I0719 03:55:57.477195      24 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-086781019
I0719 03:55:57.477226      24 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0719 03:55:57.477327      24 e2e.go:129] Starting e2e run "64ffce7d-4de7-4c5c-a3b7-b3f64d800a94" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1626666954 - Will randomize all specs
Will run 305 of 5234 specs

Jul 19 03:55:57.489: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 03:55:57.500: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 19 03:55:57.542: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 19 03:55:57.568: INFO: 2 / 2 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 19 03:55:57.568: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jul 19 03:55:57.568: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 19 03:55:57.576: INFO: e2e test version: v1.19.4
Jul 19 03:55:57.578: INFO: kube-apiserver version: v1.19.4-Fusionstage21.3.0
Jul 19 03:55:57.578: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 03:55:57.583: INFO: Cluster IP family: ipv4
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:55:57.583: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
Jul 19 03:55:57.655: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-5698/configmap-test-fc6bd752-4bb7-4e0d-b91b-53f1fe65e353
STEP: Creating a pod to test consume configMaps
Jul 19 03:55:57.692: INFO: Waiting up to 5m0s for pod "pod-configmaps-4f71c9d5-00c7-4a92-9f2e-091ceec33b40" in namespace "configmap-5698" to be "Succeeded or Failed"
Jul 19 03:55:57.699: INFO: Pod "pod-configmaps-4f71c9d5-00c7-4a92-9f2e-091ceec33b40": Phase="Pending", Reason="", readiness=false. Elapsed: 7.024193ms
Jul 19 03:55:59.737: INFO: Pod "pod-configmaps-4f71c9d5-00c7-4a92-9f2e-091ceec33b40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045594658s
Jul 19 03:56:01.741: INFO: Pod "pod-configmaps-4f71c9d5-00c7-4a92-9f2e-091ceec33b40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049525815s
STEP: Saw pod success
Jul 19 03:56:01.741: INFO: Pod "pod-configmaps-4f71c9d5-00c7-4a92-9f2e-091ceec33b40" satisfied condition "Succeeded or Failed"
Jul 19 03:56:01.744: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-configmaps-4f71c9d5-00c7-4a92-9f2e-091ceec33b40 container env-test: <nil>
STEP: delete the pod
Jul 19 03:56:01.854: INFO: Waiting for pod pod-configmaps-4f71c9d5-00c7-4a92-9f2e-091ceec33b40 to disappear
Jul 19 03:56:01.858: INFO: Pod pod-configmaps-4f71c9d5-00c7-4a92-9f2e-091ceec33b40 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:56:01.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5698" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":1,"skipped":0,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:56:01.875: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:56:18.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1796" for this suite.

• [SLOW TEST:16.252 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":2,"skipped":9,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:56:18.128: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 19 03:56:18.220: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:56:23.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7506" for this suite.

• [SLOW TEST:5.685 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":3,"skipped":18,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:56:23.814: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-6e3b444c-d80b-4b24-9dc5-2e6ac1ee71c9
STEP: Creating a pod to test consume secrets
Jul 19 03:56:24.086: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b88be5c0-d497-4c1a-8249-b344278745f4" in namespace "projected-281" to be "Succeeded or Failed"
Jul 19 03:56:24.091: INFO: Pod "pod-projected-secrets-b88be5c0-d497-4c1a-8249-b344278745f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607056ms
Jul 19 03:56:26.095: INFO: Pod "pod-projected-secrets-b88be5c0-d497-4c1a-8249-b344278745f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008676712s
Jul 19 03:56:28.100: INFO: Pod "pod-projected-secrets-b88be5c0-d497-4c1a-8249-b344278745f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013466687s
STEP: Saw pod success
Jul 19 03:56:28.100: INFO: Pod "pod-projected-secrets-b88be5c0-d497-4c1a-8249-b344278745f4" satisfied condition "Succeeded or Failed"
Jul 19 03:56:28.104: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-projected-secrets-b88be5c0-d497-4c1a-8249-b344278745f4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 19 03:56:28.139: INFO: Waiting for pod pod-projected-secrets-b88be5c0-d497-4c1a-8249-b344278745f4 to disappear
Jul 19 03:56:28.142: INFO: Pod pod-projected-secrets-b88be5c0-d497-4c1a-8249-b344278745f4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:56:28.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-281" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":4,"skipped":32,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:56:28.155: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 03:56:28.221: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:56:32.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6645" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":5,"skipped":77,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:56:32.434: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-bf69df20-5415-4ce8-b387-4f58f7e23fad
STEP: Creating a pod to test consume secrets
Jul 19 03:56:32.518: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b4065b34-6bf0-4be8-ad8a-727da06eb0f3" in namespace "projected-7121" to be "Succeeded or Failed"
Jul 19 03:56:32.522: INFO: Pod "pod-projected-secrets-b4065b34-6bf0-4be8-ad8a-727da06eb0f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.328219ms
Jul 19 03:56:34.530: INFO: Pod "pod-projected-secrets-b4065b34-6bf0-4be8-ad8a-727da06eb0f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011717566s
Jul 19 03:56:36.534: INFO: Pod "pod-projected-secrets-b4065b34-6bf0-4be8-ad8a-727da06eb0f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015814647s
STEP: Saw pod success
Jul 19 03:56:36.534: INFO: Pod "pod-projected-secrets-b4065b34-6bf0-4be8-ad8a-727da06eb0f3" satisfied condition "Succeeded or Failed"
Jul 19 03:56:36.538: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-projected-secrets-b4065b34-6bf0-4be8-ad8a-727da06eb0f3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 19 03:56:36.610: INFO: Waiting for pod pod-projected-secrets-b4065b34-6bf0-4be8-ad8a-727da06eb0f3 to disappear
Jul 19 03:56:36.613: INFO: Pod pod-projected-secrets-b4065b34-6bf0-4be8-ad8a-727da06eb0f3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:56:36.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7121" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":6,"skipped":103,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:56:36.626: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 19 03:56:36.680: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 19 03:56:36.690: INFO: Waiting for terminating namespaces to be deleted...
Jul 19 03:56:36.695: INFO: 
Logging pods the apiserver thinks is on node paas-192-168-10-158 before test
Jul 19 03:56:36.728: INFO: cse-config-center-9cb5789db-hqgs8 from fst-manage started at 2021-07-15 02:47:40 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container cse-cc-comp ready: true, restart count 0
Jul 19 03:56:36.728: INFO: cse-etcd-0 from fst-manage started at 2021-07-15 02:45:36 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container cse-etcd-comp ready: true, restart count 0
Jul 19 03:56:36.728: INFO: cse-service-center-78c89955f8-9zxb8 from fst-manage started at 2021-07-15 02:47:41 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container cse-sc-comp ready: true, restart count 0
Jul 19 03:56:36.728: INFO: cspagentmanager-84df498dd6-756cg from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container cspagentmanager-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: csphaservice-85cfb95df6-rg2zf from fst-manage started at 2021-07-15 02:45:40 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container csphaservice-soft ready: true, restart count 0
Jul 19 03:56:36.728: INFO: cspnrsmaster-0 from fst-manage started at 2021-07-15 02:45:49 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container cspnrsmaster-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: cspommgr-7b4c7c76dc-lg5xn from fst-manage started at 2021-07-15 02:45:42 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container cspommgr-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: csposmgr-666d77c8cf-5nztr from fst-manage started at 2021-07-15 02:45:52 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container csposmgr-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: deploymgr-5fc4dfbc87-skqjv from fst-manage started at 2021-07-15 02:45:39 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container deploymgr-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: febs-599995cbb8-wch4b from fst-manage started at 2021-07-15 02:46:04 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container febs-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: fileserver-0 from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container fileserver-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: gaussdb-da5b9bc8-0 from fst-manage started at 2021-07-15 02:46:01 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container gauss-softwarecomp ready: true, restart count 0
Jul 19 03:56:36.728: INFO: kafka-da5b9bc8-0 from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container kafka-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: kubeaddon-upload-package-f5vtngqpi8-4m5dx from fst-manage started at 2021-07-15 02:44:30 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container kubeaddon-upload-package ready: false, restart count 0
Jul 19 03:56:36.728: INFO: mtcenter-57c48c94fd-89d27 from fst-manage started at 2021-07-15 02:45:35 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container cspmtcenter-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: omcache-da5b9bc8-0 from fst-manage started at 2021-07-15 02:45:56 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container redis-softwarecomp ready: true, restart count 0
Jul 19 03:56:36.728: INFO: omlb-5d58b5f84f-w9p5s from fst-manage started at 2021-07-15 03:35:19 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container omlb-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: pmscalc-5744fc9c5d-x6rlp from fst-manage started at 2021-07-15 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container pmscalc-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: pmsmgr-6857b85787-q6n5l from fst-manage started at 2021-07-15 02:45:37 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container pmsmgr-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: runlog-69f7c599d-krpbr from fst-manage started at 2021-07-15 02:45:58 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container runlog-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: secmgr-65cfb48b8b-2fzvs from fst-manage started at 2021-07-15 02:45:57 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container secmgr-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: soap-76794ccdc-xtknz from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container soap-soft ready: true, restart count 0
Jul 19 03:56:36.728: INFO: upgtool-app-559557759c-jpc94 from fst-manage started at 2021-07-15 02:45:54 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container upgtool-component ready: true, restart count 0
Jul 19 03:56:36.728: INFO: zookeeper-0 from fst-manage started at 2021-07-15 02:45:41 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container zookeeper-software ready: true, restart count 0
Jul 19 03:56:36.728: INFO: pod-exec-websocket-988cc30b-4e09-4612-9be1-b37732197fb9 from pods-6645 started at 2021-07-19 03:56:28 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container main ready: true, restart count 0
Jul 19 03:56:36.728: INFO: sonobuoy from sonobuoy started at 2021-07-19 03:55:48 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 19 03:56:36.728: INFO: sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-p7chc from sonobuoy started at 2021-07-19 03:55:51 +0000 UTC (2 container statuses recorded)
Jul 19 03:56:36.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 03:56:36.728: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 19 03:56:36.728: INFO: 
Logging pods the apiserver thinks is on node paas-192-168-10-183 before test
Jul 19 03:56:36.750: INFO: cse-config-center-9cb5789db-vhnp6 from fst-manage started at 2021-07-15 02:47:40 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container cse-cc-comp ready: true, restart count 0
Jul 19 03:56:36.750: INFO: cse-etcd-1 from fst-manage started at 2021-07-15 02:45:36 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container cse-etcd-comp ready: true, restart count 0
Jul 19 03:56:36.750: INFO: cse-service-center-78c89955f8-4kfjs from fst-manage started at 2021-07-15 02:47:41 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container cse-sc-comp ready: true, restart count 0
Jul 19 03:56:36.750: INFO: cspagentmanager-84df498dd6-fbzqf from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container cspagentmanager-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: csphaservice-85cfb95df6-qch6t from fst-manage started at 2021-07-15 02:45:40 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container csphaservice-soft ready: true, restart count 0
Jul 19 03:56:36.750: INFO: cspnrsmaster-1 from fst-manage started at 2021-07-15 02:45:49 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container cspnrsmaster-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: cspommgr-7b4c7c76dc-74p5c from fst-manage started at 2021-07-15 02:45:42 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container cspommgr-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: csposmgr-666d77c8cf-xqphh from fst-manage started at 2021-07-15 02:45:53 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container csposmgr-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: deploymgr-5fc4dfbc87-d4hmv from fst-manage started at 2021-07-15 02:45:39 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container deploymgr-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: febs-599995cbb8-qsp8z from fst-manage started at 2021-07-15 02:46:04 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container febs-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: fileserver-1 from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container fileserver-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: gaussdb-da5b9bc8-1 from fst-manage started at 2021-07-15 02:46:02 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container gauss-softwarecomp ready: true, restart count 0
Jul 19 03:56:36.750: INFO: kafka-da5b9bc8-1 from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container kafka-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: mtcenter-57c48c94fd-f8h98 from fst-manage started at 2021-07-15 02:45:35 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container cspmtcenter-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: omcache-da5b9bc8-1 from fst-manage started at 2021-07-15 02:45:56 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container redis-softwarecomp ready: true, restart count 0
Jul 19 03:56:36.750: INFO: omlb-5d58b5f84f-84kpc from fst-manage started at 2021-07-15 03:24:27 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container omlb-software ready: true, restart count 2
Jul 19 03:56:36.750: INFO: pmscalc-5744fc9c5d-h54gd from fst-manage started at 2021-07-15 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container pmscalc-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: pmsmgr-6857b85787-8n4th from fst-manage started at 2021-07-15 02:45:37 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container pmsmgr-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: psm-update-69dd9dc9cb-vddnv from fst-manage started at 2021-07-15 02:44:31 +0000 UTC (0 container statuses recorded)
Jul 19 03:56:36.750: INFO: runlog-69f7c599d-mszld from fst-manage started at 2021-07-15 02:45:59 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container runlog-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: secmgr-65cfb48b8b-zz9t8 from fst-manage started at 2021-07-15 02:45:57 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container secmgr-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: soap-76794ccdc-brpbn from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container soap-soft ready: true, restart count 0
Jul 19 03:56:36.750: INFO: upgtool-app-559557759c-ssmzk from fst-manage started at 2021-07-15 02:45:54 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container upgtool-component ready: true, restart count 0
Jul 19 03:56:36.750: INFO: zookeeper-1 from fst-manage started at 2021-07-15 02:45:41 +0000 UTC (1 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container zookeeper-software ready: true, restart count 0
Jul 19 03:56:36.750: INFO: sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-gtdqw from sonobuoy started at 2021-07-19 03:55:51 +0000 UTC (2 container statuses recorded)
Jul 19 03:56:36.750: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 03:56:36.750: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c82412c6-31ec-4146-af1a-39537999dff0 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-c82412c6-31ec-4146-af1a-39537999dff0 off the node paas-192-168-10-183
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c82412c6-31ec-4146-af1a-39537999dff0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:56:52.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2352" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:16.289 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":7,"skipped":106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:56:52.915: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 19 03:56:52.991: INFO: Waiting up to 5m0s for pod "downward-api-7d764662-2eef-4285-a953-9f2975b54cf2" in namespace "downward-api-5600" to be "Succeeded or Failed"
Jul 19 03:56:52.994: INFO: Pod "downward-api-7d764662-2eef-4285-a953-9f2975b54cf2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.195063ms
Jul 19 03:56:55.002: INFO: Pod "downward-api-7d764662-2eef-4285-a953-9f2975b54cf2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011442549s
Jul 19 03:56:57.007: INFO: Pod "downward-api-7d764662-2eef-4285-a953-9f2975b54cf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016530986s
STEP: Saw pod success
Jul 19 03:56:57.007: INFO: Pod "downward-api-7d764662-2eef-4285-a953-9f2975b54cf2" satisfied condition "Succeeded or Failed"
Jul 19 03:56:57.010: INFO: Trying to get logs from node paas-192-168-10-158 pod downward-api-7d764662-2eef-4285-a953-9f2975b54cf2 container dapi-container: <nil>
STEP: delete the pod
Jul 19 03:56:57.036: INFO: Waiting for pod downward-api-7d764662-2eef-4285-a953-9f2975b54cf2 to disappear
Jul 19 03:56:57.039: INFO: Pod downward-api-7d764662-2eef-4285-a953-9f2975b54cf2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:56:57.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5600" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":141,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:56:57.052: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5311
STEP: creating service affinity-nodeport-transition in namespace services-5311
STEP: creating replication controller affinity-nodeport-transition in namespace services-5311
I0719 03:56:57.172115      24 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-5311, replica count: 3
I0719 03:57:00.222494      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 03:57:03.222820      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 03:57:03.236: INFO: Creating new exec pod
Jul 19 03:57:08.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-5311 execpod-affinity6s55h -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jul 19 03:57:11.551: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul 19 03:57:11.551: INFO: stdout: ""
Jul 19 03:57:11.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-5311 execpod-affinity6s55h -- /bin/sh -x -c nc -zv -t -w 2 10.247.108.132 80'
Jul 19 03:57:11.807: INFO: stderr: "+ nc -zv -t -w 2 10.247.108.132 80\nConnection to 10.247.108.132 80 port [tcp/http] succeeded!\n"
Jul 19 03:57:11.807: INFO: stdout: ""
Jul 19 03:57:11.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-5311 execpod-affinity6s55h -- /bin/sh -x -c nc -zv -t -w 2 192.168.10.158 32696'
Jul 19 03:57:12.093: INFO: stderr: "+ nc -zv -t -w 2 192.168.10.158 32696\nConnection to 192.168.10.158 32696 port [tcp/32696] succeeded!\n"
Jul 19 03:57:12.093: INFO: stdout: ""
Jul 19 03:57:12.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-5311 execpod-affinity6s55h -- /bin/sh -x -c nc -zv -t -w 2 192.168.10.183 32696'
Jul 19 03:57:12.368: INFO: stderr: "+ nc -zv -t -w 2 192.168.10.183 32696\nConnection to 192.168.10.183 32696 port [tcp/32696] succeeded!\n"
Jul 19 03:57:12.368: INFO: stdout: ""
Jul 19 03:57:12.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-5311 execpod-affinity6s55h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.10.158:32696/ ; done'
Jul 19 03:57:12.842: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n"
Jul 19 03:57:12.842: INFO: stdout: "\naffinity-nodeport-transition-hjcm8\naffinity-nodeport-transition-hjcm8\naffinity-nodeport-transition-rns42\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-hjcm8\naffinity-nodeport-transition-rns42\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-rns42\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-rns42\naffinity-nodeport-transition-hjcm8\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-rns42"
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-hjcm8
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-hjcm8
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-rns42
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-hjcm8
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-rns42
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-rns42
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-rns42
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-hjcm8
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:12.842: INFO: Received response from host: affinity-nodeport-transition-rns42
Jul 19 03:57:12.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-5311 execpod-affinity6s55h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.10.158:32696/ ; done'
Jul 19 03:57:13.213: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32696/\n"
Jul 19 03:57:13.213: INFO: stdout: "\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx\naffinity-nodeport-transition-6pxxx"
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Received response from host: affinity-nodeport-transition-6pxxx
Jul 19 03:57:13.214: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5311, will wait for the garbage collector to delete the pods
Jul 19 03:57:13.297: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.59307ms
Jul 19 03:57:13.397: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.17015ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:57:34.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5311" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:37.393 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":9,"skipped":164,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:57:34.446: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-70613cd1-f2a4-407d-9be7-12f0af5b4ef0
STEP: Creating a pod to test consume configMaps
Jul 19 03:57:34.533: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d363502-df46-4657-b134-ac24dc940b31" in namespace "configmap-7355" to be "Succeeded or Failed"
Jul 19 03:57:34.537: INFO: Pod "pod-configmaps-1d363502-df46-4657-b134-ac24dc940b31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.922149ms
Jul 19 03:57:36.541: INFO: Pod "pod-configmaps-1d363502-df46-4657-b134-ac24dc940b31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008048435s
Jul 19 03:57:38.545: INFO: Pod "pod-configmaps-1d363502-df46-4657-b134-ac24dc940b31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012263378s
STEP: Saw pod success
Jul 19 03:57:38.545: INFO: Pod "pod-configmaps-1d363502-df46-4657-b134-ac24dc940b31" satisfied condition "Succeeded or Failed"
Jul 19 03:57:38.549: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-configmaps-1d363502-df46-4657-b134-ac24dc940b31 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 03:57:38.574: INFO: Waiting for pod pod-configmaps-1d363502-df46-4657-b134-ac24dc940b31 to disappear
Jul 19 03:57:38.578: INFO: Pod pod-configmaps-1d363502-df46-4657-b134-ac24dc940b31 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:57:38.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7355" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":10,"skipped":180,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:57:38.588: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-090e0df2-4a7a-4d12-a7e8-72f51e3e4844
STEP: Creating a pod to test consume secrets
Jul 19 03:57:38.666: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d93158b9-0cab-4abd-8be5-7d60378b3220" in namespace "projected-5399" to be "Succeeded or Failed"
Jul 19 03:57:38.671: INFO: Pod "pod-projected-secrets-d93158b9-0cab-4abd-8be5-7d60378b3220": Phase="Pending", Reason="", readiness=false. Elapsed: 4.91666ms
Jul 19 03:57:40.675: INFO: Pod "pod-projected-secrets-d93158b9-0cab-4abd-8be5-7d60378b3220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009094172s
Jul 19 03:57:42.679: INFO: Pod "pod-projected-secrets-d93158b9-0cab-4abd-8be5-7d60378b3220": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013031034s
STEP: Saw pod success
Jul 19 03:57:42.679: INFO: Pod "pod-projected-secrets-d93158b9-0cab-4abd-8be5-7d60378b3220" satisfied condition "Succeeded or Failed"
Jul 19 03:57:42.683: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-projected-secrets-d93158b9-0cab-4abd-8be5-7d60378b3220 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 19 03:57:42.713: INFO: Waiting for pod pod-projected-secrets-d93158b9-0cab-4abd-8be5-7d60378b3220 to disappear
Jul 19 03:57:42.716: INFO: Pod pod-projected-secrets-d93158b9-0cab-4abd-8be5-7d60378b3220 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:57:42.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5399" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":11,"skipped":182,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:57:42.728: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-8d06fbfa-da0c-4ea3-8a0d-90e8b3bf3566
STEP: Creating a pod to test consume secrets
Jul 19 03:57:42.820: INFO: Waiting up to 5m0s for pod "pod-secrets-5fd5caa2-ea4b-4fc1-b311-21f407712e8b" in namespace "secrets-8238" to be "Succeeded or Failed"
Jul 19 03:57:42.823: INFO: Pod "pod-secrets-5fd5caa2-ea4b-4fc1-b311-21f407712e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.69043ms
Jul 19 03:57:44.828: INFO: Pod "pod-secrets-5fd5caa2-ea4b-4fc1-b311-21f407712e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008772144s
Jul 19 03:57:46.833: INFO: Pod "pod-secrets-5fd5caa2-ea4b-4fc1-b311-21f407712e8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013000807s
STEP: Saw pod success
Jul 19 03:57:46.833: INFO: Pod "pod-secrets-5fd5caa2-ea4b-4fc1-b311-21f407712e8b" satisfied condition "Succeeded or Failed"
Jul 19 03:57:46.836: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-secrets-5fd5caa2-ea4b-4fc1-b311-21f407712e8b container secret-env-test: <nil>
STEP: delete the pod
Jul 19 03:57:46.861: INFO: Waiting for pod pod-secrets-5fd5caa2-ea4b-4fc1-b311-21f407712e8b to disappear
Jul 19 03:57:46.865: INFO: Pod pod-secrets-5fd5caa2-ea4b-4fc1-b311-21f407712e8b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:57:46.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8238" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":12,"skipped":201,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:57:46.877: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 03:57:46.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce91545b-7806-4c03-ad4e-67ba57ca6754" in namespace "projected-6968" to be "Succeeded or Failed"
Jul 19 03:57:46.958: INFO: Pod "downwardapi-volume-ce91545b-7806-4c03-ad4e-67ba57ca6754": Phase="Pending", Reason="", readiness=false. Elapsed: 3.227632ms
Jul 19 03:57:48.962: INFO: Pod "downwardapi-volume-ce91545b-7806-4c03-ad4e-67ba57ca6754": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007328594s
Jul 19 03:57:50.966: INFO: Pod "downwardapi-volume-ce91545b-7806-4c03-ad4e-67ba57ca6754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012062476s
STEP: Saw pod success
Jul 19 03:57:50.966: INFO: Pod "downwardapi-volume-ce91545b-7806-4c03-ad4e-67ba57ca6754" satisfied condition "Succeeded or Failed"
Jul 19 03:57:50.970: INFO: Trying to get logs from node paas-192-168-10-158 pod downwardapi-volume-ce91545b-7806-4c03-ad4e-67ba57ca6754 container client-container: <nil>
STEP: delete the pod
Jul 19 03:57:50.999: INFO: Waiting for pod downwardapi-volume-ce91545b-7806-4c03-ad4e-67ba57ca6754 to disappear
Jul 19 03:57:51.004: INFO: Pod downwardapi-volume-ce91545b-7806-4c03-ad4e-67ba57ca6754 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:57:51.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6968" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":214,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:57:51.029: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul 19 03:57:51.089: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 03:57:55.127: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:08.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6004" for this suite.

• [SLOW TEST:17.847 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":14,"skipped":218,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:08.877: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-acc17dba-43cd-4f9f-9529-27eaabfc8457
STEP: Creating a pod to test consume configMaps
Jul 19 03:58:08.964: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab1ba212-3ca4-494b-ba3a-a97e0b9a5618" in namespace "configmap-7768" to be "Succeeded or Failed"
Jul 19 03:58:08.970: INFO: Pod "pod-configmaps-ab1ba212-3ca4-494b-ba3a-a97e0b9a5618": Phase="Pending", Reason="", readiness=false. Elapsed: 5.253528ms
Jul 19 03:58:10.975: INFO: Pod "pod-configmaps-ab1ba212-3ca4-494b-ba3a-a97e0b9a5618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01012952s
Jul 19 03:58:12.979: INFO: Pod "pod-configmaps-ab1ba212-3ca4-494b-ba3a-a97e0b9a5618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014682771s
STEP: Saw pod success
Jul 19 03:58:12.979: INFO: Pod "pod-configmaps-ab1ba212-3ca4-494b-ba3a-a97e0b9a5618" satisfied condition "Succeeded or Failed"
Jul 19 03:58:12.983: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-configmaps-ab1ba212-3ca4-494b-ba3a-a97e0b9a5618 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 03:58:13.013: INFO: Waiting for pod pod-configmaps-ab1ba212-3ca4-494b-ba3a-a97e0b9a5618 to disappear
Jul 19 03:58:13.019: INFO: Pod pod-configmaps-ab1ba212-3ca4-494b-ba3a-a97e0b9a5618 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:13.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7768" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":15,"skipped":218,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:13.034: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-9e7660d7-38e8-4bb4-8a26-d79756d76f58
STEP: Creating a pod to test consume secrets
Jul 19 03:58:13.146: INFO: Waiting up to 5m0s for pod "pod-secrets-47d77629-e6c5-469b-8441-15b85eaa7460" in namespace "secrets-2574" to be "Succeeded or Failed"
Jul 19 03:58:13.198: INFO: Pod "pod-secrets-47d77629-e6c5-469b-8441-15b85eaa7460": Phase="Pending", Reason="", readiness=false. Elapsed: 51.867065ms
Jul 19 03:58:15.202: INFO: Pod "pod-secrets-47d77629-e6c5-469b-8441-15b85eaa7460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055750174s
Jul 19 03:58:17.206: INFO: Pod "pod-secrets-47d77629-e6c5-469b-8441-15b85eaa7460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060049173s
STEP: Saw pod success
Jul 19 03:58:17.206: INFO: Pod "pod-secrets-47d77629-e6c5-469b-8441-15b85eaa7460" satisfied condition "Succeeded or Failed"
Jul 19 03:58:17.209: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-secrets-47d77629-e6c5-469b-8441-15b85eaa7460 container secret-volume-test: <nil>
STEP: delete the pod
Jul 19 03:58:17.232: INFO: Waiting for pod pod-secrets-47d77629-e6c5-469b-8441-15b85eaa7460 to disappear
Jul 19 03:58:17.236: INFO: Pod pod-secrets-47d77629-e6c5-469b-8441-15b85eaa7460 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:17.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2574" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":16,"skipped":233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:17.254: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-d2fde740-323b-4d78-bbb6-15099c8d301a
STEP: Creating a pod to test consume secrets
Jul 19 03:58:17.366: INFO: Waiting up to 5m0s for pod "pod-secrets-af1ad32f-6e2e-45d9-9476-4542b3cebdcf" in namespace "secrets-1451" to be "Succeeded or Failed"
Jul 19 03:58:17.370: INFO: Pod "pod-secrets-af1ad32f-6e2e-45d9-9476-4542b3cebdcf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572835ms
Jul 19 03:58:19.398: INFO: Pod "pod-secrets-af1ad32f-6e2e-45d9-9476-4542b3cebdcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031896505s
Jul 19 03:58:21.402: INFO: Pod "pod-secrets-af1ad32f-6e2e-45d9-9476-4542b3cebdcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035514222s
STEP: Saw pod success
Jul 19 03:58:21.402: INFO: Pod "pod-secrets-af1ad32f-6e2e-45d9-9476-4542b3cebdcf" satisfied condition "Succeeded or Failed"
Jul 19 03:58:21.405: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-secrets-af1ad32f-6e2e-45d9-9476-4542b3cebdcf container secret-volume-test: <nil>
STEP: delete the pod
Jul 19 03:58:21.428: INFO: Waiting for pod pod-secrets-af1ad32f-6e2e-45d9-9476-4542b3cebdcf to disappear
Jul 19 03:58:21.432: INFO: Pod pod-secrets-af1ad32f-6e2e-45d9-9476-4542b3cebdcf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:21.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1451" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":256,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:21.444: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jul 19 03:58:21.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-2181'
Jul 19 03:58:21.803: INFO: stderr: ""
Jul 19 03:58:21.803: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 19 03:58:22.815: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 03:58:22.816: INFO: Found 0 / 1
Jul 19 03:58:23.810: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 03:58:23.810: INFO: Found 0 / 1
Jul 19 03:58:24.808: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 03:58:24.808: INFO: Found 0 / 1
Jul 19 03:58:25.807: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 03:58:25.807: INFO: Found 1 / 1
Jul 19 03:58:25.807: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 19 03:58:25.811: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 03:58:25.811: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 19 03:58:25.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 patch pod agnhost-primary-7n5jh --namespace=kubectl-2181 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 19 03:58:25.925: INFO: stderr: ""
Jul 19 03:58:25.925: INFO: stdout: "pod/agnhost-primary-7n5jh patched\n"
STEP: checking annotations
Jul 19 03:58:25.929: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 03:58:25.929: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:25.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2181" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":18,"skipped":265,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:25.940: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 03:58:26.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-272c662b-718b-4014-a009-db4d0dfc528a" in namespace "downward-api-2071" to be "Succeeded or Failed"
Jul 19 03:58:26.011: INFO: Pod "downwardapi-volume-272c662b-718b-4014-a009-db4d0dfc528a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.523903ms
Jul 19 03:58:28.015: INFO: Pod "downwardapi-volume-272c662b-718b-4014-a009-db4d0dfc528a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007415591s
Jul 19 03:58:30.020: INFO: Pod "downwardapi-volume-272c662b-718b-4014-a009-db4d0dfc528a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011834848s
STEP: Saw pod success
Jul 19 03:58:30.020: INFO: Pod "downwardapi-volume-272c662b-718b-4014-a009-db4d0dfc528a" satisfied condition "Succeeded or Failed"
Jul 19 03:58:30.023: INFO: Trying to get logs from node paas-192-168-10-158 pod downwardapi-volume-272c662b-718b-4014-a009-db4d0dfc528a container client-container: <nil>
STEP: delete the pod
Jul 19 03:58:30.050: INFO: Waiting for pod downwardapi-volume-272c662b-718b-4014-a009-db4d0dfc528a to disappear
Jul 19 03:58:30.053: INFO: Pod downwardapi-volume-272c662b-718b-4014-a009-db4d0dfc528a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:30.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2071" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":19,"skipped":267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:30.065: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:30.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3228" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":20,"skipped":290,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:30.171: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 19 03:58:30.260: INFO: Waiting up to 5m0s for pod "downward-api-1e449d28-b7c2-4909-b922-42ecd6cb80cd" in namespace "downward-api-7077" to be "Succeeded or Failed"
Jul 19 03:58:30.265: INFO: Pod "downward-api-1e449d28-b7c2-4909-b922-42ecd6cb80cd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.434602ms
Jul 19 03:58:32.269: INFO: Pod "downward-api-1e449d28-b7c2-4909-b922-42ecd6cb80cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009361669s
Jul 19 03:58:34.274: INFO: Pod "downward-api-1e449d28-b7c2-4909-b922-42ecd6cb80cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014537081s
Jul 19 03:58:36.278: INFO: Pod "downward-api-1e449d28-b7c2-4909-b922-42ecd6cb80cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018621028s
STEP: Saw pod success
Jul 19 03:58:36.278: INFO: Pod "downward-api-1e449d28-b7c2-4909-b922-42ecd6cb80cd" satisfied condition "Succeeded or Failed"
Jul 19 03:58:36.282: INFO: Trying to get logs from node paas-192-168-10-158 pod downward-api-1e449d28-b7c2-4909-b922-42ecd6cb80cd container dapi-container: <nil>
STEP: delete the pod
Jul 19 03:58:36.317: INFO: Waiting for pod downward-api-1e449d28-b7c2-4909-b922-42ecd6cb80cd to disappear
Jul 19 03:58:36.322: INFO: Pod downward-api-1e449d28-b7c2-4909-b922-42ecd6cb80cd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:36.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7077" for this suite.

• [SLOW TEST:6.167 seconds]
[sig-node] Downward API
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":21,"skipped":313,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:36.339: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-8e19ce7e-b02a-4b87-9820-99c45a5656aa
STEP: Creating a pod to test consume configMaps
Jul 19 03:58:36.456: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4d97d436-a8ea-41d2-ae65-0f11725b9b0b" in namespace "projected-9157" to be "Succeeded or Failed"
Jul 19 03:58:36.461: INFO: Pod "pod-projected-configmaps-4d97d436-a8ea-41d2-ae65-0f11725b9b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86845ms
Jul 19 03:58:38.466: INFO: Pod "pod-projected-configmaps-4d97d436-a8ea-41d2-ae65-0f11725b9b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009331632s
Jul 19 03:58:40.470: INFO: Pod "pod-projected-configmaps-4d97d436-a8ea-41d2-ae65-0f11725b9b0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013594446s
STEP: Saw pod success
Jul 19 03:58:40.470: INFO: Pod "pod-projected-configmaps-4d97d436-a8ea-41d2-ae65-0f11725b9b0b" satisfied condition "Succeeded or Failed"
Jul 19 03:58:40.473: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-projected-configmaps-4d97d436-a8ea-41d2-ae65-0f11725b9b0b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 03:58:40.497: INFO: Waiting for pod pod-projected-configmaps-4d97d436-a8ea-41d2-ae65-0f11725b9b0b to disappear
Jul 19 03:58:40.501: INFO: Pod pod-projected-configmaps-4d97d436-a8ea-41d2-ae65-0f11725b9b0b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:40.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9157" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":22,"skipped":352,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:40.550: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 03:58:40.613: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9976
I0719 03:58:40.625602      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9976, replica count: 1
I0719 03:58:41.675969      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 03:58:42.676144      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 03:58:43.676315      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 03:58:44.676502      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 03:58:44.802: INFO: Created: latency-svc-9q7k5
Jul 19 03:58:44.815: INFO: Got endpoints: latency-svc-9q7k5 [38.764268ms]
Jul 19 03:58:45.209: INFO: Created: latency-svc-cs4zh
Jul 19 03:58:45.218: INFO: Got endpoints: latency-svc-cs4zh [402.329169ms]
Jul 19 03:58:45.514: INFO: Created: latency-svc-4h268
Jul 19 03:58:45.514: INFO: Created: latency-svc-l6n7g
Jul 19 03:58:45.520: INFO: Got endpoints: latency-svc-4h268 [704.282647ms]
Jul 19 03:58:45.525: INFO: Got endpoints: latency-svc-l6n7g [709.320449ms]
Jul 19 03:58:45.707: INFO: Created: latency-svc-xxqlh
Jul 19 03:58:45.714: INFO: Got endpoints: latency-svc-xxqlh [898.411432ms]
Jul 19 03:58:45.810: INFO: Created: latency-svc-86tjf
Jul 19 03:58:45.813: INFO: Created: latency-svc-rfb6q
Jul 19 03:58:45.813: INFO: Created: latency-svc-9xv2n
Jul 19 03:58:45.821: INFO: Got endpoints: latency-svc-86tjf [1.005464012s]
Jul 19 03:58:45.827: INFO: Got endpoints: latency-svc-rfb6q [1.010897617s]
Jul 19 03:58:45.831: INFO: Got endpoints: latency-svc-9xv2n [1.015588414s]
Jul 19 03:58:45.996: INFO: Created: latency-svc-7q4s4
Jul 19 03:58:46.007: INFO: Got endpoints: latency-svc-7q4s4 [1.191732145s]
Jul 19 03:58:46.013: INFO: Created: latency-svc-44pkf
Jul 19 03:58:46.032: INFO: Got endpoints: latency-svc-44pkf [1.215976691s]
Jul 19 03:58:46.122: INFO: Created: latency-svc-2zsb4
Jul 19 03:58:46.122: INFO: Created: latency-svc-mj47w
Jul 19 03:58:46.143: INFO: Got endpoints: latency-svc-mj47w [618.661853ms]
Jul 19 03:58:46.143: INFO: Got endpoints: latency-svc-2zsb4 [623.822467ms]
Jul 19 03:58:46.220: INFO: Created: latency-svc-7v4jm
Jul 19 03:58:46.220: INFO: Created: latency-svc-5ltwv
Jul 19 03:58:46.236: INFO: Got endpoints: latency-svc-5ltwv [1.420518511s]
Jul 19 03:58:46.236: INFO: Got endpoints: latency-svc-7v4jm [522.012639ms]
Jul 19 03:58:46.308: INFO: Created: latency-svc-9j7vt
Jul 19 03:58:46.311: INFO: Created: latency-svc-wztwl
Jul 19 03:58:46.317: INFO: Created: latency-svc-qphlk
Jul 19 03:58:46.324: INFO: Created: latency-svc-dssh9
Jul 19 03:58:46.325: INFO: Got endpoints: latency-svc-9j7vt [181.725692ms]
Jul 19 03:58:46.325: INFO: Got endpoints: latency-svc-wztwl [293.77787ms]
Jul 19 03:58:46.329: INFO: Got endpoints: latency-svc-qphlk [185.062957ms]
Jul 19 03:58:46.336: INFO: Got endpoints: latency-svc-dssh9 [329.013607ms]
Jul 19 03:58:46.398: INFO: Created: latency-svc-7dq27
Jul 19 03:58:46.401: INFO: Created: latency-svc-mxhth
Jul 19 03:58:46.408: INFO: Created: latency-svc-fjx6b
Jul 19 03:58:46.416: INFO: Got endpoints: latency-svc-7dq27 [180.347462ms]
Jul 19 03:58:46.423: INFO: Got endpoints: latency-svc-mxhth [186.859687ms]
Jul 19 03:58:46.425: INFO: Got endpoints: latency-svc-fjx6b [1.609702052s]
Jul 19 03:58:46.497: INFO: Created: latency-svc-ghf24
Jul 19 03:58:46.497: INFO: Created: latency-svc-pq9dk
Jul 19 03:58:46.502: INFO: Created: latency-svc-h9qkv
Jul 19 03:58:46.514: INFO: Created: latency-svc-j6v9h
Jul 19 03:58:46.515: INFO: Got endpoints: latency-svc-h9qkv [189.773952ms]
Jul 19 03:58:46.515: INFO: Got endpoints: latency-svc-pq9dk [189.7717ms]
Jul 19 03:58:46.515: INFO: Got endpoints: latency-svc-ghf24 [186.055999ms]
Jul 19 03:58:46.529: INFO: Got endpoints: latency-svc-j6v9h [192.802812ms]
Jul 19 03:58:46.602: INFO: Created: latency-svc-xf8sn
Jul 19 03:58:46.602: INFO: Created: latency-svc-8246t
Jul 19 03:58:46.602: INFO: Created: latency-svc-brhbj
Jul 19 03:58:46.615: INFO: Got endpoints: latency-svc-8246t [191.599869ms]
Jul 19 03:58:46.615: INFO: Got endpoints: latency-svc-xf8sn [189.244768ms]
Jul 19 03:58:46.616: INFO: Got endpoints: latency-svc-brhbj [199.789807ms]
Jul 19 03:58:46.701: INFO: Created: latency-svc-b5fzd
Jul 19 03:58:46.701: INFO: Created: latency-svc-5px2m
Jul 19 03:58:46.701: INFO: Created: latency-svc-mhxsg
Jul 19 03:58:46.701: INFO: Created: latency-svc-n8xzr
Jul 19 03:58:46.711: INFO: Created: latency-svc-xrp9m
Jul 19 03:58:46.720: INFO: Got endpoints: latency-svc-mhxsg [190.413438ms]
Jul 19 03:58:46.720: INFO: Got endpoints: latency-svc-5px2m [204.515317ms]
Jul 19 03:58:46.727: INFO: Got endpoints: latency-svc-xrp9m [211.43236ms]
Jul 19 03:58:46.727: INFO: Got endpoints: latency-svc-b5fzd [211.71576ms]
Jul 19 03:58:46.729: INFO: Got endpoints: latency-svc-n8xzr [1.913898053s]
Jul 19 03:58:46.804: INFO: Created: latency-svc-kfzpr
Jul 19 03:58:46.807: INFO: Created: latency-svc-dqqwk
Jul 19 03:58:46.823: INFO: Created: latency-svc-cdzkd
Jul 19 03:58:46.829: INFO: Got endpoints: latency-svc-dqqwk [214.58597ms]
Jul 19 03:58:46.832: INFO: Got endpoints: latency-svc-kfzpr [217.610712ms]
Jul 19 03:58:46.840: INFO: Got endpoints: latency-svc-cdzkd [223.468602ms]
Jul 19 03:58:46.904: INFO: Created: latency-svc-m7cdj
Jul 19 03:58:46.904: INFO: Created: latency-svc-brptp
Jul 19 03:58:46.907: INFO: Created: latency-svc-qp2xr
Jul 19 03:58:46.913: INFO: Created: latency-svc-l6vz9
Jul 19 03:58:46.924: INFO: Got endpoints: latency-svc-m7cdj [204.044937ms]
Jul 19 03:58:46.927: INFO: Got endpoints: latency-svc-brptp [199.943055ms]
Jul 19 03:58:46.927: INFO: Got endpoints: latency-svc-qp2xr [197.925495ms]
Jul 19 03:58:46.929: INFO: Got endpoints: latency-svc-l6vz9 [202.585635ms]
Jul 19 03:58:47.001: INFO: Created: latency-svc-5ncxr
Jul 19 03:58:47.001: INFO: Created: latency-svc-6rtqq
Jul 19 03:58:47.003: INFO: Created: latency-svc-49mdt
Jul 19 03:58:47.006: INFO: Created: latency-svc-vntv2
Jul 19 03:58:47.012: INFO: Created: latency-svc-67mjs
Jul 19 03:58:47.014: INFO: Created: latency-svc-hpssd
Jul 19 03:58:47.014: INFO: Created: latency-svc-4kn6v
Jul 19 03:58:47.024: INFO: Got endpoints: latency-svc-6rtqq [2.208338601s]
Jul 19 03:58:47.028: INFO: Got endpoints: latency-svc-49mdt [2.212483368s]
Jul 19 03:58:47.028: INFO: Got endpoints: latency-svc-5ncxr [198.679179ms]
Jul 19 03:58:47.034: INFO: Got endpoints: latency-svc-hpssd [1.816041095s]
Jul 19 03:58:47.034: INFO: Got endpoints: latency-svc-vntv2 [314.161644ms]
Jul 19 03:58:47.037: INFO: Got endpoints: latency-svc-4kn6v [204.850492ms]
Jul 19 03:58:47.044: INFO: Got endpoints: latency-svc-67mjs [2.228597644s]
Jul 19 03:58:47.123: INFO: Created: latency-svc-ftzrq
Jul 19 03:58:47.132: INFO: Created: latency-svc-bwx2b
Jul 19 03:58:47.132: INFO: Created: latency-svc-mhnd7
Jul 19 03:58:47.132: INFO: Created: latency-svc-ns4kh
Jul 19 03:58:47.132: INFO: Created: latency-svc-8xgg8
Jul 19 03:58:47.139: INFO: Created: latency-svc-n6tmv
Jul 19 03:58:47.139: INFO: Created: latency-svc-7gn2g
Jul 19 03:58:47.139: INFO: Created: latency-svc-mxpcn
Jul 19 03:58:47.150: INFO: Got endpoints: latency-svc-ftzrq [1.31871598s]
Jul 19 03:58:47.150: INFO: Got endpoints: latency-svc-ns4kh [223.008787ms]
Jul 19 03:58:47.150: INFO: Got endpoints: latency-svc-mhnd7 [226.14891ms]
Jul 19 03:58:47.150: INFO: Got endpoints: latency-svc-bwx2b [1.328871005s]
Jul 19 03:58:47.153: INFO: Got endpoints: latency-svc-n6tmv [1.325924332s]
Jul 19 03:58:47.159: INFO: Got endpoints: latency-svc-7gn2g [232.272379ms]
Jul 19 03:58:47.166: INFO: Got endpoints: latency-svc-8xgg8 [326.657944ms]
Jul 19 03:58:47.167: INFO: Got endpoints: latency-svc-mxpcn [237.043667ms]
Jul 19 03:58:47.211: INFO: Created: latency-svc-k4x8v
Jul 19 03:58:47.211: INFO: Created: latency-svc-cl8hz
Jul 19 03:58:47.211: INFO: Created: latency-svc-pw9g5
Jul 19 03:58:47.220: INFO: Created: latency-svc-sqxfg
Jul 19 03:58:47.220: INFO: Created: latency-svc-wnf5f
Jul 19 03:58:47.231: INFO: Got endpoints: latency-svc-cl8hz [207.023391ms]
Jul 19 03:58:47.232: INFO: Got endpoints: latency-svc-k4x8v [203.418857ms]
Jul 19 03:58:47.236: INFO: Got endpoints: latency-svc-sqxfg [201.789248ms]
Jul 19 03:58:47.236: INFO: Got endpoints: latency-svc-pw9g5 [198.968327ms]
Jul 19 03:58:47.242: INFO: Created: latency-svc-cmg6j
Jul 19 03:58:47.242: INFO: Got endpoints: latency-svc-wnf5f [197.571412ms]
Jul 19 03:58:47.256: INFO: Got endpoints: latency-svc-cmg6j [221.36986ms]
Jul 19 03:58:47.257: INFO: Created: latency-svc-krpv5
Jul 19 03:58:47.277: INFO: Got endpoints: latency-svc-krpv5 [248.830645ms]
Jul 19 03:58:47.282: INFO: Created: latency-svc-c7gzr
Jul 19 03:58:47.294: INFO: Got endpoints: latency-svc-c7gzr [141.514505ms]
Jul 19 03:58:47.296: INFO: Created: latency-svc-gsl2k
Jul 19 03:58:47.316: INFO: Got endpoints: latency-svc-gsl2k [165.738692ms]
Jul 19 03:58:47.321: INFO: Created: latency-svc-t99rz
Jul 19 03:58:47.332: INFO: Got endpoints: latency-svc-t99rz [182.330507ms]
Jul 19 03:58:47.335: INFO: Created: latency-svc-fx64j
Jul 19 03:58:47.340: INFO: Created: latency-svc-gxtj8
Jul 19 03:58:47.366: INFO: Created: latency-svc-r96dz
Jul 19 03:58:47.399: INFO: Got endpoints: latency-svc-gxtj8 [239.794786ms]
Jul 19 03:58:47.399: INFO: Got endpoints: latency-svc-r96dz [233.024323ms]
Jul 19 03:58:47.399: INFO: Got endpoints: latency-svc-fx64j [249.430943ms]
Jul 19 03:58:47.407: INFO: Created: latency-svc-rmdft
Jul 19 03:58:47.421: INFO: Created: latency-svc-m9xhx
Jul 19 03:58:47.422: INFO: Got endpoints: latency-svc-rmdft [254.930977ms]
Jul 19 03:58:47.437: INFO: Created: latency-svc-z84hw
Jul 19 03:58:47.438: INFO: Got endpoints: latency-svc-m9xhx [288.324892ms]
Jul 19 03:58:47.491: INFO: Created: latency-svc-br6f8
Jul 19 03:58:47.491: INFO: Got endpoints: latency-svc-z84hw [259.586524ms]
Jul 19 03:58:47.497: INFO: Got endpoints: latency-svc-br6f8 [266.461907ms]
Jul 19 03:58:47.507: INFO: Created: latency-svc-5dpp9
Jul 19 03:58:47.512: INFO: Got endpoints: latency-svc-5dpp9 [276.173874ms]
Jul 19 03:58:47.516: INFO: Created: latency-svc-kvvch
Jul 19 03:58:47.524: INFO: Got endpoints: latency-svc-kvvch [288.088775ms]
Jul 19 03:58:47.526: INFO: Created: latency-svc-rfgc7
Jul 19 03:58:47.535: INFO: Created: latency-svc-dpfsz
Jul 19 03:58:47.535: INFO: Got endpoints: latency-svc-rfgc7 [293.469477ms]
Jul 19 03:58:47.538: INFO: Got endpoints: latency-svc-dpfsz [281.97105ms]
Jul 19 03:58:47.554: INFO: Created: latency-svc-6zc2q
Jul 19 03:58:47.562: INFO: Created: latency-svc-t7mxp
Jul 19 03:58:47.564: INFO: Got endpoints: latency-svc-6zc2q [286.971517ms]
Jul 19 03:58:47.570: INFO: Got endpoints: latency-svc-t7mxp [275.667247ms]
Jul 19 03:58:47.573: INFO: Created: latency-svc-q5h9t
Jul 19 03:58:47.584: INFO: Got endpoints: latency-svc-q5h9t [267.707115ms]
Jul 19 03:58:47.592: INFO: Created: latency-svc-qjrhm
Jul 19 03:58:47.600: INFO: Got endpoints: latency-svc-qjrhm [267.741065ms]
Jul 19 03:58:47.607: INFO: Created: latency-svc-qmnjp
Jul 19 03:58:47.613: INFO: Created: latency-svc-vhmqd
Jul 19 03:58:47.617: INFO: Got endpoints: latency-svc-qmnjp [217.262287ms]
Jul 19 03:58:47.624: INFO: Created: latency-svc-4svd8
Jul 19 03:58:47.634: INFO: Got endpoints: latency-svc-4svd8 [234.045272ms]
Jul 19 03:58:47.647: INFO: Got endpoints: latency-svc-vhmqd [247.598846ms]
Jul 19 03:58:47.670: INFO: Created: latency-svc-bf58x
Jul 19 03:58:47.676: INFO: Created: latency-svc-bbtgt
Jul 19 03:58:47.683: INFO: Got endpoints: latency-svc-bf58x [261.859826ms]
Jul 19 03:58:47.687: INFO: Got endpoints: latency-svc-bbtgt [248.530025ms]
Jul 19 03:58:47.696: INFO: Created: latency-svc-6nxwr
Jul 19 03:58:47.708: INFO: Got endpoints: latency-svc-6nxwr [217.027429ms]
Jul 19 03:58:47.713: INFO: Created: latency-svc-b48s9
Jul 19 03:58:47.722: INFO: Got endpoints: latency-svc-b48s9 [224.242552ms]
Jul 19 03:58:47.730: INFO: Created: latency-svc-jqw9p
Jul 19 03:58:47.739: INFO: Created: latency-svc-qs9gn
Jul 19 03:58:47.739: INFO: Got endpoints: latency-svc-qs9gn [226.561989ms]
Jul 19 03:58:47.744: INFO: Got endpoints: latency-svc-jqw9p [219.664855ms]
Jul 19 03:58:47.798: INFO: Created: latency-svc-n47p6
Jul 19 03:58:47.803: INFO: Created: latency-svc-g6rm9
Jul 19 03:58:47.807: INFO: Got endpoints: latency-svc-n47p6 [271.260687ms]
Jul 19 03:58:47.816: INFO: Got endpoints: latency-svc-g6rm9 [278.011282ms]
Jul 19 03:58:47.823: INFO: Created: latency-svc-h65rr
Jul 19 03:58:47.829: INFO: Created: latency-svc-d5cmh
Jul 19 03:58:47.836: INFO: Got endpoints: latency-svc-h65rr [271.797308ms]
Jul 19 03:58:47.844: INFO: Got endpoints: latency-svc-d5cmh [273.864785ms]
Jul 19 03:58:47.866: INFO: Created: latency-svc-tzzhp
Jul 19 03:58:47.877: INFO: Created: latency-svc-ntlk4
Jul 19 03:58:47.877: INFO: Got endpoints: latency-svc-tzzhp [293.743124ms]
Jul 19 03:58:47.880: INFO: Created: latency-svc-qk6g6
Jul 19 03:58:47.886: INFO: Got endpoints: latency-svc-ntlk4 [286.317437ms]
Jul 19 03:58:47.891: INFO: Got endpoints: latency-svc-qk6g6 [273.986591ms]
Jul 19 03:58:47.899: INFO: Created: latency-svc-ptjks
Jul 19 03:58:47.901: INFO: Created: latency-svc-d5hmj
Jul 19 03:58:47.910: INFO: Got endpoints: latency-svc-d5hmj [263.089851ms]
Jul 19 03:58:47.911: INFO: Got endpoints: latency-svc-ptjks [276.990779ms]
Jul 19 03:58:47.912: INFO: Created: latency-svc-lxw99
Jul 19 03:58:47.923: INFO: Got endpoints: latency-svc-lxw99 [239.757849ms]
Jul 19 03:58:47.928: INFO: Created: latency-svc-qc79g
Jul 19 03:58:47.937: INFO: Got endpoints: latency-svc-qc79g [250.423374ms]
Jul 19 03:58:47.941: INFO: Created: latency-svc-pfc6l
Jul 19 03:58:47.948: INFO: Got endpoints: latency-svc-pfc6l [239.975611ms]
Jul 19 03:58:47.951: INFO: Created: latency-svc-bcctx
Jul 19 03:58:47.961: INFO: Got endpoints: latency-svc-bcctx [239.690876ms]
Jul 19 03:58:47.967: INFO: Created: latency-svc-lhscq
Jul 19 03:58:47.974: INFO: Created: latency-svc-gqztr
Jul 19 03:58:47.975: INFO: Got endpoints: latency-svc-lhscq [236.051563ms]
Jul 19 03:58:47.988: INFO: Got endpoints: latency-svc-gqztr [243.976356ms]
Jul 19 03:58:47.996: INFO: Created: latency-svc-xdzrg
Jul 19 03:58:48.002: INFO: Created: latency-svc-pcs97
Jul 19 03:58:48.010: INFO: Got endpoints: latency-svc-pcs97 [193.773571ms]
Jul 19 03:58:48.010: INFO: Got endpoints: latency-svc-xdzrg [203.149639ms]
Jul 19 03:58:48.015: INFO: Created: latency-svc-8kww5
Jul 19 03:58:48.023: INFO: Created: latency-svc-bxfgp
Jul 19 03:58:48.023: INFO: Got endpoints: latency-svc-8kww5 [187.602711ms]
Jul 19 03:58:48.048: INFO: Got endpoints: latency-svc-bxfgp [204.347224ms]
Jul 19 03:58:48.054: INFO: Created: latency-svc-kbfq6
Jul 19 03:58:48.080: INFO: Got endpoints: latency-svc-kbfq6 [202.820912ms]
Jul 19 03:58:48.087: INFO: Created: latency-svc-dpjff
Jul 19 03:58:48.095: INFO: Got endpoints: latency-svc-dpjff [208.062715ms]
Jul 19 03:58:48.095: INFO: Created: latency-svc-fmwkx
Jul 19 03:58:48.101: INFO: Got endpoints: latency-svc-fmwkx [210.463542ms]
Jul 19 03:58:48.110: INFO: Created: latency-svc-pbjgc
Jul 19 03:58:48.113: INFO: Created: latency-svc-4cdxf
Jul 19 03:58:48.117: INFO: Got endpoints: latency-svc-4cdxf [206.224746ms]
Jul 19 03:58:48.118: INFO: Got endpoints: latency-svc-pbjgc [207.980122ms]
Jul 19 03:58:48.128: INFO: Created: latency-svc-8n6qp
Jul 19 03:58:48.134: INFO: Got endpoints: latency-svc-8n6qp [211.069392ms]
Jul 19 03:58:48.137: INFO: Created: latency-svc-ztr7d
Jul 19 03:58:48.144: INFO: Got endpoints: latency-svc-ztr7d [206.632602ms]
Jul 19 03:58:48.151: INFO: Created: latency-svc-4nxqd
Jul 19 03:58:48.162: INFO: Got endpoints: latency-svc-4nxqd [213.758978ms]
Jul 19 03:58:48.162: INFO: Created: latency-svc-cpcxq
Jul 19 03:58:48.184: INFO: Created: latency-svc-grpxr
Jul 19 03:58:48.185: INFO: Got endpoints: latency-svc-cpcxq [223.031552ms]
Jul 19 03:58:48.199: INFO: Got endpoints: latency-svc-grpxr [224.440583ms]
Jul 19 03:58:48.213: INFO: Created: latency-svc-x2nzh
Jul 19 03:58:48.233: INFO: Got endpoints: latency-svc-x2nzh [245.257368ms]
Jul 19 03:58:48.248: INFO: Created: latency-svc-x5c47
Jul 19 03:58:48.292: INFO: Got endpoints: latency-svc-x5c47 [282.337133ms]
Jul 19 03:58:48.303: INFO: Created: latency-svc-4gqrz
Jul 19 03:58:48.311: INFO: Created: latency-svc-8p2rp
Jul 19 03:58:48.315: INFO: Got endpoints: latency-svc-4gqrz [305.110686ms]
Jul 19 03:58:48.321: INFO: Created: latency-svc-gdnl5
Jul 19 03:58:48.324: INFO: Got endpoints: latency-svc-8p2rp [300.916ms]
Jul 19 03:58:48.334: INFO: Got endpoints: latency-svc-gdnl5 [286.064824ms]
Jul 19 03:58:48.348: INFO: Created: latency-svc-62djg
Jul 19 03:58:48.393: INFO: Created: latency-svc-hw6fw
Jul 19 03:58:48.402: INFO: Created: latency-svc-2qnf9
Jul 19 03:58:48.409: INFO: Got endpoints: latency-svc-hw6fw [314.619873ms]
Jul 19 03:58:48.410: INFO: Got endpoints: latency-svc-62djg [329.264752ms]
Jul 19 03:58:48.424: INFO: Got endpoints: latency-svc-2qnf9 [322.686145ms]
Jul 19 03:58:48.425: INFO: Created: latency-svc-h9tht
Jul 19 03:58:48.427: INFO: Got endpoints: latency-svc-h9tht [310.339025ms]
Jul 19 03:58:48.498: INFO: Created: latency-svc-2pkj6
Jul 19 03:58:48.509: INFO: Got endpoints: latency-svc-2pkj6 [390.856421ms]
Jul 19 03:58:48.509: INFO: Created: latency-svc-cxhx2
Jul 19 03:58:48.523: INFO: Got endpoints: latency-svc-cxhx2 [388.698088ms]
Jul 19 03:58:48.532: INFO: Created: latency-svc-gfv5r
Jul 19 03:58:48.547: INFO: Created: latency-svc-qnd5h
Jul 19 03:58:48.547: INFO: Created: latency-svc-zwr7w
Jul 19 03:58:48.547: INFO: Got endpoints: latency-svc-gfv5r [402.408712ms]
Jul 19 03:58:48.555: INFO: Got endpoints: latency-svc-zwr7w [392.654691ms]
Jul 19 03:58:48.555: INFO: Got endpoints: latency-svc-qnd5h [370.169532ms]
Jul 19 03:58:48.557: INFO: Created: latency-svc-989rz
Jul 19 03:58:48.607: INFO: Created: latency-svc-bfwkl
Jul 19 03:58:48.607: INFO: Got endpoints: latency-svc-989rz [407.114207ms]
Jul 19 03:58:48.620: INFO: Got endpoints: latency-svc-bfwkl [386.978992ms]
Jul 19 03:58:48.623: INFO: Created: latency-svc-hnms2
Jul 19 03:58:48.691: INFO: Got endpoints: latency-svc-hnms2 [399.306139ms]
Jul 19 03:58:48.702: INFO: Created: latency-svc-prrg8
Jul 19 03:58:48.710: INFO: Got endpoints: latency-svc-prrg8 [395.367058ms]
Jul 19 03:58:48.714: INFO: Created: latency-svc-2bzmn
Jul 19 03:58:48.718: INFO: Created: latency-svc-shmgg
Jul 19 03:58:48.740: INFO: Got endpoints: latency-svc-2bzmn [405.568102ms]
Jul 19 03:58:48.740: INFO: Got endpoints: latency-svc-shmgg [415.59784ms]
Jul 19 03:58:48.760: INFO: Created: latency-svc-ttd72
Jul 19 03:58:48.762: INFO: Created: latency-svc-474vv
Jul 19 03:58:48.766: INFO: Got endpoints: latency-svc-474vv [356.514233ms]
Jul 19 03:58:48.770: INFO: Got endpoints: latency-svc-ttd72 [360.941213ms]
Jul 19 03:58:48.771: INFO: Created: latency-svc-ztq4v
Jul 19 03:58:48.780: INFO: Got endpoints: latency-svc-ztq4v [355.876538ms]
Jul 19 03:58:48.787: INFO: Created: latency-svc-4gg92
Jul 19 03:58:48.790: INFO: Created: latency-svc-zb4hr
Jul 19 03:58:48.802: INFO: Created: latency-svc-9cngs
Jul 19 03:58:48.802: INFO: Got endpoints: latency-svc-zb4hr [292.210422ms]
Jul 19 03:58:48.802: INFO: Got endpoints: latency-svc-4gg92 [374.397385ms]
Jul 19 03:58:48.806: INFO: Got endpoints: latency-svc-9cngs [282.667018ms]
Jul 19 03:58:48.809: INFO: Created: latency-svc-zjj9q
Jul 19 03:58:48.818: INFO: Got endpoints: latency-svc-zjj9q [271.516351ms]
Jul 19 03:58:48.822: INFO: Created: latency-svc-cddtf
Jul 19 03:58:48.838: INFO: Got endpoints: latency-svc-cddtf [283.127385ms]
Jul 19 03:58:48.838: INFO: Created: latency-svc-v2bk8
Jul 19 03:58:48.852: INFO: Got endpoints: latency-svc-v2bk8 [296.873454ms]
Jul 19 03:58:48.864: INFO: Created: latency-svc-m26tp
Jul 19 03:58:48.876: INFO: Created: latency-svc-2khl8
Jul 19 03:58:48.878: INFO: Got endpoints: latency-svc-m26tp [271.777551ms]
Jul 19 03:58:48.883: INFO: Created: latency-svc-4x5cn
Jul 19 03:58:48.888: INFO: Got endpoints: latency-svc-2khl8 [267.947547ms]
Jul 19 03:58:48.892: INFO: Got endpoints: latency-svc-4x5cn [200.526346ms]
Jul 19 03:58:48.898: INFO: Created: latency-svc-8484l
Jul 19 03:58:48.906: INFO: Created: latency-svc-lt6tt
Jul 19 03:58:48.910: INFO: Got endpoints: latency-svc-8484l [199.734356ms]
Jul 19 03:58:48.920: INFO: Got endpoints: latency-svc-lt6tt [180.209598ms]
Jul 19 03:58:48.920: INFO: Created: latency-svc-8bn6p
Jul 19 03:58:48.931: INFO: Created: latency-svc-w48tc
Jul 19 03:58:48.931: INFO: Got endpoints: latency-svc-8bn6p [190.743547ms]
Jul 19 03:58:48.939: INFO: Created: latency-svc-776c7
Jul 19 03:58:48.940: INFO: Got endpoints: latency-svc-w48tc [173.398897ms]
Jul 19 03:58:48.947: INFO: Created: latency-svc-277nn
Jul 19 03:58:48.953: INFO: Got endpoints: latency-svc-776c7 [182.651018ms]
Jul 19 03:58:48.954: INFO: Got endpoints: latency-svc-277nn [174.087325ms]
Jul 19 03:58:48.964: INFO: Created: latency-svc-w54ph
Jul 19 03:58:48.971: INFO: Got endpoints: latency-svc-w54ph [169.811635ms]
Jul 19 03:58:48.975: INFO: Created: latency-svc-ll54w
Jul 19 03:58:48.977: INFO: Created: latency-svc-54whv
Jul 19 03:58:48.984: INFO: Got endpoints: latency-svc-54whv [182.837877ms]
Jul 19 03:58:48.984: INFO: Got endpoints: latency-svc-ll54w [178.530362ms]
Jul 19 03:58:48.988: INFO: Created: latency-svc-swzzb
Jul 19 03:58:48.996: INFO: Created: latency-svc-pgwcz
Jul 19 03:58:48.996: INFO: Got endpoints: latency-svc-swzzb [177.936385ms]
Jul 19 03:58:49.008: INFO: Created: latency-svc-l2n2r
Jul 19 03:58:49.008: INFO: Got endpoints: latency-svc-pgwcz [169.5094ms]
Jul 19 03:58:49.014: INFO: Got endpoints: latency-svc-l2n2r [161.856714ms]
Jul 19 03:58:49.023: INFO: Created: latency-svc-fkc72
Jul 19 03:58:49.030: INFO: Created: latency-svc-bkrzx
Jul 19 03:58:49.035: INFO: Got endpoints: latency-svc-fkc72 [156.784999ms]
Jul 19 03:58:49.041: INFO: Created: latency-svc-nrsr9
Jul 19 03:58:49.044: INFO: Got endpoints: latency-svc-bkrzx [156.288608ms]
Jul 19 03:58:49.049: INFO: Got endpoints: latency-svc-nrsr9 [157.530488ms]
Jul 19 03:58:49.056: INFO: Created: latency-svc-qn9cz
Jul 19 03:58:49.071: INFO: Got endpoints: latency-svc-qn9cz [160.956318ms]
Jul 19 03:58:49.073: INFO: Created: latency-svc-brl7p
Jul 19 03:58:49.076: INFO: Created: latency-svc-dbgzn
Jul 19 03:58:49.087: INFO: Got endpoints: latency-svc-dbgzn [156.100539ms]
Jul 19 03:58:49.087: INFO: Got endpoints: latency-svc-brl7p [166.687606ms]
Jul 19 03:58:49.101: INFO: Created: latency-svc-lcb6g
Jul 19 03:58:49.101: INFO: Created: latency-svc-qxfpj
Jul 19 03:58:49.114: INFO: Got endpoints: latency-svc-qxfpj [161.270119ms]
Jul 19 03:58:49.117: INFO: Created: latency-svc-2r7n6
Jul 19 03:58:49.118: INFO: Got endpoints: latency-svc-lcb6g [178.010066ms]
Jul 19 03:58:49.121: INFO: Got endpoints: latency-svc-2r7n6 [166.880241ms]
Jul 19 03:58:49.128: INFO: Created: latency-svc-7bwpb
Jul 19 03:58:49.138: INFO: Created: latency-svc-9g8lb
Jul 19 03:58:49.144: INFO: Created: latency-svc-7fvdn
Jul 19 03:58:49.149: INFO: Got endpoints: latency-svc-9g8lb [164.45002ms]
Jul 19 03:58:49.149: INFO: Got endpoints: latency-svc-7bwpb [177.551334ms]
Jul 19 03:58:49.158: INFO: Got endpoints: latency-svc-7fvdn [173.150848ms]
Jul 19 03:58:49.163: INFO: Created: latency-svc-ghb2v
Jul 19 03:58:49.172: INFO: Got endpoints: latency-svc-ghb2v [175.759695ms]
Jul 19 03:58:49.182: INFO: Created: latency-svc-cv58x
Jul 19 03:58:49.183: INFO: Created: latency-svc-tr26k
Jul 19 03:58:49.192: INFO: Got endpoints: latency-svc-cv58x [184.259068ms]
Jul 19 03:58:49.198: INFO: Got endpoints: latency-svc-tr26k [184.637347ms]
Jul 19 03:58:49.217: INFO: Created: latency-svc-9zdzj
Jul 19 03:58:49.237: INFO: Created: latency-svc-j9l72
Jul 19 03:58:49.238: INFO: Got endpoints: latency-svc-9zdzj [202.22438ms]
Jul 19 03:58:49.247: INFO: Got endpoints: latency-svc-j9l72 [202.272306ms]
Jul 19 03:58:49.285: INFO: Created: latency-svc-8rqdw
Jul 19 03:58:49.297: INFO: Got endpoints: latency-svc-8rqdw [247.03735ms]
Jul 19 03:58:49.306: INFO: Created: latency-svc-hhg9h
Jul 19 03:58:49.318: INFO: Got endpoints: latency-svc-hhg9h [246.538023ms]
Jul 19 03:58:49.344: INFO: Created: latency-svc-v4pf4
Jul 19 03:58:49.351: INFO: Got endpoints: latency-svc-v4pf4 [264.093235ms]
Jul 19 03:58:49.356: INFO: Created: latency-svc-pgk79
Jul 19 03:58:49.362: INFO: Got endpoints: latency-svc-pgk79 [275.268294ms]
Jul 19 03:58:49.461: INFO: Created: latency-svc-44h64
Jul 19 03:58:49.467: INFO: Got endpoints: latency-svc-44h64 [352.475629ms]
Jul 19 03:58:49.472: INFO: Created: latency-svc-mr5fj
Jul 19 03:58:49.498: INFO: Got endpoints: latency-svc-mr5fj [379.982147ms]
Jul 19 03:58:49.498: INFO: Created: latency-svc-z4wg9
Jul 19 03:58:49.529: INFO: Got endpoints: latency-svc-z4wg9 [408.30283ms]
Jul 19 03:58:49.530: INFO: Created: latency-svc-rdxdz
Jul 19 03:58:49.543: INFO: Created: latency-svc-hczsb
Jul 19 03:58:49.548: INFO: Got endpoints: latency-svc-rdxdz [399.456722ms]
Jul 19 03:58:49.550: INFO: Got endpoints: latency-svc-hczsb [400.439156ms]
Jul 19 03:58:49.555: INFO: Created: latency-svc-mm9nn
Jul 19 03:58:49.580: INFO: Got endpoints: latency-svc-mm9nn [421.964114ms]
Jul 19 03:58:49.582: INFO: Created: latency-svc-fbgkt
Jul 19 03:58:49.602: INFO: Got endpoints: latency-svc-fbgkt [429.354556ms]
Jul 19 03:58:49.602: INFO: Created: latency-svc-zvw4l
Jul 19 03:58:49.614: INFO: Created: latency-svc-xp62b
Jul 19 03:58:49.614: INFO: Got endpoints: latency-svc-zvw4l [421.826969ms]
Jul 19 03:58:49.625: INFO: Got endpoints: latency-svc-xp62b [426.579189ms]
Jul 19 03:58:49.629: INFO: Created: latency-svc-8g5qv
Jul 19 03:58:49.635: INFO: Got endpoints: latency-svc-8g5qv [397.149441ms]
Jul 19 03:58:49.637: INFO: Created: latency-svc-4s7j4
Jul 19 03:58:49.647: INFO: Got endpoints: latency-svc-4s7j4 [400.160648ms]
Jul 19 03:58:49.652: INFO: Created: latency-svc-sxkft
Jul 19 03:58:49.662: INFO: Got endpoints: latency-svc-sxkft [365.911471ms]
Jul 19 03:58:49.665: INFO: Created: latency-svc-cx9sm
Jul 19 03:58:49.675: INFO: Created: latency-svc-ktgqj
Jul 19 03:58:49.679: INFO: Got endpoints: latency-svc-cx9sm [361.528101ms]
Jul 19 03:58:49.693: INFO: Created: latency-svc-9m6qt
Jul 19 03:58:49.693: INFO: Got endpoints: latency-svc-ktgqj [342.101005ms]
Jul 19 03:58:49.703: INFO: Got endpoints: latency-svc-9m6qt [340.25173ms]
Jul 19 03:58:49.714: INFO: Created: latency-svc-ndt92
Jul 19 03:58:49.724: INFO: Created: latency-svc-vfgf2
Jul 19 03:58:49.725: INFO: Got endpoints: latency-svc-ndt92 [258.736382ms]
Jul 19 03:58:49.728: INFO: Got endpoints: latency-svc-vfgf2 [230.169393ms]
Jul 19 03:58:49.731: INFO: Created: latency-svc-x5vq4
Jul 19 03:58:49.737: INFO: Got endpoints: latency-svc-x5vq4 [207.4384ms]
Jul 19 03:58:49.737: INFO: Latencies: [141.514505ms 156.100539ms 156.288608ms 156.784999ms 157.530488ms 160.956318ms 161.270119ms 161.856714ms 164.45002ms 165.738692ms 166.687606ms 166.880241ms 169.5094ms 169.811635ms 173.150848ms 173.398897ms 174.087325ms 175.759695ms 177.551334ms 177.936385ms 178.010066ms 178.530362ms 180.209598ms 180.347462ms 181.725692ms 182.330507ms 182.651018ms 182.837877ms 184.259068ms 184.637347ms 185.062957ms 186.055999ms 186.859687ms 187.602711ms 189.244768ms 189.7717ms 189.773952ms 190.413438ms 190.743547ms 191.599869ms 192.802812ms 193.773571ms 197.571412ms 197.925495ms 198.679179ms 198.968327ms 199.734356ms 199.789807ms 199.943055ms 200.526346ms 201.789248ms 202.22438ms 202.272306ms 202.585635ms 202.820912ms 203.149639ms 203.418857ms 204.044937ms 204.347224ms 204.515317ms 204.850492ms 206.224746ms 206.632602ms 207.023391ms 207.4384ms 207.980122ms 208.062715ms 210.463542ms 211.069392ms 211.43236ms 211.71576ms 213.758978ms 214.58597ms 217.027429ms 217.262287ms 217.610712ms 219.664855ms 221.36986ms 223.008787ms 223.031552ms 223.468602ms 224.242552ms 224.440583ms 226.14891ms 226.561989ms 230.169393ms 232.272379ms 233.024323ms 234.045272ms 236.051563ms 237.043667ms 239.690876ms 239.757849ms 239.794786ms 239.975611ms 243.976356ms 245.257368ms 246.538023ms 247.03735ms 247.598846ms 248.530025ms 248.830645ms 249.430943ms 250.423374ms 254.930977ms 258.736382ms 259.586524ms 261.859826ms 263.089851ms 264.093235ms 266.461907ms 267.707115ms 267.741065ms 267.947547ms 271.260687ms 271.516351ms 271.777551ms 271.797308ms 273.864785ms 273.986591ms 275.268294ms 275.667247ms 276.173874ms 276.990779ms 278.011282ms 281.97105ms 282.337133ms 282.667018ms 283.127385ms 286.064824ms 286.317437ms 286.971517ms 288.088775ms 288.324892ms 292.210422ms 293.469477ms 293.743124ms 293.77787ms 296.873454ms 300.916ms 305.110686ms 310.339025ms 314.161644ms 314.619873ms 322.686145ms 326.657944ms 329.013607ms 329.264752ms 340.25173ms 342.101005ms 352.475629ms 355.876538ms 356.514233ms 360.941213ms 361.528101ms 365.911471ms 370.169532ms 374.397385ms 379.982147ms 386.978992ms 388.698088ms 390.856421ms 392.654691ms 395.367058ms 397.149441ms 399.306139ms 399.456722ms 400.160648ms 400.439156ms 402.329169ms 402.408712ms 405.568102ms 407.114207ms 408.30283ms 415.59784ms 421.826969ms 421.964114ms 426.579189ms 429.354556ms 522.012639ms 618.661853ms 623.822467ms 704.282647ms 709.320449ms 898.411432ms 1.005464012s 1.010897617s 1.015588414s 1.191732145s 1.215976691s 1.31871598s 1.325924332s 1.328871005s 1.420518511s 1.609702052s 1.816041095s 1.913898053s 2.208338601s 2.212483368s 2.228597644s]
Jul 19 03:58:49.737: INFO: 50 %ile: 248.530025ms
Jul 19 03:58:49.737: INFO: 90 %ile: 618.661853ms
Jul 19 03:58:49.737: INFO: 99 %ile: 2.212483368s
Jul 19 03:58:49.737: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:49.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9976" for this suite.

• [SLOW TEST:9.218 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":23,"skipped":368,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:49.768: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:58:49.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-953" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":24,"skipped":375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:58:49.844: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 03:59:01.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1025" for this suite.

• [SLOW TEST:11.202 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":25,"skipped":404,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 03:59:01.047: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7048
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jul 19 03:59:01.309: INFO: Found 0 stateful pods, waiting for 3
Jul 19 03:59:11.313: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 03:59:11.313: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 03:59:11.313: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 19 03:59:21.313: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 03:59:21.313: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 03:59:21.313: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 19 03:59:21.344: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 19 03:59:31.382: INFO: Updating stateful set ss2
Jul 19 03:59:31.388: INFO: Waiting for Pod statefulset-7048/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 03:59:41.399: INFO: Waiting for Pod statefulset-7048/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jul 19 03:59:51.457: INFO: Found 2 stateful pods, waiting for 3
Jul 19 04:00:01.463: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:00:01.463: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:00:01.463: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 19 04:00:01.490: INFO: Updating stateful set ss2
Jul 19 04:00:01.500: INFO: Waiting for Pod statefulset-7048/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:00:11.508: INFO: Waiting for Pod statefulset-7048/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:00:21.527: INFO: Updating stateful set ss2
Jul 19 04:00:21.534: INFO: Waiting for StatefulSet statefulset-7048/ss2 to complete update
Jul 19 04:00:21.534: INFO: Waiting for Pod statefulset-7048/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:00:31.541: INFO: Waiting for StatefulSet statefulset-7048/ss2 to complete update
Jul 19 04:00:31.541: INFO: Waiting for Pod statefulset-7048/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:00:41.541: INFO: Waiting for StatefulSet statefulset-7048/ss2 to complete update
Jul 19 04:00:41.541: INFO: Waiting for Pod statefulset-7048/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 19 04:00:51.542: INFO: Deleting all statefulset in ns statefulset-7048
Jul 19 04:00:51.546: INFO: Scaling statefulset ss2 to 0
Jul 19 04:01:41.571: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 04:01:41.577: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:01:41.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7048" for this suite.

• [SLOW TEST:160.582 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":26,"skipped":425,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:01:41.629: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 19 04:01:41.792: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 19 04:01:41.797: INFO: starting watch
STEP: patching
STEP: updating
Jul 19 04:01:41.809: INFO: waiting for watch events with expected annotations
Jul 19 04:01:41.809: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:01:41.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9571" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":27,"skipped":443,"failed":0}

------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:01:41.928: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:01:41.998: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-db4d09c8-a3b9-43d4-9351-62bc62ff8389" in namespace "security-context-test-6385" to be "Succeeded or Failed"
Jul 19 04:01:42.013: INFO: Pod "busybox-readonly-false-db4d09c8-a3b9-43d4-9351-62bc62ff8389": Phase="Pending", Reason="", readiness=false. Elapsed: 15.049872ms
Jul 19 04:01:44.017: INFO: Pod "busybox-readonly-false-db4d09c8-a3b9-43d4-9351-62bc62ff8389": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019250414s
Jul 19 04:01:46.022: INFO: Pod "busybox-readonly-false-db4d09c8-a3b9-43d4-9351-62bc62ff8389": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023782094s
Jul 19 04:01:46.022: INFO: Pod "busybox-readonly-false-db4d09c8-a3b9-43d4-9351-62bc62ff8389" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:01:46.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6385" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":28,"skipped":443,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:01:46.034: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-d9ee7ce0-d6c9-4021-bd0a-b373ea211c13
STEP: Creating a pod to test consume secrets
Jul 19 04:01:46.117: INFO: Waiting up to 5m0s for pod "pod-secrets-1b54c59f-3a70-4ae1-b205-ad0616e67ed8" in namespace "secrets-5000" to be "Succeeded or Failed"
Jul 19 04:01:46.121: INFO: Pod "pod-secrets-1b54c59f-3a70-4ae1-b205-ad0616e67ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.166147ms
Jul 19 04:01:48.125: INFO: Pod "pod-secrets-1b54c59f-3a70-4ae1-b205-ad0616e67ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007433341s
Jul 19 04:01:50.129: INFO: Pod "pod-secrets-1b54c59f-3a70-4ae1-b205-ad0616e67ed8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011641773s
STEP: Saw pod success
Jul 19 04:01:50.129: INFO: Pod "pod-secrets-1b54c59f-3a70-4ae1-b205-ad0616e67ed8" satisfied condition "Succeeded or Failed"
Jul 19 04:01:50.132: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-secrets-1b54c59f-3a70-4ae1-b205-ad0616e67ed8 container secret-volume-test: <nil>
STEP: delete the pod
Jul 19 04:01:50.243: INFO: Waiting for pod pod-secrets-1b54c59f-3a70-4ae1-b205-ad0616e67ed8 to disappear
Jul 19 04:01:50.247: INFO: Pod pod-secrets-1b54c59f-3a70-4ae1-b205-ad0616e67ed8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:01:50.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5000" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":29,"skipped":457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:01:50.294: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:01:50.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5199" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":30,"skipped":514,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:01:50.456: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:01:51.087: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 04:01:53.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762264111, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762264111, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762264111, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762264111, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 04:01:55.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762264111, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762264111, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762264111, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762264111, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:01:58.118: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:01:58.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9651" for this suite.
STEP: Destroying namespace "webhook-9651-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.932 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":31,"skipped":532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:01:58.388: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 19 04:02:02.574: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:02:02.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5029" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":32,"skipped":572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:02:02.613: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-35c57271-cf2f-4048-b221-387aadd36b0b
STEP: Creating a pod to test consume configMaps
Jul 19 04:02:02.697: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2fcdb830-06b5-4d05-9ea5-8ac536e923e5" in namespace "projected-5829" to be "Succeeded or Failed"
Jul 19 04:02:02.714: INFO: Pod "pod-projected-configmaps-2fcdb830-06b5-4d05-9ea5-8ac536e923e5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.14616ms
Jul 19 04:02:04.719: INFO: Pod "pod-projected-configmaps-2fcdb830-06b5-4d05-9ea5-8ac536e923e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021730507s
Jul 19 04:02:06.724: INFO: Pod "pod-projected-configmaps-2fcdb830-06b5-4d05-9ea5-8ac536e923e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02745581s
Jul 19 04:02:08.729: INFO: Pod "pod-projected-configmaps-2fcdb830-06b5-4d05-9ea5-8ac536e923e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032082705s
STEP: Saw pod success
Jul 19 04:02:08.729: INFO: Pod "pod-projected-configmaps-2fcdb830-06b5-4d05-9ea5-8ac536e923e5" satisfied condition "Succeeded or Failed"
Jul 19 04:02:08.733: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-projected-configmaps-2fcdb830-06b5-4d05-9ea5-8ac536e923e5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 04:02:08.821: INFO: Waiting for pod pod-projected-configmaps-2fcdb830-06b5-4d05-9ea5-8ac536e923e5 to disappear
Jul 19 04:02:08.825: INFO: Pod pod-projected-configmaps-2fcdb830-06b5-4d05-9ea5-8ac536e923e5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:02:08.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5829" for this suite.

• [SLOW TEST:6.227 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":33,"skipped":594,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:02:08.840: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 19 04:02:13.473: INFO: Successfully updated pod "annotationupdatefefb09b2-0da5-47a1-b991-efe0de7d8571"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:02:15.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2022" for this suite.

• [SLOW TEST:6.677 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":34,"skipped":627,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:02:15.517: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jul 19 04:02:15.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-9225'
Jul 19 04:02:15.798: INFO: stderr: ""
Jul 19 04:02:15.798: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 19 04:02:15.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9225'
Jul 19 04:02:15.886: INFO: stderr: ""
Jul 19 04:02:15.886: INFO: stdout: "update-demo-nautilus-65xxp update-demo-nautilus-m5ddf "
Jul 19 04:02:15.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-65xxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9225'
Jul 19 04:02:15.972: INFO: stderr: ""
Jul 19 04:02:15.972: INFO: stdout: ""
Jul 19 04:02:15.972: INFO: update-demo-nautilus-65xxp is created but not running
Jul 19 04:02:20.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9225'
Jul 19 04:02:21.058: INFO: stderr: ""
Jul 19 04:02:21.058: INFO: stdout: "update-demo-nautilus-65xxp update-demo-nautilus-m5ddf "
Jul 19 04:02:21.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-65xxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9225'
Jul 19 04:02:21.143: INFO: stderr: ""
Jul 19 04:02:21.143: INFO: stdout: "true"
Jul 19 04:02:21.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-65xxp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9225'
Jul 19 04:02:21.232: INFO: stderr: ""
Jul 19 04:02:21.232: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 19 04:02:21.232: INFO: validating pod update-demo-nautilus-65xxp
Jul 19 04:02:21.243: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 04:02:21.243: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 04:02:21.243: INFO: update-demo-nautilus-65xxp is verified up and running
Jul 19 04:02:21.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-m5ddf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9225'
Jul 19 04:02:21.329: INFO: stderr: ""
Jul 19 04:02:21.329: INFO: stdout: "true"
Jul 19 04:02:21.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-m5ddf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9225'
Jul 19 04:02:21.413: INFO: stderr: ""
Jul 19 04:02:21.413: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 19 04:02:21.413: INFO: validating pod update-demo-nautilus-m5ddf
Jul 19 04:02:21.429: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 04:02:21.429: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 04:02:21.429: INFO: update-demo-nautilus-m5ddf is verified up and running
STEP: using delete to clean up resources
Jul 19 04:02:21.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete --grace-period=0 --force -f - --namespace=kubectl-9225'
Jul 19 04:02:21.543: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 04:02:21.543: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 19 04:02:21.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9225'
Jul 19 04:02:21.637: INFO: stderr: "No resources found in kubectl-9225 namespace.\n"
Jul 19 04:02:21.637: INFO: stdout: ""
Jul 19 04:02:21.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -l name=update-demo --namespace=kubectl-9225 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 19 04:02:21.743: INFO: stderr: ""
Jul 19 04:02:21.743: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:02:21.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9225" for this suite.

• [SLOW TEST:6.239 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":35,"skipped":628,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:02:21.757: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:02:21.828: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Creating first CR 
Jul 19 04:02:22.464: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-19T04:02:22Z generation:1 name:name1 resourceVersion:2399898 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:40e5e598-8d91-4ea0-ab84-00bf5869b24b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul 19 04:02:32.476: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-19T04:02:32Z generation:1 name:name2 resourceVersion:2399974 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ed390c6d-77cc-4203-b6e2-f13c928c14a3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul 19 04:02:42.485: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-19T04:02:22Z generation:2 name:name1 resourceVersion:2400048 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:40e5e598-8d91-4ea0-ab84-00bf5869b24b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul 19 04:02:52.491: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-19T04:02:32Z generation:2 name:name2 resourceVersion:2400118 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ed390c6d-77cc-4203-b6e2-f13c928c14a3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul 19 04:03:02.501: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-19T04:02:22Z generation:2 name:name1 resourceVersion:2400188 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:40e5e598-8d91-4ea0-ab84-00bf5869b24b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul 19 04:03:12.513: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-19T04:02:32Z generation:2 name:name2 resourceVersion:2400256 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ed390c6d-77cc-4203-b6e2-f13c928c14a3] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:03:23.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2590" for this suite.

• [SLOW TEST:61.343 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":36,"skipped":641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:03:23.100: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 19 04:03:33.256: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1250 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:03:33.256: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:03:33.436: INFO: Exec stderr: ""
Jul 19 04:03:33.436: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1250 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:03:33.436: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:03:33.656: INFO: Exec stderr: ""
Jul 19 04:03:33.656: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1250 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:03:33.656: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:03:33.889: INFO: Exec stderr: ""
Jul 19 04:03:33.889: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1250 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:03:33.889: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:03:34.108: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 19 04:03:34.108: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1250 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:03:34.108: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:03:34.274: INFO: Exec stderr: ""
Jul 19 04:03:34.274: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1250 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:03:34.274: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:03:34.460: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 19 04:03:34.460: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1250 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:03:34.460: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:03:34.628: INFO: Exec stderr: ""
Jul 19 04:03:34.628: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1250 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:03:34.628: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:03:34.789: INFO: Exec stderr: ""
Jul 19 04:03:34.789: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1250 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:03:34.790: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:03:34.970: INFO: Exec stderr: ""
Jul 19 04:03:34.970: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1250 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:03:34.970: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:03:35.130: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:03:35.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1250" for this suite.

• [SLOW TEST:12.044 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":683,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:03:35.144: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:03:52.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8749" for this suite.

• [SLOW TEST:17.178 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":38,"skipped":689,"failed":0}
SSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:03:52.322: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:03:52.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4483" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":39,"skipped":692,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:03:52.445: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 04:03:52.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ef56d11-af28-4f6f-9268-cb300081669b" in namespace "projected-967" to be "Succeeded or Failed"
Jul 19 04:03:52.528: INFO: Pod "downwardapi-volume-3ef56d11-af28-4f6f-9268-cb300081669b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.438846ms
Jul 19 04:03:54.532: INFO: Pod "downwardapi-volume-3ef56d11-af28-4f6f-9268-cb300081669b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007471235s
Jul 19 04:03:56.536: INFO: Pod "downwardapi-volume-3ef56d11-af28-4f6f-9268-cb300081669b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012235839s
STEP: Saw pod success
Jul 19 04:03:56.536: INFO: Pod "downwardapi-volume-3ef56d11-af28-4f6f-9268-cb300081669b" satisfied condition "Succeeded or Failed"
Jul 19 04:03:56.540: INFO: Trying to get logs from node paas-192-168-10-158 pod downwardapi-volume-3ef56d11-af28-4f6f-9268-cb300081669b container client-container: <nil>
STEP: delete the pod
Jul 19 04:03:56.618: INFO: Waiting for pod downwardapi-volume-3ef56d11-af28-4f6f-9268-cb300081669b to disappear
Jul 19 04:03:56.625: INFO: Pod downwardapi-volume-3ef56d11-af28-4f6f-9268-cb300081669b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:03:56.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-967" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":40,"skipped":703,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:03:56.637: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 04:03:56.741: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0039e29-03b5-481e-8f4c-80559fc29998" in namespace "projected-2541" to be "Succeeded or Failed"
Jul 19 04:03:56.752: INFO: Pod "downwardapi-volume-a0039e29-03b5-481e-8f4c-80559fc29998": Phase="Pending", Reason="", readiness=false. Elapsed: 10.278271ms
Jul 19 04:03:58.756: INFO: Pod "downwardapi-volume-a0039e29-03b5-481e-8f4c-80559fc29998": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014702634s
Jul 19 04:04:00.760: INFO: Pod "downwardapi-volume-a0039e29-03b5-481e-8f4c-80559fc29998": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018761203s
STEP: Saw pod success
Jul 19 04:04:00.760: INFO: Pod "downwardapi-volume-a0039e29-03b5-481e-8f4c-80559fc29998" satisfied condition "Succeeded or Failed"
Jul 19 04:04:00.764: INFO: Trying to get logs from node paas-192-168-10-158 pod downwardapi-volume-a0039e29-03b5-481e-8f4c-80559fc29998 container client-container: <nil>
STEP: delete the pod
Jul 19 04:04:00.794: INFO: Waiting for pod downwardapi-volume-a0039e29-03b5-481e-8f4c-80559fc29998 to disappear
Jul 19 04:04:00.798: INFO: Pod downwardapi-volume-a0039e29-03b5-481e-8f4c-80559fc29998 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:04:00.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2541" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":41,"skipped":708,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:04:00.814: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 19 04:04:00.907: INFO: Waiting up to 5m0s for pod "downward-api-2e889e7c-e871-4da9-8c3b-f661e24c214f" in namespace "downward-api-5251" to be "Succeeded or Failed"
Jul 19 04:04:00.913: INFO: Pod "downward-api-2e889e7c-e871-4da9-8c3b-f661e24c214f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209287ms
Jul 19 04:04:02.917: INFO: Pod "downward-api-2e889e7c-e871-4da9-8c3b-f661e24c214f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01060679s
Jul 19 04:04:04.921: INFO: Pod "downward-api-2e889e7c-e871-4da9-8c3b-f661e24c214f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0145316s
STEP: Saw pod success
Jul 19 04:04:04.921: INFO: Pod "downward-api-2e889e7c-e871-4da9-8c3b-f661e24c214f" satisfied condition "Succeeded or Failed"
Jul 19 04:04:04.925: INFO: Trying to get logs from node paas-192-168-10-158 pod downward-api-2e889e7c-e871-4da9-8c3b-f661e24c214f container dapi-container: <nil>
STEP: delete the pod
Jul 19 04:04:04.953: INFO: Waiting for pod downward-api-2e889e7c-e871-4da9-8c3b-f661e24c214f to disappear
Jul 19 04:04:04.958: INFO: Pod downward-api-2e889e7c-e871-4da9-8c3b-f661e24c214f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:04:04.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5251" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":42,"skipped":720,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:04:04.974: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 19 04:04:05.053: INFO: Waiting up to 5m0s for pod "pod-454cdd0e-b55f-4a38-9fc0-ceeab702e680" in namespace "emptydir-2566" to be "Succeeded or Failed"
Jul 19 04:04:05.057: INFO: Pod "pod-454cdd0e-b55f-4a38-9fc0-ceeab702e680": Phase="Pending", Reason="", readiness=false. Elapsed: 3.801504ms
Jul 19 04:04:07.061: INFO: Pod "pod-454cdd0e-b55f-4a38-9fc0-ceeab702e680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007346458s
Jul 19 04:04:09.065: INFO: Pod "pod-454cdd0e-b55f-4a38-9fc0-ceeab702e680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011984703s
STEP: Saw pod success
Jul 19 04:04:09.065: INFO: Pod "pod-454cdd0e-b55f-4a38-9fc0-ceeab702e680" satisfied condition "Succeeded or Failed"
Jul 19 04:04:09.071: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-454cdd0e-b55f-4a38-9fc0-ceeab702e680 container test-container: <nil>
STEP: delete the pod
Jul 19 04:04:09.122: INFO: Waiting for pod pod-454cdd0e-b55f-4a38-9fc0-ceeab702e680 to disappear
Jul 19 04:04:09.125: INFO: Pod pod-454cdd0e-b55f-4a38-9fc0-ceeab702e680 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:04:09.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2566" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":43,"skipped":720,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:04:09.139: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-hjzh
STEP: Creating a pod to test atomic-volume-subpath
Jul 19 04:04:09.247: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hjzh" in namespace "subpath-1276" to be "Succeeded or Failed"
Jul 19 04:04:09.250: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.960515ms
Jul 19 04:04:11.264: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016417448s
Jul 19 04:04:13.269: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Running", Reason="", readiness=true. Elapsed: 4.021146735s
Jul 19 04:04:15.272: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Running", Reason="", readiness=true. Elapsed: 6.024642101s
Jul 19 04:04:17.276: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Running", Reason="", readiness=true. Elapsed: 8.02877893s
Jul 19 04:04:19.282: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Running", Reason="", readiness=true. Elapsed: 10.034299982s
Jul 19 04:04:21.337: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Running", Reason="", readiness=true. Elapsed: 12.089101102s
Jul 19 04:04:23.359: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Running", Reason="", readiness=true. Elapsed: 14.111964498s
Jul 19 04:04:25.364: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Running", Reason="", readiness=true. Elapsed: 16.116419805s
Jul 19 04:04:27.369: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Running", Reason="", readiness=true. Elapsed: 18.121119937s
Jul 19 04:04:29.373: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Running", Reason="", readiness=true. Elapsed: 20.125974335s
Jul 19 04:04:31.378: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Running", Reason="", readiness=true. Elapsed: 22.130838965s
Jul 19 04:04:33.383: INFO: Pod "pod-subpath-test-secret-hjzh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.135516499s
STEP: Saw pod success
Jul 19 04:04:33.383: INFO: Pod "pod-subpath-test-secret-hjzh" satisfied condition "Succeeded or Failed"
Jul 19 04:04:33.387: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-subpath-test-secret-hjzh container test-container-subpath-secret-hjzh: <nil>
STEP: delete the pod
Jul 19 04:04:33.411: INFO: Waiting for pod pod-subpath-test-secret-hjzh to disappear
Jul 19 04:04:33.416: INFO: Pod pod-subpath-test-secret-hjzh no longer exists
STEP: Deleting pod pod-subpath-test-secret-hjzh
Jul 19 04:04:33.416: INFO: Deleting pod "pod-subpath-test-secret-hjzh" in namespace "subpath-1276"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:04:33.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1276" for this suite.

• [SLOW TEST:24.293 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":44,"skipped":746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:04:33.433: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:04:33.537: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-8b6e1355-35a6-47fc-b06b-8277a73b2215" in namespace "security-context-test-2603" to be "Succeeded or Failed"
Jul 19 04:04:33.542: INFO: Pod "busybox-privileged-false-8b6e1355-35a6-47fc-b06b-8277a73b2215": Phase="Pending", Reason="", readiness=false. Elapsed: 4.96398ms
Jul 19 04:04:35.547: INFO: Pod "busybox-privileged-false-8b6e1355-35a6-47fc-b06b-8277a73b2215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009658986s
Jul 19 04:04:37.552: INFO: Pod "busybox-privileged-false-8b6e1355-35a6-47fc-b06b-8277a73b2215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014418372s
Jul 19 04:04:37.552: INFO: Pod "busybox-privileged-false-8b6e1355-35a6-47fc-b06b-8277a73b2215" satisfied condition "Succeeded or Failed"
Jul 19 04:04:37.607: INFO: Got logs for pod "busybox-privileged-false-8b6e1355-35a6-47fc-b06b-8277a73b2215": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:04:37.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2603" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":45,"skipped":778,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:04:37.623: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:04:41.754: INFO: Waiting up to 5m0s for pod "client-envvars-d8fa6e1f-9aa7-46e5-8eae-6c9ee612116b" in namespace "pods-5660" to be "Succeeded or Failed"
Jul 19 04:04:41.759: INFO: Pod "client-envvars-d8fa6e1f-9aa7-46e5-8eae-6c9ee612116b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.748466ms
Jul 19 04:04:43.763: INFO: Pod "client-envvars-d8fa6e1f-9aa7-46e5-8eae-6c9ee612116b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008619947s
Jul 19 04:04:45.767: INFO: Pod "client-envvars-d8fa6e1f-9aa7-46e5-8eae-6c9ee612116b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012426317s
STEP: Saw pod success
Jul 19 04:04:45.767: INFO: Pod "client-envvars-d8fa6e1f-9aa7-46e5-8eae-6c9ee612116b" satisfied condition "Succeeded or Failed"
Jul 19 04:04:45.770: INFO: Trying to get logs from node paas-192-168-10-158 pod client-envvars-d8fa6e1f-9aa7-46e5-8eae-6c9ee612116b container env3cont: <nil>
STEP: delete the pod
Jul 19 04:04:45.793: INFO: Waiting for pod client-envvars-d8fa6e1f-9aa7-46e5-8eae-6c9ee612116b to disappear
Jul 19 04:04:45.797: INFO: Pod client-envvars-d8fa6e1f-9aa7-46e5-8eae-6c9ee612116b no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:04:45.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5660" for this suite.

• [SLOW TEST:8.194 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":46,"skipped":791,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:04:45.818: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 19 04:04:50.475: INFO: Successfully updated pod "annotationupdate705c875f-01c3-4368-b629-25903c6389a4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:04:52.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6913" for this suite.

• [SLOW TEST:6.698 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":47,"skipped":821,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:04:52.516: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 04:04:52.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6a2f240-e75b-4406-a878-9d08b0426016" in namespace "projected-6033" to be "Succeeded or Failed"
Jul 19 04:04:52.584: INFO: Pod "downwardapi-volume-c6a2f240-e75b-4406-a878-9d08b0426016": Phase="Pending", Reason="", readiness=false. Elapsed: 3.100798ms
Jul 19 04:04:54.588: INFO: Pod "downwardapi-volume-c6a2f240-e75b-4406-a878-9d08b0426016": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006716289s
Jul 19 04:04:56.592: INFO: Pod "downwardapi-volume-c6a2f240-e75b-4406-a878-9d08b0426016": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010534595s
STEP: Saw pod success
Jul 19 04:04:56.592: INFO: Pod "downwardapi-volume-c6a2f240-e75b-4406-a878-9d08b0426016" satisfied condition "Succeeded or Failed"
Jul 19 04:04:56.595: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-c6a2f240-e75b-4406-a878-9d08b0426016 container client-container: <nil>
STEP: delete the pod
Jul 19 04:04:56.621: INFO: Waiting for pod downwardapi-volume-c6a2f240-e75b-4406-a878-9d08b0426016 to disappear
Jul 19 04:04:56.625: INFO: Pod downwardapi-volume-c6a2f240-e75b-4406-a878-9d08b0426016 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:04:56.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6033" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":48,"skipped":823,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:04:56.637: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jul 19 04:05:00.745: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5479 PodName:var-expansion-72711d93-7199-4b94-b1c3-911dbb66e460 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:05:00.745: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: test for file in mounted path
Jul 19 04:05:00.925: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5479 PodName:var-expansion-72711d93-7199-4b94-b1c3-911dbb66e460 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:05:00.925: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: updating the annotation value
Jul 19 04:05:01.625: INFO: Successfully updated pod "var-expansion-72711d93-7199-4b94-b1c3-911dbb66e460"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jul 19 04:05:01.629: INFO: Deleting pod "var-expansion-72711d93-7199-4b94-b1c3-911dbb66e460" in namespace "var-expansion-5479"
Jul 19 04:05:01.640: INFO: Wait up to 5m0s for pod "var-expansion-72711d93-7199-4b94-b1c3-911dbb66e460" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:05:35.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5479" for this suite.

• [SLOW TEST:39.024 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":49,"skipped":825,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:05:35.661: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-116
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 19 04:05:35.729: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 19 04:05:35.771: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 04:05:37.775: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 04:05:39.776: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:05:41.776: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:05:43.776: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:05:45.775: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:05:47.777: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:05:49.775: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:05:51.776: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:05:53.776: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 19 04:05:53.783: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 19 04:05:57.815: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.0.73:8080/dial?request=hostname&protocol=udp&host=172.16.0.16&port=8081&tries=1'] Namespace:pod-network-test-116 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:05:57.815: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:05:58.127: INFO: Waiting for responses: map[]
Jul 19 04:05:58.133: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.0.73:8080/dial?request=hostname&protocol=udp&host=172.16.0.72&port=8081&tries=1'] Namespace:pod-network-test-116 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:05:58.133: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:05:58.336: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:05:58.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-116" for this suite.

• [SLOW TEST:22.688 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":50,"skipped":826,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:05:58.350: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Jul 19 04:05:58.418: INFO: Waiting up to 5m0s for pod "client-containers-29d3f59c-d36c-4550-8a34-aaa83fe26765" in namespace "containers-4906" to be "Succeeded or Failed"
Jul 19 04:05:58.422: INFO: Pod "client-containers-29d3f59c-d36c-4550-8a34-aaa83fe26765": Phase="Pending", Reason="", readiness=false. Elapsed: 3.812881ms
Jul 19 04:06:00.426: INFO: Pod "client-containers-29d3f59c-d36c-4550-8a34-aaa83fe26765": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008051018s
Jul 19 04:06:02.431: INFO: Pod "client-containers-29d3f59c-d36c-4550-8a34-aaa83fe26765": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013211716s
STEP: Saw pod success
Jul 19 04:06:02.431: INFO: Pod "client-containers-29d3f59c-d36c-4550-8a34-aaa83fe26765" satisfied condition "Succeeded or Failed"
Jul 19 04:06:02.434: INFO: Trying to get logs from node paas-192-168-10-158 pod client-containers-29d3f59c-d36c-4550-8a34-aaa83fe26765 container test-container: <nil>
STEP: delete the pod
Jul 19 04:06:02.468: INFO: Waiting for pod client-containers-29d3f59c-d36c-4550-8a34-aaa83fe26765 to disappear
Jul 19 04:06:02.471: INFO: Pod client-containers-29d3f59c-d36c-4550-8a34-aaa83fe26765 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:06:02.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4906" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":51,"skipped":845,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:06:02.485: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6139
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6139
I0719 04:06:02.606953      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6139, replica count: 2
I0719 04:06:05.657278      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 04:06:08.657: INFO: Creating new exec pod
I0719 04:06:08.657482      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 04:06:13.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-6139 execpodjj4tn -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 19 04:06:13.983: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 19 04:06:13.983: INFO: stdout: ""
Jul 19 04:06:13.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-6139 execpodjj4tn -- /bin/sh -x -c nc -zv -t -w 2 10.247.125.117 80'
Jul 19 04:06:14.239: INFO: stderr: "+ nc -zv -t -w 2 10.247.125.117 80\nConnection to 10.247.125.117 80 port [tcp/http] succeeded!\n"
Jul 19 04:06:14.239: INFO: stdout: ""
Jul 19 04:06:14.239: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:06:14.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6139" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:11.815 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":52,"skipped":845,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:06:14.300: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:06:14.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-9881'
Jul 19 04:06:14.748: INFO: stderr: ""
Jul 19 04:06:14.748: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul 19 04:06:14.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-9881'
Jul 19 04:06:14.972: INFO: stderr: ""
Jul 19 04:06:14.972: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 19 04:06:15.976: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 04:06:15.976: INFO: Found 0 / 1
Jul 19 04:06:16.976: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 04:06:16.976: INFO: Found 0 / 1
Jul 19 04:06:17.977: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 04:06:17.977: INFO: Found 0 / 1
Jul 19 04:06:18.977: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 04:06:18.977: INFO: Found 1 / 1
Jul 19 04:06:18.977: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 19 04:06:18.980: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 04:06:18.980: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 19 04:06:18.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 describe pod agnhost-primary-c2f6h --namespace=kubectl-9881'
Jul 19 04:06:19.089: INFO: stderr: ""
Jul 19 04:06:19.089: INFO: stdout: "Name:         agnhost-primary-c2f6h\nNamespace:    kubectl-9881\nPriority:     0\nNode:         paas-192-168-10-183/192.168.10.183\nStart Time:   Mon, 19 Jul 2021 04:06:14 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           172.16.0.75\nIPs:\n  IP:           172.16.0.75\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://bc69ad58f46b3f8e664e0c65fa6c9d64832445752fc45894a9b4ba216b931df3\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 19 Jul 2021 04:06:17 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBE_POD_NODE_NAME:   (v1:spec.nodeName)\n      KUBE_POD_NAMESPACE:  kubectl-9881 (v1:metadata.namespace)\n      KUBE_POD_CLUSTERID:  \n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-6x5mt (ro)\nConditions:\n  Type                Status\n  Initialized         True \n  Ready               True \n  ContainersReady     True \n  PodScheduled        True \n  ResourceAllocated   True \nVolumes:\n  default-token-6x5mt:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-6x5mt\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason            Age   From               Message\n  ----    ------            ----  ----               -------\n  Normal  Scheduled         4s    default-scheduler  Successfully assigned kubectl-9881/agnhost-primary-c2f6h to paas-192-168-10-183\n  Normal  Pulled            2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  SuccessfulCreate  2s    kubelet            Created container agnhost-primary\n  Normal  Started           2s    kubelet            Started container agnhost-primary\n"
Jul 19 04:06:19.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 describe rc agnhost-primary --namespace=kubectl-9881'
Jul 19 04:06:19.203: INFO: stderr: ""
Jul 19 04:06:19.203: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9881\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-c2f6h\n"
Jul 19 04:06:19.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 describe service agnhost-primary --namespace=kubectl-9881'
Jul 19 04:06:19.370: INFO: stderr: ""
Jul 19 04:06:19.370: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9881\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       service.alpha.kubernetes.io/ipstack: ipv4\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.247.2.102\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.16.0.75:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 19 04:06:19.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 describe node paas-192-168-10-143'
Jul 19 04:06:19.538: INFO: stderr: ""
Jul 19 04:06:19.538: INFO: stdout: "Name:               paas-192-168-10-143\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    com.huawei.project/name=fst-manage\n                    fst-manage.aad=aad\n                    fst-manage.alm=alm\n                    fst-manage.almreceiver=almreceiver\n                    fst-manage.als=als\n                    fst-manage.ams=ams\n                    fst-manage.aos=aos\n                    fst-manage.aosaddon=aosaddon\n                    fst-manage.aosworkflowengine=aosworkflowengine\n                    fst-manage.apm-etcd=apm-etcd\n                    fst-manage.audit=audit\n                    fst-manage.auth=auth\n                    fst-manage.base-upload=base-upload\n                    fst-manage.cam=cam\n                    fst-manage.canal=canal\n                    fst-manage.cfe=cfe\n                    fst-manage.cinder=cinder\n                    fst-manage.clustermanager=clustermanager\n                    fst-manage.core-upload=core-upload\n                    fst-manage.cse=cse\n                    fst-manage.csewebsite=csewebsite\n                    fst-manage.csms=csms\n                    fst-manage.dbproxy=dbproxy\n                    fst-manage.dns=dns\n                    fst-manage.dpa=dpa\n                    fst-manage.etcd=etcd\n                    fst-manage.eviewwebsite=eviewwebsite\n                    fst-manage.febs=febs\n                    fst-manage.fs=fs\n                    fst-manage.haproxy=haproxy\n                    fst-manage.icmgr=icmgr\n                    fst-manage.ingresslb=ingresslb\n                    fst-manage.ingressproxyir=ingressproxyir\n                    fst-manage.k8s=k8s\n                    fst-manage.keepalived=keepalived\n                    fst-manage.kube-addon=kube-addon\n                    fst-manage.kubelet=kubelet\n                    fst-manage.license=license\n                    fst-manage.mns=mns\n                    fst-manage.monit=monit\n                    fst-manage.node=node\n                    fst-manage.om=om\n                    fst-manage.pbs=pbs\n                    fst-manage.pe=pe\n                    fst-manage.portal=portal\n                    fst-manage.privilege=privilege\n                    fst-manage.psm=psm\n                    fst-manage.rhm=rhm\n                    fst-manage.smapp=smapp\n                    fst-manage.source2image=source2image\n                    fst-manage.swr=swr\n                    fst-manage.syslog=syslog\n                    fst-manage.trm=trm\n                    fst-manage.unisession=unisession\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/fsnodetype=fsmanagenode\n                    kubernetes.io/hostname=paas-192-168-10-143\n                    kubernetes.io/os=linux\n                    os.architecture=x86_64\n                    os.name=EulerOS_2.0_SP9x86_64\n                    os.version=4.18.0-147.5.1.6.h451.eulerosv2r9.x86_64\nAnnotations:        kubernetes.io/kubelet.common.envs: HOSTING_SERVER_IP=192.168.10.143\n                    node.alpha.kubernetes.io/ips: {\"name\":\"eth0\",\"ips\":[\"192.168.10.143\",\"192.168.10.40\"]}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.provision.alpha.kubernetes.io/node-address: 192.168.10.143\n                    node.provision.alpha.kubernetes.io/vmname: \n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 15 Jul 2021 02:24:44 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  paas-192-168-10-143\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 19 Jul 2021 04:06:06 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 19 Jul 2021 04:01:43 +0000   Thu, 15 Jul 2021 02:24:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 19 Jul 2021 04:01:43 +0000   Thu, 15 Jul 2021 02:24:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 19 Jul 2021 04:01:43 +0000   Thu, 15 Jul 2021 02:24:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 19 Jul 2021 04:01:43 +0000   Thu, 15 Jul 2021 02:25:04 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.10.143\n  Hostname:    paas-192-168-10-143\n  DataIP:      192.168.10.143\nCapacity:\n  cpu:                4\n  ephemeral-storage:  51343840Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7622940Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  47318482866\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7520540Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 952d77d81a474ab4940e73ff9aa9cc75\n  System UUID:                059be0b2-1020-4154-922c-203c5c2efe35\n  Boot ID:                    3bd094c3-223f-43e5-897e-af52b6145e8f\n  Kernel Version:             4.18.0-147.5.1.6.h451.eulerosv2r9.x86_64\n  OS Image:                   EulerOS 2.0 (SP9x86_64)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.9.0\n  Kubelet Version:            v1.19.4-Fusionstage21.3.0\n  Kube-Proxy Version:         v1.19.4-Fusionstage21.3.0\nNon-terminated Pods:          (25 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  fst-manage                  alarmnotifyservice-59b7f89f7f-bkppv                        50m (1%)      100m (2%)   100Mi (1%)       300Mi (4%)     4d1h\n  fst-manage                  aos-apiserver-7899c88bb5-kbhj9                             50m (1%)      500m (12%)  100Mi (1%)       3Gi (41%)      4d1h\n  fst-manage                  aos-cmdbserver-6d7558969b-hhk52                            50m (1%)      500m (12%)  100Mi (1%)       1Gi (13%)      4d1h\n  fst-manage                  aos-controller-f76d459dc-4zwhs                             50m (1%)      500m (12%)  100Mi (1%)       500Mi (6%)     4d1h\n  fst-manage                  aos-workflowengine-59db7cc6cc-nnzh9                        50m (1%)      500m (12%)  100Mi (1%)       4Gi (55%)      4d1h\n  fst-manage                  cam-registry-5c65595bb-s6ncf                               50m (1%)      500m (12%)  100Mi (1%)       500Mi (6%)     4d1h\n  fst-manage                  cfe-apiserver-cff99d887-mdpfp                              50m (1%)      200m (5%)   100Mi (1%)       800Mi (10%)    4d1h\n  fst-manage                  coredns-544b57bf6f-bqwz2                                   50m (1%)      1 (25%)     70Mi (0%)        1Gi (13%)      4d1h\n  fst-manage                  csms-storagemgr-7c9db466ff-d8785                           50m (1%)      500m (12%)  100Mi (1%)       2Gi (27%)      4d1h\n  fst-manage                  extension-controller-manager-7c85986667-2chlb              50m (1%)      100m (2%)   100Mi (1%)       2Gi (27%)      4d1h\n  fst-manage                  haproxy-paas-192-168-10-143                                50m (1%)      200m (5%)   100Mi (1%)       512Mi (6%)     4d1h\n  fst-manage                  k8s-etcd-backup-server-paas-192-168-10-143                 50m (1%)      500m (12%)  100Mi (1%)       500Mi (6%)     4d1h\n  fst-manage                  k8s-etcd-event-server-paas-192-168-10-143                  50m (1%)      500m (12%)  100Mi (1%)       3Gi (41%)      4d1h\n  fst-manage                  k8s-etcd-network-server-paas-192-168-10-143                50m (1%)      500m (12%)  100Mi (1%)       4Gi (55%)      4d1h\n  fst-manage                  k8s-etcd-server-paas-192-168-10-143                        50m (1%)      500m (12%)  100Mi (1%)       4Gi (55%)      4d1h\n  fst-manage                  keepalived-paas-192-168-10-143                             50m (1%)      500m (12%)  10Mi (0%)        100Mi (1%)     4d1h\n  fst-manage                  kube-canal-apiserver-paas-192-168-10-143                   50m (1%)      500m (12%)  100Mi (1%)       5Gi (69%)      4d1h\n  fst-manage                  kube-canal-controller-paas-192-168-10-143                  50m (1%)      500m (12%)  100Mi (1%)       500Mi (6%)     4d1h\n  fst-manage                  kube-scheduler-paas-192-168-10-143                         100m (2%)     700m (17%)  200Mi (2%)       6Gi (83%)      4d1h\n  fst-manage                  provision-controller-manager-6cc684677b-cf2dp              50m (1%)      100m (2%)   100Mi (1%)       3Gi (41%)      4d1h\n  fst-manage                  psm-75cb494bd4-5jp8v                                       50m (1%)      500m (12%)  100Mi (1%)       500Mi (6%)     4d1h\n  fst-manage                  swr-api-server-87f5fb947-r7tb5                             100m (2%)     1 (25%)     200Mi (2%)       2548Mi (34%)   4d1h\n  fst-manage                  trm-apiserver-8656dffc46-pdwt5                             50m (1%)      500m (12%)  100Mi (1%)       500Mi (6%)     4d1h\n  fst-manage                  webhook-paas-192-168-10-143                                50m (1%)      250m (6%)   100Mi (1%)       2Gi (27%)      4d1h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-cd7fc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1300m (32%)   11150m (278%)\n  memory             2480Mi (33%)  48220Mi (656%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-1Gi      0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:              <none>\n"
Jul 19 04:06:19.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 describe namespace kubectl-9881'
Jul 19 04:06:19.644: INFO: stderr: ""
Jul 19 04:06:19.644: INFO: stdout: "Name:         kubectl-9881\nLabels:       e2e-framework=kubectl\n              e2e-run=64ffce7d-4de7-4c5c-a3b7-b3f64d800a94\nAnnotations:  namespace.alpha.kubernetes.io/ipstack: ipv4\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:06:19.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9881" for this suite.

• [SLOW TEST:5.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":53,"skipped":847,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:06:19.656: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9715
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-9715
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9715
Jul 19 04:06:19.740: INFO: Found 0 stateful pods, waiting for 1
Jul 19 04:06:29.745: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 19 04:06:29.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 04:06:30.034: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 04:06:30.034: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 04:06:30.034: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 04:06:30.041: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 19 04:06:40.046: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 04:06:40.046: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 04:06:40.063: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:06:40.063: INFO: ss-0  paas-192-168-10-158  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:06:40.063: INFO: 
Jul 19 04:06:40.063: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 19 04:06:41.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995783101s
Jul 19 04:06:42.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990699215s
Jul 19 04:06:43.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985981935s
Jul 19 04:06:44.087: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976423979s
Jul 19 04:06:45.092: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972257344s
Jul 19 04:06:46.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967664534s
Jul 19 04:06:47.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962657103s
Jul 19 04:06:48.106: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957211562s
Jul 19 04:06:49.111: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.250055ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9715
Jul 19 04:06:50.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:06:50.369: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 04:06:50.369: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 04:06:50.369: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 04:06:50.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:06:50.649: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 19 04:06:50.649: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 04:06:50.649: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 04:06:50.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:06:50.886: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 19 04:06:50.886: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 04:06:50.886: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 04:06:50.895: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:06:50.895: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:06:50.895: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 19 04:06:50.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 04:06:51.214: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 04:06:51.215: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 04:06:51.215: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 04:06:51.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 04:06:51.594: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 04:06:51.594: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 04:06:51.594: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 04:06:51.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 04:06:51.850: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 04:06:51.850: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 04:06:51.850: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 04:06:51.850: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 04:06:51.854: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 19 04:07:01.865: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 04:07:01.865: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 04:07:01.865: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 04:07:01.880: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:07:01.880: INFO: ss-0  paas-192-168-10-158  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:07:01.880: INFO: ss-1  paas-192-168-10-183  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:01.880: INFO: ss-2  paas-192-168-10-158  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:01.880: INFO: 
Jul 19 04:07:01.880: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 19 04:07:02.886: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:07:02.886: INFO: ss-0  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:07:02.886: INFO: ss-1  paas-192-168-10-183  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:02.886: INFO: ss-2  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:02.886: INFO: 
Jul 19 04:07:02.886: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 19 04:07:03.892: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:07:03.892: INFO: ss-0  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:07:03.892: INFO: ss-1  paas-192-168-10-183  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:03.892: INFO: ss-2  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:03.892: INFO: 
Jul 19 04:07:03.892: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 19 04:07:04.897: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:07:04.897: INFO: ss-0  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:07:04.897: INFO: ss-2  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:04.897: INFO: 
Jul 19 04:07:04.897: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 19 04:07:05.902: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:07:05.902: INFO: ss-0  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:07:05.902: INFO: ss-2  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:05.902: INFO: 
Jul 19 04:07:05.902: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 19 04:07:06.907: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:07:06.907: INFO: ss-0  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:07:06.907: INFO: ss-2  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:06.907: INFO: 
Jul 19 04:07:06.907: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 19 04:07:07.912: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:07:07.912: INFO: ss-0  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:07:07.912: INFO: ss-2  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:07.912: INFO: 
Jul 19 04:07:07.912: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 19 04:07:08.917: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:07:08.917: INFO: ss-0  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:07:08.917: INFO: ss-2  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:08.917: INFO: 
Jul 19 04:07:08.917: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 19 04:07:09.923: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:07:09.923: INFO: ss-0  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:07:09.923: INFO: ss-2  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:09.924: INFO: 
Jul 19 04:07:09.924: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 19 04:07:10.929: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jul 19 04:07:10.929: INFO: ss-0  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:19 +0000 UTC  }]
Jul 19 04:07:10.929: INFO: ss-2  paas-192-168-10-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  } {ResourceAllocated True 0001-01-01 00:00:00 +0000 UTC 2021-07-19 04:06:40 +0000 UTC  }]
Jul 19 04:07:10.929: INFO: 
Jul 19 04:07:10.929: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9715
Jul 19 04:07:11.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:07:12.472: INFO: rc: 1
Jul 19 04:07:12.472: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jul 19 04:07:22.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:07:22.567: INFO: rc: 1
Jul 19 04:07:22.567: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:07:32.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:07:32.663: INFO: rc: 1
Jul 19 04:07:32.663: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:07:42.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:07:42.757: INFO: rc: 1
Jul 19 04:07:42.757: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:07:52.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:07:52.855: INFO: rc: 1
Jul 19 04:07:52.855: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:08:02.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:08:02.943: INFO: rc: 1
Jul 19 04:08:02.943: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:08:12.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:08:13.032: INFO: rc: 1
Jul 19 04:08:13.032: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:08:23.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:08:23.127: INFO: rc: 1
Jul 19 04:08:23.127: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:08:33.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:08:33.210: INFO: rc: 1
Jul 19 04:08:33.210: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:08:43.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:08:43.390: INFO: rc: 1
Jul 19 04:08:43.390: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:08:53.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:08:53.490: INFO: rc: 1
Jul 19 04:08:53.490: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:09:03.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:09:03.583: INFO: rc: 1
Jul 19 04:09:03.583: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:09:13.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:09:13.672: INFO: rc: 1
Jul 19 04:09:13.672: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:09:23.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:09:23.763: INFO: rc: 1
Jul 19 04:09:23.763: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:09:33.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:09:33.855: INFO: rc: 1
Jul 19 04:09:33.855: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:09:43.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:09:43.941: INFO: rc: 1
Jul 19 04:09:43.941: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:09:53.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:09:54.053: INFO: rc: 1
Jul 19 04:09:54.053: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:10:04.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:10:04.139: INFO: rc: 1
Jul 19 04:10:04.139: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:10:14.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:10:14.238: INFO: rc: 1
Jul 19 04:10:14.238: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:10:24.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:10:24.327: INFO: rc: 1
Jul 19 04:10:24.327: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:10:34.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:10:34.415: INFO: rc: 1
Jul 19 04:10:34.415: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:10:44.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:10:44.505: INFO: rc: 1
Jul 19 04:10:44.505: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:10:54.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:10:54.610: INFO: rc: 1
Jul 19 04:10:54.610: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:11:04.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:11:04.707: INFO: rc: 1
Jul 19 04:11:04.707: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:11:14.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:11:14.801: INFO: rc: 1
Jul 19 04:11:14.801: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:11:24.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:11:24.926: INFO: rc: 1
Jul 19 04:11:24.926: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:11:34.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:11:35.021: INFO: rc: 1
Jul 19 04:11:35.021: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:11:45.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:11:45.115: INFO: rc: 1
Jul 19 04:11:45.115: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:11:55.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:11:55.211: INFO: rc: 1
Jul 19 04:11:55.211: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:12:05.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:12:05.304: INFO: rc: 1
Jul 19 04:12:05.304: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 19 04:12:15.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-9715 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:12:15.401: INFO: rc: 1
Jul 19 04:12:15.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Jul 19 04:12:15.401: INFO: Scaling statefulset ss to 0
Jul 19 04:12:15.480: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 19 04:12:15.485: INFO: Deleting all statefulset in ns statefulset-9715
Jul 19 04:12:15.489: INFO: Scaling statefulset ss to 0
Jul 19 04:12:15.514: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 04:12:15.519: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:12:15.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9715" for this suite.

• [SLOW TEST:355.900 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":54,"skipped":868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:12:15.557: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 19 04:12:23.691: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:23.698: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:25.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:25.703: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:27.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:27.715: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:29.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:29.703: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:31.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:31.702: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:33.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:33.702: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:35.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:35.703: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:37.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:37.702: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:39.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:39.703: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:41.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:41.703: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:43.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:43.706: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 04:12:45.698: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 04:12:45.703: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:12:45.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2657" for this suite.

• [SLOW TEST:30.203 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":55,"skipped":919,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:12:45.760: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-ef7937e5-0e24-4b22-846d-fb0ab2ec0ad8
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:12:45.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-796" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":56,"skipped":931,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:12:45.853: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4800
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jul 19 04:12:46.129: INFO: Found 0 stateful pods, waiting for 3
Jul 19 04:12:56.135: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:12:56.135: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:12:56.135: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 19 04:13:06.139: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:13:06.139: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:13:06.139: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:13:06.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-4800 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 04:13:06.414: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 04:13:06.414: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 04:13:06.414: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 19 04:13:16.462: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 19 04:13:26.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-4800 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:13:26.744: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 04:13:26.744: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 04:13:26.744: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 04:13:36.768: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
Jul 19 04:13:36.768: INFO: Waiting for Pod statefulset-4800/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:13:36.768: INFO: Waiting for Pod statefulset-4800/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:13:36.768: INFO: Waiting for Pod statefulset-4800/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:13:46.776: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
Jul 19 04:13:46.776: INFO: Waiting for Pod statefulset-4800/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:13:46.776: INFO: Waiting for Pod statefulset-4800/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:13:56.779: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
Jul 19 04:13:56.779: INFO: Waiting for Pod statefulset-4800/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:14:06.780: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
Jul 19 04:14:06.780: INFO: Waiting for Pod statefulset-4800/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 19 04:14:16.777: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
STEP: Rolling back to a previous revision
Jul 19 04:14:26.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-4800 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 04:14:27.015: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 04:14:27.015: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 04:14:27.015: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 04:14:27.054: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 19 04:14:37.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-4800 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:14:37.331: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 04:14:37.331: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 04:14:37.331: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 04:14:47.374: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
Jul 19 04:14:47.374: INFO: Waiting for Pod statefulset-4800/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:14:47.374: INFO: Waiting for Pod statefulset-4800/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:14:47.374: INFO: Waiting for Pod statefulset-4800/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:14:57.382: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
Jul 19 04:14:57.382: INFO: Waiting for Pod statefulset-4800/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:14:57.382: INFO: Waiting for Pod statefulset-4800/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:14:57.382: INFO: Waiting for Pod statefulset-4800/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:15:07.387: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
Jul 19 04:15:07.387: INFO: Waiting for Pod statefulset-4800/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:15:07.387: INFO: Waiting for Pod statefulset-4800/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:15:17.382: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
Jul 19 04:15:17.382: INFO: Waiting for Pod statefulset-4800/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:15:17.382: INFO: Waiting for Pod statefulset-4800/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:15:27.383: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
Jul 19 04:15:27.383: INFO: Waiting for Pod statefulset-4800/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:15:37.383: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
Jul 19 04:15:37.383: INFO: Waiting for Pod statefulset-4800/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 19 04:15:47.381: INFO: Waiting for StatefulSet statefulset-4800/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 19 04:15:57.384: INFO: Deleting all statefulset in ns statefulset-4800
Jul 19 04:15:57.387: INFO: Scaling statefulset ss2 to 0
Jul 19 04:16:57.411: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 04:16:57.414: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:16:57.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4800" for this suite.

• [SLOW TEST:251.637 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":57,"skipped":945,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:16:57.491: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-6ef124ff-d2be-43e8-b830-359ba63a2ad6
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-6ef124ff-d2be-43e8-b830-359ba63a2ad6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:17:03.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4885" for this suite.

• [SLOW TEST:6.254 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":58,"skipped":1002,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:17:03.745: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:17:04.453: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 04:17:06.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265024, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265024, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265024, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265024, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 04:17:08.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265024, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265024, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265024, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265024, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:17:11.483: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:17:11.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-413" for this suite.
STEP: Destroying namespace "webhook-413-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.928 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":59,"skipped":1023,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:17:11.673: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:19:11.756: INFO: Deleting pod "var-expansion-a4c2f1c1-4cab-44b6-8cdc-8b60e82e343b" in namespace "var-expansion-4917"
Jul 19 04:19:11.765: INFO: Wait up to 5m0s for pod "var-expansion-a4c2f1c1-4cab-44b6-8cdc-8b60e82e343b" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:19:39.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4917" for this suite.

• [SLOW TEST:148.111 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":60,"skipped":1036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:19:39.786: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3947
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3947
STEP: creating replication controller externalsvc in namespace services-3947
I0719 04:19:39.911331      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3947, replica count: 2
I0719 04:19:42.961665      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 04:19:45.961841      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul 19 04:19:45.994: INFO: Creating new exec pod
Jul 19 04:19:50.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-3947 execpodtqwkp -- /bin/sh -x -c nslookup clusterip-service.services-3947.svc.cluster.local'
Jul 19 04:19:50.761: INFO: stderr: "+ nslookup clusterip-service.services-3947.svc.cluster.local\n"
Jul 19 04:19:50.761: INFO: stdout: "Server:\t\t10.247.0.20\nAddress:\t10.247.0.20#53\n\nclusterip-service.services-3947.svc.cluster.local\tcanonical name = externalsvc.services-3947.svc.cluster.local.\nName:\texternalsvc.services-3947.svc.cluster.local\nAddress: 10.247.205.70\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3947, will wait for the garbage collector to delete the pods
Jul 19 04:19:50.824: INFO: Deleting ReplicationController externalsvc took: 10.271373ms
Jul 19 04:19:50.925: INFO: Terminating ReplicationController externalsvc pods took: 100.193556ms
Jul 19 04:20:09.549: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:20:09.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3947" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:29.798 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":61,"skipped":1058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:20:09.584: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:20:21.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1177" for this suite.

• [SLOW TEST:12.094 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":62,"skipped":1124,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:20:21.678: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 19 04:20:21.733: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:20:28.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9003" for this suite.

• [SLOW TEST:7.170 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":63,"skipped":1145,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:20:28.848: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:20:32.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8356" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":1148,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:20:32.991: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:20:33.734: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 04:20:35.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265233, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265233, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265233, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265233, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:20:38.771: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:20:38.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6320" for this suite.
STEP: Destroying namespace "webhook-6320-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.004 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":65,"skipped":1165,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:20:38.996: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 19 04:20:39.106: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1603 /api/v1/namespaces/watch-1603/configmaps/e2e-watch-test-resource-version e175dc10-da18-4809-9d5f-31f7b85e265d 2408309 0 2021-07-19 04:20:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 04:20:39.111: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1603 /api/v1/namespaces/watch-1603/configmaps/e2e-watch-test-resource-version e175dc10-da18-4809-9d5f-31f7b85e265d 2408310 0 2021-07-19 04:20:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:20:39.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1603" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":66,"skipped":1170,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:20:39.123: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:20:39.923: INFO: Checking APIGroup: apiregistration.k8s.io
Jul 19 04:20:39.924: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul 19 04:20:39.924: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.924: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul 19 04:20:39.924: INFO: Checking APIGroup: extensions
Jul 19 04:20:39.925: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jul 19 04:20:39.925: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jul 19 04:20:39.925: INFO: extensions/v1beta1 matches extensions/v1beta1
Jul 19 04:20:39.925: INFO: Checking APIGroup: apps
Jul 19 04:20:39.926: INFO: PreferredVersion.GroupVersion: apps/v1
Jul 19 04:20:39.926: INFO: Versions found [{apps/v1 v1}]
Jul 19 04:20:39.926: INFO: apps/v1 matches apps/v1
Jul 19 04:20:39.926: INFO: Checking APIGroup: events.k8s.io
Jul 19 04:20:39.926: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul 19 04:20:39.926: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.927: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul 19 04:20:39.927: INFO: Checking APIGroup: authentication.k8s.io
Jul 19 04:20:39.927: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul 19 04:20:39.927: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.927: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul 19 04:20:39.927: INFO: Checking APIGroup: authorization.k8s.io
Jul 19 04:20:39.928: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul 19 04:20:39.928: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.928: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul 19 04:20:39.928: INFO: Checking APIGroup: autoscaling
Jul 19 04:20:39.929: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jul 19 04:20:39.929: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jul 19 04:20:39.929: INFO: autoscaling/v1 matches autoscaling/v1
Jul 19 04:20:39.929: INFO: Checking APIGroup: batch
Jul 19 04:20:39.930: INFO: PreferredVersion.GroupVersion: batch/v1
Jul 19 04:20:39.930: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1} {batch/v2alpha1 v2alpha1}]
Jul 19 04:20:39.930: INFO: batch/v1 matches batch/v1
Jul 19 04:20:39.930: INFO: Checking APIGroup: certificates.k8s.io
Jul 19 04:20:39.931: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul 19 04:20:39.931: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.931: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul 19 04:20:39.931: INFO: Checking APIGroup: networking.k8s.io
Jul 19 04:20:39.932: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul 19 04:20:39.932: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.932: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul 19 04:20:39.932: INFO: Checking APIGroup: policy
Jul 19 04:20:39.932: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jul 19 04:20:39.932: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jul 19 04:20:39.932: INFO: policy/v1beta1 matches policy/v1beta1
Jul 19 04:20:39.932: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul 19 04:20:39.933: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul 19 04:20:39.933: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.933: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul 19 04:20:39.933: INFO: Checking APIGroup: storage.k8s.io
Jul 19 04:20:39.934: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul 19 04:20:39.934: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.934: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul 19 04:20:39.934: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul 19 04:20:39.935: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul 19 04:20:39.935: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.935: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul 19 04:20:39.935: INFO: Checking APIGroup: apiextensions.k8s.io
Jul 19 04:20:39.936: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul 19 04:20:39.936: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.936: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul 19 04:20:39.936: INFO: Checking APIGroup: scheduling.k8s.io
Jul 19 04:20:39.937: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul 19 04:20:39.937: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.937: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul 19 04:20:39.937: INFO: Checking APIGroup: coordination.k8s.io
Jul 19 04:20:39.937: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul 19 04:20:39.937: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.937: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul 19 04:20:39.938: INFO: Checking APIGroup: node.k8s.io
Jul 19 04:20:39.938: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Jul 19 04:20:39.938: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.938: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Jul 19 04:20:39.938: INFO: Checking APIGroup: discovery.k8s.io
Jul 19 04:20:39.939: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jul 19 04:20:39.939: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jul 19 04:20:39.939: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jul 19 04:20:39.939: INFO: Checking APIGroup: k8s.cni.cncf.io
Jul 19 04:20:39.940: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jul 19 04:20:39.940: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jul 19 04:20:39.940: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jul 19 04:20:39.940: INFO: Checking APIGroup: paas
Jul 19 04:20:39.941: INFO: PreferredVersion.GroupVersion: paas/v1alpha1
Jul 19 04:20:39.941: INFO: Versions found [{paas/v1alpha1 v1alpha1}]
Jul 19 04:20:39.941: INFO: paas/v1alpha1 matches paas/v1alpha1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:20:39.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1431" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":67,"skipped":1180,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:20:39.953: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:20:40.011: INFO: Creating deployment "test-recreate-deployment"
Jul 19 04:20:40.016: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 19 04:20:40.043: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 19 04:20:42.051: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 19 04:20:42.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265240, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265240, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265240, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265240, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-66f594cbb7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 04:20:44.058: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 19 04:20:44.112: INFO: Updating deployment test-recreate-deployment
Jul 19 04:20:44.112: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 19 04:20:44.379: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7291 /apis/apps/v1/namespaces/deployment-7291/deployments/test-recreate-deployment b570eea4-8214-4b52-bf58-e2b552fb4930 2408393 2 2021-07-19 04:20:40 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003778c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*5,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-19 04:20:44 +0000 UTC,LastTransitionTime:2021-07-19 04:20:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-957fbd757" is progressing.,LastUpdateTime:2021-07-19 04:20:44 +0000 UTC,LastTransitionTime:2021-07-19 04:20:40 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 19 04:20:44.386: INFO: New ReplicaSet "test-recreate-deployment-957fbd757" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-957fbd757  deployment-7291 /apis/apps/v1/namespaces/deployment-7291/replicasets/test-recreate-deployment-957fbd757 0c5261df-ec1e-4b62-bb17-0b8fe21684db 2408391 1 2021-07-19 04:20:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:957fbd757] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/internalStrategyType:Recreate deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b570eea4-8214-4b52-bf58-e2b552fb4930 0xc00377952f 0xc003779540}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 957fbd757,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:957fbd757] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037795a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 04:20:44.386: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 19 04:20:44.386: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-66f594cbb7  deployment-7291 /apis/apps/v1/namespaces/deployment-7291/replicasets/test-recreate-deployment-66f594cbb7 25f403de-cfae-4530-94dc-0770dcf40494 2408383 2 2021-07-19 04:20:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:66f594cbb7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/internalStrategyType:Recreate deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b570eea4-8214-4b52-bf58-e2b552fb4930 0xc00377930f 0xc003779320}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 66f594cbb7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:66f594cbb7] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037794c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 04:20:44.414: INFO: Pod "test-recreate-deployment-957fbd757-9rgdh" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-957fbd757-9rgdh test-recreate-deployment-957fbd757- deployment-7291 /api/v1/namespaces/deployment-7291/pods/test-recreate-deployment-957fbd757-9rgdh 897b0aca-7866-435f-951a-bdee285fbfe4 2408394 0 2021-07-19 04:20:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:957fbd757] map[] [{apps/v1 ReplicaSet test-recreate-deployment-957fbd757 0c5261df-ec1e-4b62-bb17-0b8fe21684db 0xc003779a90 0xc003779a91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-z7fx2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-z7fx2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-z7fx2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:20:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:20:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:20:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:20:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:20:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:,StartTime:2021-07-19 04:20:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:20:44.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7291" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":68,"skipped":1214,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:20:44.453: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Jul 19 04:20:44.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-2426 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 19 04:20:44.682: INFO: stderr: ""
Jul 19 04:20:44.682: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Jul 19 04:20:44.682: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 19 04:20:44.682: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2426" to be "running and ready, or succeeded"
Jul 19 04:20:44.687: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.638597ms
Jul 19 04:20:46.692: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00930978s
Jul 19 04:20:48.696: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.013559042s
Jul 19 04:20:48.696: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 19 04:20:48.696: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul 19 04:20:48.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 logs logs-generator logs-generator --namespace=kubectl-2426'
Jul 19 04:20:48.808: INFO: stderr: ""
Jul 19 04:20:48.808: INFO: stdout: "I0719 04:20:47.300303       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/2nj 479\nI0719 04:20:47.500427       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/jz2 337\nI0719 04:20:47.700437       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/mlbz 489\nI0719 04:20:47.900416       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/2j99 350\nI0719 04:20:48.100421       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/rjhn 234\nI0719 04:20:48.300390       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/bh6 329\nI0719 04:20:48.500427       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/nnxb 541\nI0719 04:20:48.700413       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/xv5 210\n"
STEP: limiting log lines
Jul 19 04:20:48.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 logs logs-generator logs-generator --namespace=kubectl-2426 --tail=1'
Jul 19 04:20:48.910: INFO: stderr: ""
Jul 19 04:20:48.910: INFO: stdout: "I0719 04:20:48.900422       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/vnvk 337\n"
Jul 19 04:20:48.910: INFO: got output "I0719 04:20:48.900422       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/vnvk 337\n"
STEP: limiting log bytes
Jul 19 04:20:48.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 logs logs-generator logs-generator --namespace=kubectl-2426 --limit-bytes=1'
Jul 19 04:20:49.021: INFO: stderr: ""
Jul 19 04:20:49.021: INFO: stdout: "I"
Jul 19 04:20:49.021: INFO: got output "I"
STEP: exposing timestamps
Jul 19 04:20:49.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 logs logs-generator logs-generator --namespace=kubectl-2426 --tail=1 --timestamps'
Jul 19 04:20:49.131: INFO: stderr: ""
Jul 19 04:20:49.131: INFO: stdout: "2021-07-19T04:20:49.100538681Z I0719 04:20:49.100432       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/c24 260\n"
Jul 19 04:20:49.131: INFO: got output "2021-07-19T04:20:49.100538681Z I0719 04:20:49.100432       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/c24 260\n"
STEP: restricting to a time range
Jul 19 04:20:51.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 logs logs-generator logs-generator --namespace=kubectl-2426 --since=1s'
Jul 19 04:20:51.730: INFO: stderr: ""
Jul 19 04:20:51.730: INFO: stdout: "I0719 04:20:50.900436       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/csm 290\nI0719 04:20:51.100423       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/d269 276\nI0719 04:20:51.300440       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/gz6 345\nI0719 04:20:51.500481       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/m9x 327\nI0719 04:20:51.700342       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/25st 571\n"
Jul 19 04:20:51.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 logs logs-generator logs-generator --namespace=kubectl-2426 --since=24h'
Jul 19 04:20:51.841: INFO: stderr: ""
Jul 19 04:20:51.841: INFO: stdout: "I0719 04:20:47.300303       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/2nj 479\nI0719 04:20:47.500427       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/jz2 337\nI0719 04:20:47.700437       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/mlbz 489\nI0719 04:20:47.900416       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/2j99 350\nI0719 04:20:48.100421       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/rjhn 234\nI0719 04:20:48.300390       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/bh6 329\nI0719 04:20:48.500427       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/nnxb 541\nI0719 04:20:48.700413       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/xv5 210\nI0719 04:20:48.900422       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/vnvk 337\nI0719 04:20:49.100432       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/c24 260\nI0719 04:20:49.300444       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/vbl8 507\nI0719 04:20:49.500432       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/rvr 436\nI0719 04:20:49.700384       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/gft 497\nI0719 04:20:49.900441       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/hj99 302\nI0719 04:20:50.100424       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/fq6 411\nI0719 04:20:50.300443       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/65mf 351\nI0719 04:20:50.500423       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/nfmm 543\nI0719 04:20:50.700432       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/jrc 270\nI0719 04:20:50.900436       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/csm 290\nI0719 04:20:51.100423       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/d269 276\nI0719 04:20:51.300440       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/gz6 345\nI0719 04:20:51.500481       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/m9x 327\nI0719 04:20:51.700342       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/25st 571\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Jul 19 04:20:51.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete pod logs-generator --namespace=kubectl-2426'
Jul 19 04:21:09.492: INFO: stderr: ""
Jul 19 04:21:09.492: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:21:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2426" for this suite.

• [SLOW TEST:25.055 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":69,"skipped":1223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:21:09.508: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-23d27c92-a9e0-425d-bece-6ed423384199
STEP: Creating a pod to test consume configMaps
Jul 19 04:21:09.581: INFO: Waiting up to 5m0s for pod "pod-configmaps-15314b5e-4396-4609-9175-282d84cb87ad" in namespace "configmap-8449" to be "Succeeded or Failed"
Jul 19 04:21:09.584: INFO: Pod "pod-configmaps-15314b5e-4396-4609-9175-282d84cb87ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.030798ms
Jul 19 04:21:11.589: INFO: Pod "pod-configmaps-15314b5e-4396-4609-9175-282d84cb87ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007508648s
Jul 19 04:21:13.593: INFO: Pod "pod-configmaps-15314b5e-4396-4609-9175-282d84cb87ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012000435s
STEP: Saw pod success
Jul 19 04:21:13.593: INFO: Pod "pod-configmaps-15314b5e-4396-4609-9175-282d84cb87ad" satisfied condition "Succeeded or Failed"
Jul 19 04:21:13.596: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-configmaps-15314b5e-4396-4609-9175-282d84cb87ad container configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 04:21:13.626: INFO: Waiting for pod pod-configmaps-15314b5e-4396-4609-9175-282d84cb87ad to disappear
Jul 19 04:21:13.629: INFO: Pod pod-configmaps-15314b5e-4396-4609-9175-282d84cb87ad no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:21:13.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8449" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":70,"skipped":1256,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:21:13.650: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 19 04:21:13.757: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:13.758: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:13.758: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:13.758: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:13.758: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:13.761: INFO: Number of nodes with available pods: 0
Jul 19 04:21:13.761: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:21:14.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:14.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:14.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:14.767: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:14.767: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:14.771: INFO: Number of nodes with available pods: 0
Jul 19 04:21:14.771: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:21:15.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:15.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:15.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:15.767: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:15.767: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:15.771: INFO: Number of nodes with available pods: 0
Jul 19 04:21:15.771: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:21:16.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:16.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:16.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:16.767: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:16.767: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:16.770: INFO: Number of nodes with available pods: 0
Jul 19 04:21:16.770: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:21:17.766: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:17.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:17.767: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:17.767: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:17.767: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:17.770: INFO: Number of nodes with available pods: 2
Jul 19 04:21:17.770: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 19 04:21:17.796: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:17.796: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:17.797: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:17.797: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:17.797: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:17.810: INFO: Number of nodes with available pods: 1
Jul 19 04:21:17.810: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:21:18.821: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:18.821: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:18.821: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:18.821: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:18.821: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:18.825: INFO: Number of nodes with available pods: 1
Jul 19 04:21:18.825: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:21:19.816: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:19.816: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:19.816: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:19.816: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:19.816: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:19.820: INFO: Number of nodes with available pods: 1
Jul 19 04:21:19.820: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:21:20.819: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:20.819: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:20.819: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:20.819: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:20.819: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:20.825: INFO: Number of nodes with available pods: 1
Jul 19 04:21:20.825: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:21:21.815: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:21.815: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:21.815: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:21.815: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:21.815: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:21.819: INFO: Number of nodes with available pods: 1
Jul 19 04:21:21.819: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:21:22.816: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:22.816: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:22.816: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:22.816: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:22.816: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:21:22.819: INFO: Number of nodes with available pods: 2
Jul 19 04:21:22.819: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5303, will wait for the garbage collector to delete the pods
Jul 19 04:21:22.891: INFO: Deleting DaemonSet.extensions daemon-set took: 9.796983ms
Jul 19 04:21:22.992: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.292989ms
Jul 19 04:21:44.399: INFO: Number of nodes with available pods: 0
Jul 19 04:21:44.399: INFO: Number of running nodes: 0, number of available pods: 0
Jul 19 04:21:44.404: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5303/daemonsets","resourceVersion":"2408899"},"items":null}

Jul 19 04:21:44.410: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5303/pods","resourceVersion":"2408899"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:21:44.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5303" for this suite.

• [SLOW TEST:30.790 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":71,"skipped":1278,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:21:44.440: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:21:44.926: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 19 04:21:46.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265304, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265304, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265304, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265304, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:21:49.968: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul 19 04:21:50.006: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:21:50.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-635" for this suite.
STEP: Destroying namespace "webhook-635-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.721 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":72,"skipped":1290,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:21:50.161: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 19 04:21:50.278: INFO: PodSpec: initContainers in spec.initContainers
Jul 19 04:22:46.065: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c7482bd1-e30f-4832-8285-9217643f47b5", GenerateName:"", Namespace:"init-container-3368", SelfLink:"/api/v1/namespaces/init-container-3368/pods/pod-init-c7482bd1-e30f-4832-8285-9217643f47b5", UID:"8697a7bf-3575-45d3-979e-dea805c9becc", ResourceVersion:"2409381", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63762265310, loc:(*time.Location)(0x77108c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"278227820"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-wxbj4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002c5e180), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wxbj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wxbj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBE_POD_NODE_NAME", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc001699e60)}, v1.EnvVar{Name:"KUBE_POD_NAMESPACE", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc001699ec0)}, v1.EnvVar{Name:"KUBE_POD_CLUSTERID", Value:"", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wxbj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0030b8c88), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"paas-192-168-10-158", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0029b89a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0030b8d00)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0030b8d20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0030b8d28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0030b8d2c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002559a70), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265310, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265310, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265310, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265310, loc:(*time.Location)(0x77108c0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ResourceAllocated", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265310, loc:(*time.Location)(0x77108c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.10.158", PodIP:"172.16.0.31", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.0.31"}}, StartTime:(*v1.Time)(0xc0038e2000), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0029b8a80)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0029b8af0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker://sha256:758ec7f3a1ee85f8f08399b55641bfb13e8c1109287ddc5e22b68c3d653152ee", ContainerID:"docker://44919cb8aa95af0dea1ac2688aff34caef4d223c955ad161f1b1a7371dd9c4e9", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038e2040), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038e2020), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0030b8da4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:22:46.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3368" for this suite.

• [SLOW TEST:55.916 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":73,"skipped":1305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:22:46.078: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jul 19 04:22:46.164: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:22:46.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5571" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":74,"skipped":1328,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:22:46.204: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 19 04:22:46.269: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 19 04:22:46.393: INFO: Waiting for terminating namespaces to be deleted...
Jul 19 04:22:46.397: INFO: 
Logging pods the apiserver thinks is on node paas-192-168-10-158 before test
Jul 19 04:22:46.416: INFO: cse-config-center-9cb5789db-hqgs8 from fst-manage started at 2021-07-15 02:47:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container cse-cc-comp ready: true, restart count 0
Jul 19 04:22:46.417: INFO: cse-etcd-0 from fst-manage started at 2021-07-15 02:45:36 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container cse-etcd-comp ready: true, restart count 0
Jul 19 04:22:46.417: INFO: cse-service-center-78c89955f8-9zxb8 from fst-manage started at 2021-07-15 02:47:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container cse-sc-comp ready: true, restart count 0
Jul 19 04:22:46.417: INFO: cspagentmanager-84df498dd6-756cg from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container cspagentmanager-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: csphaservice-85cfb95df6-rg2zf from fst-manage started at 2021-07-15 02:45:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container csphaservice-soft ready: true, restart count 0
Jul 19 04:22:46.417: INFO: cspnrsmaster-0 from fst-manage started at 2021-07-15 02:45:49 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container cspnrsmaster-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: cspommgr-7b4c7c76dc-lg5xn from fst-manage started at 2021-07-15 02:45:42 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container cspommgr-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: csposmgr-666d77c8cf-5nztr from fst-manage started at 2021-07-15 02:45:52 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container csposmgr-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: deploymgr-5fc4dfbc87-skqjv from fst-manage started at 2021-07-15 02:45:39 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container deploymgr-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: febs-599995cbb8-wch4b from fst-manage started at 2021-07-15 02:46:04 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container febs-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: fileserver-0 from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container fileserver-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: gaussdb-da5b9bc8-0 from fst-manage started at 2021-07-15 02:46:01 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container gauss-softwarecomp ready: true, restart count 0
Jul 19 04:22:46.417: INFO: kafka-da5b9bc8-0 from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container kafka-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: kubeaddon-upload-package-f5vtngqpi8-4m5dx from fst-manage started at 2021-07-15 02:44:30 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container kubeaddon-upload-package ready: false, restart count 0
Jul 19 04:22:46.417: INFO: mtcenter-57c48c94fd-89d27 from fst-manage started at 2021-07-15 02:45:35 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container cspmtcenter-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: omcache-da5b9bc8-0 from fst-manage started at 2021-07-15 02:45:56 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container redis-softwarecomp ready: true, restart count 0
Jul 19 04:22:46.417: INFO: omlb-5d58b5f84f-w9p5s from fst-manage started at 2021-07-15 03:35:19 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container omlb-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: pmscalc-5744fc9c5d-x6rlp from fst-manage started at 2021-07-15 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container pmscalc-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: pmsmgr-6857b85787-q6n5l from fst-manage started at 2021-07-15 02:45:37 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container pmsmgr-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: runlog-69f7c599d-krpbr from fst-manage started at 2021-07-15 02:45:58 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container runlog-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: secmgr-65cfb48b8b-2fzvs from fst-manage started at 2021-07-15 02:45:57 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container secmgr-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: soap-76794ccdc-xtknz from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container soap-soft ready: true, restart count 0
Jul 19 04:22:46.417: INFO: upgtool-app-559557759c-jpc94 from fst-manage started at 2021-07-15 02:45:54 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container upgtool-component ready: true, restart count 0
Jul 19 04:22:46.417: INFO: zookeeper-0 from fst-manage started at 2021-07-15 02:45:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container zookeeper-software ready: true, restart count 0
Jul 19 04:22:46.417: INFO: pod-init-c7482bd1-e30f-4832-8285-9217643f47b5 from init-container-3368 started at 2021-07-19 04:21:50 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container run1 ready: false, restart count 0
Jul 19 04:22:46.417: INFO: sonobuoy from sonobuoy started at 2021-07-19 03:55:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 19 04:22:46.417: INFO: sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-p7chc from sonobuoy started at 2021-07-19 03:55:51 +0000 UTC (2 container statuses recorded)
Jul 19 04:22:46.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 04:22:46.417: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 19 04:22:46.417: INFO: 
Logging pods the apiserver thinks is on node paas-192-168-10-183 before test
Jul 19 04:22:46.442: INFO: cse-config-center-9cb5789db-vhnp6 from fst-manage started at 2021-07-15 02:47:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container cse-cc-comp ready: true, restart count 0
Jul 19 04:22:46.442: INFO: cse-etcd-1 from fst-manage started at 2021-07-15 02:45:36 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container cse-etcd-comp ready: true, restart count 0
Jul 19 04:22:46.442: INFO: cse-service-center-78c89955f8-4kfjs from fst-manage started at 2021-07-15 02:47:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container cse-sc-comp ready: true, restart count 0
Jul 19 04:22:46.442: INFO: cspagentmanager-84df498dd6-fbzqf from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container cspagentmanager-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: csphaservice-85cfb95df6-qch6t from fst-manage started at 2021-07-15 02:45:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container csphaservice-soft ready: true, restart count 0
Jul 19 04:22:46.442: INFO: cspnrsmaster-1 from fst-manage started at 2021-07-15 02:45:49 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container cspnrsmaster-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: cspommgr-7b4c7c76dc-74p5c from fst-manage started at 2021-07-15 02:45:42 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container cspommgr-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: csposmgr-666d77c8cf-xqphh from fst-manage started at 2021-07-15 02:45:53 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container csposmgr-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: deploymgr-5fc4dfbc87-d4hmv from fst-manage started at 2021-07-15 02:45:39 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container deploymgr-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: febs-599995cbb8-qsp8z from fst-manage started at 2021-07-15 02:46:04 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container febs-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: fileserver-1 from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container fileserver-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: gaussdb-da5b9bc8-1 from fst-manage started at 2021-07-15 02:46:02 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container gauss-softwarecomp ready: true, restart count 0
Jul 19 04:22:46.442: INFO: kafka-da5b9bc8-1 from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container kafka-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: mtcenter-57c48c94fd-f8h98 from fst-manage started at 2021-07-15 02:45:35 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container cspmtcenter-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: omcache-da5b9bc8-1 from fst-manage started at 2021-07-15 02:45:56 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container redis-softwarecomp ready: true, restart count 0
Jul 19 04:22:46.442: INFO: omlb-5d58b5f84f-84kpc from fst-manage started at 2021-07-15 03:24:27 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container omlb-software ready: true, restart count 2
Jul 19 04:22:46.442: INFO: pmscalc-5744fc9c5d-h54gd from fst-manage started at 2021-07-15 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container pmscalc-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: pmsmgr-6857b85787-8n4th from fst-manage started at 2021-07-15 02:45:37 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container pmsmgr-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: psm-update-69dd9dc9cb-vddnv from fst-manage started at 2021-07-15 02:44:31 +0000 UTC (0 container statuses recorded)
Jul 19 04:22:46.442: INFO: runlog-69f7c599d-mszld from fst-manage started at 2021-07-15 02:45:59 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container runlog-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: secmgr-65cfb48b8b-zz9t8 from fst-manage started at 2021-07-15 02:45:57 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container secmgr-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: soap-76794ccdc-brpbn from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container soap-soft ready: true, restart count 0
Jul 19 04:22:46.442: INFO: upgtool-app-559557759c-ssmzk from fst-manage started at 2021-07-15 02:45:54 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container upgtool-component ready: true, restart count 0
Jul 19 04:22:46.442: INFO: zookeeper-1 from fst-manage started at 2021-07-15 02:45:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container zookeeper-software ready: true, restart count 0
Jul 19 04:22:46.442: INFO: sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-gtdqw from sonobuoy started at 2021-07-19 03:55:51 +0000 UTC (2 container statuses recorded)
Jul 19 04:22:46.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 04:22:46.442: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3608c946-147d-4750-8e5e-adc8b6be3113 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-3608c946-147d-4750-8e5e-adc8b6be3113 off the node paas-192-168-10-183
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3608c946-147d-4750-8e5e-adc8b6be3113
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:27:56.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6037" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:310.389 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":75,"skipped":1336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:27:56.593: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 19 04:27:56.681: INFO: Waiting up to 5m0s for pod "pod-8595c5d7-bf53-4610-ac14-50dd50917905" in namespace "emptydir-4912" to be "Succeeded or Failed"
Jul 19 04:27:56.686: INFO: Pod "pod-8595c5d7-bf53-4610-ac14-50dd50917905": Phase="Pending", Reason="", readiness=false. Elapsed: 4.824863ms
Jul 19 04:27:58.693: INFO: Pod "pod-8595c5d7-bf53-4610-ac14-50dd50917905": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011982181s
Jul 19 04:28:00.696: INFO: Pod "pod-8595c5d7-bf53-4610-ac14-50dd50917905": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015245395s
STEP: Saw pod success
Jul 19 04:28:00.696: INFO: Pod "pod-8595c5d7-bf53-4610-ac14-50dd50917905" satisfied condition "Succeeded or Failed"
Jul 19 04:28:00.699: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-8595c5d7-bf53-4610-ac14-50dd50917905 container test-container: <nil>
STEP: delete the pod
Jul 19 04:28:00.762: INFO: Waiting for pod pod-8595c5d7-bf53-4610-ac14-50dd50917905 to disappear
Jul 19 04:28:00.765: INFO: Pod pod-8595c5d7-bf53-4610-ac14-50dd50917905 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:28:00.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4912" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":1364,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:28:00.777: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jul 19 04:28:00.874: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 04:29:00.979: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:29:00.984: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jul 19 04:29:05.168: INFO: found a healthy node: paas-192-168-10-158
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:29:17.252: INFO: pods created so far: [1 1 1]
Jul 19 04:29:17.252: INFO: length of pods created so far: 3
Jul 19 04:29:35.264: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:29:42.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3445" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:29:42.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9446" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:101.683 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":77,"skipped":1381,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:29:42.461: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jul 19 04:29:42.541: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 04:30:42.644: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jul 19 04:30:42.675: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 19 04:30:42.703: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:31:24.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8242" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:102.334 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":78,"skipped":1392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:31:24.795: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-17d2eaec-7223-494a-a17e-3424ee40307e
STEP: Creating a pod to test consume configMaps
Jul 19 04:31:24.874: INFO: Waiting up to 5m0s for pod "pod-configmaps-1492e066-ba4a-4bae-8f1b-ef45d8a7e90c" in namespace "configmap-3965" to be "Succeeded or Failed"
Jul 19 04:31:24.877: INFO: Pod "pod-configmaps-1492e066-ba4a-4bae-8f1b-ef45d8a7e90c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.71753ms
Jul 19 04:31:26.881: INFO: Pod "pod-configmaps-1492e066-ba4a-4bae-8f1b-ef45d8a7e90c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007793734s
Jul 19 04:31:28.886: INFO: Pod "pod-configmaps-1492e066-ba4a-4bae-8f1b-ef45d8a7e90c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012311749s
STEP: Saw pod success
Jul 19 04:31:28.886: INFO: Pod "pod-configmaps-1492e066-ba4a-4bae-8f1b-ef45d8a7e90c" satisfied condition "Succeeded or Failed"
Jul 19 04:31:28.889: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-configmaps-1492e066-ba4a-4bae-8f1b-ef45d8a7e90c container configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 04:31:28.951: INFO: Waiting for pod pod-configmaps-1492e066-ba4a-4bae-8f1b-ef45d8a7e90c to disappear
Jul 19 04:31:28.954: INFO: Pod pod-configmaps-1492e066-ba4a-4bae-8f1b-ef45d8a7e90c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:31:28.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3965" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":1414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:31:28.972: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 19 04:31:29.055: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:29.055: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:29.055: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:29.055: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:29.055: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:29.059: INFO: Number of nodes with available pods: 0
Jul 19 04:31:29.059: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:30.065: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:30.065: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:30.065: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:30.065: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:30.065: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:30.072: INFO: Number of nodes with available pods: 0
Jul 19 04:31:30.072: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:31.065: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:31.065: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:31.065: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:31.065: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:31.065: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:31.068: INFO: Number of nodes with available pods: 0
Jul 19 04:31:31.068: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:32.065: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:32.065: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:32.065: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:32.065: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:32.065: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:32.068: INFO: Number of nodes with available pods: 0
Jul 19 04:31:32.068: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:33.066: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:33.067: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:33.067: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:33.067: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:33.067: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:33.070: INFO: Number of nodes with available pods: 2
Jul 19 04:31:33.070: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 19 04:31:33.100: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:33.100: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:33.100: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:33.100: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:33.100: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:33.104: INFO: Number of nodes with available pods: 1
Jul 19 04:31:33.104: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:34.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:34.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:34.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:34.110: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:34.110: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:34.113: INFO: Number of nodes with available pods: 1
Jul 19 04:31:34.113: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:35.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:35.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:35.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:35.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:35.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:35.115: INFO: Number of nodes with available pods: 1
Jul 19 04:31:35.115: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:36.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:36.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:36.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:36.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:36.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:36.114: INFO: Number of nodes with available pods: 1
Jul 19 04:31:36.114: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:37.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:37.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:37.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:37.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:37.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:37.115: INFO: Number of nodes with available pods: 1
Jul 19 04:31:37.115: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:38.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:38.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:38.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:38.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:38.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:38.114: INFO: Number of nodes with available pods: 1
Jul 19 04:31:38.114: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:39.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:39.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:39.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:39.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:39.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:39.114: INFO: Number of nodes with available pods: 1
Jul 19 04:31:39.114: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:40.115: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:40.115: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:40.115: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:40.115: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:40.115: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:40.121: INFO: Number of nodes with available pods: 1
Jul 19 04:31:40.121: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:41.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:41.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:41.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:41.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:41.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:41.115: INFO: Number of nodes with available pods: 1
Jul 19 04:31:41.115: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:42.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:42.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:42.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:42.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:42.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:42.115: INFO: Number of nodes with available pods: 1
Jul 19 04:31:42.115: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:43.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:43.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:43.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:43.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:43.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:43.115: INFO: Number of nodes with available pods: 1
Jul 19 04:31:43.115: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:44.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:44.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:44.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:44.110: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:44.110: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:44.114: INFO: Number of nodes with available pods: 1
Jul 19 04:31:44.114: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:45.116: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:45.116: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:45.116: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:45.116: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:45.116: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:45.121: INFO: Number of nodes with available pods: 1
Jul 19 04:31:45.121: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:46.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:46.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:46.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:46.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:46.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:46.115: INFO: Number of nodes with available pods: 1
Jul 19 04:31:46.115: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:47.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:47.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:47.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:47.112: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:47.112: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:47.115: INFO: Number of nodes with available pods: 1
Jul 19 04:31:47.115: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:48.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:48.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:48.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:48.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:48.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:48.115: INFO: Number of nodes with available pods: 1
Jul 19 04:31:48.115: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:49.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:49.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:49.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:49.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:49.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:49.115: INFO: Number of nodes with available pods: 1
Jul 19 04:31:49.115: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:50.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:50.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:50.110: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:50.110: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:50.110: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:50.113: INFO: Number of nodes with available pods: 1
Jul 19 04:31:50.114: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:51.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:51.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:51.111: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:51.111: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:51.111: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:51.114: INFO: Number of nodes with available pods: 1
Jul 19 04:31:51.114: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:31:52.117: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:52.117: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:52.117: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:52.117: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:52.117: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:31:52.120: INFO: Number of nodes with available pods: 2
Jul 19 04:31:52.120: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8358, will wait for the garbage collector to delete the pods
Jul 19 04:31:52.201: INFO: Deleting DaemonSet.extensions daemon-set took: 23.592724ms
Jul 19 04:31:52.301: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.174243ms
Jul 19 04:32:09.506: INFO: Number of nodes with available pods: 0
Jul 19 04:32:09.506: INFO: Number of running nodes: 0, number of available pods: 0
Jul 19 04:32:09.509: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8358/daemonsets","resourceVersion":"2413438"},"items":null}

Jul 19 04:32:09.512: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8358/pods","resourceVersion":"2413438"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:32:09.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8358" for this suite.

• [SLOW TEST:40.567 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":80,"skipped":1448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:32:09.539: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-121bb4e3-2419-48c0-9ca5-7cfa33a60939
STEP: Creating a pod to test consume configMaps
Jul 19 04:32:09.614: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-30600d35-de51-416f-8c0c-f990a18fe5bb" in namespace "projected-6526" to be "Succeeded or Failed"
Jul 19 04:32:09.618: INFO: Pod "pod-projected-configmaps-30600d35-de51-416f-8c0c-f990a18fe5bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.430747ms
Jul 19 04:32:11.622: INFO: Pod "pod-projected-configmaps-30600d35-de51-416f-8c0c-f990a18fe5bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007661912s
Jul 19 04:32:13.627: INFO: Pod "pod-projected-configmaps-30600d35-de51-416f-8c0c-f990a18fe5bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012771561s
STEP: Saw pod success
Jul 19 04:32:13.627: INFO: Pod "pod-projected-configmaps-30600d35-de51-416f-8c0c-f990a18fe5bb" satisfied condition "Succeeded or Failed"
Jul 19 04:32:13.630: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-projected-configmaps-30600d35-de51-416f-8c0c-f990a18fe5bb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 04:32:13.660: INFO: Waiting for pod pod-projected-configmaps-30600d35-de51-416f-8c0c-f990a18fe5bb to disappear
Jul 19 04:32:13.663: INFO: Pod pod-projected-configmaps-30600d35-de51-416f-8c0c-f990a18fe5bb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:32:13.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6526" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":81,"skipped":1478,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:32:13.675: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:32:17.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7164" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":82,"skipped":1483,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:32:17.831: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:32:22.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2803" for this suite.

• [SLOW TEST:5.126 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":83,"skipped":1495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:32:22.957: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 19 04:32:23.046: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 19 04:32:28.056: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:32:29.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2696" for this suite.

• [SLOW TEST:6.129 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":84,"skipped":1524,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:32:29.086: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:32:30.012: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 04:32:32.024: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265950, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265950, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265950, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762265950, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:32:35.095: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:32:35.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-36" for this suite.
STEP: Destroying namespace "webhook-36-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.424 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":85,"skipped":1540,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:32:35.511: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 04:32:35.587: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e38bd79c-328b-4dda-82dd-bf65f0c3ffaf" in namespace "downward-api-3142" to be "Succeeded or Failed"
Jul 19 04:32:35.591: INFO: Pod "downwardapi-volume-e38bd79c-328b-4dda-82dd-bf65f0c3ffaf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700955ms
Jul 19 04:32:37.596: INFO: Pod "downwardapi-volume-e38bd79c-328b-4dda-82dd-bf65f0c3ffaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009287568s
Jul 19 04:32:39.601: INFO: Pod "downwardapi-volume-e38bd79c-328b-4dda-82dd-bf65f0c3ffaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013708051s
STEP: Saw pod success
Jul 19 04:32:39.601: INFO: Pod "downwardapi-volume-e38bd79c-328b-4dda-82dd-bf65f0c3ffaf" satisfied condition "Succeeded or Failed"
Jul 19 04:32:39.604: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-e38bd79c-328b-4dda-82dd-bf65f0c3ffaf container client-container: <nil>
STEP: delete the pod
Jul 19 04:32:39.636: INFO: Waiting for pod downwardapi-volume-e38bd79c-328b-4dda-82dd-bf65f0c3ffaf to disappear
Jul 19 04:32:39.642: INFO: Pod downwardapi-volume-e38bd79c-328b-4dda-82dd-bf65f0c3ffaf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:32:39.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3142" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":86,"skipped":1561,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:32:39.655: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-3234/configmap-test-cb3e4d2f-6c01-4bec-ac0d-74bf5480e5ea
STEP: Creating a pod to test consume configMaps
Jul 19 04:32:39.738: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd538cca-e7fd-49ba-9338-fdc2f8e44179" in namespace "configmap-3234" to be "Succeeded or Failed"
Jul 19 04:32:39.741: INFO: Pod "pod-configmaps-dd538cca-e7fd-49ba-9338-fdc2f8e44179": Phase="Pending", Reason="", readiness=false. Elapsed: 3.027022ms
Jul 19 04:32:41.746: INFO: Pod "pod-configmaps-dd538cca-e7fd-49ba-9338-fdc2f8e44179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007943233s
Jul 19 04:32:43.751: INFO: Pod "pod-configmaps-dd538cca-e7fd-49ba-9338-fdc2f8e44179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012414515s
STEP: Saw pod success
Jul 19 04:32:43.751: INFO: Pod "pod-configmaps-dd538cca-e7fd-49ba-9338-fdc2f8e44179" satisfied condition "Succeeded or Failed"
Jul 19 04:32:43.755: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-configmaps-dd538cca-e7fd-49ba-9338-fdc2f8e44179 container env-test: <nil>
STEP: delete the pod
Jul 19 04:32:43.780: INFO: Waiting for pod pod-configmaps-dd538cca-e7fd-49ba-9338-fdc2f8e44179 to disappear
Jul 19 04:32:43.784: INFO: Pod pod-configmaps-dd538cca-e7fd-49ba-9338-fdc2f8e44179 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:32:43.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3234" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1597,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:32:43.801: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:32:43.898: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 19 04:32:43.911: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:43.911: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:43.911: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:43.911: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:43.911: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:43.914: INFO: Number of nodes with available pods: 0
Jul 19 04:32:43.914: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:32:44.926: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:44.926: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:44.926: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:44.926: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:44.926: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:44.932: INFO: Number of nodes with available pods: 0
Jul 19 04:32:44.932: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:32:45.920: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:45.920: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:45.920: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:45.920: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:45.920: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:45.923: INFO: Number of nodes with available pods: 0
Jul 19 04:32:45.923: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:32:46.920: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:46.920: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:46.920: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:46.920: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:46.920: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:46.923: INFO: Number of nodes with available pods: 0
Jul 19 04:32:46.923: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 04:32:47.919: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:47.919: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:47.919: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:47.920: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:47.920: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:47.923: INFO: Number of nodes with available pods: 2
Jul 19 04:32:47.923: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 19 04:32:47.949: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:47.949: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:47.954: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:47.954: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:47.954: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:47.954: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:47.954: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:48.976: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:48.976: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:48.983: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:48.984: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:48.984: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:48.984: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:48.984: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:49.959: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:49.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:49.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:49.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:49.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:49.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:49.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:50.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:50.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:32:50.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:50.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:50.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:50.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:50.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:50.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:51.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:51.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:32:51.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:51.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:51.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:51.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:51.963: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:51.963: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:52.959: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:52.959: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:32:52.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:52.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:52.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:52.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:52.965: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:52.965: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:53.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:53.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:32:53.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:53.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:53.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:53.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:53.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:53.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:54.961: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:54.961: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:32:54.961: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:54.967: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:54.967: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:54.967: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:54.967: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:54.967: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:55.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:55.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:32:55.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:55.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:55.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:55.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:55.963: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:55.963: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:56.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:56.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:32:56.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:56.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:56.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:56.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:56.963: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:56.963: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:57.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:57.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:32:57.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:57.977: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:57.977: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:57.977: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:57.977: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:57.977: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:58.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:58.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:32:58.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:58.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:58.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:58.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:58.963: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:58.963: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:59.959: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:59.959: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:32:59.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:32:59.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:59.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:59.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:59.965: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:32:59.965: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:00.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:00.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:33:00.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:00.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:00.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:00.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:00.963: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:00.963: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:01.959: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:01.959: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:33:01.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:01.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:01.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:01.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:01.966: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:01.966: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:02.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:02.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:33:02.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:02.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:02.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:02.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:02.963: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:02.963: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:03.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:03.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:33:03.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:03.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:03.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:03.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:03.963: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:03.963: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:04.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:04.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:33:04.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:04.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:04.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:04.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:04.963: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:04.963: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:05.960: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:05.960: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:33:05.960: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:05.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:05.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:05.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:05.966: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:05.966: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:06.958: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:06.958: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:33:06.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:06.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:06.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:06.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:06.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:06.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:07.961: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:07.961: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:33:07.961: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:07.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:07.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:07.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:07.966: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:07.966: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:08.960: INFO: Wrong image for pod: daemon-set-fzrmp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:08.960: INFO: Pod daemon-set-fzrmp is not available
Jul 19 04:33:08.960: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:08.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:08.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:08.966: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:08.966: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:08.966: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:09.959: INFO: Pod daemon-set-j4zvn is not available
Jul 19 04:33:09.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:09.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:09.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:09.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:09.965: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:09.966: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:10.958: INFO: Pod daemon-set-j4zvn is not available
Jul 19 04:33:10.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:10.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:10.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:10.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:10.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:10.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:11.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:11.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:11.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:11.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:11.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:11.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:12.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:12.959: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:12.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:12.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:12.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:12.965: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:12.965: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:13.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:13.959: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:13.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:13.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:13.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:13.965: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:13.965: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:14.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:14.958: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:14.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:14.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:14.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:14.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:14.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:15.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:15.958: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:15.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:15.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:15.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:15.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:15.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:16.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:16.958: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:16.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:16.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:16.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:16.963: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:16.963: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:17.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:17.959: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:17.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:17.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:17.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:17.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:17.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:18.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:18.959: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:18.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:18.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:18.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:18.965: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:18.965: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:19.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:19.958: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:19.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:19.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:19.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:19.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:19.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:20.960: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:20.960: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:20.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:20.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:20.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:20.965: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:20.965: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:21.960: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:21.960: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:21.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:21.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:21.965: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:21.965: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:21.965: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:22.958: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:22.958: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:22.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:22.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:22.963: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:22.963: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:22.963: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:23.959: INFO: Wrong image for pod: daemon-set-trs48. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 19 04:33:23.959: INFO: Pod daemon-set-trs48 is not available
Jul 19 04:33:23.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:23.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:23.964: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:23.964: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:23.964: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:24.961: INFO: Pod daemon-set-s8f6b is not available
Jul 19 04:33:24.969: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:24.969: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:24.969: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:24.969: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:24.969: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 19 04:33:24.974: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:24.974: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:24.974: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:24.974: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:24.974: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:24.977: INFO: Number of nodes with available pods: 1
Jul 19 04:33:24.977: INFO: Node paas-192-168-10-183 is running more than one daemon pod
Jul 19 04:33:25.983: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:25.983: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:25.983: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:25.983: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:25.983: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:25.987: INFO: Number of nodes with available pods: 1
Jul 19 04:33:25.987: INFO: Node paas-192-168-10-183 is running more than one daemon pod
Jul 19 04:33:26.986: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:26.986: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:26.986: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:26.986: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:26.986: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:26.991: INFO: Number of nodes with available pods: 1
Jul 19 04:33:26.991: INFO: Node paas-192-168-10-183 is running more than one daemon pod
Jul 19 04:33:27.984: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:27.984: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:27.984: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:27.984: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:27.984: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 04:33:27.987: INFO: Number of nodes with available pods: 2
Jul 19 04:33:27.987: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2889, will wait for the garbage collector to delete the pods
Jul 19 04:33:28.082: INFO: Deleting DaemonSet.extensions daemon-set took: 14.719614ms
Jul 19 04:33:28.182: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.224355ms
Jul 19 04:33:44.393: INFO: Number of nodes with available pods: 0
Jul 19 04:33:44.393: INFO: Number of running nodes: 0, number of available pods: 0
Jul 19 04:33:44.400: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2889/daemonsets","resourceVersion":"2414333"},"items":null}

Jul 19 04:33:44.410: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2889/pods","resourceVersion":"2414333"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:33:44.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2889" for this suite.

• [SLOW TEST:60.641 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":88,"skipped":1600,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:33:44.441: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-aea1ab7b-7285-4a2e-a4db-e7e151c9d2c5 in namespace container-probe-2735
Jul 19 04:33:48.520: INFO: Started pod busybox-aea1ab7b-7285-4a2e-a4db-e7e151c9d2c5 in namespace container-probe-2735
STEP: checking the pod's current state and verifying that restartCount is present
Jul 19 04:33:48.523: INFO: Initial restart count of pod busybox-aea1ab7b-7285-4a2e-a4db-e7e151c9d2c5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:37:49.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2735" for this suite.

• [SLOW TEST:244.729 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":89,"skipped":1610,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:37:49.171: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-hpzn
STEP: Creating a pod to test atomic-volume-subpath
Jul 19 04:37:49.275: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-hpzn" in namespace "subpath-3737" to be "Succeeded or Failed"
Jul 19 04:37:49.338: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Pending", Reason="", readiness=false. Elapsed: 63.105985ms
Jul 19 04:37:51.358: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083594499s
Jul 19 04:37:53.364: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Running", Reason="", readiness=true. Elapsed: 4.088803124s
Jul 19 04:37:55.367: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Running", Reason="", readiness=true. Elapsed: 6.092506499s
Jul 19 04:37:57.372: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Running", Reason="", readiness=true. Elapsed: 8.096961612s
Jul 19 04:37:59.380: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Running", Reason="", readiness=true. Elapsed: 10.105594355s
Jul 19 04:38:01.391: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Running", Reason="", readiness=true. Elapsed: 12.115941756s
Jul 19 04:38:03.396: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Running", Reason="", readiness=true. Elapsed: 14.121088774s
Jul 19 04:38:05.400: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Running", Reason="", readiness=true. Elapsed: 16.125515759s
Jul 19 04:38:07.404: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Running", Reason="", readiness=true. Elapsed: 18.129614096s
Jul 19 04:38:09.409: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Running", Reason="", readiness=true. Elapsed: 20.134467596s
Jul 19 04:38:11.413: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Running", Reason="", readiness=true. Elapsed: 22.138243911s
Jul 19 04:38:13.418: INFO: Pod "pod-subpath-test-downwardapi-hpzn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.14279622s
STEP: Saw pod success
Jul 19 04:38:13.418: INFO: Pod "pod-subpath-test-downwardapi-hpzn" satisfied condition "Succeeded or Failed"
Jul 19 04:38:13.421: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-subpath-test-downwardapi-hpzn container test-container-subpath-downwardapi-hpzn: <nil>
STEP: delete the pod
Jul 19 04:38:13.488: INFO: Waiting for pod pod-subpath-test-downwardapi-hpzn to disappear
Jul 19 04:38:13.494: INFO: Pod pod-subpath-test-downwardapi-hpzn no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-hpzn
Jul 19 04:38:13.494: INFO: Deleting pod "pod-subpath-test-downwardapi-hpzn" in namespace "subpath-3737"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:38:13.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3737" for this suite.

• [SLOW TEST:24.347 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":90,"skipped":1616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:38:13.518: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 19 04:38:13.579: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 19 04:38:13.593: INFO: Waiting for terminating namespaces to be deleted...
Jul 19 04:38:13.598: INFO: 
Logging pods the apiserver thinks is on node paas-192-168-10-158 before test
Jul 19 04:38:13.619: INFO: cse-config-center-9cb5789db-hqgs8 from fst-manage started at 2021-07-15 02:47:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container cse-cc-comp ready: true, restart count 0
Jul 19 04:38:13.619: INFO: cse-etcd-0 from fst-manage started at 2021-07-15 02:45:36 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container cse-etcd-comp ready: true, restart count 0
Jul 19 04:38:13.619: INFO: cse-service-center-78c89955f8-9zxb8 from fst-manage started at 2021-07-15 02:47:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container cse-sc-comp ready: true, restart count 0
Jul 19 04:38:13.619: INFO: cspagentmanager-84df498dd6-756cg from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container cspagentmanager-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: csphaservice-85cfb95df6-rg2zf from fst-manage started at 2021-07-15 02:45:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container csphaservice-soft ready: true, restart count 0
Jul 19 04:38:13.619: INFO: cspnrsmaster-0 from fst-manage started at 2021-07-15 02:45:49 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container cspnrsmaster-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: cspommgr-7b4c7c76dc-lg5xn from fst-manage started at 2021-07-15 02:45:42 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container cspommgr-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: csposmgr-666d77c8cf-5nztr from fst-manage started at 2021-07-15 02:45:52 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container csposmgr-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: deploymgr-5fc4dfbc87-skqjv from fst-manage started at 2021-07-15 02:45:39 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container deploymgr-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: febs-599995cbb8-wch4b from fst-manage started at 2021-07-15 02:46:04 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container febs-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: fileserver-0 from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container fileserver-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: gaussdb-da5b9bc8-0 from fst-manage started at 2021-07-15 02:46:01 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container gauss-softwarecomp ready: true, restart count 0
Jul 19 04:38:13.619: INFO: kafka-da5b9bc8-0 from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container kafka-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: kubeaddon-upload-package-f5vtngqpi8-4m5dx from fst-manage started at 2021-07-15 02:44:30 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container kubeaddon-upload-package ready: false, restart count 0
Jul 19 04:38:13.619: INFO: mtcenter-57c48c94fd-89d27 from fst-manage started at 2021-07-15 02:45:35 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container cspmtcenter-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: omcache-da5b9bc8-0 from fst-manage started at 2021-07-15 02:45:56 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container redis-softwarecomp ready: true, restart count 0
Jul 19 04:38:13.619: INFO: omlb-5d58b5f84f-w9p5s from fst-manage started at 2021-07-15 03:35:19 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container omlb-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: pmscalc-5744fc9c5d-x6rlp from fst-manage started at 2021-07-15 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container pmscalc-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: pmsmgr-6857b85787-q6n5l from fst-manage started at 2021-07-15 02:45:37 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container pmsmgr-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: runlog-69f7c599d-krpbr from fst-manage started at 2021-07-15 02:45:58 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container runlog-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: secmgr-65cfb48b8b-2fzvs from fst-manage started at 2021-07-15 02:45:57 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container secmgr-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: soap-76794ccdc-xtknz from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container soap-soft ready: true, restart count 0
Jul 19 04:38:13.619: INFO: upgtool-app-559557759c-jpc94 from fst-manage started at 2021-07-15 02:45:54 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container upgtool-component ready: true, restart count 0
Jul 19 04:38:13.619: INFO: zookeeper-0 from fst-manage started at 2021-07-15 02:45:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container zookeeper-software ready: true, restart count 0
Jul 19 04:38:13.619: INFO: sonobuoy from sonobuoy started at 2021-07-19 03:55:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 19 04:38:13.619: INFO: sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-p7chc from sonobuoy started at 2021-07-19 03:55:51 +0000 UTC (2 container statuses recorded)
Jul 19 04:38:13.619: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 04:38:13.619: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 19 04:38:13.619: INFO: 
Logging pods the apiserver thinks is on node paas-192-168-10-183 before test
Jul 19 04:38:13.640: INFO: cse-config-center-9cb5789db-vhnp6 from fst-manage started at 2021-07-15 02:47:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.640: INFO: 	Container cse-cc-comp ready: true, restart count 0
Jul 19 04:38:13.640: INFO: cse-etcd-1 from fst-manage started at 2021-07-15 02:45:36 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.640: INFO: 	Container cse-etcd-comp ready: true, restart count 0
Jul 19 04:38:13.640: INFO: cse-service-center-78c89955f8-4kfjs from fst-manage started at 2021-07-15 02:47:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container cse-sc-comp ready: true, restart count 0
Jul 19 04:38:13.641: INFO: cspagentmanager-84df498dd6-fbzqf from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container cspagentmanager-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: csphaservice-85cfb95df6-qch6t from fst-manage started at 2021-07-15 02:45:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container csphaservice-soft ready: true, restart count 0
Jul 19 04:38:13.641: INFO: cspnrsmaster-1 from fst-manage started at 2021-07-15 02:45:49 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container cspnrsmaster-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: cspommgr-7b4c7c76dc-74p5c from fst-manage started at 2021-07-15 02:45:42 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container cspommgr-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: csposmgr-666d77c8cf-xqphh from fst-manage started at 2021-07-15 02:45:53 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container csposmgr-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: deploymgr-5fc4dfbc87-d4hmv from fst-manage started at 2021-07-15 02:45:39 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container deploymgr-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: febs-599995cbb8-qsp8z from fst-manage started at 2021-07-15 02:46:04 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container febs-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: fileserver-1 from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container fileserver-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: gaussdb-da5b9bc8-1 from fst-manage started at 2021-07-15 02:46:02 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container gauss-softwarecomp ready: true, restart count 0
Jul 19 04:38:13.641: INFO: kafka-da5b9bc8-1 from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container kafka-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: mtcenter-57c48c94fd-f8h98 from fst-manage started at 2021-07-15 02:45:35 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container cspmtcenter-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: omcache-da5b9bc8-1 from fst-manage started at 2021-07-15 02:45:56 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container redis-softwarecomp ready: true, restart count 0
Jul 19 04:38:13.641: INFO: omlb-5d58b5f84f-84kpc from fst-manage started at 2021-07-15 03:24:27 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container omlb-software ready: true, restart count 2
Jul 19 04:38:13.641: INFO: pmscalc-5744fc9c5d-h54gd from fst-manage started at 2021-07-15 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container pmscalc-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: pmsmgr-6857b85787-8n4th from fst-manage started at 2021-07-15 02:45:37 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container pmsmgr-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: psm-update-69dd9dc9cb-vddnv from fst-manage started at 2021-07-15 02:44:31 +0000 UTC (0 container statuses recorded)
Jul 19 04:38:13.641: INFO: runlog-69f7c599d-mszld from fst-manage started at 2021-07-15 02:45:59 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container runlog-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: secmgr-65cfb48b8b-zz9t8 from fst-manage started at 2021-07-15 02:45:57 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container secmgr-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: soap-76794ccdc-brpbn from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container soap-soft ready: true, restart count 0
Jul 19 04:38:13.641: INFO: upgtool-app-559557759c-ssmzk from fst-manage started at 2021-07-15 02:45:54 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container upgtool-component ready: true, restart count 0
Jul 19 04:38:13.641: INFO: zookeeper-1 from fst-manage started at 2021-07-15 02:45:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container zookeeper-software ready: true, restart count 0
Jul 19 04:38:13.641: INFO: sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-gtdqw from sonobuoy started at 2021-07-19 03:55:51 +0000 UTC (2 container statuses recorded)
Jul 19 04:38:13.641: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 04:38:13.641: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.169317493bb30fdf], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:38:14.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7714" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":91,"skipped":1640,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:38:14.725: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Jul 19 04:38:14.788: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-086781019 proxy --unix-socket=/tmp/kubectl-proxy-unix216969014/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:38:14.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9973" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":92,"skipped":1654,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:38:14.866: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 19 04:38:19.521: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4fe6409d-a1ec-4a8e-895e-4f940d607b5f"
Jul 19 04:38:19.521: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4fe6409d-a1ec-4a8e-895e-4f940d607b5f" in namespace "pods-6053" to be "terminated due to deadline exceeded"
Jul 19 04:38:19.526: INFO: Pod "pod-update-activedeadlineseconds-4fe6409d-a1ec-4a8e-895e-4f940d607b5f": Phase="Running", Reason="", readiness=true. Elapsed: 4.871949ms
Jul 19 04:38:21.530: INFO: Pod "pod-update-activedeadlineseconds-4fe6409d-a1ec-4a8e-895e-4f940d607b5f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.008738494s
Jul 19 04:38:21.530: INFO: Pod "pod-update-activedeadlineseconds-4fe6409d-a1ec-4a8e-895e-4f940d607b5f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:38:21.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6053" for this suite.

• [SLOW TEST:6.676 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:38:21.542: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 04:38:21.608: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3e78517-4fbb-4713-bc1a-52f9b065dbd2" in namespace "projected-9821" to be "Succeeded or Failed"
Jul 19 04:38:21.611: INFO: Pod "downwardapi-volume-d3e78517-4fbb-4713-bc1a-52f9b065dbd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.760083ms
Jul 19 04:38:23.615: INFO: Pod "downwardapi-volume-d3e78517-4fbb-4713-bc1a-52f9b065dbd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00699369s
Jul 19 04:38:25.619: INFO: Pod "downwardapi-volume-d3e78517-4fbb-4713-bc1a-52f9b065dbd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010968276s
STEP: Saw pod success
Jul 19 04:38:25.619: INFO: Pod "downwardapi-volume-d3e78517-4fbb-4713-bc1a-52f9b065dbd2" satisfied condition "Succeeded or Failed"
Jul 19 04:38:25.622: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-d3e78517-4fbb-4713-bc1a-52f9b065dbd2 container client-container: <nil>
STEP: delete the pod
Jul 19 04:38:25.686: INFO: Waiting for pod downwardapi-volume-d3e78517-4fbb-4713-bc1a-52f9b065dbd2 to disappear
Jul 19 04:38:25.690: INFO: Pod downwardapi-volume-d3e78517-4fbb-4713-bc1a-52f9b065dbd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:38:25.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9821" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":94,"skipped":1702,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:38:25.701: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:38:25.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7902" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":95,"skipped":1711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:38:25.778: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7834
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7834
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7834
Jul 19 04:38:25.856: INFO: Found 0 stateful pods, waiting for 1
Jul 19 04:38:35.861: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 19 04:38:35.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-7834 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 04:38:36.517: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 04:38:36.517: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 04:38:36.517: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 04:38:36.521: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 19 04:38:46.526: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 04:38:46.526: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 04:38:46.540: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999754s
Jul 19 04:38:47.545: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996586786s
Jul 19 04:38:48.550: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992119278s
Jul 19 04:38:49.554: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987163945s
Jul 19 04:38:50.559: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98265983s
Jul 19 04:38:51.563: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978179755s
Jul 19 04:38:52.568: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.973616578s
Jul 19 04:38:53.573: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968495943s
Jul 19 04:38:54.577: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.964330211s
Jul 19 04:38:55.581: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.825388ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7834
Jul 19 04:38:56.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-7834 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:38:56.832: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 04:38:56.832: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 04:38:56.832: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 04:38:56.836: INFO: Found 1 stateful pods, waiting for 3
Jul 19 04:39:06.841: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:39:06.841: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 04:39:06.841: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 19 04:39:06.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-7834 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 04:39:07.084: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 04:39:07.084: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 04:39:07.084: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 04:39:07.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-7834 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 04:39:07.353: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 04:39:07.353: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 04:39:07.353: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 04:39:07.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-7834 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 04:39:07.620: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 04:39:07.620: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 04:39:07.620: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 04:39:07.620: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 04:39:07.624: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 19 04:39:17.633: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 04:39:17.633: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 04:39:17.633: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 04:39:17.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999646s
Jul 19 04:39:18.649: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996298902s
Jul 19 04:39:19.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99226038s
Jul 19 04:39:20.660: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986671276s
Jul 19 04:39:21.664: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.9818564s
Jul 19 04:39:22.669: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977690729s
Jul 19 04:39:23.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972925905s
Jul 19 04:39:24.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968570807s
Jul 19 04:39:25.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963468519s
Jul 19 04:39:26.688: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.241435ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7834
Jul 19 04:39:27.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-7834 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:39:27.987: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 04:39:27.987: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 04:39:27.987: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 04:39:27.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-7834 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:39:28.298: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 04:39:28.298: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 04:39:28.298: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 04:39:28.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=statefulset-7834 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 04:39:28.543: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 04:39:28.543: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 04:39:28.543: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 04:39:28.543: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 19 04:40:28.572: INFO: Deleting all statefulset in ns statefulset-7834
Jul 19 04:40:28.575: INFO: Scaling statefulset ss to 0
Jul 19 04:40:28.586: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 04:40:28.588: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:40:28.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7834" for this suite.

• [SLOW TEST:122.850 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":96,"skipped":1735,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:40:28.629: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Jul 19 04:40:28.695: INFO: created test-event-1
Jul 19 04:40:28.700: INFO: created test-event-2
Jul 19 04:40:28.705: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jul 19 04:40:28.710: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jul 19 04:40:28.727: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:40:28.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4616" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":97,"skipped":1740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:40:28.742: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:40:29.195: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 04:40:31.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266429, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266429, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266429, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266429, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:40:34.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:40:34.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1503" for this suite.
STEP: Destroying namespace "webhook-1503-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.913 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":98,"skipped":1765,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:40:34.656: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:41:05.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7287" for this suite.

• [SLOW TEST:30.433 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":99,"skipped":1828,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:41:05.089: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:41:09.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8241" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":100,"skipped":1849,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:41:09.202: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3744.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3744.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 19 04:41:15.399: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:15.465: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:15.465: INFO: Lookups using dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:41:20.470: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:20.475: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:20.475: INFO: Lookups using dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:41:25.476: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:25.480: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:25.480: INFO: Lookups using dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:41:30.470: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:30.475: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:30.475: INFO: Lookups using dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:41:35.470: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:35.474: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:35.474: INFO: Lookups using dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:41:40.470: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:40.473: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 contains '' instead of 'foo.example.com.'
Jul 19 04:41:40.473: INFO: Lookups using dns-3744/dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:41:45.476: INFO: DNS probes using dns-test-450e97ba-1d20-45c9-bc2f-3792779766c1 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3744.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3744.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 19 04:41:51.598: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 04:41:51.608: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 04:41:51.608: INFO: Lookups using dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:41:56.614: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 04:41:56.618: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 04:41:56.618: INFO: Lookups using dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:01.614: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 04:42:01.619: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 04:42:01.619: INFO: Lookups using dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:06.614: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 04:42:06.618: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 04:42:06.618: INFO: Lookups using dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:11.613: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 04:42:11.618: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 04:42:11.618: INFO: Lookups using dns-3744/dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:16.617: INFO: DNS probes using dns-test-0e829351-8e5a-489b-b8f8-2ca125350c1a succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3744.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3744.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 19 04:42:22.704: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:22.708: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:22.709: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:27.713: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:27.717: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:27.717: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:32.714: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:32.718: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:32.718: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:37.714: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:37.719: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:37.719: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:42.714: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:42.719: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:42.719: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:47.713: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:47.717: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:47.717: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:52.714: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:52.717: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:52.717: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:42:57.714: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:57.719: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:42:57.719: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:43:02.716: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:43:02.724: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:43:02.724: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:43:07.714: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains ';; connection timed out; no servers could be reached
' instead of '10.247.58.38'
Jul 19 04:43:07.718: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains ';; connection timed out; no servers could be reached
' instead of '10.247.58.38'
Jul 19 04:43:07.718: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:43:12.713: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:43:12.718: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:43:12.718: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:43:17.713: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:43:17.719: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:43:17.719: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:43:22.714: INFO: File wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:43:22.719: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:43:22.719: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [wheezy_udp@dns-test-service-3.dns-3744.svc.cluster.local jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:43:27.716: INFO: File jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local from pod  dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 contains '' instead of '10.247.58.38'
Jul 19 04:43:27.716: INFO: Lookups using dns-3744/dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 failed for: [jessie_udp@dns-test-service-3.dns-3744.svc.cluster.local]

Jul 19 04:43:32.719: INFO: DNS probes using dns-test-ddc0639e-9c14-43d5-bb86-abd62fcf5d18 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:43:32.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3744" for this suite.

• [SLOW TEST:143.577 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":101,"skipped":1858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:43:32.779: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Jul 19 04:43:32.867: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul 19 04:43:32.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-6626'
Jul 19 04:43:33.101: INFO: stderr: ""
Jul 19 04:43:33.102: INFO: stdout: "service/agnhost-replica created\n"
Jul 19 04:43:33.102: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul 19 04:43:33.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-6626'
Jul 19 04:43:33.345: INFO: stderr: ""
Jul 19 04:43:33.345: INFO: stdout: "service/agnhost-primary created\n"
Jul 19 04:43:33.345: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 19 04:43:33.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-6626'
Jul 19 04:43:33.583: INFO: stderr: ""
Jul 19 04:43:33.583: INFO: stdout: "service/frontend created\n"
Jul 19 04:43:33.583: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 19 04:43:33.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-6626'
Jul 19 04:43:33.859: INFO: stderr: ""
Jul 19 04:43:33.860: INFO: stdout: "deployment.apps/frontend created\n"
Jul 19 04:43:33.860: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 19 04:43:33.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-6626'
Jul 19 04:43:34.107: INFO: stderr: ""
Jul 19 04:43:34.107: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul 19 04:43:34.107: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 19 04:43:34.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-6626'
Jul 19 04:43:34.323: INFO: stderr: ""
Jul 19 04:43:34.323: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jul 19 04:43:34.323: INFO: Waiting for all frontend pods to be Running.
Jul 19 04:43:39.373: INFO: Waiting for frontend to serve content.
Jul 19 04:43:40.414: INFO: Trying to add a new entry to the guestbook.
Jul 19 04:43:40.455: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 19 04:43:40.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete --grace-period=0 --force -f - --namespace=kubectl-6626'
Jul 19 04:43:40.622: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 04:43:40.623: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jul 19 04:43:40.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete --grace-period=0 --force -f - --namespace=kubectl-6626'
Jul 19 04:43:40.763: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 04:43:40.763: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 19 04:43:40.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete --grace-period=0 --force -f - --namespace=kubectl-6626'
Jul 19 04:43:40.907: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 04:43:40.907: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 19 04:43:40.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete --grace-period=0 --force -f - --namespace=kubectl-6626'
Jul 19 04:43:41.020: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 04:43:41.020: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 19 04:43:41.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete --grace-period=0 --force -f - --namespace=kubectl-6626'
Jul 19 04:43:41.119: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 04:43:41.119: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 19 04:43:41.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete --grace-period=0 --force -f - --namespace=kubectl-6626'
Jul 19 04:43:41.226: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 04:43:41.226: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:43:41.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6626" for this suite.

• [SLOW TEST:8.461 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":102,"skipped":1880,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:43:41.241: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jul 19 04:43:41.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-9035'
Jul 19 04:43:41.564: INFO: stderr: ""
Jul 19 04:43:41.564: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 19 04:43:41.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9035'
Jul 19 04:43:41.672: INFO: stderr: ""
Jul 19 04:43:41.673: INFO: stdout: "update-demo-nautilus-f9dhz update-demo-nautilus-fcmq8 "
Jul 19 04:43:41.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-f9dhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:43:41.770: INFO: stderr: ""
Jul 19 04:43:41.770: INFO: stdout: ""
Jul 19 04:43:41.770: INFO: update-demo-nautilus-f9dhz is created but not running
Jul 19 04:43:46.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9035'
Jul 19 04:43:46.860: INFO: stderr: ""
Jul 19 04:43:46.860: INFO: stdout: "update-demo-nautilus-f9dhz update-demo-nautilus-fcmq8 "
Jul 19 04:43:46.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-f9dhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:43:46.965: INFO: stderr: ""
Jul 19 04:43:46.965: INFO: stdout: "true"
Jul 19 04:43:46.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-f9dhz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:43:47.057: INFO: stderr: ""
Jul 19 04:43:47.057: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 19 04:43:47.057: INFO: validating pod update-demo-nautilus-f9dhz
Jul 19 04:43:47.068: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 04:43:47.068: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 04:43:47.068: INFO: update-demo-nautilus-f9dhz is verified up and running
Jul 19 04:43:47.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-fcmq8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:43:47.162: INFO: stderr: ""
Jul 19 04:43:47.162: INFO: stdout: "true"
Jul 19 04:43:47.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-fcmq8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:43:47.246: INFO: stderr: ""
Jul 19 04:43:47.246: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 19 04:43:47.246: INFO: validating pod update-demo-nautilus-fcmq8
Jul 19 04:43:47.261: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 04:43:47.261: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 04:43:47.261: INFO: update-demo-nautilus-fcmq8 is verified up and running
STEP: scaling down the replication controller
Jul 19 04:43:47.262: INFO: scanned /root for discovery docs: <nil>
Jul 19 04:43:47.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9035'
Jul 19 04:43:48.364: INFO: stderr: ""
Jul 19 04:43:48.364: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 19 04:43:48.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9035'
Jul 19 04:43:48.461: INFO: stderr: ""
Jul 19 04:43:48.461: INFO: stdout: "update-demo-nautilus-f9dhz update-demo-nautilus-fcmq8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 19 04:43:53.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9035'
Jul 19 04:43:53.573: INFO: stderr: ""
Jul 19 04:43:53.573: INFO: stdout: "update-demo-nautilus-f9dhz update-demo-nautilus-fcmq8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 19 04:43:58.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9035'
Jul 19 04:43:58.666: INFO: stderr: ""
Jul 19 04:43:58.666: INFO: stdout: "update-demo-nautilus-f9dhz update-demo-nautilus-fcmq8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 19 04:44:03.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9035'
Jul 19 04:44:03.754: INFO: stderr: ""
Jul 19 04:44:03.754: INFO: stdout: "update-demo-nautilus-f9dhz update-demo-nautilus-fcmq8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 19 04:44:08.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9035'
Jul 19 04:44:08.861: INFO: stderr: ""
Jul 19 04:44:08.861: INFO: stdout: "update-demo-nautilus-f9dhz "
Jul 19 04:44:08.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-f9dhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:44:08.957: INFO: stderr: ""
Jul 19 04:44:08.957: INFO: stdout: "true"
Jul 19 04:44:08.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-f9dhz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:44:09.045: INFO: stderr: ""
Jul 19 04:44:09.045: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 19 04:44:09.045: INFO: validating pod update-demo-nautilus-f9dhz
Jul 19 04:44:09.071: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 04:44:09.071: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 04:44:09.071: INFO: update-demo-nautilus-f9dhz is verified up and running
STEP: scaling up the replication controller
Jul 19 04:44:09.072: INFO: scanned /root for discovery docs: <nil>
Jul 19 04:44:09.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9035'
Jul 19 04:44:10.187: INFO: stderr: ""
Jul 19 04:44:10.187: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 19 04:44:10.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9035'
Jul 19 04:44:10.275: INFO: stderr: ""
Jul 19 04:44:10.275: INFO: stdout: "update-demo-nautilus-2w7z9 update-demo-nautilus-f9dhz "
Jul 19 04:44:10.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-2w7z9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:44:10.364: INFO: stderr: ""
Jul 19 04:44:10.364: INFO: stdout: ""
Jul 19 04:44:10.364: INFO: update-demo-nautilus-2w7z9 is created but not running
Jul 19 04:44:15.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9035'
Jul 19 04:44:15.469: INFO: stderr: ""
Jul 19 04:44:15.470: INFO: stdout: "update-demo-nautilus-2w7z9 update-demo-nautilus-f9dhz "
Jul 19 04:44:15.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-2w7z9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:44:15.562: INFO: stderr: ""
Jul 19 04:44:15.562: INFO: stdout: "true"
Jul 19 04:44:15.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-2w7z9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:44:15.645: INFO: stderr: ""
Jul 19 04:44:15.645: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 19 04:44:15.645: INFO: validating pod update-demo-nautilus-2w7z9
Jul 19 04:44:15.661: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 04:44:15.661: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 04:44:15.661: INFO: update-demo-nautilus-2w7z9 is verified up and running
Jul 19 04:44:15.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-f9dhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:44:15.747: INFO: stderr: ""
Jul 19 04:44:15.747: INFO: stdout: "true"
Jul 19 04:44:15.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods update-demo-nautilus-f9dhz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9035'
Jul 19 04:44:15.837: INFO: stderr: ""
Jul 19 04:44:15.837: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 19 04:44:15.837: INFO: validating pod update-demo-nautilus-f9dhz
Jul 19 04:44:15.841: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 04:44:15.841: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 04:44:15.841: INFO: update-demo-nautilus-f9dhz is verified up and running
STEP: using delete to clean up resources
Jul 19 04:44:15.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete --grace-period=0 --force -f - --namespace=kubectl-9035'
Jul 19 04:44:15.947: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 04:44:15.947: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 19 04:44:15.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9035'
Jul 19 04:44:16.042: INFO: stderr: "No resources found in kubectl-9035 namespace.\n"
Jul 19 04:44:16.042: INFO: stdout: ""
Jul 19 04:44:16.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -l name=update-demo --namespace=kubectl-9035 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 19 04:44:16.132: INFO: stderr: ""
Jul 19 04:44:16.132: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:16.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9035" for this suite.

• [SLOW TEST:34.907 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":103,"skipped":1909,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:16.148: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 04:44:16.317: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd7c7ba3-1819-4763-9fab-872f23579bd4" in namespace "downward-api-3998" to be "Succeeded or Failed"
Jul 19 04:44:16.322: INFO: Pod "downwardapi-volume-bd7c7ba3-1819-4763-9fab-872f23579bd4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.244502ms
Jul 19 04:44:18.326: INFO: Pod "downwardapi-volume-bd7c7ba3-1819-4763-9fab-872f23579bd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009802213s
Jul 19 04:44:20.331: INFO: Pod "downwardapi-volume-bd7c7ba3-1819-4763-9fab-872f23579bd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014106969s
STEP: Saw pod success
Jul 19 04:44:20.331: INFO: Pod "downwardapi-volume-bd7c7ba3-1819-4763-9fab-872f23579bd4" satisfied condition "Succeeded or Failed"
Jul 19 04:44:20.349: INFO: Trying to get logs from node paas-192-168-10-158 pod downwardapi-volume-bd7c7ba3-1819-4763-9fab-872f23579bd4 container client-container: <nil>
STEP: delete the pod
Jul 19 04:44:20.429: INFO: Waiting for pod downwardapi-volume-bd7c7ba3-1819-4763-9fab-872f23579bd4 to disappear
Jul 19 04:44:20.432: INFO: Pod downwardapi-volume-bd7c7ba3-1819-4763-9fab-872f23579bd4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:20.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3998" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":104,"skipped":1922,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:20.457: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-3a216667-0177-4ac2-a1e6-2737448fd966
STEP: Creating secret with name s-test-opt-upd-365c393f-0c65-43c0-8b7f-b4dc3af8f18b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3a216667-0177-4ac2-a1e6-2737448fd966
STEP: Updating secret s-test-opt-upd-365c393f-0c65-43c0-8b7f-b4dc3af8f18b
STEP: Creating secret with name s-test-opt-create-cab15764-0d1d-4c42-8444-26ddb860565b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:26.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8372" for this suite.

• [SLOW TEST:6.459 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":105,"skipped":1931,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:26.916: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 19 04:44:27.084: INFO: Waiting up to 5m0s for pod "pod-e5dcde75-8b43-4270-ab8c-4d034f13b926" in namespace "emptydir-5836" to be "Succeeded or Failed"
Jul 19 04:44:27.167: INFO: Pod "pod-e5dcde75-8b43-4270-ab8c-4d034f13b926": Phase="Pending", Reason="", readiness=false. Elapsed: 82.314875ms
Jul 19 04:44:29.172: INFO: Pod "pod-e5dcde75-8b43-4270-ab8c-4d034f13b926": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087387502s
Jul 19 04:44:31.176: INFO: Pod "pod-e5dcde75-8b43-4270-ab8c-4d034f13b926": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.091766474s
STEP: Saw pod success
Jul 19 04:44:31.176: INFO: Pod "pod-e5dcde75-8b43-4270-ab8c-4d034f13b926" satisfied condition "Succeeded or Failed"
Jul 19 04:44:31.181: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-e5dcde75-8b43-4270-ab8c-4d034f13b926 container test-container: <nil>
STEP: delete the pod
Jul 19 04:44:31.273: INFO: Waiting for pod pod-e5dcde75-8b43-4270-ab8c-4d034f13b926 to disappear
Jul 19 04:44:31.279: INFO: Pod pod-e5dcde75-8b43-4270-ab8c-4d034f13b926 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:31.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5836" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":106,"skipped":1947,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:31.299: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:44:31.391: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:32.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9463" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":107,"skipped":1950,"failed":0}
SSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:32.507: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:44:32.720: INFO: Waiting up to 5m0s for pod "busybox-user-65534-669034ff-a81f-4ade-8fa9-4989d3863880" in namespace "security-context-test-6955" to be "Succeeded or Failed"
Jul 19 04:44:32.726: INFO: Pod "busybox-user-65534-669034ff-a81f-4ade-8fa9-4989d3863880": Phase="Pending", Reason="", readiness=false. Elapsed: 6.292973ms
Jul 19 04:44:34.730: INFO: Pod "busybox-user-65534-669034ff-a81f-4ade-8fa9-4989d3863880": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010264994s
Jul 19 04:44:36.797: INFO: Pod "busybox-user-65534-669034ff-a81f-4ade-8fa9-4989d3863880": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077631038s
Jul 19 04:44:36.797: INFO: Pod "busybox-user-65534-669034ff-a81f-4ade-8fa9-4989d3863880" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:36.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6955" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1953,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:36.823: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 19 04:44:36.977: INFO: Waiting up to 5m0s for pod "downward-api-f6ce249e-63aa-4e07-84a1-2b8a8c4d7889" in namespace "downward-api-1380" to be "Succeeded or Failed"
Jul 19 04:44:36.981: INFO: Pod "downward-api-f6ce249e-63aa-4e07-84a1-2b8a8c4d7889": Phase="Pending", Reason="", readiness=false. Elapsed: 3.566891ms
Jul 19 04:44:38.987: INFO: Pod "downward-api-f6ce249e-63aa-4e07-84a1-2b8a8c4d7889": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009639568s
Jul 19 04:44:40.991: INFO: Pod "downward-api-f6ce249e-63aa-4e07-84a1-2b8a8c4d7889": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014095104s
STEP: Saw pod success
Jul 19 04:44:40.991: INFO: Pod "downward-api-f6ce249e-63aa-4e07-84a1-2b8a8c4d7889" satisfied condition "Succeeded or Failed"
Jul 19 04:44:40.995: INFO: Trying to get logs from node paas-192-168-10-183 pod downward-api-f6ce249e-63aa-4e07-84a1-2b8a8c4d7889 container dapi-container: <nil>
STEP: delete the pod
Jul 19 04:44:41.022: INFO: Waiting for pod downward-api-f6ce249e-63aa-4e07-84a1-2b8a8c4d7889 to disappear
Jul 19 04:44:41.027: INFO: Pod downward-api-f6ce249e-63aa-4e07-84a1-2b8a8c4d7889 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:41.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1380" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":109,"skipped":1962,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:41.039: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 19 04:44:41.117: INFO: Waiting up to 5m0s for pod "pod-11981b7a-790f-460a-bfd5-34274bddfdb4" in namespace "emptydir-9885" to be "Succeeded or Failed"
Jul 19 04:44:41.122: INFO: Pod "pod-11981b7a-790f-460a-bfd5-34274bddfdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.976186ms
Jul 19 04:44:43.126: INFO: Pod "pod-11981b7a-790f-460a-bfd5-34274bddfdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009156367s
Jul 19 04:44:45.133: INFO: Pod "pod-11981b7a-790f-460a-bfd5-34274bddfdb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015988817s
STEP: Saw pod success
Jul 19 04:44:45.133: INFO: Pod "pod-11981b7a-790f-460a-bfd5-34274bddfdb4" satisfied condition "Succeeded or Failed"
Jul 19 04:44:45.136: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-11981b7a-790f-460a-bfd5-34274bddfdb4 container test-container: <nil>
STEP: delete the pod
Jul 19 04:44:45.187: INFO: Waiting for pod pod-11981b7a-790f-460a-bfd5-34274bddfdb4 to disappear
Jul 19 04:44:45.192: INFO: Pod pod-11981b7a-790f-460a-bfd5-34274bddfdb4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:45.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9885" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":110,"skipped":1969,"failed":0}

------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:45.217: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 19 04:44:45.875: INFO: starting watch
STEP: patching
STEP: updating
Jul 19 04:44:45.888: INFO: waiting for watch events with expected annotations
Jul 19 04:44:45.888: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:45.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8141" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":111,"skipped":1969,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:45.967: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 19 04:44:46.027: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 19 04:44:46.038: INFO: Waiting for terminating namespaces to be deleted...
Jul 19 04:44:46.042: INFO: 
Logging pods the apiserver thinks is on node paas-192-168-10-158 before test
Jul 19 04:44:46.070: INFO: cse-config-center-9cb5789db-hqgs8 from fst-manage started at 2021-07-15 02:47:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container cse-cc-comp ready: true, restart count 0
Jul 19 04:44:46.070: INFO: cse-etcd-0 from fst-manage started at 2021-07-15 02:45:36 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container cse-etcd-comp ready: true, restart count 0
Jul 19 04:44:46.070: INFO: cse-service-center-78c89955f8-9zxb8 from fst-manage started at 2021-07-15 02:47:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container cse-sc-comp ready: true, restart count 0
Jul 19 04:44:46.070: INFO: cspagentmanager-84df498dd6-756cg from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container cspagentmanager-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: csphaservice-85cfb95df6-rg2zf from fst-manage started at 2021-07-15 02:45:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container csphaservice-soft ready: true, restart count 0
Jul 19 04:44:46.070: INFO: cspnrsmaster-0 from fst-manage started at 2021-07-15 02:45:49 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container cspnrsmaster-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: cspommgr-7b4c7c76dc-lg5xn from fst-manage started at 2021-07-15 02:45:42 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container cspommgr-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: csposmgr-666d77c8cf-5nztr from fst-manage started at 2021-07-15 02:45:52 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container csposmgr-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: deploymgr-5fc4dfbc87-skqjv from fst-manage started at 2021-07-15 02:45:39 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container deploymgr-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: febs-599995cbb8-wch4b from fst-manage started at 2021-07-15 02:46:04 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container febs-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: fileserver-0 from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container fileserver-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: gaussdb-da5b9bc8-0 from fst-manage started at 2021-07-15 02:46:01 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container gauss-softwarecomp ready: true, restart count 0
Jul 19 04:44:46.070: INFO: kafka-da5b9bc8-0 from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container kafka-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: kubeaddon-upload-package-f5vtngqpi8-4m5dx from fst-manage started at 2021-07-15 02:44:30 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container kubeaddon-upload-package ready: false, restart count 0
Jul 19 04:44:46.070: INFO: mtcenter-57c48c94fd-89d27 from fst-manage started at 2021-07-15 02:45:35 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container cspmtcenter-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: omcache-da5b9bc8-0 from fst-manage started at 2021-07-15 02:45:56 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container redis-softwarecomp ready: true, restart count 0
Jul 19 04:44:46.070: INFO: omlb-5d58b5f84f-w9p5s from fst-manage started at 2021-07-15 03:35:19 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container omlb-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: pmscalc-5744fc9c5d-x6rlp from fst-manage started at 2021-07-15 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container pmscalc-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: pmsmgr-6857b85787-q6n5l from fst-manage started at 2021-07-15 02:45:37 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container pmsmgr-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: runlog-69f7c599d-krpbr from fst-manage started at 2021-07-15 02:45:58 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container runlog-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: secmgr-65cfb48b8b-2fzvs from fst-manage started at 2021-07-15 02:45:57 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container secmgr-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: soap-76794ccdc-xtknz from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container soap-soft ready: true, restart count 0
Jul 19 04:44:46.070: INFO: upgtool-app-559557759c-jpc94 from fst-manage started at 2021-07-15 02:45:54 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container upgtool-component ready: true, restart count 0
Jul 19 04:44:46.070: INFO: zookeeper-0 from fst-manage started at 2021-07-15 02:45:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container zookeeper-software ready: true, restart count 0
Jul 19 04:44:46.070: INFO: pod-projected-secrets-277b91b8-4c96-4d83-ba13-37c0237c495b from projected-8372 started at 2021-07-19 04:44:20 +0000 UTC (3 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container creates-volume-test ready: false, restart count 0
Jul 19 04:44:46.070: INFO: 	Container dels-volume-test ready: false, restart count 0
Jul 19 04:44:46.070: INFO: 	Container upds-volume-test ready: false, restart count 0
Jul 19 04:44:46.070: INFO: sonobuoy from sonobuoy started at 2021-07-19 03:55:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 19 04:44:46.070: INFO: sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-p7chc from sonobuoy started at 2021-07-19 03:55:51 +0000 UTC (2 container statuses recorded)
Jul 19 04:44:46.070: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 04:44:46.070: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 19 04:44:46.070: INFO: 
Logging pods the apiserver thinks is on node paas-192-168-10-183 before test
Jul 19 04:44:46.093: INFO: cse-config-center-9cb5789db-vhnp6 from fst-manage started at 2021-07-15 02:47:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container cse-cc-comp ready: true, restart count 0
Jul 19 04:44:46.093: INFO: cse-etcd-1 from fst-manage started at 2021-07-15 02:45:36 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container cse-etcd-comp ready: true, restart count 0
Jul 19 04:44:46.093: INFO: cse-service-center-78c89955f8-4kfjs from fst-manage started at 2021-07-15 02:47:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container cse-sc-comp ready: true, restart count 0
Jul 19 04:44:46.093: INFO: cspagentmanager-84df498dd6-fbzqf from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container cspagentmanager-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: csphaservice-85cfb95df6-qch6t from fst-manage started at 2021-07-15 02:45:40 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container csphaservice-soft ready: true, restart count 0
Jul 19 04:44:46.093: INFO: cspnrsmaster-1 from fst-manage started at 2021-07-15 02:45:49 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container cspnrsmaster-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: cspommgr-7b4c7c76dc-74p5c from fst-manage started at 2021-07-15 02:45:42 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container cspommgr-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: csposmgr-666d77c8cf-xqphh from fst-manage started at 2021-07-15 02:45:53 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container csposmgr-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: deploymgr-5fc4dfbc87-d4hmv from fst-manage started at 2021-07-15 02:45:39 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container deploymgr-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: febs-599995cbb8-qsp8z from fst-manage started at 2021-07-15 02:46:04 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container febs-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: fileserver-1 from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container fileserver-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: gaussdb-da5b9bc8-1 from fst-manage started at 2021-07-15 02:46:02 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container gauss-softwarecomp ready: true, restart count 0
Jul 19 04:44:46.093: INFO: kafka-da5b9bc8-1 from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container kafka-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: mtcenter-57c48c94fd-f8h98 from fst-manage started at 2021-07-15 02:45:35 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container cspmtcenter-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: omcache-da5b9bc8-1 from fst-manage started at 2021-07-15 02:45:56 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container redis-softwarecomp ready: true, restart count 0
Jul 19 04:44:46.093: INFO: omlb-5d58b5f84f-84kpc from fst-manage started at 2021-07-15 03:24:27 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container omlb-software ready: true, restart count 2
Jul 19 04:44:46.093: INFO: pmscalc-5744fc9c5d-h54gd from fst-manage started at 2021-07-15 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container pmscalc-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: pmsmgr-6857b85787-8n4th from fst-manage started at 2021-07-15 02:45:37 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container pmsmgr-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: psm-update-69dd9dc9cb-vddnv from fst-manage started at 2021-07-15 02:44:31 +0000 UTC (0 container statuses recorded)
Jul 19 04:44:46.093: INFO: runlog-69f7c599d-mszld from fst-manage started at 2021-07-15 02:45:59 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container runlog-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: secmgr-65cfb48b8b-zz9t8 from fst-manage started at 2021-07-15 02:45:57 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container secmgr-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: soap-76794ccdc-brpbn from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container soap-soft ready: true, restart count 0
Jul 19 04:44:46.093: INFO: upgtool-app-559557759c-ssmzk from fst-manage started at 2021-07-15 02:45:54 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container upgtool-component ready: true, restart count 0
Jul 19 04:44:46.093: INFO: zookeeper-1 from fst-manage started at 2021-07-15 02:45:41 +0000 UTC (1 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container zookeeper-software ready: true, restart count 0
Jul 19 04:44:46.093: INFO: sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-gtdqw from sonobuoy started at 2021-07-19 03:55:51 +0000 UTC (2 container statuses recorded)
Jul 19 04:44:46.093: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 04:44:46.093: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c17ee43f-fc5b-4964-8dd5-7316cfa56655 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c17ee43f-fc5b-4964-8dd5-7316cfa56655 off the node paas-192-168-10-183
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c17ee43f-fc5b-4964-8dd5-7316cfa56655
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:54.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8931" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.278 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":112,"skipped":1996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:54.245: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:44:54.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6405" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":113,"skipped":2023,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:44:54.427: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-1b4ab8f0-1cc6-4b05-a89f-4ef6c9095cf1
STEP: Creating configMap with name cm-test-opt-upd-18a36d02-1ead-45b2-90aa-657c6fef57ab
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1b4ab8f0-1cc6-4b05-a89f-4ef6c9095cf1
STEP: Updating configmap cm-test-opt-upd-18a36d02-1ead-45b2-90aa-657c6fef57ab
STEP: Creating configMap with name cm-test-opt-create-05c41d86-ec99-461e-a4f5-916f46fcfd17
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:46:25.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7786" for this suite.

• [SLOW TEST:90.956 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":2030,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:46:25.383: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 04:46:25.470: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48a9e41f-43c5-4e1d-b304-cb69f96197fd" in namespace "projected-8382" to be "Succeeded or Failed"
Jul 19 04:46:25.474: INFO: Pod "downwardapi-volume-48a9e41f-43c5-4e1d-b304-cb69f96197fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.495535ms
Jul 19 04:46:27.479: INFO: Pod "downwardapi-volume-48a9e41f-43c5-4e1d-b304-cb69f96197fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008588441s
Jul 19 04:46:29.484: INFO: Pod "downwardapi-volume-48a9e41f-43c5-4e1d-b304-cb69f96197fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013495555s
STEP: Saw pod success
Jul 19 04:46:29.484: INFO: Pod "downwardapi-volume-48a9e41f-43c5-4e1d-b304-cb69f96197fd" satisfied condition "Succeeded or Failed"
Jul 19 04:46:29.487: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-48a9e41f-43c5-4e1d-b304-cb69f96197fd container client-container: <nil>
STEP: delete the pod
Jul 19 04:46:29.552: INFO: Waiting for pod downwardapi-volume-48a9e41f-43c5-4e1d-b304-cb69f96197fd to disappear
Jul 19 04:46:29.556: INFO: Pod downwardapi-volume-48a9e41f-43c5-4e1d-b304-cb69f96197fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:46:29.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8382" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":115,"skipped":2030,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:46:29.571: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 19 04:46:33.658: INFO: &Pod{ObjectMeta:{send-events-184e09a2-c1d4-43a6-b62d-8e921e488a06  events-9779 /api/v1/namespaces/events-9779/pods/send-events-184e09a2-c1d4-43a6-b62d-8e921e488a06 a14e3871-3048-44f4-a646-a6418cf407fe 2420280 0 2021-07-19 04:46:29 +0000 UTC <nil> <nil> map[name:foo time:630168167] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f8gcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f8gcd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f8gcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:46:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:46:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:46:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:46:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:46:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:172.16.0.77,StartTime:2021-07-19 04:46:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 04:46:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b,ContainerID:docker://a9f64123b3a4a510a4fb1b303c2438f22c10dd498e6132faafc584c00d4c75d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jul 19 04:46:35.663: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 19 04:46:37.668: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:46:37.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9779" for this suite.

• [SLOW TEST:8.118 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":116,"skipped":2032,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:46:37.689: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 19 04:46:37.782: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3553 /api/v1/namespaces/watch-3553/configmaps/e2e-watch-test-watch-closed ecef9db7-6619-4f6d-acd1-358fcea5b025 2420319 0 2021-07-19 04:46:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 04:46:37.782: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3553 /api/v1/namespaces/watch-3553/configmaps/e2e-watch-test-watch-closed ecef9db7-6619-4f6d-acd1-358fcea5b025 2420320 0 2021-07-19 04:46:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 19 04:46:37.804: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3553 /api/v1/namespaces/watch-3553/configmaps/e2e-watch-test-watch-closed ecef9db7-6619-4f6d-acd1-358fcea5b025 2420321 0 2021-07-19 04:46:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 04:46:37.804: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3553 /api/v1/namespaces/watch-3553/configmaps/e2e-watch-test-watch-closed ecef9db7-6619-4f6d-acd1-358fcea5b025 2420322 0 2021-07-19 04:46:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:46:37.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3553" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":117,"skipped":2036,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:46:37.820: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:46:37.888: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 19 04:46:37.903: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 19 04:46:42.911: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 19 04:46:42.911: INFO: Creating deployment "test-rolling-update-deployment"
Jul 19 04:46:42.921: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 19 04:46:42.934: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 19 04:46:44.941: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 19 04:46:44.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266803, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266803, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266803, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266802, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7f559dfc67\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 04:46:46.949: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 19 04:46:46.962: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1894 /apis/apps/v1/namespaces/deployment-1894/deployments/test-rolling-update-deployment d0a53f84-c5f3-441a-81bd-b445eabcc804 2420423 1 2021-07-19 04:46:42 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00379c988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:1,MaxSurge:1,},},MinReadySeconds:0,RevisionHistoryLimit:*5,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-19 04:46:43 +0000 UTC,LastTransitionTime:2021-07-19 04:46:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7f559dfc67" has successfully progressed.,LastUpdateTime:2021-07-19 04:46:46 +0000 UTC,LastTransitionTime:2021-07-19 04:46:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 19 04:46:46.966: INFO: New ReplicaSet "test-rolling-update-deployment-7f559dfc67" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7f559dfc67  deployment-1894 /apis/apps/v1/namespaces/deployment-1894/replicasets/test-rolling-update-deployment-7f559dfc67 3bfb740f-feaa-494b-a112-4591f9a8519c 2420422 1 2021-07-19 04:46:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7f559dfc67] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/internalStrategyType:RollingUpdate deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d0a53f84-c5f3-441a-81bd-b445eabcc804 0xc00379cfb7 0xc00379cfb8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7f559dfc67,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7f559dfc67] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00379d058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 19 04:46:46.966: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 19 04:46:46.966: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1894 /apis/apps/v1/namespaces/deployment-1894/replicasets/test-rolling-update-controller b021b35a-815c-4cdd-a034-ef05ea8a0428 2420390 2 2021-07-19 04:46:37 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d0a53f84-c5f3-441a-81bd-b445eabcc804 0xc00379ce6f 0xc00379ceb0}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00379cf38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 04:46:46.970: INFO: Pod "test-rolling-update-deployment-7f559dfc67-sqwwq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7f559dfc67-sqwwq test-rolling-update-deployment-7f559dfc67- deployment-1894 /api/v1/namespaces/deployment-1894/pods/test-rolling-update-deployment-7f559dfc67-sqwwq c38f0a63-4942-49a4-9b0d-d36999a02bb4 2420421 0 2021-07-19 04:46:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7f559dfc67] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7f559dfc67 3bfb740f-feaa-494b-a112-4591f9a8519c 0xc00379d6b7 0xc00379d6b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2xtw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2xtw9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2xtw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:46:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:46:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:46:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:46:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:46:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:172.16.0.78,StartTime:2021-07-19 04:46:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 04:46:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b,ContainerID:docker://cdf3fae742356349a8557e4619517b0818044a305caf831c697f1b692c5d7871,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:46:46.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1894" for this suite.

• [SLOW TEST:9.162 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":118,"skipped":2043,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:46:46.982: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4137
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4137
STEP: creating replication controller externalsvc in namespace services-4137
I0719 04:46:47.091493      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4137, replica count: 2
I0719 04:46:50.141830      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 04:46:53.141970      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul 19 04:46:53.172: INFO: Creating new exec pod
Jul 19 04:46:57.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-4137 execpodf9znx -- /bin/sh -x -c nslookup nodeport-service.services-4137.svc.cluster.local'
Jul 19 04:46:57.562: INFO: stderr: "+ nslookup nodeport-service.services-4137.svc.cluster.local\n"
Jul 19 04:46:57.562: INFO: stdout: "Server:\t\t10.247.0.20\nAddress:\t10.247.0.20#53\n\nnodeport-service.services-4137.svc.cluster.local\tcanonical name = externalsvc.services-4137.svc.cluster.local.\nName:\texternalsvc.services-4137.svc.cluster.local\nAddress: 10.247.105.70\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4137, will wait for the garbage collector to delete the pods
Jul 19 04:46:57.627: INFO: Deleting ReplicationController externalsvc took: 10.541458ms
Jul 19 04:46:57.727: INFO: Terminating ReplicationController externalsvc pods took: 100.183495ms
Jul 19 04:47:19.559: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:47:19.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4137" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:32.632 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":119,"skipped":2048,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:47:19.614: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-2fff058c-fed9-4369-aa8b-11edb6056101
STEP: Creating a pod to test consume secrets
Jul 19 04:47:19.778: INFO: Waiting up to 5m0s for pod "pod-secrets-49af6436-cb72-4380-bcf5-fef9a66f3aa5" in namespace "secrets-5540" to be "Succeeded or Failed"
Jul 19 04:47:19.781: INFO: Pod "pod-secrets-49af6436-cb72-4380-bcf5-fef9a66f3aa5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081828ms
Jul 19 04:47:21.786: INFO: Pod "pod-secrets-49af6436-cb72-4380-bcf5-fef9a66f3aa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007676094s
Jul 19 04:47:23.791: INFO: Pod "pod-secrets-49af6436-cb72-4380-bcf5-fef9a66f3aa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012541026s
STEP: Saw pod success
Jul 19 04:47:23.791: INFO: Pod "pod-secrets-49af6436-cb72-4380-bcf5-fef9a66f3aa5" satisfied condition "Succeeded or Failed"
Jul 19 04:47:23.806: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-secrets-49af6436-cb72-4380-bcf5-fef9a66f3aa5 container secret-volume-test: <nil>
STEP: delete the pod
Jul 19 04:47:23.834: INFO: Waiting for pod pod-secrets-49af6436-cb72-4380-bcf5-fef9a66f3aa5 to disappear
Jul 19 04:47:23.837: INFO: Pod pod-secrets-49af6436-cb72-4380-bcf5-fef9a66f3aa5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:47:23.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5540" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":120,"skipped":2056,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:47:23.850: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 19 04:47:23.927: INFO: Waiting up to 5m0s for pod "pod-b49c732e-4d77-499e-b953-efcc6eb6406d" in namespace "emptydir-3685" to be "Succeeded or Failed"
Jul 19 04:47:23.930: INFO: Pod "pod-b49c732e-4d77-499e-b953-efcc6eb6406d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09739ms
Jul 19 04:47:25.933: INFO: Pod "pod-b49c732e-4d77-499e-b953-efcc6eb6406d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006893127s
Jul 19 04:47:27.937: INFO: Pod "pod-b49c732e-4d77-499e-b953-efcc6eb6406d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010697461s
STEP: Saw pod success
Jul 19 04:47:27.937: INFO: Pod "pod-b49c732e-4d77-499e-b953-efcc6eb6406d" satisfied condition "Succeeded or Failed"
Jul 19 04:47:27.941: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-b49c732e-4d77-499e-b953-efcc6eb6406d container test-container: <nil>
STEP: delete the pod
Jul 19 04:47:27.962: INFO: Waiting for pod pod-b49c732e-4d77-499e-b953-efcc6eb6406d to disappear
Jul 19 04:47:27.966: INFO: Pod pod-b49c732e-4d77-499e-b953-efcc6eb6406d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:47:27.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3685" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":2066,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:47:27.978: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Jul 19 04:47:28.047: INFO: Waiting up to 5m0s for pod "var-expansion-1c288b64-62ef-4c1a-a477-afd0ebeba31d" in namespace "var-expansion-9784" to be "Succeeded or Failed"
Jul 19 04:47:28.051: INFO: Pod "var-expansion-1c288b64-62ef-4c1a-a477-afd0ebeba31d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.988939ms
Jul 19 04:47:30.058: INFO: Pod "var-expansion-1c288b64-62ef-4c1a-a477-afd0ebeba31d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011473726s
Jul 19 04:47:32.062: INFO: Pod "var-expansion-1c288b64-62ef-4c1a-a477-afd0ebeba31d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015364743s
STEP: Saw pod success
Jul 19 04:47:32.063: INFO: Pod "var-expansion-1c288b64-62ef-4c1a-a477-afd0ebeba31d" satisfied condition "Succeeded or Failed"
Jul 19 04:47:32.066: INFO: Trying to get logs from node paas-192-168-10-183 pod var-expansion-1c288b64-62ef-4c1a-a477-afd0ebeba31d container dapi-container: <nil>
STEP: delete the pod
Jul 19 04:47:32.098: INFO: Waiting for pod var-expansion-1c288b64-62ef-4c1a-a477-afd0ebeba31d to disappear
Jul 19 04:47:32.103: INFO: Pod var-expansion-1c288b64-62ef-4c1a-a477-afd0ebeba31d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:47:32.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9784" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":122,"skipped":2070,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:47:32.116: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-25a3fe28-530b-471b-a85c-75c19191287c
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-25a3fe28-530b-471b-a85c-75c19191287c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:47:38.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5577" for this suite.

• [SLOW TEST:6.206 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":2076,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:47:38.322: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-871.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-871.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-871.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-871.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-871.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-871.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 19 04:47:42.477: INFO: Unable to read jessie_udp@PodARecord from pod dns-871/dns-test-d564bab0-07b8-4662-917e-ad950f44ef3b: the server could not find the requested resource (get pods dns-test-d564bab0-07b8-4662-917e-ad950f44ef3b)
Jul 19 04:47:42.480: INFO: Unable to read jessie_tcp@PodARecord from pod dns-871/dns-test-d564bab0-07b8-4662-917e-ad950f44ef3b: the server could not find the requested resource (get pods dns-test-d564bab0-07b8-4662-917e-ad950f44ef3b)
Jul 19 04:47:42.480: INFO: Lookups using dns-871/dns-test-d564bab0-07b8-4662-917e-ad950f44ef3b failed for: [jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul 19 04:47:47.520: INFO: DNS probes using dns-871/dns-test-d564bab0-07b8-4662-917e-ad950f44ef3b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:47:47.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-871" for this suite.

• [SLOW TEST:9.330 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":124,"skipped":2094,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:47:47.652: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 19 04:47:55.937: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:47:55.942: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 04:47:57.942: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:47:57.946: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 04:47:59.942: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:47:59.946: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 04:48:01.942: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:48:01.947: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 04:48:03.942: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:48:03.946: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 04:48:05.942: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:48:05.949: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 04:48:07.942: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:48:07.947: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 04:48:09.942: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:48:09.947: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 04:48:11.942: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:48:11.953: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 04:48:13.942: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:48:13.947: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 04:48:15.942: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 04:48:15.947: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:48:15.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-175" for this suite.

• [SLOW TEST:28.311 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":2103,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:48:15.963: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:48:16.490: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 04:48:18.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266896, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266896, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266896, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266896, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:48:21.523: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:48:21.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9685" for this suite.
STEP: Destroying namespace "webhook-9685-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":126,"skipped":2110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:48:21.637: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:48:21.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-4504" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":127,"skipped":2139,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:48:21.753: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 19 04:48:26.363: INFO: Successfully updated pod "labelsupdate938169ea-92cb-489f-974f-281534ed2be6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:48:28.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-491" for this suite.

• [SLOW TEST:6.651 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":128,"skipped":2151,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:48:28.404: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-0074787f-1359-4517-9da8-82e22bd15bef
STEP: Creating a pod to test consume configMaps
Jul 19 04:48:28.489: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b09323e2-c0f8-4e00-bd6f-a9ae2ff482cb" in namespace "projected-6265" to be "Succeeded or Failed"
Jul 19 04:48:28.493: INFO: Pod "pod-projected-configmaps-b09323e2-c0f8-4e00-bd6f-a9ae2ff482cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.123785ms
Jul 19 04:48:30.496: INFO: Pod "pod-projected-configmaps-b09323e2-c0f8-4e00-bd6f-a9ae2ff482cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006884324s
Jul 19 04:48:32.504: INFO: Pod "pod-projected-configmaps-b09323e2-c0f8-4e00-bd6f-a9ae2ff482cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01472206s
STEP: Saw pod success
Jul 19 04:48:32.504: INFO: Pod "pod-projected-configmaps-b09323e2-c0f8-4e00-bd6f-a9ae2ff482cb" satisfied condition "Succeeded or Failed"
Jul 19 04:48:32.508: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-projected-configmaps-b09323e2-c0f8-4e00-bd6f-a9ae2ff482cb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 04:48:32.536: INFO: Waiting for pod pod-projected-configmaps-b09323e2-c0f8-4e00-bd6f-a9ae2ff482cb to disappear
Jul 19 04:48:32.540: INFO: Pod pod-projected-configmaps-b09323e2-c0f8-4e00-bd6f-a9ae2ff482cb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:48:32.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6265" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":129,"skipped":2151,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:48:32.553: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8158
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8158
I0719 04:48:32.692558      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8158, replica count: 2
I0719 04:48:35.742887      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 04:48:38.743036      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 04:48:38.743: INFO: Creating new exec pod
Jul 19 04:48:43.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-8158 execpodxjp66 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 19 04:48:44.464: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 19 04:48:44.464: INFO: stdout: ""
Jul 19 04:48:44.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-8158 execpodxjp66 -- /bin/sh -x -c nc -zv -t -w 2 10.247.175.157 80'
Jul 19 04:48:44.722: INFO: stderr: "+ nc -zv -t -w 2 10.247.175.157 80\nConnection to 10.247.175.157 80 port [tcp/http] succeeded!\n"
Jul 19 04:48:44.722: INFO: stdout: ""
Jul 19 04:48:44.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-8158 execpodxjp66 -- /bin/sh -x -c nc -zv -t -w 2 192.168.10.158 31601'
Jul 19 04:48:45.008: INFO: stderr: "+ nc -zv -t -w 2 192.168.10.158 31601\nConnection to 192.168.10.158 31601 port [tcp/31601] succeeded!\n"
Jul 19 04:48:45.008: INFO: stdout: ""
Jul 19 04:48:45.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-8158 execpodxjp66 -- /bin/sh -x -c nc -zv -t -w 2 192.168.10.183 31601'
Jul 19 04:48:45.280: INFO: stderr: "+ nc -zv -t -w 2 192.168.10.183 31601\nConnection to 192.168.10.183 31601 port [tcp/31601] succeeded!\n"
Jul 19 04:48:45.280: INFO: stdout: ""
Jul 19 04:48:45.280: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:48:45.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8158" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:12.817 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":130,"skipped":2162,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:48:45.370: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:48:45.471: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 19 04:48:47.517: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:48:47.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6393" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":131,"skipped":2167,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:48:47.535: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:48:47.860: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 04:48:49.873: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266927, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266927, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266927, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266927, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 04:48:51.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266927, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266927, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266927, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762266927, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:48:54.895: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:48:55.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5071" for this suite.
STEP: Destroying namespace "webhook-5071-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.689 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":132,"skipped":2204,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:48:55.225: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 04:48:55.479: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9155ec31-664d-4ef8-9275-00a181641e9e" in namespace "downward-api-8288" to be "Succeeded or Failed"
Jul 19 04:48:55.497: INFO: Pod "downwardapi-volume-9155ec31-664d-4ef8-9275-00a181641e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.801421ms
Jul 19 04:48:57.500: INFO: Pod "downwardapi-volume-9155ec31-664d-4ef8-9275-00a181641e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021170616s
Jul 19 04:48:59.505: INFO: Pod "downwardapi-volume-9155ec31-664d-4ef8-9275-00a181641e9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025901033s
STEP: Saw pod success
Jul 19 04:48:59.505: INFO: Pod "downwardapi-volume-9155ec31-664d-4ef8-9275-00a181641e9e" satisfied condition "Succeeded or Failed"
Jul 19 04:48:59.509: INFO: Trying to get logs from node paas-192-168-10-158 pod downwardapi-volume-9155ec31-664d-4ef8-9275-00a181641e9e container client-container: <nil>
STEP: delete the pod
Jul 19 04:48:59.534: INFO: Waiting for pod downwardapi-volume-9155ec31-664d-4ef8-9275-00a181641e9e to disappear
Jul 19 04:48:59.538: INFO: Pod downwardapi-volume-9155ec31-664d-4ef8-9275-00a181641e9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:48:59.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8288" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2258,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:48:59.556: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3641
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 19 04:48:59.625: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 19 04:48:59.660: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 04:49:01.665: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 04:49:03.665: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:49:05.664: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:49:07.664: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:49:09.664: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:49:11.665: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:49:13.664: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 04:49:15.664: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 19 04:49:15.670: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 19 04:49:17.676: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 19 04:49:19.675: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 19 04:49:23.725: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.0.31:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3641 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:49:23.725: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:49:24.291: INFO: Found all expected endpoints: [netserver-0]
Jul 19 04:49:24.295: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.0.76:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3641 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:49:24.295: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:49:24.471: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:49:24.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3641" for this suite.

• [SLOW TEST:24.941 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":134,"skipped":2271,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:49:24.497: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 19 04:49:24.564: INFO: Waiting up to 5m0s for pod "downward-api-56112f2d-d110-43b9-8c65-b6429d321ecf" in namespace "downward-api-2870" to be "Succeeded or Failed"
Jul 19 04:49:24.570: INFO: Pod "downward-api-56112f2d-d110-43b9-8c65-b6429d321ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.811495ms
Jul 19 04:49:26.575: INFO: Pod "downward-api-56112f2d-d110-43b9-8c65-b6429d321ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01112002s
Jul 19 04:49:28.579: INFO: Pod "downward-api-56112f2d-d110-43b9-8c65-b6429d321ecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015476421s
STEP: Saw pod success
Jul 19 04:49:28.579: INFO: Pod "downward-api-56112f2d-d110-43b9-8c65-b6429d321ecf" satisfied condition "Succeeded or Failed"
Jul 19 04:49:28.583: INFO: Trying to get logs from node paas-192-168-10-183 pod downward-api-56112f2d-d110-43b9-8c65-b6429d321ecf container dapi-container: <nil>
STEP: delete the pod
Jul 19 04:49:28.607: INFO: Waiting for pod downward-api-56112f2d-d110-43b9-8c65-b6429d321ecf to disappear
Jul 19 04:49:28.611: INFO: Pod downward-api-56112f2d-d110-43b9-8c65-b6429d321ecf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:49:28.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2870" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2281,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:49:28.627: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Jul 19 04:49:28.697: INFO: Waiting up to 5m0s for pod "var-expansion-547d7ce6-9b95-486e-b464-aaea1ee71fa7" in namespace "var-expansion-1805" to be "Succeeded or Failed"
Jul 19 04:49:28.701: INFO: Pod "var-expansion-547d7ce6-9b95-486e-b464-aaea1ee71fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369347ms
Jul 19 04:49:30.705: INFO: Pod "var-expansion-547d7ce6-9b95-486e-b464-aaea1ee71fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007824615s
Jul 19 04:49:32.710: INFO: Pod "var-expansion-547d7ce6-9b95-486e-b464-aaea1ee71fa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012786904s
STEP: Saw pod success
Jul 19 04:49:32.710: INFO: Pod "var-expansion-547d7ce6-9b95-486e-b464-aaea1ee71fa7" satisfied condition "Succeeded or Failed"
Jul 19 04:49:32.713: INFO: Trying to get logs from node paas-192-168-10-158 pod var-expansion-547d7ce6-9b95-486e-b464-aaea1ee71fa7 container dapi-container: <nil>
STEP: delete the pod
Jul 19 04:49:32.767: INFO: Waiting for pod var-expansion-547d7ce6-9b95-486e-b464-aaea1ee71fa7 to disappear
Jul 19 04:49:32.770: INFO: Pod var-expansion-547d7ce6-9b95-486e-b464-aaea1ee71fa7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:49:32.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1805" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":136,"skipped":2293,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:49:32.783: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 19 04:49:32.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2593'
Jul 19 04:49:32.951: INFO: stderr: ""
Jul 19 04:49:32.951: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul 19 04:49:38.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pod e2e-test-httpd-pod --namespace=kubectl-2593 -o json'
Jul 19 04:49:38.124: INFO: stderr: ""
Jul 19 04:49:38.124: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-07-19T04:49:32Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2593\",\n        \"resourceVersion\": \"2422112\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2593/pods/e2e-test-httpd-pod\",\n        \"uid\": \"838dba3c-a16d-48d7-8bf9-11410c29285a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBE_POD_NODE_NAME\",\n                        \"valueFrom\": {\n                            \"fieldRef\": {\n                                \"apiVersion\": \"v1\",\n                                \"fieldPath\": \"spec.nodeName\"\n                            }\n                        }\n                    },\n                    {\n                        \"name\": \"KUBE_POD_NAMESPACE\",\n                        \"valueFrom\": {\n                            \"fieldRef\": {\n                                \"apiVersion\": \"v1\",\n                                \"fieldPath\": \"metadata.namespace\"\n                            }\n                        }\n                    },\n                    {\n                        \"name\": \"KUBE_POD_CLUSTERID\"\n                    }\n                ],\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-qg82p\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"paas-192-168-10-158\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-qg82p\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-qg82p\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-19T04:49:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-19T04:49:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-19T04:49:35Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-19T04:49:32Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-19T04:49:32Z\",\n                \"status\": \"True\",\n                \"type\": \"ResourceAllocated\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://fcc9916ec8383fd692a484aa7ad2b5efa607d385e8369f1e2c8cf46bfa4b1bec\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-07-19T04:49:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.10.158\",\n        \"managementIP\": \"192.168.10.158\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.0.19\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.0.19\"\n            }\n        ],\n        \"podNetworks\": [\n            {\n                \"iP\": [\n                    \"172.16.0.19\"\n                ],\n                \"name\": \"eth0\",\n                \"network\": \"fst-manage\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-07-19T04:49:32Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 19 04:49:38.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 replace -f - --namespace=kubectl-2593'
Jul 19 04:49:38.424: INFO: stderr: ""
Jul 19 04:49:38.424: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Jul 19 04:49:38.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete pods e2e-test-httpd-pod --namespace=kubectl-2593'
Jul 19 04:49:59.444: INFO: stderr: ""
Jul 19 04:49:59.444: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:49:59.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2593" for this suite.

• [SLOW TEST:26.677 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":137,"skipped":2297,"failed":0}
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:49:59.460: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jul 19 04:49:59.525: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 04:50:59.621: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:50:59.624: INFO: Starting informer...
STEP: Starting pods...
Jul 19 04:50:59.889: INFO: Pod1 is running on paas-192-168-10-183. Tainting Node
Jul 19 04:51:04.184: INFO: Pod2 is running on paas-192-168-10-183. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul 19 04:51:24.375: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 19 04:51:51.003: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:51:51.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7892" for this suite.

• [SLOW TEST:111.608 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":138,"skipped":2297,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:51:51.068: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-vr6s
STEP: Creating a pod to test atomic-volume-subpath
Jul 19 04:51:51.975: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vr6s" in namespace "subpath-8448" to be "Succeeded or Failed"
Jul 19 04:51:51.979: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.237669ms
Jul 19 04:51:53.983: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008831929s
Jul 19 04:51:55.992: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01779672s
Jul 19 04:51:57.996: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021754897s
Jul 19 04:52:00.000: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025479264s
Jul 19 04:52:02.005: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029937454s
Jul 19 04:52:04.009: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034156454s
Jul 19 04:52:06.017: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Running", Reason="", readiness=true. Elapsed: 14.042808527s
Jul 19 04:52:08.021: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Running", Reason="", readiness=true. Elapsed: 16.04674648s
Jul 19 04:52:10.025: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Running", Reason="", readiness=true. Elapsed: 18.050291279s
Jul 19 04:52:12.030: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Running", Reason="", readiness=true. Elapsed: 20.054953303s
Jul 19 04:52:14.035: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Running", Reason="", readiness=true. Elapsed: 22.059835462s
Jul 19 04:52:16.039: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Running", Reason="", readiness=true. Elapsed: 24.064248874s
Jul 19 04:52:18.043: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Running", Reason="", readiness=true. Elapsed: 26.067929713s
Jul 19 04:52:20.047: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Running", Reason="", readiness=true. Elapsed: 28.072395851s
Jul 19 04:52:22.052: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Running", Reason="", readiness=true. Elapsed: 30.076958449s
Jul 19 04:52:24.057: INFO: Pod "pod-subpath-test-configmap-vr6s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.081987628s
STEP: Saw pod success
Jul 19 04:52:24.057: INFO: Pod "pod-subpath-test-configmap-vr6s" satisfied condition "Succeeded or Failed"
Jul 19 04:52:24.061: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-subpath-test-configmap-vr6s container test-container-subpath-configmap-vr6s: <nil>
STEP: delete the pod
Jul 19 04:52:24.935: INFO: Waiting for pod pod-subpath-test-configmap-vr6s to disappear
Jul 19 04:52:24.940: INFO: Pod pod-subpath-test-configmap-vr6s no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vr6s
Jul 19 04:52:24.940: INFO: Deleting pod "pod-subpath-test-configmap-vr6s" in namespace "subpath-8448"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:52:24.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8448" for this suite.

• [SLOW TEST:33.891 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":139,"skipped":2302,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:52:24.959: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:52:25.037: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 19 04:52:28.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-1900 create -f -'
Jul 19 04:52:29.269: INFO: stderr: ""
Jul 19 04:52:29.269: INFO: stdout: "e2e-test-crd-publish-openapi-9009-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 19 04:52:29.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-1900 delete e2e-test-crd-publish-openapi-9009-crds test-cr'
Jul 19 04:52:29.367: INFO: stderr: ""
Jul 19 04:52:29.367: INFO: stdout: "e2e-test-crd-publish-openapi-9009-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 19 04:52:29.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-1900 apply -f -'
Jul 19 04:52:29.592: INFO: stderr: ""
Jul 19 04:52:29.592: INFO: stdout: "e2e-test-crd-publish-openapi-9009-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 19 04:52:29.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-1900 delete e2e-test-crd-publish-openapi-9009-crds test-cr'
Jul 19 04:52:29.688: INFO: stderr: ""
Jul 19 04:52:29.688: INFO: stdout: "e2e-test-crd-publish-openapi-9009-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 19 04:52:29.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 explain e2e-test-crd-publish-openapi-9009-crds'
Jul 19 04:52:29.883: INFO: stderr: ""
Jul 19 04:52:29.883: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9009-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:52:33.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1900" for this suite.

• [SLOW TEST:8.442 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":140,"skipped":2302,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:52:33.402: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 19 04:52:33.477: INFO: Waiting up to 5m0s for pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225" in namespace "emptydir-5916" to be "Succeeded or Failed"
Jul 19 04:52:33.480: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 3.026676ms
Jul 19 04:52:35.483: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006757893s
Jul 19 04:52:37.488: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011403349s
Jul 19 04:52:39.493: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015819277s
Jul 19 04:52:41.498: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020907678s
Jul 19 04:52:43.502: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025291534s
Jul 19 04:52:45.506: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029150597s
Jul 19 04:52:47.510: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 14.033588234s
Jul 19 04:52:49.515: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 16.038128363s
Jul 19 04:52:51.519: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 18.041987723s
Jul 19 04:52:53.523: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 20.046730438s
Jul 19 04:52:55.527: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 22.050656536s
Jul 19 04:52:57.533: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 24.056377559s
Jul 19 04:52:59.537: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 26.060742801s
Jul 19 04:53:01.542: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 28.065729218s
Jul 19 04:53:03.547: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 30.069984743s
Jul 19 04:53:05.551: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 32.074002009s
Jul 19 04:53:07.556: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 34.079383901s
Jul 19 04:53:09.560: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 36.083736593s
Jul 19 04:53:11.564: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 38.087285942s
Jul 19 04:53:13.569: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 40.092040257s
Jul 19 04:53:15.573: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 42.096063403s
Jul 19 04:53:17.578: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 44.100796624s
Jul 19 04:53:19.581: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 46.104664697s
Jul 19 04:53:21.586: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 48.109102649s
Jul 19 04:53:23.591: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 50.11393283s
Jul 19 04:53:25.594: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 52.117470252s
Jul 19 04:53:27.599: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 54.121822573s
Jul 19 04:53:29.602: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Pending", Reason="", readiness=false. Elapsed: 56.125719908s
Jul 19 04:53:31.607: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Running", Reason="", readiness=true. Elapsed: 58.12977022s
Jul 19 04:53:33.613: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Running", Reason="", readiness=true. Elapsed: 1m0.135866376s
Jul 19 04:53:35.617: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Running", Reason="", readiness=true. Elapsed: 1m2.140070115s
Jul 19 04:53:37.622: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Running", Reason="", readiness=true. Elapsed: 1m4.144772461s
Jul 19 04:53:39.626: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Running", Reason="", readiness=true. Elapsed: 1m6.148794595s
Jul 19 04:53:41.629: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Running", Reason="", readiness=true. Elapsed: 1m8.152597379s
Jul 19 04:53:43.634: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Running", Reason="", readiness=true. Elapsed: 1m10.157587376s
Jul 19 04:53:45.639: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Running", Reason="", readiness=true. Elapsed: 1m12.162177508s
Jul 19 04:53:47.643: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Running", Reason="", readiness=true. Elapsed: 1m14.166157286s
Jul 19 04:53:49.648: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m16.170839389s
STEP: Saw pod success
Jul 19 04:53:49.648: INFO: Pod "pod-b86bc43e-91d8-442f-bdd0-8e0617db4225" satisfied condition "Succeeded or Failed"
Jul 19 04:53:49.669: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-b86bc43e-91d8-442f-bdd0-8e0617db4225 container test-container: <nil>
STEP: delete the pod
Jul 19 04:53:49.694: INFO: Waiting for pod pod-b86bc43e-91d8-442f-bdd0-8e0617db4225 to disappear
Jul 19 04:53:49.697: INFO: Pod pod-b86bc43e-91d8-442f-bdd0-8e0617db4225 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:53:49.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5916" for this suite.

• [SLOW TEST:76.306 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":141,"skipped":2309,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:53:49.708: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-3282
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3282
STEP: Deleting pre-stop pod
Jul 19 04:55:22.875: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:55:22.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3282" for this suite.

• [SLOW TEST:93.191 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":142,"skipped":2320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:55:22.899: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Jul 19 04:55:22.962: INFO: Waiting up to 5m0s for pod "client-containers-c73a8484-dfd9-4ff5-9901-7caeba7cb585" in namespace "containers-4694" to be "Succeeded or Failed"
Jul 19 04:55:22.965: INFO: Pod "client-containers-c73a8484-dfd9-4ff5-9901-7caeba7cb585": Phase="Pending", Reason="", readiness=false. Elapsed: 2.846542ms
Jul 19 04:55:24.969: INFO: Pod "client-containers-c73a8484-dfd9-4ff5-9901-7caeba7cb585": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006982301s
Jul 19 04:55:26.973: INFO: Pod "client-containers-c73a8484-dfd9-4ff5-9901-7caeba7cb585": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011283811s
STEP: Saw pod success
Jul 19 04:55:26.973: INFO: Pod "client-containers-c73a8484-dfd9-4ff5-9901-7caeba7cb585" satisfied condition "Succeeded or Failed"
Jul 19 04:55:26.977: INFO: Trying to get logs from node paas-192-168-10-158 pod client-containers-c73a8484-dfd9-4ff5-9901-7caeba7cb585 container test-container: <nil>
STEP: delete the pod
Jul 19 04:55:27.045: INFO: Waiting for pod client-containers-c73a8484-dfd9-4ff5-9901-7caeba7cb585 to disappear
Jul 19 04:55:27.049: INFO: Pod client-containers-c73a8484-dfd9-4ff5-9901-7caeba7cb585 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:55:27.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4694" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":143,"skipped":2342,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:55:27.063: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-8643b799-8896-4b48-80a0-1289301b860a
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:55:27.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5381" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":144,"skipped":2347,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:55:27.141: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 19 04:55:27.205: INFO: Waiting up to 5m0s for pod "pod-1f22da7c-f3a3-4eca-a704-301552b33548" in namespace "emptydir-8309" to be "Succeeded or Failed"
Jul 19 04:55:27.209: INFO: Pod "pod-1f22da7c-f3a3-4eca-a704-301552b33548": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056225ms
Jul 19 04:55:29.216: INFO: Pod "pod-1f22da7c-f3a3-4eca-a704-301552b33548": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011244765s
Jul 19 04:55:31.222: INFO: Pod "pod-1f22da7c-f3a3-4eca-a704-301552b33548": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017417329s
STEP: Saw pod success
Jul 19 04:55:31.222: INFO: Pod "pod-1f22da7c-f3a3-4eca-a704-301552b33548" satisfied condition "Succeeded or Failed"
Jul 19 04:55:31.226: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-1f22da7c-f3a3-4eca-a704-301552b33548 container test-container: <nil>
STEP: delete the pod
Jul 19 04:55:31.297: INFO: Waiting for pod pod-1f22da7c-f3a3-4eca-a704-301552b33548 to disappear
Jul 19 04:55:31.301: INFO: Pod pod-1f22da7c-f3a3-4eca-a704-301552b33548 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:55:31.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8309" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2374,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:55:31.410: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Jul 19 04:55:31.494: INFO: Waiting up to 5m0s for pod "var-expansion-3ef00d51-7ea3-4424-b0f8-ddd714214faf" in namespace "var-expansion-9799" to be "Succeeded or Failed"
Jul 19 04:55:31.498: INFO: Pod "var-expansion-3ef00d51-7ea3-4424-b0f8-ddd714214faf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.26401ms
Jul 19 04:55:33.502: INFO: Pod "var-expansion-3ef00d51-7ea3-4424-b0f8-ddd714214faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007800735s
Jul 19 04:55:35.507: INFO: Pod "var-expansion-3ef00d51-7ea3-4424-b0f8-ddd714214faf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012470971s
STEP: Saw pod success
Jul 19 04:55:35.507: INFO: Pod "var-expansion-3ef00d51-7ea3-4424-b0f8-ddd714214faf" satisfied condition "Succeeded or Failed"
Jul 19 04:55:35.511: INFO: Trying to get logs from node paas-192-168-10-183 pod var-expansion-3ef00d51-7ea3-4424-b0f8-ddd714214faf container dapi-container: <nil>
STEP: delete the pod
Jul 19 04:55:35.539: INFO: Waiting for pod var-expansion-3ef00d51-7ea3-4424-b0f8-ddd714214faf to disappear
Jul 19 04:55:35.544: INFO: Pod var-expansion-3ef00d51-7ea3-4424-b0f8-ddd714214faf no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:55:35.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9799" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":146,"skipped":2384,"failed":0}

------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:55:35.558: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:55:35.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4908" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":147,"skipped":2384,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:55:35.664: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-4945
STEP: creating replication controller nodeport-test in namespace services-4945
I0719 04:55:35.778770      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-4945, replica count: 2
I0719 04:55:38.829041      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 04:55:41.829: INFO: Creating new exec pod
I0719 04:55:41.829245      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 04:55:46.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-4945 execpodspjjr -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jul 19 04:55:47.144: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 19 04:55:47.144: INFO: stdout: ""
Jul 19 04:55:47.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-4945 execpodspjjr -- /bin/sh -x -c nc -zv -t -w 2 10.247.185.34 80'
Jul 19 04:55:47.407: INFO: stderr: "+ nc -zv -t -w 2 10.247.185.34 80\nConnection to 10.247.185.34 80 port [tcp/http] succeeded!\n"
Jul 19 04:55:47.407: INFO: stdout: ""
Jul 19 04:55:47.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-4945 execpodspjjr -- /bin/sh -x -c nc -zv -t -w 2 192.168.10.158 30519'
Jul 19 04:55:47.705: INFO: stderr: "+ nc -zv -t -w 2 192.168.10.158 30519\nConnection to 192.168.10.158 30519 port [tcp/30519] succeeded!\n"
Jul 19 04:55:47.705: INFO: stdout: ""
Jul 19 04:55:47.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-4945 execpodspjjr -- /bin/sh -x -c nc -zv -t -w 2 192.168.10.183 30519'
Jul 19 04:55:47.969: INFO: stderr: "+ nc -zv -t -w 2 192.168.10.183 30519\nConnection to 192.168.10.183 30519 port [tcp/30519] succeeded!\n"
Jul 19 04:55:47.969: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:55:47.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4945" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:12.335 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":148,"skipped":2392,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:55:48.000: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-d8bl
STEP: Creating a pod to test atomic-volume-subpath
Jul 19 04:55:48.095: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d8bl" in namespace "subpath-4308" to be "Succeeded or Failed"
Jul 19 04:55:48.101: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.177973ms
Jul 19 04:55:50.106: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010278095s
Jul 19 04:55:52.110: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Running", Reason="", readiness=true. Elapsed: 4.014388149s
Jul 19 04:55:54.115: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Running", Reason="", readiness=true. Elapsed: 6.019336316s
Jul 19 04:55:56.119: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Running", Reason="", readiness=true. Elapsed: 8.023473423s
Jul 19 04:55:58.123: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Running", Reason="", readiness=true. Elapsed: 10.027871017s
Jul 19 04:56:00.128: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Running", Reason="", readiness=true. Elapsed: 12.032227786s
Jul 19 04:56:02.134: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Running", Reason="", readiness=true. Elapsed: 14.038229724s
Jul 19 04:56:04.139: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Running", Reason="", readiness=true. Elapsed: 16.043253342s
Jul 19 04:56:06.143: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Running", Reason="", readiness=true. Elapsed: 18.047436799s
Jul 19 04:56:08.162: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Running", Reason="", readiness=true. Elapsed: 20.066652705s
Jul 19 04:56:10.167: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Running", Reason="", readiness=true. Elapsed: 22.07140518s
Jul 19 04:56:12.176: INFO: Pod "pod-subpath-test-configmap-d8bl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.080595434s
STEP: Saw pod success
Jul 19 04:56:12.176: INFO: Pod "pod-subpath-test-configmap-d8bl" satisfied condition "Succeeded or Failed"
Jul 19 04:56:12.180: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-subpath-test-configmap-d8bl container test-container-subpath-configmap-d8bl: <nil>
STEP: delete the pod
Jul 19 04:56:12.210: INFO: Waiting for pod pod-subpath-test-configmap-d8bl to disappear
Jul 19 04:56:12.213: INFO: Pod pod-subpath-test-configmap-d8bl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d8bl
Jul 19 04:56:12.213: INFO: Deleting pod "pod-subpath-test-configmap-d8bl" in namespace "subpath-4308"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:56:12.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4308" for this suite.

• [SLOW TEST:24.232 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":149,"skipped":2393,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:56:12.232: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 19 04:56:16.856: INFO: Successfully updated pod "labelsupdateddef444c-c51c-4c60-869f-150338a30b6c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:56:20.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8738" for this suite.

• [SLOW TEST:8.672 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":150,"skipped":2394,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:56:20.905: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:56:20.971: INFO: Creating deployment "webserver-deployment"
Jul 19 04:56:20.978: INFO: Waiting for observed generation 1
Jul 19 04:56:22.986: INFO: Waiting for all required pods to come up
Jul 19 04:56:22.990: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 19 04:56:29.026: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 19 04:56:29.032: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 19 04:56:29.049: INFO: Updating deployment webserver-deployment
Jul 19 04:56:29.049: INFO: Waiting for observed generation 2
Jul 19 04:56:31.061: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 19 04:56:31.064: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 19 04:56:31.067: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 19 04:56:31.081: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 19 04:56:31.081: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 19 04:56:31.084: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 19 04:56:31.093: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 19 04:56:31.093: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 19 04:56:31.102: INFO: Updating deployment webserver-deployment
Jul 19 04:56:31.102: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 19 04:56:31.120: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 19 04:56:31.128: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 19 04:56:31.214: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6812 /apis/apps/v1/namespaces/deployment-6812/deployments/webserver-deployment b3eeaf6c-a348-41d6-8ed4-aea1962ed48c 2425726 3 2021-07-19 04:56:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035fb4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*5,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9db756f4" is progressing.,LastUpdateTime:2021-07-19 04:56:29 +0000 UTC,LastTransitionTime:2021-07-19 04:56:20 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-19 04:56:31 +0000 UTC,LastTransitionTime:2021-07-19 04:56:31 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 19 04:56:31.243: INFO: New ReplicaSet "webserver-deployment-d9db756f4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9db756f4  deployment-6812 /apis/apps/v1/namespaces/deployment-6812/replicasets/webserver-deployment-d9db756f4 4abe5be7-c420-4831-b2df-437f898547f7 2425721 3 2021-07-19 04:56:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9db756f4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/internalStrategyType:RollingUpdate deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b3eeaf6c-a348-41d6-8ed4-aea1962ed48c 0xc0035fbdf7 0xc0035fbdf8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9db756f4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9db756f4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035fbef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 04:56:31.243: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 19 04:56:31.243: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-d6c5f64cc  deployment-6812 /apis/apps/v1/namespaces/deployment-6812/replicasets/webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 2425720 3 2021-07-19 04:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/internalStrategyType:RollingUpdate deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b3eeaf6c-a348-41d6-8ed4-aea1962ed48c 0xc0035fbd07 0xc0035fbd08}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d6c5f64cc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035fbd68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 19 04:56:31.316: INFO: Pod "webserver-deployment-d6c5f64cc-6cwhl" is available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-6cwhl webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-6cwhl a6e420cb-e643-4a73-8b8b-2d8c17e3b049 2425644 0 2021-07-19 04:56:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b8ce7 0xc0030b8ce8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:172.16.0.75,StartTime:2021-07-19 04:56:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 04:56:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://16480fbdd0424a6f55bd8406b9f27954be2d3fd33ef658094ae7f06474cf6ee2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.316: INFO: Pod "webserver-deployment-d6c5f64cc-6hrj9" is available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-6hrj9 webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-6hrj9 38a4e4e6-322f-4e11-9996-923e12731b25 2425639 0 2021-07-19 04:56:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b8e67 0xc0030b8e68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.158,PodIP:172.16.0.28,StartTime:2021-07-19 04:56:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 04:56:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://daf4549bf0d690a4f3d7d74c168a3375155f74d93e8f38c549cb45291583425b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.316: INFO: Pod "webserver-deployment-d6c5f64cc-6hzrv" is not available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-6hzrv webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-6hzrv ac73ddeb-d7ca-4176-987b-0b41d40d8b00 2425732 0 2021-07-19 04:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b8fe7 0xc0030b8fe8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.316: INFO: Pod "webserver-deployment-d6c5f64cc-7wd6v" is available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-7wd6v webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-7wd6v d5432b79-b409-4b3c-82f7-ba6285a4861b 2425612 0 2021-07-19 04:56:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b90f0 0xc0030b90f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.158,PodIP:172.16.0.27,StartTime:2021-07-19 04:56:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 04:56:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://be905f3a26c4c0151a15eb93a23bde3a7425429e029e8ec589909b11376dd12a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.316: INFO: Pod "webserver-deployment-d6c5f64cc-8qn6n" is not available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-8qn6n webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-8qn6n 109b5467-0eab-468c-be9c-d791cf7c3784 2425728 0 2021-07-19 04:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b9267 0xc0030b9268}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.316: INFO: Pod "webserver-deployment-d6c5f64cc-dlfzr" is available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-dlfzr webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-dlfzr 144a9882-1f3b-4752-a0c8-7a94ddb25c44 2425635 0 2021-07-19 04:56:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b93a7 0xc0030b93a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:172.16.0.73,StartTime:2021-07-19 04:56:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 04:56:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://2aed1ac316a7bcdb8eb750f0122fb05967f8ffaa80e31a2187a5414f44d211b1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.317: INFO: Pod "webserver-deployment-d6c5f64cc-fjhbb" is available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-fjhbb webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-fjhbb 82b9650f-c3af-48ce-9d1a-b78fb377a219 2425610 0 2021-07-19 04:56:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b9967 0xc0030b9968}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.158,PodIP:172.16.0.25,StartTime:2021-07-19 04:56:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 04:56:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://1a29b89a7ef8dcbfa7bc7292a07e479ff41a70fc6597401da8132269d048dcd7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.317: INFO: Pod "webserver-deployment-d6c5f64cc-l6pwz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-l6pwz webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-l6pwz e09d5f1e-dfbe-4549-a73e-7e5aa12c6194 2425731 0 2021-07-19 04:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b9af7 0xc0030b9af8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.317: INFO: Pod "webserver-deployment-d6c5f64cc-mtdpd" is available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-mtdpd webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-mtdpd a1d7a3de-d67a-4dd5-b7ff-cb18be49b8c8 2425646 0 2021-07-19 04:56:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b9c00 0xc0030b9c01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:172.16.0.76,StartTime:2021-07-19 04:56:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 04:56:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://8adac686c9576f08c5b688742071789e24410e00d6ef0945fb68992935a80ec0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.317: INFO: Pod "webserver-deployment-d6c5f64cc-pjx59" is not available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-pjx59 webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-pjx59 8443d85f-f723-4b9e-978e-1aa6142988d5 2425725 0 2021-07-19 04:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b9d77 0xc0030b9d78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.317: INFO: Pod "webserver-deployment-d6c5f64cc-qg6nt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-qg6nt webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-qg6nt ce9f872f-10d6-4414-a7de-aefe0841cc1f 2425729 0 2021-07-19 04:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b9e97 0xc0030b9e98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.317: INFO: Pod "webserver-deployment-d6c5f64cc-qmzfm" is available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-qmzfm webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-qmzfm df15baa4-8d57-4b7d-a612-33bc0a620880 2425632 0 2021-07-19 04:56:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0030b9fb7 0xc0030b9fb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:172.16.0.74,StartTime:2021-07-19 04:56:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 04:56:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://bcd9e08a65b2adb2c1d6a395ac4eefc33b18a55013adffe6ee3e4f068da292fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.317: INFO: Pod "webserver-deployment-d6c5f64cc-w2d9k" is available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-w2d9k webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-w2d9k 43ee623f-8bca-4c3a-ae98-65536a313022 2425614 0 2021-07-19 04:56:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0036f2147 0xc0036f2148}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.158,PodIP:172.16.0.26,StartTime:2021-07-19 04:56:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 04:56:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://8e6f41aec6e91857a27cca619eb2aaf7290f6a18f9c36c851211cfe38fd7b57e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.317: INFO: Pod "webserver-deployment-d6c5f64cc-xq9bq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d6c5f64cc-xq9bq webserver-deployment-d6c5f64cc- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d6c5f64cc-xq9bq acefabfd-683b-49ab-9dd4-efe0253b5d7c 2425730 0 2021-07-19 04:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d6c5f64cc] map[] [{apps/v1 ReplicaSet webserver-deployment-d6c5f64cc bc546dae-419a-4e27-a08d-92fd72402f1f 0xc0036f22c7 0xc0036f22c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.318: INFO: Pod "webserver-deployment-d9db756f4-9zrtg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9db756f4-9zrtg webserver-deployment-d9db756f4- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d9db756f4-9zrtg 89d9b214-3a30-4a95-9193-bc95b5165f2d 2425703 0 2021-07-19 04:56:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9db756f4] map[] [{apps/v1 ReplicaSet webserver-deployment-d9db756f4 4abe5be7-c420-4831-b2df-437f898547f7 0xc0036f23d0 0xc0036f23d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:,StartTime:2021-07-19 04:56:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.318: INFO: Pod "webserver-deployment-d9db756f4-gjspn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9db756f4-gjspn webserver-deployment-d9db756f4- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d9db756f4-gjspn d56e9e3f-7ee1-4541-9a65-369a4635c276 2425690 0 2021-07-19 04:56:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9db756f4] map[] [{apps/v1 ReplicaSet webserver-deployment-d9db756f4 4abe5be7-c420-4831-b2df-437f898547f7 0xc0036f25b7 0xc0036f25b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.158,PodIP:,StartTime:2021-07-19 04:56:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.318: INFO: Pod "webserver-deployment-d9db756f4-kqq2v" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9db756f4-kqq2v webserver-deployment-d9db756f4- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d9db756f4-kqq2v 7367c4f1-4900-4394-88e9-a8af4cfef412 2425687 0 2021-07-19 04:56:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9db756f4] map[] [{apps/v1 ReplicaSet webserver-deployment-d9db756f4 4abe5be7-c420-4831-b2df-437f898547f7 0xc0036f2887 0xc0036f2888}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:,StartTime:2021-07-19 04:56:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.318: INFO: Pod "webserver-deployment-d9db756f4-rq5gt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9db756f4-rq5gt webserver-deployment-d9db756f4- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d9db756f4-rq5gt 231dcaae-c866-4c6b-80d2-64e3a4bc6239 2425700 0 2021-07-19 04:56:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9db756f4] map[] [{apps/v1 ReplicaSet webserver-deployment-d9db756f4 4abe5be7-c420-4831-b2df-437f898547f7 0xc0036f2a87 0xc0036f2a88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.158,PodIP:,StartTime:2021-07-19 04:56:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:56:31.318: INFO: Pod "webserver-deployment-d9db756f4-wpb9p" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9db756f4-wpb9p webserver-deployment-d9db756f4- deployment-6812 /api/v1/namespaces/deployment-6812/pods/webserver-deployment-d9db756f4-wpb9p 8ace8678-40ed-400e-993b-9465536a3f8a 2425692 0 2021-07-19 04:56:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9db756f4] map[] [{apps/v1 ReplicaSet webserver-deployment-d9db756f4 4abe5be7-c420-4831-b2df-437f898547f7 0xc0036f2c27 0xc0036f2c28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxklf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxklf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxklf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 04:56:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:,StartTime:2021-07-19 04:56:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:56:31.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6812" for this suite.

• [SLOW TEST:10.533 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":151,"skipped":2397,"failed":0}
SS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:56:31.438: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jul 19 04:56:31.744: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jul 19 04:56:31.751: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 19 04:56:31.751: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jul 19 04:56:31.764: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 19 04:56:31.764: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jul 19 04:56:31.776: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul 19 04:56:31.776: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jul 19 04:56:38.829: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:56:38.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8979" for this suite.

• [SLOW TEST:7.426 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":152,"skipped":2399,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:56:38.864: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-8744ad42-ad0d-4731-bed2-e69697896ba3
STEP: Creating secret with name secret-projected-all-test-volume-0667ee18-94ac-4997-8082-7be2a74fa599
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 19 04:56:39.009: INFO: Waiting up to 5m0s for pod "projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef" in namespace "projected-8034" to be "Succeeded or Failed"
Jul 19 04:56:39.013: INFO: Pod "projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.971786ms
Jul 19 04:56:41.018: INFO: Pod "projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008385039s
Jul 19 04:56:43.022: INFO: Pod "projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01251521s
Jul 19 04:56:45.026: INFO: Pod "projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016384874s
Jul 19 04:56:47.029: INFO: Pod "projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020109461s
Jul 19 04:56:49.035: INFO: Pod "projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025578909s
Jul 19 04:56:51.040: INFO: Pod "projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.030278761s
STEP: Saw pod success
Jul 19 04:56:51.040: INFO: Pod "projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef" satisfied condition "Succeeded or Failed"
Jul 19 04:56:51.043: INFO: Trying to get logs from node paas-192-168-10-183 pod projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 19 04:56:51.072: INFO: Waiting for pod projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef to disappear
Jul 19 04:56:51.076: INFO: Pod projected-volume-93985e74-aad6-45fc-b6a1-48c7edcc16ef no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:56:51.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8034" for this suite.

• [SLOW TEST:12.226 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":153,"skipped":2426,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:56:51.091: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:56:52.101: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 19 04:56:54.113: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267412, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267412, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267412, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267412, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 04:56:56.118: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267412, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267412, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267412, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267412, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:56:59.146: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:56:59.151: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:00.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2106" for this suite.
STEP: Destroying namespace "webhook-2106-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.607 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":154,"skipped":2433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:00.698: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Jul 19 04:57:00.787: INFO: created test-podtemplate-1
Jul 19 04:57:00.793: INFO: created test-podtemplate-2
Jul 19 04:57:00.801: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jul 19 04:57:00.811: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jul 19 04:57:00.825: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:00.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7291" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":155,"skipped":2463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:00.841: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:57:01.509: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 04:57:03.520: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267421, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267421, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267421, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267421, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:57:06.558: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:57:06.562: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9065-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:07.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9934" for this suite.
STEP: Destroying namespace "webhook-9934-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.145 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":156,"skipped":2493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:07.986: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 19 04:57:08.100: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4305 /api/v1/namespaces/watch-4305/configmaps/e2e-watch-test-label-changed 7c2133c6-e75d-44a3-a20e-94f2343b4e15 2426285 0 2021-07-19 04:57:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 04:57:08.100: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4305 /api/v1/namespaces/watch-4305/configmaps/e2e-watch-test-label-changed 7c2133c6-e75d-44a3-a20e-94f2343b4e15 2426286 0 2021-07-19 04:57:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 04:57:08.100: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4305 /api/v1/namespaces/watch-4305/configmaps/e2e-watch-test-label-changed 7c2133c6-e75d-44a3-a20e-94f2343b4e15 2426287 0 2021-07-19 04:57:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 19 04:57:18.141: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4305 /api/v1/namespaces/watch-4305/configmaps/e2e-watch-test-label-changed 7c2133c6-e75d-44a3-a20e-94f2343b4e15 2426366 0 2021-07-19 04:57:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 04:57:18.141: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4305 /api/v1/namespaces/watch-4305/configmaps/e2e-watch-test-label-changed 7c2133c6-e75d-44a3-a20e-94f2343b4e15 2426367 0 2021-07-19 04:57:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 04:57:18.141: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4305 /api/v1/namespaces/watch-4305/configmaps/e2e-watch-test-label-changed 7c2133c6-e75d-44a3-a20e-94f2343b4e15 2426368 0 2021-07-19 04:57:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:18.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4305" for this suite.

• [SLOW TEST:10.172 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":157,"skipped":2525,"failed":0}
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:18.158: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul 19 04:57:18.250: INFO: Created pod &Pod{ObjectMeta:{dns-7521  dns-7521 /api/v1/namespaces/dns-7521/pods/dns-7521 61eebc0a-29a2-4a24-978e-4e80922be3cd 2426374 0 2021-07-19 04:57:18 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qhhs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qhhs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qhhs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 04:57:18.253: INFO: The status of Pod dns-7521 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 04:57:20.258: INFO: The status of Pod dns-7521 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 04:57:22.267: INFO: The status of Pod dns-7521 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jul 19 04:57:22.267: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7521 PodName:dns-7521 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:57:22.267: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Verifying customized DNS server is configured on pod...
Jul 19 04:57:22.450: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7521 PodName:dns-7521 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 04:57:22.450: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 04:57:22.640: INFO: Deleting pod dns-7521...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:22.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7521" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":158,"skipped":2525,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:22.682: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0719 04:57:32.840244      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0719 04:57:32.840412      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0719 04:57:32.840488      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 19 04:57:32.840: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 19 04:57:32.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-256js" in namespace "gc-651"
Jul 19 04:57:32.855: INFO: Deleting pod "simpletest-rc-to-be-deleted-47b98" in namespace "gc-651"
Jul 19 04:57:32.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-jrxkw" in namespace "gc-651"
Jul 19 04:57:32.881: INFO: Deleting pod "simpletest-rc-to-be-deleted-jslw8" in namespace "gc-651"
Jul 19 04:57:32.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-mmtdm" in namespace "gc-651"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:32.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-651" for this suite.

• [SLOW TEST:10.272 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":159,"skipped":2534,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:32.954: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 19 04:57:33.060: INFO: Waiting up to 5m0s for pod "pod-a70da7b0-5a82-48cb-82ac-1b7aa9519485" in namespace "emptydir-2259" to be "Succeeded or Failed"
Jul 19 04:57:33.065: INFO: Pod "pod-a70da7b0-5a82-48cb-82ac-1b7aa9519485": Phase="Pending", Reason="", readiness=false. Elapsed: 4.521593ms
Jul 19 04:57:35.068: INFO: Pod "pod-a70da7b0-5a82-48cb-82ac-1b7aa9519485": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008003036s
Jul 19 04:57:37.072: INFO: Pod "pod-a70da7b0-5a82-48cb-82ac-1b7aa9519485": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011841619s
Jul 19 04:57:39.076: INFO: Pod "pod-a70da7b0-5a82-48cb-82ac-1b7aa9519485": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015621868s
STEP: Saw pod success
Jul 19 04:57:39.076: INFO: Pod "pod-a70da7b0-5a82-48cb-82ac-1b7aa9519485" satisfied condition "Succeeded or Failed"
Jul 19 04:57:39.079: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-a70da7b0-5a82-48cb-82ac-1b7aa9519485 container test-container: <nil>
STEP: delete the pod
Jul 19 04:57:39.112: INFO: Waiting for pod pod-a70da7b0-5a82-48cb-82ac-1b7aa9519485 to disappear
Jul 19 04:57:39.117: INFO: Pod pod-a70da7b0-5a82-48cb-82ac-1b7aa9519485 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:39.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2259" for this suite.

• [SLOW TEST:6.175 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":160,"skipped":2534,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:39.130: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Jul 19 04:57:39.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f -'
Jul 19 04:57:39.474: INFO: stderr: ""
Jul 19 04:57:39.474: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jul 19 04:57:39.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 diff -f -'
Jul 19 04:57:39.886: INFO: rc: 1
Jul 19 04:57:39.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete -f -'
Jul 19 04:57:39.980: INFO: stderr: ""
Jul 19 04:57:39.980: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:39.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1546" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":161,"skipped":2545,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:40.010: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:45.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6706" for this suite.

• [SLOW TEST:5.574 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":162,"skipped":2597,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:45.585: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 19 04:57:45.847: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jul 19 04:57:47.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267465, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267465, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267465, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267465, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59d7ccfb84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 04:57:49.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267465, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267465, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267465, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267465, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59d7ccfb84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 04:57:52.888: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:57:52.893: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:54.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5210" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.719 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":163,"skipped":2599,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:54.304: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 04:57:54.440: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:57:58.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7776" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2612,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:57:58.510: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 04:58:58.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8164" for this suite.

• [SLOW TEST:60.103 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":165,"skipped":2623,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 04:58:58.614: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1550
Jul 19 04:59:02.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 19 04:59:03.171: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 19 04:59:03.171: INFO: stdout: "iptables"
Jul 19 04:59:03.171: INFO: proxyMode: iptables
Jul 19 04:59:03.183: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 04:59:03.187: INFO: Pod kube-proxy-mode-detector still exists
Jul 19 04:59:05.187: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 04:59:05.192: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1550
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1550
I0719 04:59:05.230300      24 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1550, replica count: 3
I0719 04:59:08.280655      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 04:59:11.280836      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 04:59:11.293: INFO: Creating new exec pod
Jul 19 04:59:16.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 execpod-affinityggtdb -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jul 19 04:59:16.647: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jul 19 04:59:16.647: INFO: stdout: ""
Jul 19 04:59:16.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 execpod-affinityggtdb -- /bin/sh -x -c nc -zv -t -w 2 10.247.70.47 80'
Jul 19 04:59:16.904: INFO: stderr: "+ nc -zv -t -w 2 10.247.70.47 80\nConnection to 10.247.70.47 80 port [tcp/http] succeeded!\n"
Jul 19 04:59:16.904: INFO: stdout: ""
Jul 19 04:59:16.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 execpod-affinityggtdb -- /bin/sh -x -c nc -zv -t -w 2 192.168.10.158 32676'
Jul 19 04:59:17.167: INFO: stderr: "+ nc -zv -t -w 2 192.168.10.158 32676\nConnection to 192.168.10.158 32676 port [tcp/32676] succeeded!\n"
Jul 19 04:59:17.167: INFO: stdout: ""
Jul 19 04:59:17.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 execpod-affinityggtdb -- /bin/sh -x -c nc -zv -t -w 2 192.168.10.183 32676'
Jul 19 04:59:17.414: INFO: stderr: "+ nc -zv -t -w 2 192.168.10.183 32676\nConnection to 192.168.10.183 32676 port [tcp/32676] succeeded!\n"
Jul 19 04:59:17.414: INFO: stdout: ""
Jul 19 04:59:17.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 execpod-affinityggtdb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.10.158:32676/ ; done'
Jul 19 04:59:17.832: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n"
Jul 19 04:59:17.832: INFO: stdout: "\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474\naffinity-nodeport-timeout-br474"
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Received response from host: affinity-nodeport-timeout-br474
Jul 19 04:59:17.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 execpod-affinityggtdb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.10.158:32676/'
Jul 19 04:59:18.088: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n"
Jul 19 04:59:18.088: INFO: stdout: "affinity-nodeport-timeout-br474"
Jul 19 04:59:33.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 execpod-affinityggtdb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.10.158:32676/'
Jul 19 04:59:33.340: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n"
Jul 19 04:59:33.340: INFO: stdout: "affinity-nodeport-timeout-br474"
Jul 19 04:59:48.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 execpod-affinityggtdb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.10.158:32676/'
Jul 19 04:59:48.599: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n"
Jul 19 04:59:48.599: INFO: stdout: "affinity-nodeport-timeout-br474"
Jul 19 05:00:03.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 execpod-affinityggtdb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.10.158:32676/'
Jul 19 05:00:03.859: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n"
Jul 19 05:00:03.859: INFO: stdout: "affinity-nodeport-timeout-br474"
Jul 19 05:00:18.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-1550 execpod-affinityggtdb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.10.158:32676/'
Jul 19 05:00:19.177: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.10.158:32676/\n"
Jul 19 05:00:19.177: INFO: stdout: "affinity-nodeport-timeout-f7wfs"
Jul 19 05:00:19.177: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1550, will wait for the garbage collector to delete the pods
Jul 19 05:00:19.267: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.612796ms
Jul 19 05:00:19.367: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.163186ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:00:39.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1550" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:100.939 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":166,"skipped":2634,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:00:39.554: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-b222f39d-3506-48a2-acb2-286784d9cc05
STEP: Creating a pod to test consume configMaps
Jul 19 05:00:39.675: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2142ea53-e085-4ba1-89ce-532fd44afabd" in namespace "projected-9963" to be "Succeeded or Failed"
Jul 19 05:00:39.682: INFO: Pod "pod-projected-configmaps-2142ea53-e085-4ba1-89ce-532fd44afabd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083872ms
Jul 19 05:00:41.686: INFO: Pod "pod-projected-configmaps-2142ea53-e085-4ba1-89ce-532fd44afabd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010722874s
Jul 19 05:00:43.691: INFO: Pod "pod-projected-configmaps-2142ea53-e085-4ba1-89ce-532fd44afabd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015206192s
STEP: Saw pod success
Jul 19 05:00:43.691: INFO: Pod "pod-projected-configmaps-2142ea53-e085-4ba1-89ce-532fd44afabd" satisfied condition "Succeeded or Failed"
Jul 19 05:00:43.694: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-projected-configmaps-2142ea53-e085-4ba1-89ce-532fd44afabd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 05:00:43.756: INFO: Waiting for pod pod-projected-configmaps-2142ea53-e085-4ba1-89ce-532fd44afabd to disappear
Jul 19 05:00:43.760: INFO: Pod pod-projected-configmaps-2142ea53-e085-4ba1-89ce-532fd44afabd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:00:43.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9963" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":167,"skipped":2652,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:00:43.775: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:00:43.876: INFO: Create a RollingUpdate DaemonSet
Jul 19 05:00:43.881: INFO: Check that daemon pods launch on every node of the cluster
Jul 19 05:00:43.890: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:43.890: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:43.890: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:43.890: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:43.890: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:43.893: INFO: Number of nodes with available pods: 0
Jul 19 05:00:43.893: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:00:44.900: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:44.900: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:44.900: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:44.900: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:44.901: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:44.903: INFO: Number of nodes with available pods: 0
Jul 19 05:00:44.903: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:00:45.899: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:45.899: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:45.899: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:45.899: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:45.899: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:45.903: INFO: Number of nodes with available pods: 0
Jul 19 05:00:45.903: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:00:46.899: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:46.899: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:46.899: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:46.899: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:46.899: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:46.903: INFO: Number of nodes with available pods: 0
Jul 19 05:00:46.903: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:00:47.908: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:47.908: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:47.908: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:47.908: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:47.908: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:00:47.913: INFO: Number of nodes with available pods: 2
Jul 19 05:00:47.913: INFO: Number of running nodes: 2, number of available pods: 2
Jul 19 05:00:47.913: INFO: Update the DaemonSet to trigger a rollout
Jul 19 05:00:47.922: INFO: Updating DaemonSet daemon-set
Jul 19 05:01:09.940: INFO: Roll back the DaemonSet before rollout is complete
Jul 19 05:01:09.949: INFO: Updating DaemonSet daemon-set
Jul 19 05:01:09.949: INFO: Make sure DaemonSet rollback is complete
Jul 19 05:01:09.952: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:09.952: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:09.958: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:09.958: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:09.958: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:09.958: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:09.958: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:10.963: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:10.963: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:10.969: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:10.969: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:10.969: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:10.969: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:10.969: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:11.962: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:11.962: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:11.967: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:11.967: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:11.967: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:11.967: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:11.967: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:12.962: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:12.962: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:12.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:12.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:12.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:12.968: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:12.968: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:13.963: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:13.963: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:13.970: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:13.970: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:13.970: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:13.970: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:13.970: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:14.963: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:14.963: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:14.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:14.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:14.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:14.968: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:14.968: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:15.963: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:15.963: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:15.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:15.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:15.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:15.968: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:15.969: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:16.962: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:16.962: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:16.967: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:16.967: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:16.967: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:16.967: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:16.967: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:17.962: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:17.962: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:17.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:17.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:17.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:17.968: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:17.968: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:18.963: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:18.963: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:18.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:18.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:18.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:18.968: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:18.968: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:19.965: INFO: Wrong image for pod: daemon-set-8wts5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 19 05:01:19.965: INFO: Pod daemon-set-8wts5 is not available
Jul 19 05:01:19.970: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:19.970: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:19.970: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:19.970: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:19.970: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:20.962: INFO: Pod daemon-set-7km57 is not available
Jul 19 05:01:20.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-143 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:20.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-176 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:20.968: INFO: DaemonSet pods can't tolerate node paas-192-168-10-223 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:20.968: INFO: DaemonSet pods can't tolerate node paas-addon-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 19 05:01:20.968: INFO: DaemonSet pods can't tolerate node paas-addon-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6074, will wait for the garbage collector to delete the pods
Jul 19 05:01:21.038: INFO: Deleting DaemonSet.extensions daemon-set took: 8.664838ms
Jul 19 05:01:21.138: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.227372ms
Jul 19 05:01:34.343: INFO: Number of nodes with available pods: 0
Jul 19 05:01:34.343: INFO: Number of running nodes: 0, number of available pods: 0
Jul 19 05:01:34.348: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6074/daemonsets","resourceVersion":"2428592"},"items":null}

Jul 19 05:01:34.387: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6074/pods","resourceVersion":"2428592"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:01:34.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6074" for this suite.

• [SLOW TEST:50.635 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":168,"skipped":2661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:01:34.410: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:01:45.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8106" for this suite.

• [SLOW TEST:11.134 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":169,"skipped":2730,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:01:45.544: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 19 05:01:45.610: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:02:04.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5378" for this suite.

• [SLOW TEST:18.757 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":170,"skipped":2739,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:02:04.301: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Jul 19 05:02:04.424: INFO: created test-pod-1
Jul 19 05:02:04.439: INFO: created test-pod-2
Jul 19 05:02:04.451: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:02:04.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9898" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":171,"skipped":2749,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:02:04.549: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-2ca1ce44-cf90-4f95-93fd-81cd00d717a2
STEP: Creating a pod to test consume secrets
Jul 19 05:02:04.643: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-740354fd-f2b2-4961-9610-3d19eea6c138" in namespace "projected-7360" to be "Succeeded or Failed"
Jul 19 05:02:04.652: INFO: Pod "pod-projected-secrets-740354fd-f2b2-4961-9610-3d19eea6c138": Phase="Pending", Reason="", readiness=false. Elapsed: 8.611172ms
Jul 19 05:02:06.657: INFO: Pod "pod-projected-secrets-740354fd-f2b2-4961-9610-3d19eea6c138": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014279862s
Jul 19 05:02:08.661: INFO: Pod "pod-projected-secrets-740354fd-f2b2-4961-9610-3d19eea6c138": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018352889s
STEP: Saw pod success
Jul 19 05:02:08.661: INFO: Pod "pod-projected-secrets-740354fd-f2b2-4961-9610-3d19eea6c138" satisfied condition "Succeeded or Failed"
Jul 19 05:02:08.665: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-projected-secrets-740354fd-f2b2-4961-9610-3d19eea6c138 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 19 05:02:08.688: INFO: Waiting for pod pod-projected-secrets-740354fd-f2b2-4961-9610-3d19eea6c138 to disappear
Jul 19 05:02:08.692: INFO: Pod pod-projected-secrets-740354fd-f2b2-4961-9610-3d19eea6c138 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:02:08.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7360" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2792,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:02:08.703: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2405, will wait for the garbage collector to delete the pods
Jul 19 05:02:12.839: INFO: Deleting Job.batch foo took: 9.688712ms
Jul 19 05:02:12.939: INFO: Terminating Job.batch foo pods took: 100.141138ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:03:04.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2405" for this suite.

• [SLOW TEST:55.657 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":173,"skipped":2815,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:03:04.361: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5014 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5014;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5014 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5014;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5014.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5014.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5014.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5014.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5014.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5014.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5014.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5014.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5014.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5014.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5014.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5014.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5014.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 247.44.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.44.247_udp@PTR;check="$$(dig +tcp +noall +answer +search 247.44.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.44.247_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5014 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5014;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5014 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5014;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5014.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5014.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5014.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5014.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5014.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5014.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5014.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5014.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5014.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5014.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5014.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5014.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5014.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 247.44.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.44.247_udp@PTR;check="$$(dig +tcp +noall +answer +search 247.44.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.44.247_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 19 05:03:10.512: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.517: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.525: INFO: Unable to read wheezy_udp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.529: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.536: INFO: Unable to read wheezy_udp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.544: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.548: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.552: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.556: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.560: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.564: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.568: INFO: Unable to read 10.247.44.247_udp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.571: INFO: Unable to read 10.247.44.247_tcp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.575: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.579: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.583: INFO: Unable to read jessie_udp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.587: INFO: Unable to read jessie_tcp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.591: INFO: Unable to read jessie_udp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.595: INFO: Unable to read jessie_tcp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.599: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.602: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.606: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.624: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.628: INFO: Unable to read jessie_udp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.632: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.636: INFO: Unable to read 10.247.44.247_udp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.640: INFO: Unable to read 10.247.44.247_tcp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:10.640: INFO: Lookups using dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5014 wheezy_tcp@dns-test-service.dns-5014 wheezy_udp@dns-test-service.dns-5014.svc wheezy_tcp@dns-test-service.dns-5014.svc wheezy_udp@_http._tcp.dns-test-service.dns-5014.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5014.svc wheezy_udp@_http._tcp.test-service-2.dns-5014.svc wheezy_tcp@_http._tcp.test-service-2.dns-5014.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.247.44.247_udp@PTR 10.247.44.247_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5014 jessie_tcp@dns-test-service.dns-5014 jessie_udp@dns-test-service.dns-5014.svc jessie_tcp@dns-test-service.dns-5014.svc jessie_udp@_http._tcp.dns-test-service.dns-5014.svc jessie_tcp@_http._tcp.dns-test-service.dns-5014.svc jessie_udp@_http._tcp.test-service-2.dns-5014.svc jessie_tcp@_http._tcp.test-service-2.dns-5014.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.247.44.247_udp@PTR 10.247.44.247_tcp@PTR]

Jul 19 05:03:15.645: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.650: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.653: INFO: Unable to read wheezy_udp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.657: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.661: INFO: Unable to read wheezy_udp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.668: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.672: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.675: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.682: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.686: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.689: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.694: INFO: Unable to read 10.247.44.247_udp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.698: INFO: Unable to read 10.247.44.247_tcp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.702: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.707: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.710: INFO: Unable to read jessie_udp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.714: INFO: Unable to read jessie_tcp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.719: INFO: Unable to read jessie_udp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.723: INFO: Unable to read jessie_tcp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.726: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.730: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.734: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.738: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.742: INFO: Unable to read jessie_udp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.747: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.750: INFO: Unable to read 10.247.44.247_udp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.755: INFO: Unable to read 10.247.44.247_tcp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:15.755: INFO: Lookups using dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5014 wheezy_tcp@dns-test-service.dns-5014 wheezy_udp@dns-test-service.dns-5014.svc wheezy_tcp@dns-test-service.dns-5014.svc wheezy_udp@_http._tcp.dns-test-service.dns-5014.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5014.svc wheezy_udp@_http._tcp.test-service-2.dns-5014.svc wheezy_tcp@_http._tcp.test-service-2.dns-5014.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.247.44.247_udp@PTR 10.247.44.247_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5014 jessie_tcp@dns-test-service.dns-5014 jessie_udp@dns-test-service.dns-5014.svc jessie_tcp@dns-test-service.dns-5014.svc jessie_udp@_http._tcp.dns-test-service.dns-5014.svc jessie_tcp@_http._tcp.dns-test-service.dns-5014.svc jessie_udp@_http._tcp.test-service-2.dns-5014.svc jessie_tcp@_http._tcp.test-service-2.dns-5014.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.247.44.247_udp@PTR 10.247.44.247_tcp@PTR]

Jul 19 05:03:20.645: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.650: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.654: INFO: Unable to read wheezy_udp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.658: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.671: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.674: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.679: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.682: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.686: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.691: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.694: INFO: Unable to read 10.247.44.247_udp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.698: INFO: Unable to read 10.247.44.247_tcp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.702: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.705: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.709: INFO: Unable to read jessie_udp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.712: INFO: Unable to read jessie_tcp@dns-test-service.dns-5014 from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.716: INFO: Unable to read jessie_udp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.719: INFO: Unable to read jessie_tcp@dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.723: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.727: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.731: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.736: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5014.svc from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.739: INFO: Unable to read jessie_udp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.743: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.747: INFO: Unable to read 10.247.44.247_udp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.750: INFO: Unable to read 10.247.44.247_tcp@PTR from pod dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b: the server could not find the requested resource (get pods dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b)
Jul 19 05:03:20.750: INFO: Lookups using dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5014 wheezy_tcp@dns-test-service.dns-5014 wheezy_udp@dns-test-service.dns-5014.svc wheezy_tcp@dns-test-service.dns-5014.svc wheezy_udp@_http._tcp.dns-test-service.dns-5014.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5014.svc wheezy_udp@_http._tcp.test-service-2.dns-5014.svc wheezy_tcp@_http._tcp.test-service-2.dns-5014.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.247.44.247_udp@PTR 10.247.44.247_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5014 jessie_tcp@dns-test-service.dns-5014 jessie_udp@dns-test-service.dns-5014.svc jessie_tcp@dns-test-service.dns-5014.svc jessie_udp@_http._tcp.dns-test-service.dns-5014.svc jessie_tcp@_http._tcp.dns-test-service.dns-5014.svc jessie_udp@_http._tcp.test-service-2.dns-5014.svc jessie_tcp@_http._tcp.test-service-2.dns-5014.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.247.44.247_udp@PTR 10.247.44.247_tcp@PTR]

Jul 19 05:03:25.747: INFO: DNS probes using dns-5014/dns-test-8d0d2a06-c7f7-4f14-bcc7-288fb0630e1b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:03:25.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5014" for this suite.

• [SLOW TEST:21.512 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":174,"skipped":2821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:03:25.873: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:03:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6613" for this suite.

• [SLOW TEST:7.101 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":175,"skipped":2867,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:03:32.974: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 19 05:03:33.035: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 19 05:03:33.044: INFO: Waiting for terminating namespaces to be deleted...
Jul 19 05:03:33.048: INFO: 
Logging pods the apiserver thinks is on node paas-192-168-10-158 before test
Jul 19 05:03:33.067: INFO: cse-config-center-9cb5789db-hqgs8 from fst-manage started at 2021-07-15 02:47:40 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container cse-cc-comp ready: true, restart count 0
Jul 19 05:03:33.067: INFO: cse-etcd-0 from fst-manage started at 2021-07-15 02:45:36 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container cse-etcd-comp ready: true, restart count 0
Jul 19 05:03:33.067: INFO: cse-service-center-78c89955f8-9zxb8 from fst-manage started at 2021-07-15 02:47:41 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container cse-sc-comp ready: true, restart count 0
Jul 19 05:03:33.067: INFO: cspagentmanager-84df498dd6-756cg from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container cspagentmanager-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: csphaservice-85cfb95df6-rg2zf from fst-manage started at 2021-07-15 02:45:40 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container csphaservice-soft ready: true, restart count 0
Jul 19 05:03:33.067: INFO: cspnrsmaster-0 from fst-manage started at 2021-07-15 02:45:49 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container cspnrsmaster-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: cspommgr-7b4c7c76dc-lg5xn from fst-manage started at 2021-07-15 02:45:42 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container cspommgr-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: csposmgr-666d77c8cf-5nztr from fst-manage started at 2021-07-15 02:45:52 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container csposmgr-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: deploymgr-5fc4dfbc87-skqjv from fst-manage started at 2021-07-15 02:45:39 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container deploymgr-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: febs-599995cbb8-wch4b from fst-manage started at 2021-07-15 02:46:04 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container febs-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: fileserver-0 from fst-manage started at 2021-07-15 02:45:48 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container fileserver-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: gaussdb-da5b9bc8-0 from fst-manage started at 2021-07-15 02:46:01 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container gauss-softwarecomp ready: true, restart count 0
Jul 19 05:03:33.067: INFO: kafka-da5b9bc8-0 from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container kafka-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: kubeaddon-upload-package-f5vtngqpi8-4m5dx from fst-manage started at 2021-07-15 02:44:30 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container kubeaddon-upload-package ready: false, restart count 0
Jul 19 05:03:33.067: INFO: mtcenter-57c48c94fd-89d27 from fst-manage started at 2021-07-15 02:45:35 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container cspmtcenter-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: omcache-da5b9bc8-0 from fst-manage started at 2021-07-15 02:45:56 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container redis-softwarecomp ready: true, restart count 0
Jul 19 05:03:33.067: INFO: omlb-5d58b5f84f-w9p5s from fst-manage started at 2021-07-15 03:35:19 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container omlb-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: pmscalc-5744fc9c5d-x6rlp from fst-manage started at 2021-07-15 02:46:00 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container pmscalc-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: pmsmgr-6857b85787-q6n5l from fst-manage started at 2021-07-15 02:45:37 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container pmsmgr-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: runlog-69f7c599d-krpbr from fst-manage started at 2021-07-15 02:45:58 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container runlog-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: secmgr-65cfb48b8b-2fzvs from fst-manage started at 2021-07-15 02:45:57 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container secmgr-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: soap-76794ccdc-xtknz from fst-manage started at 2021-07-15 02:45:45 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container soap-soft ready: true, restart count 0
Jul 19 05:03:33.067: INFO: upgtool-app-559557759c-jpc94 from fst-manage started at 2021-07-15 02:45:54 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container upgtool-component ready: true, restart count 0
Jul 19 05:03:33.067: INFO: zookeeper-0 from fst-manage started at 2021-07-15 02:45:41 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container zookeeper-software ready: true, restart count 0
Jul 19 05:03:33.067: INFO: sonobuoy from sonobuoy started at 2021-07-19 03:55:48 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 19 05:03:33.067: INFO: sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-p7chc from sonobuoy started at 2021-07-19 03:55:51 +0000 UTC (2 container statuses recorded)
Jul 19 05:03:33.067: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 05:03:33.067: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 19 05:03:33.067: INFO: 
Logging pods the apiserver thinks is on node paas-192-168-10-183 before test
Jul 19 05:03:33.099: INFO: cse-config-center-9cb5789db-28zp9 from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container cse-cc-comp ready: true, restart count 0
Jul 19 05:03:33.099: INFO: cse-etcd-1 from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container cse-etcd-comp ready: true, restart count 0
Jul 19 05:03:33.099: INFO: cse-service-center-78c89955f8-5prtz from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container cse-sc-comp ready: true, restart count 0
Jul 19 05:03:33.099: INFO: cspagentmanager-84df498dd6-b4cjd from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container cspagentmanager-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: csphaservice-85cfb95df6-qbb2n from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container csphaservice-soft ready: true, restart count 0
Jul 19 05:03:33.099: INFO: cspnrsmaster-1 from fst-manage started at 2021-07-19 04:51:55 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container cspnrsmaster-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: cspommgr-7b4c7c76dc-vdphx from fst-manage started at 2021-07-19 04:51:55 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container cspommgr-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: csposmgr-666d77c8cf-z6rmr from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container csposmgr-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: deploymgr-5fc4dfbc87-8vh4t from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container deploymgr-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: febs-599995cbb8-2pddj from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container febs-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: fileserver-1 from fst-manage started at 2021-07-19 04:51:51 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container fileserver-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: gaussdb-da5b9bc8-1 from fst-manage started at 2021-07-19 04:51:52 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container gauss-softwarecomp ready: true, restart count 0
Jul 19 05:03:33.099: INFO: kafka-da5b9bc8-1 from fst-manage started at 2021-07-19 04:51:54 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container kafka-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: mtcenter-57c48c94fd-ds7n9 from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container cspmtcenter-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: omcache-da5b9bc8-1 from fst-manage started at 2021-07-19 04:51:52 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container redis-softwarecomp ready: true, restart count 0
Jul 19 05:03:33.099: INFO: omlb-5d58b5f84f-rf8jn from fst-manage started at 2021-07-19 04:51:55 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container omlb-software ready: false, restart count 2
Jul 19 05:03:33.099: INFO: pmscalc-5744fc9c5d-zwbq5 from fst-manage started at 2021-07-19 04:51:55 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container pmscalc-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: pmsmgr-6857b85787-mktnz from fst-manage started at 2021-07-19 04:51:58 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container pmsmgr-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: runlog-69f7c599d-4zpnj from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container runlog-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: secmgr-65cfb48b8b-bttbd from fst-manage started at 2021-07-19 04:51:55 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container secmgr-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: soap-76794ccdc-s5sj5 from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container soap-soft ready: true, restart count 0
Jul 19 05:03:33.099: INFO: upgtool-app-559557759c-8lcrn from fst-manage started at 2021-07-19 04:51:59 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container upgtool-component ready: true, restart count 0
Jul 19 05:03:33.099: INFO: zookeeper-1 from fst-manage started at 2021-07-19 04:51:51 +0000 UTC (1 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container zookeeper-software ready: true, restart count 0
Jul 19 05:03:33.099: INFO: sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-gtdqw from sonobuoy started at 2021-07-19 03:55:51 +0000 UTC (2 container statuses recorded)
Jul 19 05:03:33.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 05:03:33.099: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node paas-192-168-10-158
STEP: verifying the node has the label node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod cse-config-center-9cb5789db-28zp9 requesting resource cpu=50m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod cse-config-center-9cb5789db-hqgs8 requesting resource cpu=50m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod cse-etcd-0 requesting resource cpu=100m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod cse-etcd-1 requesting resource cpu=100m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod cse-service-center-78c89955f8-5prtz requesting resource cpu=200m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod cse-service-center-78c89955f8-9zxb8 requesting resource cpu=200m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod cspagentmanager-84df498dd6-756cg requesting resource cpu=50m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod cspagentmanager-84df498dd6-b4cjd requesting resource cpu=50m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod csphaservice-85cfb95df6-qbb2n requesting resource cpu=100m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod csphaservice-85cfb95df6-rg2zf requesting resource cpu=100m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod cspnrsmaster-0 requesting resource cpu=100m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod cspnrsmaster-1 requesting resource cpu=100m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod cspommgr-7b4c7c76dc-lg5xn requesting resource cpu=1000m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod cspommgr-7b4c7c76dc-vdphx requesting resource cpu=1000m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod csposmgr-666d77c8cf-5nztr requesting resource cpu=100m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod csposmgr-666d77c8cf-z6rmr requesting resource cpu=100m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod deploymgr-5fc4dfbc87-8vh4t requesting resource cpu=100m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod deploymgr-5fc4dfbc87-skqjv requesting resource cpu=100m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod febs-599995cbb8-2pddj requesting resource cpu=50m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod febs-599995cbb8-wch4b requesting resource cpu=50m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod fileserver-0 requesting resource cpu=50m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod fileserver-1 requesting resource cpu=50m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod gaussdb-da5b9bc8-0 requesting resource cpu=100m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod gaussdb-da5b9bc8-1 requesting resource cpu=100m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod kafka-da5b9bc8-0 requesting resource cpu=200m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod kafka-da5b9bc8-1 requesting resource cpu=200m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod mtcenter-57c48c94fd-89d27 requesting resource cpu=50m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod mtcenter-57c48c94fd-ds7n9 requesting resource cpu=50m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod omcache-da5b9bc8-0 requesting resource cpu=100m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod omcache-da5b9bc8-1 requesting resource cpu=100m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod omlb-5d58b5f84f-rf8jn requesting resource cpu=70m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod omlb-5d58b5f84f-w9p5s requesting resource cpu=70m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod pmscalc-5744fc9c5d-x6rlp requesting resource cpu=70m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod pmscalc-5744fc9c5d-zwbq5 requesting resource cpu=70m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod pmsmgr-6857b85787-mktnz requesting resource cpu=100m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod pmsmgr-6857b85787-q6n5l requesting resource cpu=100m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod runlog-69f7c599d-4zpnj requesting resource cpu=50m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod runlog-69f7c599d-krpbr requesting resource cpu=50m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod secmgr-65cfb48b8b-2fzvs requesting resource cpu=100m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod secmgr-65cfb48b8b-bttbd requesting resource cpu=100m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod soap-76794ccdc-s5sj5 requesting resource cpu=10m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod soap-76794ccdc-xtknz requesting resource cpu=10m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod upgtool-app-559557759c-8lcrn requesting resource cpu=50m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod upgtool-app-559557759c-jpc94 requesting resource cpu=50m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod zookeeper-0 requesting resource cpu=20m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod zookeeper-1 requesting resource cpu=20m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod sonobuoy requesting resource cpu=0m on Node paas-192-168-10-158
Jul 19 05:03:33.221: INFO: Pod sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-gtdqw requesting resource cpu=0m on Node paas-192-168-10-183
Jul 19 05:03:33.221: INFO: Pod sonobuoy-systemd-logs-daemon-set-dc03aa0be08d4f8c-p7chc requesting resource cpu=0m on Node paas-192-168-10-158
STEP: Starting Pods to consume most of the cluster CPU.
Jul 19 05:03:33.221: INFO: Creating a pod which consumes cpu=826m on Node paas-192-168-10-158
Jul 19 05:03:33.233: INFO: Creating a pod which consumes cpu=826m on Node paas-192-168-10-183
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-78c2baca-83ad-4682-96d1-183ea696e6e8.169318ab075653d6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9146/filler-pod-78c2baca-83ad-4682-96d1-183ea696e6e8 to paas-192-168-10-183]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-78c2baca-83ad-4682-96d1-183ea696e6e8.169318ab818eb1ce], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-78c2baca-83ad-4682-96d1-183ea696e6e8.169318ab9305596a], Reason = [SuccessfulCreate], Message = [Created container filler-pod-78c2baca-83ad-4682-96d1-183ea696e6e8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-78c2baca-83ad-4682-96d1-183ea696e6e8.169318ab9f7a9154], Reason = [Started], Message = [Started container filler-pod-78c2baca-83ad-4682-96d1-183ea696e6e8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95d13fb1-e7e0-4b10-ac09-051faae1d7eb.169318ab069fde29], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9146/filler-pod-95d13fb1-e7e0-4b10-ac09-051faae1d7eb to paas-192-168-10-158]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95d13fb1-e7e0-4b10-ac09-051faae1d7eb.169318ab75b10f11], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95d13fb1-e7e0-4b10-ac09-051faae1d7eb.169318ab85cfc2a7], Reason = [SuccessfulCreate], Message = [Created container filler-pod-95d13fb1-e7e0-4b10-ac09-051faae1d7eb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95d13fb1-e7e0-4b10-ac09-051faae1d7eb.169318ab907fd326], Reason = [Started], Message = [Started container filler-pod-95d13fb1-e7e0-4b10-ac09-051faae1d7eb]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.169318abf770eebb], Reason = [FailedScheduling], Message = [0/7 nodes are available: 2 Insufficient cpu, 5 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node paas-192-168-10-158
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node paas-192-168-10-183
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:03:38.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9146" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.400 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":176,"skipped":2877,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:03:38.374: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 19 05:03:46.520: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 05:03:46.528: INFO: Pod pod-with-prestop-http-hook still exists
Jul 19 05:03:48.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 05:03:48.533: INFO: Pod pod-with-prestop-http-hook still exists
Jul 19 05:03:50.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 05:03:50.532: INFO: Pod pod-with-prestop-http-hook still exists
Jul 19 05:03:52.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 05:03:52.532: INFO: Pod pod-with-prestop-http-hook still exists
Jul 19 05:03:54.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 05:03:54.533: INFO: Pod pod-with-prestop-http-hook still exists
Jul 19 05:03:56.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 05:03:56.533: INFO: Pod pod-with-prestop-http-hook still exists
Jul 19 05:03:58.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 05:03:58.533: INFO: Pod pod-with-prestop-http-hook still exists
Jul 19 05:04:00.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 05:04:00.533: INFO: Pod pod-with-prestop-http-hook still exists
Jul 19 05:04:02.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 05:04:02.533: INFO: Pod pod-with-prestop-http-hook still exists
Jul 19 05:04:04.528: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 05:04:04.538: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:04:04.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3894" for this suite.

• [SLOW TEST:26.449 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":177,"skipped":2890,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:04:04.824: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 19 05:04:09.002: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:04:09.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4356" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":178,"skipped":2891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:04:09.033: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:04:09.103: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 19 05:04:12.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-430 create -f -'
Jul 19 05:04:13.306: INFO: stderr: ""
Jul 19 05:04:13.306: INFO: stdout: "e2e-test-crd-publish-openapi-7140-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 19 05:04:13.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-430 delete e2e-test-crd-publish-openapi-7140-crds test-cr'
Jul 19 05:04:13.404: INFO: stderr: ""
Jul 19 05:04:13.404: INFO: stdout: "e2e-test-crd-publish-openapi-7140-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 19 05:04:13.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-430 apply -f -'
Jul 19 05:04:13.624: INFO: stderr: ""
Jul 19 05:04:13.624: INFO: stdout: "e2e-test-crd-publish-openapi-7140-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 19 05:04:13.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-430 delete e2e-test-crd-publish-openapi-7140-crds test-cr'
Jul 19 05:04:13.729: INFO: stderr: ""
Jul 19 05:04:13.729: INFO: stdout: "e2e-test-crd-publish-openapi-7140-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 19 05:04:13.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 explain e2e-test-crd-publish-openapi-7140-crds'
Jul 19 05:04:13.930: INFO: stderr: ""
Jul 19 05:04:13.930: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7140-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:04:17.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-430" for this suite.

• [SLOW TEST:8.408 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":179,"skipped":2922,"failed":0}
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:04:17.440: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2224
Jul 19 05:04:21.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2224 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 19 05:04:21.874: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 19 05:04:21.874: INFO: stdout: "iptables"
Jul 19 05:04:21.874: INFO: proxyMode: iptables
Jul 19 05:04:21.924: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 05:04:21.927: INFO: Pod kube-proxy-mode-detector still exists
Jul 19 05:04:23.927: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 05:04:23.932: INFO: Pod kube-proxy-mode-detector still exists
Jul 19 05:04:25.927: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 05:04:25.932: INFO: Pod kube-proxy-mode-detector still exists
Jul 19 05:04:27.927: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 05:04:27.931: INFO: Pod kube-proxy-mode-detector still exists
Jul 19 05:04:29.927: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 05:04:29.933: INFO: Pod kube-proxy-mode-detector still exists
Jul 19 05:04:31.927: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 05:04:31.932: INFO: Pod kube-proxy-mode-detector still exists
Jul 19 05:04:33.927: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 05:04:33.931: INFO: Pod kube-proxy-mode-detector still exists
Jul 19 05:04:35.927: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 05:04:35.932: INFO: Pod kube-proxy-mode-detector still exists
Jul 19 05:04:37.927: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 05:04:37.932: INFO: Pod kube-proxy-mode-detector still exists
Jul 19 05:04:39.927: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 19 05:04:39.932: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-2224
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2224
I0719 05:04:39.956697      24 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2224, replica count: 3
I0719 05:04:43.006993      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 05:04:46.007180      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 05:04:46.014: INFO: Creating new exec pod
Jul 19 05:04:51.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2224 execpod-affinityfsmk6 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jul 19 05:04:51.328: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jul 19 05:04:51.328: INFO: stdout: ""
Jul 19 05:04:51.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2224 execpod-affinityfsmk6 -- /bin/sh -x -c nc -zv -t -w 2 10.247.80.224 80'
Jul 19 05:04:51.601: INFO: stderr: "+ nc -zv -t -w 2 10.247.80.224 80\nConnection to 10.247.80.224 80 port [tcp/http] succeeded!\n"
Jul 19 05:04:51.601: INFO: stdout: ""
Jul 19 05:04:51.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2224 execpod-affinityfsmk6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.247.80.224:80/ ; done'
Jul 19 05:04:52.111: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n"
Jul 19 05:04:52.111: INFO: stdout: "\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs\naffinity-clusterip-timeout-9whjs"
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Received response from host: affinity-clusterip-timeout-9whjs
Jul 19 05:04:52.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2224 execpod-affinityfsmk6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.247.80.224:80/'
Jul 19 05:04:52.425: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n"
Jul 19 05:04:52.425: INFO: stdout: "affinity-clusterip-timeout-9whjs"
Jul 19 05:05:07.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2224 execpod-affinityfsmk6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.247.80.224:80/'
Jul 19 05:05:07.820: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.247.80.224:80/\n"
Jul 19 05:05:07.820: INFO: stdout: "affinity-clusterip-timeout-mdjsh"
Jul 19 05:05:07.820: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2224, will wait for the garbage collector to delete the pods
Jul 19 05:05:07.898: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 9.891805ms
Jul 19 05:05:07.998: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.166785ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:05:29.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2224" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:72.101 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":180,"skipped":2922,"failed":0}
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:05:29.542: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 19 05:05:29.605: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Jul 19 05:05:30.114: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 19 05:05:32.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267930, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267930, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267930, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762267930, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5d4db574\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:05:35.628: INFO: Waited 1.421854776s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:05:36.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8413" for this suite.

• [SLOW TEST:6.961 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":181,"skipped":2924,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:05:36.504: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-61e684c0-464c-4bf1-8448-1dcbfcf8e7d1
STEP: Creating a pod to test consume configMaps
Jul 19 05:05:36.625: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-86e1e788-c60b-40bb-b649-c242f73eb507" in namespace "projected-8790" to be "Succeeded or Failed"
Jul 19 05:05:36.629: INFO: Pod "pod-projected-configmaps-86e1e788-c60b-40bb-b649-c242f73eb507": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017666ms
Jul 19 05:05:38.635: INFO: Pod "pod-projected-configmaps-86e1e788-c60b-40bb-b649-c242f73eb507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009520937s
Jul 19 05:05:40.639: INFO: Pod "pod-projected-configmaps-86e1e788-c60b-40bb-b649-c242f73eb507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014128389s
STEP: Saw pod success
Jul 19 05:05:40.639: INFO: Pod "pod-projected-configmaps-86e1e788-c60b-40bb-b649-c242f73eb507" satisfied condition "Succeeded or Failed"
Jul 19 05:05:40.642: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-projected-configmaps-86e1e788-c60b-40bb-b649-c242f73eb507 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 05:05:40.714: INFO: Waiting for pod pod-projected-configmaps-86e1e788-c60b-40bb-b649-c242f73eb507 to disappear
Jul 19 05:05:40.717: INFO: Pod pod-projected-configmaps-86e1e788-c60b-40bb-b649-c242f73eb507 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:05:40.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8790" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":182,"skipped":2924,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:05:40.735: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0719 05:05:50.833386      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0719 05:05:50.833402      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0719 05:05:50.833407      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 19 05:05:50.833: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:05:50.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2122" for this suite.

• [SLOW TEST:10.111 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":183,"skipped":2938,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:05:50.846: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 05:05:50.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ea644a8-fa23-42af-b761-0ea9020dd0ec" in namespace "downward-api-3611" to be "Succeeded or Failed"
Jul 19 05:05:50.925: INFO: Pod "downwardapi-volume-6ea644a8-fa23-42af-b761-0ea9020dd0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.021591ms
Jul 19 05:05:52.930: INFO: Pod "downwardapi-volume-6ea644a8-fa23-42af-b761-0ea9020dd0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01017444s
Jul 19 05:05:54.934: INFO: Pod "downwardapi-volume-6ea644a8-fa23-42af-b761-0ea9020dd0ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013713879s
STEP: Saw pod success
Jul 19 05:05:54.934: INFO: Pod "downwardapi-volume-6ea644a8-fa23-42af-b761-0ea9020dd0ec" satisfied condition "Succeeded or Failed"
Jul 19 05:05:54.937: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-6ea644a8-fa23-42af-b761-0ea9020dd0ec container client-container: <nil>
STEP: delete the pod
Jul 19 05:05:54.958: INFO: Waiting for pod downwardapi-volume-6ea644a8-fa23-42af-b761-0ea9020dd0ec to disappear
Jul 19 05:05:54.962: INFO: Pod downwardapi-volume-6ea644a8-fa23-42af-b761-0ea9020dd0ec no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:05:54.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3611" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":184,"skipped":2954,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:05:54.977: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 19 05:05:55.045: INFO: Waiting up to 5m0s for pod "pod-64ddeaa0-8973-4e04-90c9-a749a89a1fd7" in namespace "emptydir-3108" to be "Succeeded or Failed"
Jul 19 05:05:55.048: INFO: Pod "pod-64ddeaa0-8973-4e04-90c9-a749a89a1fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.239208ms
Jul 19 05:05:57.052: INFO: Pod "pod-64ddeaa0-8973-4e04-90c9-a749a89a1fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007032343s
Jul 19 05:05:59.056: INFO: Pod "pod-64ddeaa0-8973-4e04-90c9-a749a89a1fd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011762264s
STEP: Saw pod success
Jul 19 05:05:59.056: INFO: Pod "pod-64ddeaa0-8973-4e04-90c9-a749a89a1fd7" satisfied condition "Succeeded or Failed"
Jul 19 05:05:59.060: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-64ddeaa0-8973-4e04-90c9-a749a89a1fd7 container test-container: <nil>
STEP: delete the pod
Jul 19 05:05:59.102: INFO: Waiting for pod pod-64ddeaa0-8973-4e04-90c9-a749a89a1fd7 to disappear
Jul 19 05:05:59.106: INFO: Pod pod-64ddeaa0-8973-4e04-90c9-a749a89a1fd7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:05:59.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3108" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":185,"skipped":2964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:05:59.118: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 19 05:06:03.726: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3726 pod-service-account-89210342-a364-43ae-bdf5-c50e2ee649c9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 19 05:06:03.994: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3726 pod-service-account-89210342-a364-43ae-bdf5-c50e2ee649c9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 19 05:06:04.304: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3726 pod-service-account-89210342-a364-43ae-bdf5-c50e2ee649c9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:06:04.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3726" for this suite.

• [SLOW TEST:5.529 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":186,"skipped":3004,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:06:04.648: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:06:15.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1092" for this suite.

• [SLOW TEST:11.124 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":187,"skipped":3016,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:06:15.772: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-fdd69f1d-872a-4902-92ae-36ddab0707fa
STEP: Creating a pod to test consume secrets
Jul 19 05:06:15.930: INFO: Waiting up to 5m0s for pod "pod-secrets-291df731-9d65-4fe7-8894-d385d59f7b68" in namespace "secrets-6381" to be "Succeeded or Failed"
Jul 19 05:06:15.934: INFO: Pod "pod-secrets-291df731-9d65-4fe7-8894-d385d59f7b68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.718549ms
Jul 19 05:06:17.941: INFO: Pod "pod-secrets-291df731-9d65-4fe7-8894-d385d59f7b68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010627668s
Jul 19 05:06:19.945: INFO: Pod "pod-secrets-291df731-9d65-4fe7-8894-d385d59f7b68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014729828s
STEP: Saw pod success
Jul 19 05:06:19.945: INFO: Pod "pod-secrets-291df731-9d65-4fe7-8894-d385d59f7b68" satisfied condition "Succeeded or Failed"
Jul 19 05:06:19.949: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-secrets-291df731-9d65-4fe7-8894-d385d59f7b68 container secret-volume-test: <nil>
STEP: delete the pod
Jul 19 05:06:19.977: INFO: Waiting for pod pod-secrets-291df731-9d65-4fe7-8894-d385d59f7b68 to disappear
Jul 19 05:06:19.980: INFO: Pod pod-secrets-291df731-9d65-4fe7-8894-d385d59f7b68 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:06:19.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6381" for this suite.
STEP: Destroying namespace "secret-namespace-5773" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":188,"skipped":3035,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:06:20.002: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:06:20.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3525" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":189,"skipped":3108,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:06:20.144: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-eb3d7d0f-f660-4db5-a2b1-ee3b8ef9946b
STEP: Creating a pod to test consume secrets
Jul 19 05:06:20.264: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5bad0190-a40b-44f3-8777-135856d17b26" in namespace "projected-4541" to be "Succeeded or Failed"
Jul 19 05:06:20.270: INFO: Pod "pod-projected-secrets-5bad0190-a40b-44f3-8777-135856d17b26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.492045ms
Jul 19 05:06:22.285: INFO: Pod "pod-projected-secrets-5bad0190-a40b-44f3-8777-135856d17b26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021450226s
Jul 19 05:06:24.289: INFO: Pod "pod-projected-secrets-5bad0190-a40b-44f3-8777-135856d17b26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025395028s
STEP: Saw pod success
Jul 19 05:06:24.289: INFO: Pod "pod-projected-secrets-5bad0190-a40b-44f3-8777-135856d17b26" satisfied condition "Succeeded or Failed"
Jul 19 05:06:24.292: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-projected-secrets-5bad0190-a40b-44f3-8777-135856d17b26 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 19 05:06:24.329: INFO: Waiting for pod pod-projected-secrets-5bad0190-a40b-44f3-8777-135856d17b26 to disappear
Jul 19 05:06:24.333: INFO: Pod pod-projected-secrets-5bad0190-a40b-44f3-8777-135856d17b26 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:06:24.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4541" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":190,"skipped":3109,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:06:24.344: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8974.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8974.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8974.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8974.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 109.51.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.51.109_udp@PTR;check="$$(dig +tcp +noall +answer +search 109.51.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.51.109_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8974.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8974.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8974.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8974.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 109.51.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.51.109_udp@PTR;check="$$(dig +tcp +noall +answer +search 109.51.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.51.109_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 19 05:06:30.485: INFO: Unable to read wheezy_udp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.490: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.494: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.499: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.502: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.507: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.511: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.514: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.519: INFO: Unable to read 10.247.51.109_udp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.523: INFO: Unable to read 10.247.51.109_tcp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.527: INFO: Unable to read jessie_udp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.536: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.539: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.548: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.561: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.565: INFO: Unable to read jessie_udp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.569: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.573: INFO: Unable to read 10.247.51.109_udp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.576: INFO: Unable to read 10.247.51.109_tcp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:30.576: INFO: Lookups using dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291 failed for: [wheezy_udp@dns-test-service.dns-8974.svc.cluster.local wheezy_tcp@dns-test-service.dns-8974.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.247.51.109_udp@PTR 10.247.51.109_tcp@PTR jessie_udp@dns-test-service.dns-8974.svc.cluster.local jessie_tcp@dns-test-service.dns-8974.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord 10.247.51.109_udp@PTR 10.247.51.109_tcp@PTR]

Jul 19 05:06:35.581: INFO: Unable to read wheezy_udp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.586: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.590: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.594: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.597: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.600: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.604: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.607: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.611: INFO: Unable to read 10.247.51.109_udp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.614: INFO: Unable to read 10.247.51.109_tcp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.617: INFO: Unable to read jessie_udp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.621: INFO: Unable to read jessie_tcp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.625: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.630: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.633: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.637: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.640: INFO: Unable to read jessie_udp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.643: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.647: INFO: Unable to read 10.247.51.109_udp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.650: INFO: Unable to read 10.247.51.109_tcp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:35.650: INFO: Lookups using dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291 failed for: [wheezy_udp@dns-test-service.dns-8974.svc.cluster.local wheezy_tcp@dns-test-service.dns-8974.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.247.51.109_udp@PTR 10.247.51.109_tcp@PTR jessie_udp@dns-test-service.dns-8974.svc.cluster.local jessie_tcp@dns-test-service.dns-8974.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord 10.247.51.109_udp@PTR 10.247.51.109_tcp@PTR]

Jul 19 05:06:40.582: INFO: Unable to read wheezy_udp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.585: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.589: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.593: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.597: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.601: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.606: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.610: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.613: INFO: Unable to read 10.247.51.109_udp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.618: INFO: Unable to read 10.247.51.109_tcp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.622: INFO: Unable to read jessie_udp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.626: INFO: Unable to read jessie_tcp@dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.632: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.636: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.642: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.646: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.649: INFO: Unable to read jessie_udp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.654: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.659: INFO: Unable to read 10.247.51.109_udp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.663: INFO: Unable to read 10.247.51.109_tcp@PTR from pod dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291: the server could not find the requested resource (get pods dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291)
Jul 19 05:06:40.663: INFO: Lookups using dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291 failed for: [wheezy_udp@dns-test-service.dns-8974.svc.cluster.local wheezy_tcp@dns-test-service.dns-8974.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.247.51.109_udp@PTR 10.247.51.109_tcp@PTR jessie_udp@dns-test-service.dns-8974.svc.cluster.local jessie_tcp@dns-test-service.dns-8974.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8974.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-8974.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-8974.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord 10.247.51.109_udp@PTR 10.247.51.109_tcp@PTR]

Jul 19 05:06:45.661: INFO: DNS probes using dns-8974/dns-test-c4ef4625-5492-43d4-91c4-eac3b55fd291 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:06:45.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8974" for this suite.

• [SLOW TEST:21.429 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":191,"skipped":3115,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:06:45.774: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:07:17.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9954" for this suite.
STEP: Destroying namespace "nsdeletetest-2849" for this suite.
Jul 19 05:07:18.009: INFO: Namespace nsdeletetest-2849 was already deleted
STEP: Destroying namespace "nsdeletetest-8620" for this suite.

• [SLOW TEST:32.250 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":192,"skipped":3120,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:07:18.024: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:07:18.096: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Pending, waiting for it to be Running (with Ready = true)
Jul 19 05:07:20.102: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Pending, waiting for it to be Running (with Ready = true)
Jul 19 05:07:22.104: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Running (Ready = false)
Jul 19 05:07:24.101: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Running (Ready = false)
Jul 19 05:07:26.100: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Running (Ready = false)
Jul 19 05:07:28.100: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Running (Ready = false)
Jul 19 05:07:30.100: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Running (Ready = false)
Jul 19 05:07:32.101: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Running (Ready = false)
Jul 19 05:07:34.101: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Running (Ready = false)
Jul 19 05:07:36.100: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Running (Ready = false)
Jul 19 05:07:38.101: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Running (Ready = false)
Jul 19 05:07:40.101: INFO: The status of Pod test-webserver-09151ee8-3621-4f3b-b0e3-06f978b1e17d is Running (Ready = true)
Jul 19 05:07:40.106: INFO: Container started at 2021-07-19 05:07:20 +0000 UTC, pod became ready at 2021-07-19 05:07:39 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:07:40.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8228" for this suite.

• [SLOW TEST:22.094 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3130,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:07:40.118: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jul 19 05:07:40.182: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 05:08:40.276: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:08:40.285: INFO: Starting informer...
STEP: Starting pod...
Jul 19 05:08:40.506: INFO: Pod is running on paas-192-168-10-183. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul 19 05:08:40.533: INFO: Pod wasn't evicted. Proceeding
Jul 19 05:08:40.533: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul 19 05:09:55.605: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:09:55.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2949" for this suite.

• [SLOW TEST:135.500 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":194,"skipped":3132,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:09:55.619: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 05:09:55.690: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b" in namespace "downward-api-1129" to be "Succeeded or Failed"
Jul 19 05:09:55.695: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.859723ms
Jul 19 05:09:57.699: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009133473s
Jul 19 05:09:59.707: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017061274s
Jul 19 05:10:01.711: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020885915s
Jul 19 05:10:03.715: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025152674s
Jul 19 05:10:05.720: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029522291s
Jul 19 05:10:07.725: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034270282s
Jul 19 05:10:09.729: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.038622933s
Jul 19 05:10:11.734: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.043454541s
Jul 19 05:10:13.738: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.04768178s
Jul 19 05:10:15.741: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.051153158s
Jul 19 05:10:17.747: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.056860986s
Jul 19 05:10:19.751: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.061110634s
Jul 19 05:10:21.756: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.065278522s
Jul 19 05:10:23.759: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 28.068952853s
Jul 19 05:10:25.764: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.073663528s
Jul 19 05:10:27.769: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 32.078562196s
Jul 19 05:10:29.774: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.084157205s
Jul 19 05:10:31.779: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 36.088528326s
Jul 19 05:10:33.784: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 38.093968492s
Jul 19 05:10:35.788: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 40.098057559s
Jul 19 05:10:37.793: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Pending", Reason="", readiness=false. Elapsed: 42.103104004s
Jul 19 05:10:39.799: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Running", Reason="", readiness=true. Elapsed: 44.109001783s
Jul 19 05:10:41.803: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Running", Reason="", readiness=true. Elapsed: 46.11272098s
Jul 19 05:10:43.807: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Running", Reason="", readiness=true. Elapsed: 48.117197816s
Jul 19 05:10:45.812: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Running", Reason="", readiness=true. Elapsed: 50.121453054s
Jul 19 05:10:47.828: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Running", Reason="", readiness=true. Elapsed: 52.137677316s
Jul 19 05:10:49.833: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Running", Reason="", readiness=true. Elapsed: 54.142529343s
Jul 19 05:10:51.837: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Running", Reason="", readiness=true. Elapsed: 56.14705182s
Jul 19 05:10:53.841: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Running", Reason="", readiness=true. Elapsed: 58.15102212s
Jul 19 05:10:55.846: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Running", Reason="", readiness=true. Elapsed: 1m0.155655164s
Jul 19 05:10:57.850: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m2.160084781s
STEP: Saw pod success
Jul 19 05:10:57.850: INFO: Pod "downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b" satisfied condition "Succeeded or Failed"
Jul 19 05:10:57.854: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b container client-container: <nil>
STEP: delete the pod
Jul 19 05:10:57.940: INFO: Waiting for pod downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b to disappear
Jul 19 05:10:57.943: INFO: Pod downwardapi-volume-99691dd5-e904-40a3-95a7-a8bb63d0681b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:10:57.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1129" for this suite.

• [SLOW TEST:62.343 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":195,"skipped":3141,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:10:57.962: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 19 05:10:58.053: INFO: Waiting up to 5m0s for pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab" in namespace "emptydir-861" to be "Succeeded or Failed"
Jul 19 05:10:58.058: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.983242ms
Jul 19 05:11:00.063: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009768372s
Jul 19 05:11:02.067: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013693773s
Jul 19 05:11:04.073: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019009766s
Jul 19 05:11:06.077: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023600742s
Jul 19 05:11:08.082: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028836886s
Jul 19 05:11:10.088: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034280203s
Jul 19 05:11:12.094: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 14.040075065s
Jul 19 05:11:14.098: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 16.044581572s
Jul 19 05:11:16.102: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 18.048526701s
Jul 19 05:11:18.107: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 20.053196275s
Jul 19 05:11:20.112: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 22.058364064s
Jul 19 05:11:22.116: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 24.062618976s
Jul 19 05:11:24.120: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 26.066579189s
Jul 19 05:11:26.124: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 28.070159283s
Jul 19 05:11:28.128: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 30.074321523s
Jul 19 05:11:30.133: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 32.079950114s
Jul 19 05:11:32.137: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 34.083838216s
Jul 19 05:11:34.142: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 36.088317142s
Jul 19 05:11:36.147: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 38.093196223s
Jul 19 05:11:38.151: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 40.09704237s
Jul 19 05:11:40.171: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 42.11751409s
Jul 19 05:11:42.176: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 44.122937458s
Jul 19 05:11:44.184: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 46.129987992s
Jul 19 05:11:46.190: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 48.136923044s
Jul 19 05:11:48.196: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 50.142422587s
Jul 19 05:11:50.201: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 52.147700781s
Jul 19 05:11:52.206: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 54.152439731s
Jul 19 05:11:54.216: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 56.162451993s
Jul 19 05:11:56.221: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 58.167438693s
Jul 19 05:11:58.225: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.171689888s
Jul 19 05:12:00.229: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.175817405s
Jul 19 05:12:02.235: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.181274052s
Jul 19 05:12:04.240: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.186066797s
Jul 19 05:12:06.244: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.190056995s
Jul 19 05:12:08.248: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.194438203s
Jul 19 05:12:10.266: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.212026826s
Jul 19 05:12:12.271: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.217024961s
Jul 19 05:12:14.277: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.223116462s
Jul 19 05:12:16.296: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.242305065s
Jul 19 05:12:18.301: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.246971813s
Jul 19 05:12:20.305: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Running", Reason="", readiness=true. Elapsed: 1m22.251110542s
Jul 19 05:12:22.310: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Running", Reason="", readiness=true. Elapsed: 1m24.256064417s
Jul 19 05:12:24.314: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Running", Reason="", readiness=true. Elapsed: 1m26.260603586s
Jul 19 05:12:26.391: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Running", Reason="", readiness=true. Elapsed: 1m28.337835125s
Jul 19 05:12:28.396: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Running", Reason="", readiness=true. Elapsed: 1m30.342484277s
Jul 19 05:12:30.400: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Running", Reason="", readiness=true. Elapsed: 1m32.346278609s
Jul 19 05:12:32.404: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m34.350533313s
STEP: Saw pod success
Jul 19 05:12:32.404: INFO: Pod "pod-866e4d14-027d-4f74-aa02-72e9f391a1ab" satisfied condition "Succeeded or Failed"
Jul 19 05:12:32.407: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-866e4d14-027d-4f74-aa02-72e9f391a1ab container test-container: <nil>
STEP: delete the pod
Jul 19 05:12:32.477: INFO: Waiting for pod pod-866e4d14-027d-4f74-aa02-72e9f391a1ab to disappear
Jul 19 05:12:32.480: INFO: Pod pod-866e4d14-027d-4f74-aa02-72e9f391a1ab no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:12:32.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-861" for this suite.

• [SLOW TEST:94.530 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":196,"skipped":3151,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:12:32.492: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 19 05:12:44.619: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 05:12:44.623: INFO: Pod pod-with-poststart-http-hook still exists
Jul 19 05:12:46.623: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 05:12:46.627: INFO: Pod pod-with-poststart-http-hook still exists
Jul 19 05:12:48.623: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 05:12:48.627: INFO: Pod pod-with-poststart-http-hook still exists
Jul 19 05:12:50.623: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 05:12:50.627: INFO: Pod pod-with-poststart-http-hook still exists
Jul 19 05:12:52.623: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 05:12:52.628: INFO: Pod pod-with-poststart-http-hook still exists
Jul 19 05:12:54.623: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 05:12:54.628: INFO: Pod pod-with-poststart-http-hook still exists
Jul 19 05:12:56.623: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 05:12:56.628: INFO: Pod pod-with-poststart-http-hook still exists
Jul 19 05:12:58.623: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 05:12:58.627: INFO: Pod pod-with-poststart-http-hook still exists
Jul 19 05:13:00.623: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 05:13:00.627: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:13:00.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6492" for this suite.

• [SLOW TEST:28.146 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":197,"skipped":3158,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:13:00.638: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 05:13:00.719: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d268db2-6f25-403f-bc09-a11b2863de0a" in namespace "downward-api-1694" to be "Succeeded or Failed"
Jul 19 05:13:00.722: INFO: Pod "downwardapi-volume-8d268db2-6f25-403f-bc09-a11b2863de0a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442764ms
Jul 19 05:13:02.726: INFO: Pod "downwardapi-volume-8d268db2-6f25-403f-bc09-a11b2863de0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007462438s
Jul 19 05:13:04.730: INFO: Pod "downwardapi-volume-8d268db2-6f25-403f-bc09-a11b2863de0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011254826s
STEP: Saw pod success
Jul 19 05:13:04.730: INFO: Pod "downwardapi-volume-8d268db2-6f25-403f-bc09-a11b2863de0a" satisfied condition "Succeeded or Failed"
Jul 19 05:13:04.734: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-8d268db2-6f25-403f-bc09-a11b2863de0a container client-container: <nil>
STEP: delete the pod
Jul 19 05:13:04.787: INFO: Waiting for pod downwardapi-volume-8d268db2-6f25-403f-bc09-a11b2863de0a to disappear
Jul 19 05:13:04.792: INFO: Pod downwardapi-volume-8d268db2-6f25-403f-bc09-a11b2863de0a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:13:04.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1694" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":198,"skipped":3160,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:13:04.803: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 19 05:13:09.952: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:13:09.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-969" for this suite.

• [SLOW TEST:5.190 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":199,"skipped":3175,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:13:09.994: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 05:13:10.427: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 05:13:12.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268390, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268390, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268390, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268390, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:13:14.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268390, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268390, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268390, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268390, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 05:13:17.460: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:13:17.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6137" for this suite.
STEP: Destroying namespace "webhook-6137-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.625 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":200,"skipped":3190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:13:17.619: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:13:17.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2975" for this suite.
STEP: Destroying namespace "nspatchtest-fd3e4a59-b8ac-4c53-a17c-b550d2c72acd-9521" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":201,"skipped":3219,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:13:17.911: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:13:22.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-339" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":202,"skipped":3228,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:13:22.034: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:13:22.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 version'
Jul 19 05:13:22.194: INFO: stderr: ""
Jul 19 05:13:22.194: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:17:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19+\", GitVersion:\"v1.19.4-Fusionstage21.3.0\", GitCommit:\"04232fc5f4bcbb27943acd6073e83c9bca959771\", GitTreeState:\"clean\", BuildDate:\"2021-06-24T20:28:11Z\", GoVersion:\"go1.16.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:13:22.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8583" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":203,"skipped":3248,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:13:22.213: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-6471/secret-test-4f8d764d-8910-41b7-8068-89cc7a6d67a8
STEP: Creating a pod to test consume secrets
Jul 19 05:13:22.318: INFO: Waiting up to 5m0s for pod "pod-configmaps-3cdb58a2-665a-495e-9ba6-6ad70bb66e38" in namespace "secrets-6471" to be "Succeeded or Failed"
Jul 19 05:13:22.324: INFO: Pod "pod-configmaps-3cdb58a2-665a-495e-9ba6-6ad70bb66e38": Phase="Pending", Reason="", readiness=false. Elapsed: 6.097319ms
Jul 19 05:13:24.329: INFO: Pod "pod-configmaps-3cdb58a2-665a-495e-9ba6-6ad70bb66e38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011338654s
Jul 19 05:13:26.333: INFO: Pod "pod-configmaps-3cdb58a2-665a-495e-9ba6-6ad70bb66e38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015350798s
STEP: Saw pod success
Jul 19 05:13:26.333: INFO: Pod "pod-configmaps-3cdb58a2-665a-495e-9ba6-6ad70bb66e38" satisfied condition "Succeeded or Failed"
Jul 19 05:13:26.337: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-configmaps-3cdb58a2-665a-495e-9ba6-6ad70bb66e38 container env-test: <nil>
STEP: delete the pod
Jul 19 05:13:26.396: INFO: Waiting for pod pod-configmaps-3cdb58a2-665a-495e-9ba6-6ad70bb66e38 to disappear
Jul 19 05:13:26.400: INFO: Pod pod-configmaps-3cdb58a2-665a-495e-9ba6-6ad70bb66e38 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:13:26.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6471" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":204,"skipped":3259,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:13:26.415: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-38d9fab3-bd12-4ed2-9833-d11186096b25 in namespace container-probe-6940
Jul 19 05:13:30.557: INFO: Started pod liveness-38d9fab3-bd12-4ed2-9833-d11186096b25 in namespace container-probe-6940
STEP: checking the pod's current state and verifying that restartCount is present
Jul 19 05:13:30.561: INFO: Initial restart count of pod liveness-38d9fab3-bd12-4ed2-9833-d11186096b25 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:17:31.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6940" for this suite.

• [SLOW TEST:245.062 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":205,"skipped":3260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:17:31.478: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-5vzf
STEP: Creating a pod to test atomic-volume-subpath
Jul 19 05:17:31.564: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5vzf" in namespace "subpath-9194" to be "Succeeded or Failed"
Jul 19 05:17:31.567: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.19659ms
Jul 19 05:17:33.571: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007085045s
Jul 19 05:17:35.576: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Running", Reason="", readiness=true. Elapsed: 4.011658403s
Jul 19 05:17:37.580: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Running", Reason="", readiness=true. Elapsed: 6.016377066s
Jul 19 05:17:39.585: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Running", Reason="", readiness=true. Elapsed: 8.02121409s
Jul 19 05:17:41.590: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Running", Reason="", readiness=true. Elapsed: 10.026069407s
Jul 19 05:17:43.595: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Running", Reason="", readiness=true. Elapsed: 12.031363991s
Jul 19 05:17:45.600: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Running", Reason="", readiness=true. Elapsed: 14.035903281s
Jul 19 05:17:47.605: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Running", Reason="", readiness=true. Elapsed: 16.040562519s
Jul 19 05:17:49.608: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Running", Reason="", readiness=true. Elapsed: 18.044545944s
Jul 19 05:17:51.612: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Running", Reason="", readiness=true. Elapsed: 20.048483202s
Jul 19 05:17:53.617: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Running", Reason="", readiness=true. Elapsed: 22.053534246s
Jul 19 05:17:55.622: INFO: Pod "pod-subpath-test-projected-5vzf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.058058538s
STEP: Saw pod success
Jul 19 05:17:55.622: INFO: Pod "pod-subpath-test-projected-5vzf" satisfied condition "Succeeded or Failed"
Jul 19 05:17:55.626: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-subpath-test-projected-5vzf container test-container-subpath-projected-5vzf: <nil>
STEP: delete the pod
Jul 19 05:17:55.726: INFO: Waiting for pod pod-subpath-test-projected-5vzf to disappear
Jul 19 05:17:55.730: INFO: Pod pod-subpath-test-projected-5vzf no longer exists
STEP: Deleting pod pod-subpath-test-projected-5vzf
Jul 19 05:17:55.730: INFO: Deleting pod "pod-subpath-test-projected-5vzf" in namespace "subpath-9194"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:17:55.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9194" for this suite.

• [SLOW TEST:24.267 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":206,"skipped":3290,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:17:55.745: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 05:17:55.830: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ec6303a-8210-4d95-a092-d08c35a4c762" in namespace "projected-4782" to be "Succeeded or Failed"
Jul 19 05:17:55.833: INFO: Pod "downwardapi-volume-7ec6303a-8210-4d95-a092-d08c35a4c762": Phase="Pending", Reason="", readiness=false. Elapsed: 3.207261ms
Jul 19 05:17:57.837: INFO: Pod "downwardapi-volume-7ec6303a-8210-4d95-a092-d08c35a4c762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007844232s
Jul 19 05:17:59.843: INFO: Pod "downwardapi-volume-7ec6303a-8210-4d95-a092-d08c35a4c762": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013534288s
Jul 19 05:18:01.848: INFO: Pod "downwardapi-volume-7ec6303a-8210-4d95-a092-d08c35a4c762": Phase="Running", Reason="", readiness=true. Elapsed: 6.017888911s
Jul 19 05:18:03.853: INFO: Pod "downwardapi-volume-7ec6303a-8210-4d95-a092-d08c35a4c762": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023448331s
STEP: Saw pod success
Jul 19 05:18:03.853: INFO: Pod "downwardapi-volume-7ec6303a-8210-4d95-a092-d08c35a4c762" satisfied condition "Succeeded or Failed"
Jul 19 05:18:03.857: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-7ec6303a-8210-4d95-a092-d08c35a4c762 container client-container: <nil>
STEP: delete the pod
Jul 19 05:18:03.887: INFO: Waiting for pod downwardapi-volume-7ec6303a-8210-4d95-a092-d08c35a4c762 to disappear
Jul 19 05:18:03.891: INFO: Pod downwardapi-volume-7ec6303a-8210-4d95-a092-d08c35a4c762 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:18:03.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4782" for this suite.

• [SLOW TEST:8.158 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":207,"skipped":3306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:18:03.904: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 19 05:18:04.008: INFO: Waiting up to 5m0s for pod "pod-060dfe58-8002-4ca1-89d4-92c96110d59d" in namespace "emptydir-7318" to be "Succeeded or Failed"
Jul 19 05:18:04.011: INFO: Pod "pod-060dfe58-8002-4ca1-89d4-92c96110d59d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.464502ms
Jul 19 05:18:06.016: INFO: Pod "pod-060dfe58-8002-4ca1-89d4-92c96110d59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007834727s
Jul 19 05:18:08.020: INFO: Pod "pod-060dfe58-8002-4ca1-89d4-92c96110d59d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011977583s
Jul 19 05:18:10.024: INFO: Pod "pod-060dfe58-8002-4ca1-89d4-92c96110d59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016471654s
STEP: Saw pod success
Jul 19 05:18:10.024: INFO: Pod "pod-060dfe58-8002-4ca1-89d4-92c96110d59d" satisfied condition "Succeeded or Failed"
Jul 19 05:18:10.028: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-060dfe58-8002-4ca1-89d4-92c96110d59d container test-container: <nil>
STEP: delete the pod
Jul 19 05:18:10.056: INFO: Waiting for pod pod-060dfe58-8002-4ca1-89d4-92c96110d59d to disappear
Jul 19 05:18:10.059: INFO: Pod pod-060dfe58-8002-4ca1-89d4-92c96110d59d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:18:10.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7318" for this suite.

• [SLOW TEST:6.170 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":208,"skipped":3334,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:18:10.074: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 19 05:18:10.511: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 19 05:18:12.523: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268690, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268690, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268690, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268690, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59d7ccfb84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:18:14.527: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268690, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268690, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268690, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762268690, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59d7ccfb84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 05:18:17.540: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:18:17.545: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:18:18.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1450" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.216 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":209,"skipped":3342,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:18:19.290: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:18:23.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7613" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":210,"skipped":3346,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:18:23.515: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-023b350e-dae9-4863-bc1d-b40ea10cac31
STEP: Creating configMap with name cm-test-opt-upd-32f85683-55a8-4fcf-b766-f82678a7e394
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-023b350e-dae9-4863-bc1d-b40ea10cac31
STEP: Updating configmap cm-test-opt-upd-32f85683-55a8-4fcf-b766-f82678a7e394
STEP: Creating configMap with name cm-test-opt-create-68635bef-1261-4094-b82c-1197806fc3ff
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:18:31.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6203" for this suite.

• [SLOW TEST:8.314 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":211,"skipped":3364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:18:31.830: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-e6d3c7b9-0aec-46f6-8515-0c76ad10ff57
STEP: Creating a pod to test consume configMaps
Jul 19 05:18:31.947: INFO: Waiting up to 5m0s for pod "pod-configmaps-e3874193-917c-4717-8dfe-da08dcfec782" in namespace "configmap-8133" to be "Succeeded or Failed"
Jul 19 05:18:31.950: INFO: Pod "pod-configmaps-e3874193-917c-4717-8dfe-da08dcfec782": Phase="Pending", Reason="", readiness=false. Elapsed: 3.354253ms
Jul 19 05:18:33.954: INFO: Pod "pod-configmaps-e3874193-917c-4717-8dfe-da08dcfec782": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007143718s
Jul 19 05:18:35.958: INFO: Pod "pod-configmaps-e3874193-917c-4717-8dfe-da08dcfec782": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011510927s
STEP: Saw pod success
Jul 19 05:18:35.958: INFO: Pod "pod-configmaps-e3874193-917c-4717-8dfe-da08dcfec782" satisfied condition "Succeeded or Failed"
Jul 19 05:18:35.962: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-configmaps-e3874193-917c-4717-8dfe-da08dcfec782 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 05:18:36.024: INFO: Waiting for pod pod-configmaps-e3874193-917c-4717-8dfe-da08dcfec782 to disappear
Jul 19 05:18:36.030: INFO: Pod pod-configmaps-e3874193-917c-4717-8dfe-da08dcfec782 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:18:36.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8133" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":212,"skipped":3386,"failed":0}

------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:18:36.053: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4560
STEP: creating service affinity-clusterip-transition in namespace services-4560
STEP: creating replication controller affinity-clusterip-transition in namespace services-4560
I0719 05:18:36.146831      24 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-4560, replica count: 3
I0719 05:18:39.197137      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 05:18:42.197343      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 05:18:42.204: INFO: Creating new exec pod
Jul 19 05:18:47.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-4560 execpod-affinityv2s28 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jul 19 05:18:47.926: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul 19 05:18:47.926: INFO: stdout: ""
Jul 19 05:18:47.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-4560 execpod-affinityv2s28 -- /bin/sh -x -c nc -zv -t -w 2 10.247.61.41 80'
Jul 19 05:18:48.154: INFO: stderr: "+ nc -zv -t -w 2 10.247.61.41 80\nConnection to 10.247.61.41 80 port [tcp/http] succeeded!\n"
Jul 19 05:18:48.154: INFO: stdout: ""
Jul 19 05:18:48.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-4560 execpod-affinityv2s28 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.247.61.41:80/ ; done'
Jul 19 05:18:48.576: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n"
Jul 19 05:18:48.576: INFO: stdout: "\naffinity-clusterip-transition-ddq8x\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-ddq8x\naffinity-clusterip-transition-ddq8x\naffinity-clusterip-transition-mkxgp\naffinity-clusterip-transition-ddq8x\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-ddq8x\naffinity-clusterip-transition-mkxgp\naffinity-clusterip-transition-mkxgp\naffinity-clusterip-transition-ddq8x\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-mkxgp"
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-ddq8x
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-ddq8x
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-ddq8x
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-mkxgp
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-ddq8x
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-ddq8x
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-mkxgp
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-mkxgp
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-ddq8x
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.576: INFO: Received response from host: affinity-clusterip-transition-mkxgp
Jul 19 05:18:48.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-4560 execpod-affinityv2s28 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.247.61.41:80/ ; done'
Jul 19 05:18:48.946: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.61.41:80/\n"
Jul 19 05:18:48.946: INFO: stdout: "\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd\naffinity-clusterip-transition-td5kd"
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Received response from host: affinity-clusterip-transition-td5kd
Jul 19 05:18:48.946: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4560, will wait for the garbage collector to delete the pods
Jul 19 05:18:49.027: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.59796ms
Jul 19 05:18:49.127: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.187575ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:19:09.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4560" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:33.558 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":213,"skipped":3386,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:19:09.611: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:19:25.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-987" for this suite.

• [SLOW TEST:16.262 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":214,"skipped":3403,"failed":0}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:19:25.873: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 19 05:19:25.925: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:19:34.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7091" for this suite.

• [SLOW TEST:8.821 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":215,"skipped":3410,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:19:34.694: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Jul 19 05:19:34.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-6233'
Jul 19 05:19:35.074: INFO: stderr: ""
Jul 19 05:19:35.074: INFO: stdout: "pod/pause created\n"
Jul 19 05:19:35.074: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 19 05:19:35.074: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6233" to be "running and ready"
Jul 19 05:19:35.080: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.436703ms
Jul 19 05:19:37.085: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011113418s
Jul 19 05:19:39.090: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.015703987s
Jul 19 05:19:39.090: INFO: Pod "pause" satisfied condition "running and ready"
Jul 19 05:19:39.090: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 19 05:19:39.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 label pods pause testing-label=testing-label-value --namespace=kubectl-6233'
Jul 19 05:19:39.187: INFO: stderr: ""
Jul 19 05:19:39.187: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 19 05:19:39.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pod pause -L testing-label --namespace=kubectl-6233'
Jul 19 05:19:39.285: INFO: stderr: ""
Jul 19 05:19:39.285: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 19 05:19:39.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 label pods pause testing-label- --namespace=kubectl-6233'
Jul 19 05:19:39.390: INFO: stderr: ""
Jul 19 05:19:39.390: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 19 05:19:39.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pod pause -L testing-label --namespace=kubectl-6233'
Jul 19 05:19:39.483: INFO: stderr: ""
Jul 19 05:19:39.483: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Jul 19 05:19:39.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete --grace-period=0 --force -f - --namespace=kubectl-6233'
Jul 19 05:19:39.583: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 05:19:39.583: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 19 05:19:39.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get rc,svc -l name=pause --no-headers --namespace=kubectl-6233'
Jul 19 05:19:39.676: INFO: stderr: "No resources found in kubectl-6233 namespace.\n"
Jul 19 05:19:39.676: INFO: stdout: ""
Jul 19 05:19:39.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pods -l name=pause --namespace=kubectl-6233 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 19 05:19:39.768: INFO: stderr: ""
Jul 19 05:19:39.768: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:19:39.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6233" for this suite.

• [SLOW TEST:5.087 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1330
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":216,"skipped":3434,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:19:39.781: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 19 05:19:43.904: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9172 PodName:pod-sharedvolume-58a8f56e-7307-4fa4-ae65-1f71635ac0d7 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 05:19:43.904: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 05:19:44.068: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:19:44.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9172" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":217,"skipped":3453,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:19:44.085: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jul 19 05:19:44.182: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:20:03.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1035" for this suite.

• [SLOW TEST:19.588 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":218,"skipped":3463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:20:03.674: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Jul 19 05:22:04.276: INFO: Successfully updated pod "var-expansion-fff70f1a-2d4d-40ad-90df-6e4b32db7197"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jul 19 05:22:06.300: INFO: Deleting pod "var-expansion-fff70f1a-2d4d-40ad-90df-6e4b32db7197" in namespace "var-expansion-1446"
Jul 19 05:22:06.308: INFO: Wait up to 5m0s for pod "var-expansion-fff70f1a-2d4d-40ad-90df-6e4b32db7197" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:22:54.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1446" for this suite.

• [SLOW TEST:170.660 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":219,"skipped":3497,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:22:54.333: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:22:54.453: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 19 05:22:58.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-7055 create -f -'
Jul 19 05:22:59.092: INFO: stderr: ""
Jul 19 05:22:59.092: INFO: stdout: "e2e-test-crd-publish-openapi-3504-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 19 05:22:59.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-7055 delete e2e-test-crd-publish-openapi-3504-crds test-cr'
Jul 19 05:22:59.184: INFO: stderr: ""
Jul 19 05:22:59.184: INFO: stdout: "e2e-test-crd-publish-openapi-3504-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 19 05:22:59.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-7055 apply -f -'
Jul 19 05:22:59.446: INFO: stderr: ""
Jul 19 05:22:59.446: INFO: stdout: "e2e-test-crd-publish-openapi-3504-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 19 05:22:59.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-7055 delete e2e-test-crd-publish-openapi-3504-crds test-cr'
Jul 19 05:22:59.542: INFO: stderr: ""
Jul 19 05:22:59.542: INFO: stdout: "e2e-test-crd-publish-openapi-3504-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul 19 05:22:59.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 explain e2e-test-crd-publish-openapi-3504-crds'
Jul 19 05:22:59.749: INFO: stderr: ""
Jul 19 05:22:59.749: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3504-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:23:03.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7055" for this suite.

• [SLOW TEST:8.903 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":220,"skipped":3499,"failed":0}
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:23:03.236: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jul 19 05:23:03.358: INFO: namespace kubectl-2336
Jul 19 05:23:03.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 create -f - --namespace=kubectl-2336'
Jul 19 05:23:03.611: INFO: stderr: ""
Jul 19 05:23:03.611: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 19 05:23:04.615: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 05:23:04.615: INFO: Found 0 / 1
Jul 19 05:23:05.615: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 05:23:05.615: INFO: Found 0 / 1
Jul 19 05:23:06.615: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 05:23:06.615: INFO: Found 0 / 1
Jul 19 05:23:07.615: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 05:23:07.615: INFO: Found 1 / 1
Jul 19 05:23:07.615: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 19 05:23:07.620: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 05:23:07.620: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 19 05:23:07.620: INFO: wait on agnhost-primary startup in kubectl-2336 
Jul 19 05:23:07.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 logs agnhost-primary-qtv5b agnhost-primary --namespace=kubectl-2336'
Jul 19 05:23:07.756: INFO: stderr: ""
Jul 19 05:23:07.756: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul 19 05:23:07.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2336'
Jul 19 05:23:07.882: INFO: stderr: ""
Jul 19 05:23:07.882: INFO: stdout: "service/rm2 exposed\n"
Jul 19 05:23:07.886: INFO: Service rm2 in namespace kubectl-2336 found.
STEP: exposing service
Jul 19 05:23:09.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2336'
Jul 19 05:23:10.011: INFO: stderr: ""
Jul 19 05:23:10.011: INFO: stdout: "service/rm3 exposed\n"
Jul 19 05:23:10.015: INFO: Service rm3 in namespace kubectl-2336 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:23:12.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2336" for this suite.

• [SLOW TEST:8.809 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":221,"skipped":3499,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:23:12.045: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:23:12.112: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 19 05:23:17.116: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 19 05:23:17.116: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 19 05:23:17.138: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5036 /apis/apps/v1/namespaces/deployment-5036/deployments/test-cleanup-deployment 802aca14-dc18-4d6f-8c1b-f6019eda2f43 2439079 1 2021-07-19 05:23:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d6c8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:1,MaxSurge:1,},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jul 19 05:23:17.142: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jul 19 05:23:17.142: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul 19 05:23:17.142: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5036 /apis/apps/v1/namespaces/deployment-5036/replicasets/test-cleanup-controller bf4096ef-c5bd-4ba8-be11-225bcd87b025 2439080 1 2021-07-19 05:23:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 802aca14-dc18-4d6f-8c1b-f6019eda2f43 0xc004d6cde7 0xc004d6cde8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004d6ce68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 19 05:23:17.163: INFO: Pod "test-cleanup-controller-mhv74" is available:
&Pod{ObjectMeta:{test-cleanup-controller-mhv74 test-cleanup-controller- deployment-5036 /api/v1/namespaces/deployment-5036/pods/test-cleanup-controller-mhv74 f1a73d99-28fd-4b6f-811e-ec72c5bb558e 2439065 0 2021-07-19 05:23:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller bf4096ef-c5bd-4ba8-be11-225bcd87b025 0xc004d6d337 0xc004d6d338}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sqxz5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sqxz5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sqxz5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 05:23:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 05:23:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 05:23:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 05:23:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 05:23:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.183,PodIP:172.16.0.71,StartTime:2021-07-19 05:23:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 05:23:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker://sha256:0c388cccfd046fb7f46560e6605e128f0bd0c2bb2f5858b84b0f16d1497e32a6,ContainerID:docker://233a529780f21eeaa037de3a1afe5232cb1bedc452931680a5366489d92955e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:23:17.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5036" for this suite.

• [SLOW TEST:5.153 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":222,"skipped":3500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:23:17.198: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 19 05:23:20.400: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:23:20.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8966" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":223,"skipped":3540,"failed":0}

------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:23:20.476: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 19 05:23:20.976: INFO: Pod name wrapped-volume-race-4d97204e-17eb-410f-987f-5ecfeb2d3647: Found 0 pods out of 5
Jul 19 05:23:25.982: INFO: Pod name wrapped-volume-race-4d97204e-17eb-410f-987f-5ecfeb2d3647: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4d97204e-17eb-410f-987f-5ecfeb2d3647 in namespace emptydir-wrapper-8544, will wait for the garbage collector to delete the pods
Jul 19 05:23:38.070: INFO: Deleting ReplicationController wrapped-volume-race-4d97204e-17eb-410f-987f-5ecfeb2d3647 took: 12.116944ms
Jul 19 05:23:38.170: INFO: Terminating ReplicationController wrapped-volume-race-4d97204e-17eb-410f-987f-5ecfeb2d3647 pods took: 100.184875ms
STEP: Creating RC which spawns configmap-volume pods
Jul 19 05:23:54.493: INFO: Pod name wrapped-volume-race-01faa899-c4ec-4f1c-ad4a-1ea460b8d384: Found 0 pods out of 5
Jul 19 05:23:59.500: INFO: Pod name wrapped-volume-race-01faa899-c4ec-4f1c-ad4a-1ea460b8d384: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-01faa899-c4ec-4f1c-ad4a-1ea460b8d384 in namespace emptydir-wrapper-8544, will wait for the garbage collector to delete the pods
Jul 19 05:24:13.605: INFO: Deleting ReplicationController wrapped-volume-race-01faa899-c4ec-4f1c-ad4a-1ea460b8d384 took: 26.281319ms
Jul 19 05:24:13.705: INFO: Terminating ReplicationController wrapped-volume-race-01faa899-c4ec-4f1c-ad4a-1ea460b8d384 pods took: 100.17277ms
STEP: Creating RC which spawns configmap-volume pods
Jul 19 05:24:29.626: INFO: Pod name wrapped-volume-race-541cee5d-30f0-48cb-83ce-a8dcff2df0a4: Found 0 pods out of 5
Jul 19 05:24:34.633: INFO: Pod name wrapped-volume-race-541cee5d-30f0-48cb-83ce-a8dcff2df0a4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-541cee5d-30f0-48cb-83ce-a8dcff2df0a4 in namespace emptydir-wrapper-8544, will wait for the garbage collector to delete the pods
Jul 19 05:24:48.723: INFO: Deleting ReplicationController wrapped-volume-race-541cee5d-30f0-48cb-83ce-a8dcff2df0a4 took: 10.0037ms
Jul 19 05:24:48.823: INFO: Terminating ReplicationController wrapped-volume-race-541cee5d-30f0-48cb-83ce-a8dcff2df0a4 pods took: 100.168699ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:25:05.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8544" for this suite.

• [SLOW TEST:104.560 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":224,"skipped":3540,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:25:05.037: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jul 19 05:25:05.087: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:25:23.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1961" for this suite.

• [SLOW TEST:18.046 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":225,"skipped":3580,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:25:23.083: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:25:23.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1325" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":226,"skipped":3598,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:25:23.177: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 05:25:23.988: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 05:25:25.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269124, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269124, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269124, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269123, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:25:28.006: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269124, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269124, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269124, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269123, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 05:25:31.033: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:25:31.038: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1005-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:25:32.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5628" for this suite.
STEP: Destroying namespace "webhook-5628-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.240 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":227,"skipped":3614,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:25:32.418: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9971.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9971.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9971.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9971.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9971.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9971.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 19 05:25:38.878: INFO: DNS probes using dns-9971/dns-test-21c27977-1e0f-49f5-9042-677cb7700306 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:25:38.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9971" for this suite.

• [SLOW TEST:6.490 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":228,"skipped":3619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:25:38.908: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-04d977a3-b581-4f59-9cbc-92db574d84de
STEP: Creating a pod to test consume secrets
Jul 19 05:25:39.018: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-24a41f32-be63-499c-a898-c3e5d911562d" in namespace "projected-7364" to be "Succeeded or Failed"
Jul 19 05:25:39.024: INFO: Pod "pod-projected-secrets-24a41f32-be63-499c-a898-c3e5d911562d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.18511ms
Jul 19 05:25:41.028: INFO: Pod "pod-projected-secrets-24a41f32-be63-499c-a898-c3e5d911562d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010593802s
Jul 19 05:25:43.033: INFO: Pod "pod-projected-secrets-24a41f32-be63-499c-a898-c3e5d911562d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015736632s
STEP: Saw pod success
Jul 19 05:25:43.034: INFO: Pod "pod-projected-secrets-24a41f32-be63-499c-a898-c3e5d911562d" satisfied condition "Succeeded or Failed"
Jul 19 05:25:43.038: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-projected-secrets-24a41f32-be63-499c-a898-c3e5d911562d container secret-volume-test: <nil>
STEP: delete the pod
Jul 19 05:25:43.132: INFO: Waiting for pod pod-projected-secrets-24a41f32-be63-499c-a898-c3e5d911562d to disappear
Jul 19 05:25:43.136: INFO: Pod pod-projected-secrets-24a41f32-be63-499c-a898-c3e5d911562d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:25:43.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7364" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":229,"skipped":3666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:25:43.150: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 19 05:25:43.262: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-a 499f73c2-9c4f-42b1-8e57-9ae33d786273 2440500 0 2021-07-19 05:25:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 05:25:43.262: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-a 499f73c2-9c4f-42b1-8e57-9ae33d786273 2440500 0 2021-07-19 05:25:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 19 05:25:53.272: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-a 499f73c2-9c4f-42b1-8e57-9ae33d786273 2440581 0 2021-07-19 05:25:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 05:25:53.272: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-a 499f73c2-9c4f-42b1-8e57-9ae33d786273 2440581 0 2021-07-19 05:25:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 19 05:26:03.283: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-a 499f73c2-9c4f-42b1-8e57-9ae33d786273 2440645 0 2021-07-19 05:25:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 05:26:03.284: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-a 499f73c2-9c4f-42b1-8e57-9ae33d786273 2440645 0 2021-07-19 05:25:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 19 05:26:13.300: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-a 499f73c2-9c4f-42b1-8e57-9ae33d786273 2440714 0 2021-07-19 05:25:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 05:26:13.300: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-a 499f73c2-9c4f-42b1-8e57-9ae33d786273 2440714 0 2021-07-19 05:25:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 19 05:26:23.309: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-b caaa7405-de05-4ed4-ae2c-3073a3c66aad 2440778 0 2021-07-19 05:26:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 05:26:23.309: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-b caaa7405-de05-4ed4-ae2c-3073a3c66aad 2440778 0 2021-07-19 05:26:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 19 05:26:33.319: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-b caaa7405-de05-4ed4-ae2c-3073a3c66aad 2440848 0 2021-07-19 05:26:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 05:26:33.319: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1418 /api/v1/namespaces/watch-1418/configmaps/e2e-watch-test-configmap-b caaa7405-de05-4ed4-ae2c-3073a3c66aad 2440848 0 2021-07-19 05:26:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:26:43.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1418" for this suite.

• [SLOW TEST:60.188 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":230,"skipped":3705,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:26:43.338: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 05:26:43.895: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 05:26:45.908: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269203, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269203, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269203, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269203, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 05:26:48.927: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:26:59.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2168" for this suite.
STEP: Destroying namespace "webhook-2168-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.824 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":231,"skipped":3710,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:26:59.163: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-3506d3d2-00c8-4c59-a5bd-6a86ed0d3f15 in namespace container-probe-8007
Jul 19 05:27:03.283: INFO: Started pod busybox-3506d3d2-00c8-4c59-a5bd-6a86ed0d3f15 in namespace container-probe-8007
STEP: checking the pod's current state and verifying that restartCount is present
Jul 19 05:27:03.287: INFO: Initial restart count of pod busybox-3506d3d2-00c8-4c59-a5bd-6a86ed0d3f15 is 0
Jul 19 05:27:55.474: INFO: Restart count of pod container-probe-8007/busybox-3506d3d2-00c8-4c59-a5bd-6a86ed0d3f15 is now 1 (52.186546088s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:27:55.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8007" for this suite.

• [SLOW TEST:56.341 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":232,"skipped":3731,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:27:55.504: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:29:55.589: INFO: Deleting pod "var-expansion-69aa57ef-0645-4e9c-9941-6c9d6f90c5fc" in namespace "var-expansion-5361"
Jul 19 05:29:55.596: INFO: Wait up to 5m0s for pod "var-expansion-69aa57ef-0645-4e9c-9941-6c9d6f90c5fc" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:30:25.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5361" for this suite.

• [SLOW TEST:150.124 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":233,"skipped":3733,"failed":0}
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:30:25.628: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:30:31.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3924" for this suite.
STEP: Destroying namespace "nsdeletetest-5648" for this suite.
Jul 19 05:30:31.845: INFO: Namespace nsdeletetest-5648 was already deleted
STEP: Destroying namespace "nsdeletetest-8200" for this suite.

• [SLOW TEST:6.224 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":234,"skipped":3733,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:30:31.851: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:30:45.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-816" for this suite.

• [SLOW TEST:13.190 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":235,"skipped":3740,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:30:45.041: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:30:45.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2090" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":236,"skipped":3742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:30:45.600: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul 19 05:30:45.664: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 05:30:49.184: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:31:03.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3576" for this suite.

• [SLOW TEST:18.114 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":237,"skipped":3782,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:31:03.715: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-7868
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 19 05:31:03.784: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 19 05:31:03.828: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 05:31:05.833: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 05:31:07.832: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:31:09.831: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:31:11.831: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:31:13.832: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:31:15.832: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:31:17.832: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:31:19.833: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:31:21.831: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:31:23.831: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 19 05:31:23.841: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 19 05:31:27.868: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.0.74:8080/dial?request=hostname&protocol=http&host=172.16.0.29&port=8080&tries=1'] Namespace:pod-network-test-7868 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 05:31:27.868: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 05:31:28.223: INFO: Waiting for responses: map[]
Jul 19 05:31:28.227: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.0.74:8080/dial?request=hostname&protocol=http&host=172.16.0.73&port=8080&tries=1'] Namespace:pod-network-test-7868 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 05:31:28.227: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 05:31:28.440: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:31:28.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7868" for this suite.

• [SLOW TEST:24.739 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":238,"skipped":3796,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:31:28.454: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 05:31:28.816: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 19 05:31:30.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269488, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269488, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269488, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269488, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 05:31:33.853: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul 19 05:31:37.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 attach --namespace=webhook-1265 to-be-attached-pod -i -c=container1'
Jul 19 05:31:38.033: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:31:38.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1265" for this suite.
STEP: Destroying namespace "webhook-1265-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.705 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":239,"skipped":3798,"failed":0}
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:31:38.159: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul 19 05:32:18.277: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0719 05:32:18.277168      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0719 05:32:18.277193      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0719 05:32:18.277199      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 19 05:32:18.277: INFO: Deleting pod "simpletest.rc-27jj6" in namespace "gc-8802"
Jul 19 05:32:18.290: INFO: Deleting pod "simpletest.rc-2tdxl" in namespace "gc-8802"
Jul 19 05:32:18.306: INFO: Deleting pod "simpletest.rc-77f7c" in namespace "gc-8802"
Jul 19 05:32:18.322: INFO: Deleting pod "simpletest.rc-7hpx8" in namespace "gc-8802"
Jul 19 05:32:18.336: INFO: Deleting pod "simpletest.rc-7rxzl" in namespace "gc-8802"
Jul 19 05:32:18.350: INFO: Deleting pod "simpletest.rc-hg777" in namespace "gc-8802"
Jul 19 05:32:18.364: INFO: Deleting pod "simpletest.rc-lmnw2" in namespace "gc-8802"
Jul 19 05:32:18.378: INFO: Deleting pod "simpletest.rc-m87vw" in namespace "gc-8802"
Jul 19 05:32:18.393: INFO: Deleting pod "simpletest.rc-qqsx8" in namespace "gc-8802"
Jul 19 05:32:18.405: INFO: Deleting pod "simpletest.rc-rwgdn" in namespace "gc-8802"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:32:18.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8802" for this suite.

• [SLOW TEST:40.273 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":240,"skipped":3798,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:32:18.433: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 05:32:18.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc29d866-bfc8-49c3-97c6-5fed619ee7aa" in namespace "projected-1296" to be "Succeeded or Failed"
Jul 19 05:32:18.570: INFO: Pod "downwardapi-volume-dc29d866-bfc8-49c3-97c6-5fed619ee7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386293ms
Jul 19 05:32:20.575: INFO: Pod "downwardapi-volume-dc29d866-bfc8-49c3-97c6-5fed619ee7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00865481s
Jul 19 05:32:22.579: INFO: Pod "downwardapi-volume-dc29d866-bfc8-49c3-97c6-5fed619ee7aa": Phase="Running", Reason="", readiness=true. Elapsed: 4.012658067s
Jul 19 05:32:24.583: INFO: Pod "downwardapi-volume-dc29d866-bfc8-49c3-97c6-5fed619ee7aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016637634s
STEP: Saw pod success
Jul 19 05:32:24.583: INFO: Pod "downwardapi-volume-dc29d866-bfc8-49c3-97c6-5fed619ee7aa" satisfied condition "Succeeded or Failed"
Jul 19 05:32:24.587: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-dc29d866-bfc8-49c3-97c6-5fed619ee7aa container client-container: <nil>
STEP: delete the pod
Jul 19 05:32:24.673: INFO: Waiting for pod downwardapi-volume-dc29d866-bfc8-49c3-97c6-5fed619ee7aa to disappear
Jul 19 05:32:24.676: INFO: Pod downwardapi-volume-dc29d866-bfc8-49c3-97c6-5fed619ee7aa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:32:24.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1296" for this suite.

• [SLOW TEST:6.260 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":3799,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:32:24.694: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 19 05:32:29.311: INFO: Successfully updated pod "pod-update-d41ff640-f7ac-4dea-a993-b60d477b2445"
STEP: verifying the updated pod is in kubernetes
Jul 19 05:32:29.336: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:32:29.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8548" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":242,"skipped":3832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:32:29.348: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0719 05:32:35.462626      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0719 05:32:35.462750      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0719 05:32:35.462761      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 19 05:32:35.462: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:32:35.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-23" for this suite.

• [SLOW TEST:6.127 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":243,"skipped":3889,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:32:35.475: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
Jul 19 05:32:36.582: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0719 05:32:36.582107      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0719 05:32:36.582128      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0719 05:32:36.582133      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:32:36.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8000" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":244,"skipped":3898,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:32:36.593: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:32:36.683: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 19 05:32:41.688: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 19 05:32:41.688: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 19 05:32:43.693: INFO: Creating deployment "test-rollover-deployment"
Jul 19 05:32:43.701: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 19 05:32:45.708: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 19 05:32:45.714: INFO: Ensure that both replica sets have 1 created replica
Jul 19 05:32:45.721: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 19 05:32:45.731: INFO: Updating deployment test-rollover-deployment
Jul 19 05:32:45.731: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 19 05:32:47.738: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 19 05:32:47.746: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 19 05:32:47.753: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 05:32:47.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269565, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6dc79dcb94\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:32:49.761: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 05:32:49.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6dc79dcb94\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:32:51.761: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 05:32:51.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6dc79dcb94\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:32:53.762: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 05:32:53.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6dc79dcb94\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:32:55.762: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 05:32:55.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6dc79dcb94\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:32:57.760: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 05:32:57.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762269563, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6dc79dcb94\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:32:59.775: INFO: 
Jul 19 05:32:59.775: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 19 05:32:59.786: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2884 /apis/apps/v1/namespaces/deployment-2884/deployments/test-rollover-deployment 0454d071-6071-4579-8da7-4f724ec5ce3c 2444024 2 2021-07-19 05:32:43 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042fadd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*5,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-19 05:32:43 +0000 UTC,LastTransitionTime:2021-07-19 05:32:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6dc79dcb94" has successfully progressed.,LastUpdateTime:2021-07-19 05:32:59 +0000 UTC,LastTransitionTime:2021-07-19 05:32:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 19 05:32:59.794: INFO: New ReplicaSet "test-rollover-deployment-6dc79dcb94" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6dc79dcb94  deployment-2884 /apis/apps/v1/namespaces/deployment-2884/replicasets/test-rollover-deployment-6dc79dcb94 d9cbdd41-33e3-4d03-9745-dc8fa56fc7f9 2444017 2 2021-07-19 05:32:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6dc79dcb94] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/internalStrategyType:RollingUpdate deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0454d071-6071-4579-8da7-4f724ec5ce3c 0xc0042fb387 0xc0042fb388}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6dc79dcb94,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6dc79dcb94] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042fb3f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 19 05:32:59.794: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 19 05:32:59.794: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2884 /apis/apps/v1/namespaces/deployment-2884/replicasets/test-rollover-controller 80dcb201-ce89-45b9-92c4-4af3be72df3b 2444023 2 2021-07-19 05:32:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0454d071-6071-4579-8da7-4f724ec5ce3c 0xc0042fb2a7 0xc0042fb2a8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042fb308 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 05:32:59.794: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-74f4b8597  deployment-2884 /apis/apps/v1/namespaces/deployment-2884/replicasets/test-rollover-deployment-74f4b8597 fd5cc110-10e5-4ad8-a742-5aabe49607fb 2443910 2 2021-07-19 05:32:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:74f4b8597] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/internalStrategyType:RollingUpdate deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0454d071-6071-4579-8da7-4f724ec5ce3c 0xc0042fb4b0 0xc0042fb4b1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 74f4b8597,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:74f4b8597] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042fb528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 05:32:59.799: INFO: Pod "test-rollover-deployment-6dc79dcb94-6m2pk" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6dc79dcb94-6m2pk test-rollover-deployment-6dc79dcb94- deployment-2884 /api/v1/namespaces/deployment-2884/pods/test-rollover-deployment-6dc79dcb94-6m2pk 299c8a0b-349d-4b8a-833b-7710f447a429 2443942 0 2021-07-19 05:32:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6dc79dcb94] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6dc79dcb94 d9cbdd41-33e3-4d03-9745-dc8fa56fc7f9 0xc0042fbc27 0xc0042fbc28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sh2bq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sh2bq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBE_POD_NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:KUBE_POD_CLUSTERID,Value:,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sh2bq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:paas-192-168-10-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 05:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 05:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 05:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 05:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ResourceAllocated,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-19 05:32:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.158,PodIP:172.16.0.29,StartTime:2021-07-19 05:32:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-19 05:32:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker://sha256:adf0c90de619c8a6df92961ab786efa495d63cce0a4a9ade43a0723e340f1d3b,ContainerID:docker://f13abed6094e2723c404747d1a624dd9cdd3c86a77225570bcaae0808914c54d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:32:59.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2884" for this suite.

• [SLOW TEST:23.222 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":245,"skipped":3902,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:32:59.815: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul 19 05:33:06.412: INFO: Successfully updated pod "adopt-release-tqcmp"
STEP: Checking that the Job readopts the Pod
Jul 19 05:33:06.412: INFO: Waiting up to 15m0s for pod "adopt-release-tqcmp" in namespace "job-2563" to be "adopted"
Jul 19 05:33:06.416: INFO: Pod "adopt-release-tqcmp": Phase="Running", Reason="", readiness=true. Elapsed: 3.703821ms
Jul 19 05:33:08.421: INFO: Pod "adopt-release-tqcmp": Phase="Running", Reason="", readiness=true. Elapsed: 2.008762468s
Jul 19 05:33:08.421: INFO: Pod "adopt-release-tqcmp" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul 19 05:33:08.934: INFO: Successfully updated pod "adopt-release-tqcmp"
STEP: Checking that the Job releases the Pod
Jul 19 05:33:08.934: INFO: Waiting up to 15m0s for pod "adopt-release-tqcmp" in namespace "job-2563" to be "released"
Jul 19 05:33:08.938: INFO: Pod "adopt-release-tqcmp": Phase="Running", Reason="", readiness=true. Elapsed: 3.715781ms
Jul 19 05:33:10.941: INFO: Pod "adopt-release-tqcmp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007409702s
Jul 19 05:33:10.941: INFO: Pod "adopt-release-tqcmp" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:33:10.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2563" for this suite.

• [SLOW TEST:11.140 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":246,"skipped":3904,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:33:10.955: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-456w4 in namespace proxy-2020
I0719 05:33:11.037841      24 runners.go:190] Created replication controller with name: proxy-service-456w4, namespace: proxy-2020, replica count: 1
I0719 05:33:12.088144      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 05:33:13.088241      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 05:33:14.088409      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0719 05:33:15.088586      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0719 05:33:16.088787      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0719 05:33:17.088932      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0719 05:33:18.089085      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0719 05:33:19.089267      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0719 05:33:20.089427      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0719 05:33:21.089607      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0719 05:33:22.089797      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0719 05:33:23.089919      24 runners.go:190] proxy-service-456w4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 05:33:23.094: INFO: setup took 12.081320366s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 19 05:33:23.103: INFO: (0) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 8.780562ms)
Jul 19 05:33:23.103: INFO: (0) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 8.572605ms)
Jul 19 05:33:23.103: INFO: (0) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 8.673181ms)
Jul 19 05:33:23.103: INFO: (0) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 8.856324ms)
Jul 19 05:33:23.103: INFO: (0) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 8.92283ms)
Jul 19 05:33:23.103: INFO: (0) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 8.876584ms)
Jul 19 05:33:23.103: INFO: (0) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 8.982142ms)
Jul 19 05:33:23.103: INFO: (0) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 8.988074ms)
Jul 19 05:33:23.107: INFO: (0) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 12.442594ms)
Jul 19 05:33:23.110: INFO: (0) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 15.482396ms)
Jul 19 05:33:23.111: INFO: (0) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 16.444407ms)
Jul 19 05:33:23.111: INFO: (0) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 16.417603ms)
Jul 19 05:33:23.111: INFO: (0) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 16.597722ms)
Jul 19 05:33:23.111: INFO: (0) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 16.524666ms)
Jul 19 05:33:23.111: INFO: (0) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 16.458838ms)
Jul 19 05:33:23.112: INFO: (0) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 17.882501ms)
Jul 19 05:33:23.117: INFO: (1) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 4.870902ms)
Jul 19 05:33:23.117: INFO: (1) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 4.951209ms)
Jul 19 05:33:23.117: INFO: (1) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 5.00175ms)
Jul 19 05:33:23.117: INFO: (1) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 5.055108ms)
Jul 19 05:33:23.117: INFO: (1) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 4.999056ms)
Jul 19 05:33:23.117: INFO: (1) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 4.963597ms)
Jul 19 05:33:23.118: INFO: (1) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 5.192006ms)
Jul 19 05:33:23.118: INFO: (1) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 6.113844ms)
Jul 19 05:33:23.119: INFO: (1) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 6.362403ms)
Jul 19 05:33:23.120: INFO: (1) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 7.435332ms)
Jul 19 05:33:23.120: INFO: (1) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 7.761706ms)
Jul 19 05:33:23.134: INFO: (1) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 21.716633ms)
Jul 19 05:33:23.134: INFO: (1) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 21.887237ms)
Jul 19 05:33:23.134: INFO: (1) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 21.805107ms)
Jul 19 05:33:23.135: INFO: (1) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 22.363186ms)
Jul 19 05:33:23.136: INFO: (1) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 23.822897ms)
Jul 19 05:33:23.140: INFO: (2) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 3.675768ms)
Jul 19 05:33:23.140: INFO: (2) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 3.499213ms)
Jul 19 05:33:23.140: INFO: (2) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 3.678716ms)
Jul 19 05:33:23.140: INFO: (2) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 3.693634ms)
Jul 19 05:33:23.141: INFO: (2) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 4.854107ms)
Jul 19 05:33:23.141: INFO: (2) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 4.835717ms)
Jul 19 05:33:23.141: INFO: (2) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 4.92709ms)
Jul 19 05:33:23.141: INFO: (2) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 4.947304ms)
Jul 19 05:33:23.141: INFO: (2) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 5.056671ms)
Jul 19 05:33:23.142: INFO: (2) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 5.162662ms)
Jul 19 05:33:23.142: INFO: (2) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 5.12309ms)
Jul 19 05:33:23.142: INFO: (2) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 6.062758ms)
Jul 19 05:33:23.142: INFO: (2) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 6.048971ms)
Jul 19 05:33:23.142: INFO: (2) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 6.134868ms)
Jul 19 05:33:23.143: INFO: (2) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 6.783173ms)
Jul 19 05:33:23.143: INFO: (2) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 6.501268ms)
Jul 19 05:33:23.146: INFO: (3) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 3.034747ms)
Jul 19 05:33:23.146: INFO: (3) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 3.043549ms)
Jul 19 05:33:23.147: INFO: (3) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 4.211465ms)
Jul 19 05:33:23.147: INFO: (3) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 4.13238ms)
Jul 19 05:33:23.147: INFO: (3) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 4.306065ms)
Jul 19 05:33:23.148: INFO: (3) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 4.91311ms)
Jul 19 05:33:23.148: INFO: (3) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 5.085987ms)
Jul 19 05:33:23.148: INFO: (3) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 5.183099ms)
Jul 19 05:33:23.149: INFO: (3) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 5.429612ms)
Jul 19 05:33:23.149: INFO: (3) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 5.929816ms)
Jul 19 05:33:23.149: INFO: (3) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 5.838111ms)
Jul 19 05:33:23.149: INFO: (3) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 6.133217ms)
Jul 19 05:33:23.150: INFO: (3) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 6.342247ms)
Jul 19 05:33:23.150: INFO: (3) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 6.881193ms)
Jul 19 05:33:23.150: INFO: (3) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 7.243798ms)
Jul 19 05:33:23.150: INFO: (3) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 7.180705ms)
Jul 19 05:33:23.154: INFO: (4) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 3.628891ms)
Jul 19 05:33:23.154: INFO: (4) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 3.657818ms)
Jul 19 05:33:23.155: INFO: (4) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 4.256937ms)
Jul 19 05:33:23.155: INFO: (4) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 4.32806ms)
Jul 19 05:33:23.155: INFO: (4) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 4.296128ms)
Jul 19 05:33:23.155: INFO: (4) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 4.557045ms)
Jul 19 05:33:23.156: INFO: (4) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 5.602953ms)
Jul 19 05:33:23.156: INFO: (4) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 5.567179ms)
Jul 19 05:33:23.156: INFO: (4) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 5.671703ms)
Jul 19 05:33:23.157: INFO: (4) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 6.037327ms)
Jul 19 05:33:23.157: INFO: (4) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 6.153558ms)
Jul 19 05:33:23.157: INFO: (4) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 6.353225ms)
Jul 19 05:33:23.157: INFO: (4) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 6.733113ms)
Jul 19 05:33:23.157: INFO: (4) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 6.725207ms)
Jul 19 05:33:23.158: INFO: (4) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 7.180207ms)
Jul 19 05:33:23.159: INFO: (4) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 8.463105ms)
Jul 19 05:33:23.165: INFO: (5) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 5.640782ms)
Jul 19 05:33:23.165: INFO: (5) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 5.456092ms)
Jul 19 05:33:23.165: INFO: (5) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 5.768555ms)
Jul 19 05:33:23.165: INFO: (5) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 6.002888ms)
Jul 19 05:33:23.165: INFO: (5) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 5.986228ms)
Jul 19 05:33:23.166: INFO: (5) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 6.309065ms)
Jul 19 05:33:23.166: INFO: (5) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 5.994781ms)
Jul 19 05:33:23.166: INFO: (5) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 6.156759ms)
Jul 19 05:33:23.166: INFO: (5) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 7.06284ms)
Jul 19 05:33:23.166: INFO: (5) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 7.069026ms)
Jul 19 05:33:23.166: INFO: (5) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 7.075763ms)
Jul 19 05:33:23.167: INFO: (5) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 7.494079ms)
Jul 19 05:33:23.168: INFO: (5) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 8.607605ms)
Jul 19 05:33:23.169: INFO: (5) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 9.259166ms)
Jul 19 05:33:23.169: INFO: (5) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 9.451883ms)
Jul 19 05:33:23.169: INFO: (5) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 9.52475ms)
Jul 19 05:33:23.173: INFO: (6) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 3.637963ms)
Jul 19 05:33:23.174: INFO: (6) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 5.02171ms)
Jul 19 05:33:23.174: INFO: (6) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 4.949405ms)
Jul 19 05:33:23.174: INFO: (6) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 5.078503ms)
Jul 19 05:33:23.176: INFO: (6) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 7.121808ms)
Jul 19 05:33:23.176: INFO: (6) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 7.086119ms)
Jul 19 05:33:23.177: INFO: (6) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 7.638983ms)
Jul 19 05:33:23.185: INFO: (6) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 15.488993ms)
Jul 19 05:33:23.185: INFO: (6) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 15.62636ms)
Jul 19 05:33:23.185: INFO: (6) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 15.431395ms)
Jul 19 05:33:23.185: INFO: (6) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 15.500229ms)
Jul 19 05:33:23.185: INFO: (6) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 15.57738ms)
Jul 19 05:33:23.185: INFO: (6) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 15.448167ms)
Jul 19 05:33:23.185: INFO: (6) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 15.444501ms)
Jul 19 05:33:23.189: INFO: (6) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 19.427755ms)
Jul 19 05:33:23.189: INFO: (6) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 19.827062ms)
Jul 19 05:33:23.197: INFO: (7) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 7.53426ms)
Jul 19 05:33:23.197: INFO: (7) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 7.493024ms)
Jul 19 05:33:23.197: INFO: (7) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 7.5336ms)
Jul 19 05:33:23.197: INFO: (7) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 7.507966ms)
Jul 19 05:33:23.197: INFO: (7) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 7.535658ms)
Jul 19 05:33:23.197: INFO: (7) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 7.901234ms)
Jul 19 05:33:23.199: INFO: (7) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 9.651742ms)
Jul 19 05:33:23.200: INFO: (7) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 10.535803ms)
Jul 19 05:33:23.200: INFO: (7) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 10.731185ms)
Jul 19 05:33:23.200: INFO: (7) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 10.761053ms)
Jul 19 05:33:23.200: INFO: (7) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 11.189942ms)
Jul 19 05:33:23.201: INFO: (7) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 11.592712ms)
Jul 19 05:33:23.201: INFO: (7) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 11.811205ms)
Jul 19 05:33:23.235: INFO: (7) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 45.63237ms)
Jul 19 05:33:23.236: INFO: (7) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 46.245616ms)
Jul 19 05:33:23.237: INFO: (7) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 47.399676ms)
Jul 19 05:33:23.241: INFO: (8) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 4.480999ms)
Jul 19 05:33:23.242: INFO: (8) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 5.090379ms)
Jul 19 05:33:23.243: INFO: (8) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 5.833612ms)
Jul 19 05:33:23.245: INFO: (8) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 7.732176ms)
Jul 19 05:33:23.245: INFO: (8) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 7.766771ms)
Jul 19 05:33:23.245: INFO: (8) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 7.860213ms)
Jul 19 05:33:23.245: INFO: (8) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 7.764677ms)
Jul 19 05:33:23.245: INFO: (8) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 8.24224ms)
Jul 19 05:33:23.245: INFO: (8) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 8.17356ms)
Jul 19 05:33:23.245: INFO: (8) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 8.696194ms)
Jul 19 05:33:23.246: INFO: (8) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 9.248641ms)
Jul 19 05:33:23.246: INFO: (8) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 9.10129ms)
Jul 19 05:33:23.246: INFO: (8) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 9.101872ms)
Jul 19 05:33:23.246: INFO: (8) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 9.490914ms)
Jul 19 05:33:23.246: INFO: (8) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 9.640812ms)
Jul 19 05:33:23.247: INFO: (8) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 10.167422ms)
Jul 19 05:33:23.252: INFO: (9) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 4.726537ms)
Jul 19 05:33:23.254: INFO: (9) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 6.654235ms)
Jul 19 05:33:23.254: INFO: (9) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 6.584169ms)
Jul 19 05:33:23.254: INFO: (9) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 6.570071ms)
Jul 19 05:33:23.254: INFO: (9) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 6.827722ms)
Jul 19 05:33:23.254: INFO: (9) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 6.290444ms)
Jul 19 05:33:23.255: INFO: (9) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 7.848535ms)
Jul 19 05:33:23.255: INFO: (9) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 8.334336ms)
Jul 19 05:33:23.255: INFO: (9) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 8.138126ms)
Jul 19 05:33:23.255: INFO: (9) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 7.865394ms)
Jul 19 05:33:23.255: INFO: (9) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 7.837686ms)
Jul 19 05:33:23.255: INFO: (9) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 7.971182ms)
Jul 19 05:33:23.256: INFO: (9) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 8.377041ms)
Jul 19 05:33:23.256: INFO: (9) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 8.490759ms)
Jul 19 05:33:23.256: INFO: (9) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 8.590374ms)
Jul 19 05:33:23.256: INFO: (9) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 8.945331ms)
Jul 19 05:33:23.261: INFO: (10) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 4.141277ms)
Jul 19 05:33:23.261: INFO: (10) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 4.895035ms)
Jul 19 05:33:23.261: INFO: (10) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 5.024677ms)
Jul 19 05:33:23.261: INFO: (10) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 4.789382ms)
Jul 19 05:33:23.262: INFO: (10) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 5.316953ms)
Jul 19 05:33:23.268: INFO: (10) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 11.817995ms)
Jul 19 05:33:23.268: INFO: (10) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 11.774628ms)
Jul 19 05:33:23.268: INFO: (10) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 11.647061ms)
Jul 19 05:33:23.268: INFO: (10) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 12.032674ms)
Jul 19 05:33:23.272: INFO: (10) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 15.130125ms)
Jul 19 05:33:23.272: INFO: (10) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 15.086964ms)
Jul 19 05:33:23.272: INFO: (10) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 15.239578ms)
Jul 19 05:33:23.272: INFO: (10) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 15.295886ms)
Jul 19 05:33:23.272: INFO: (10) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 15.40223ms)
Jul 19 05:33:23.272: INFO: (10) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 15.530746ms)
Jul 19 05:33:23.275: INFO: (10) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 17.805426ms)
Jul 19 05:33:23.284: INFO: (11) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 9.575183ms)
Jul 19 05:33:23.284: INFO: (11) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 9.649723ms)
Jul 19 05:33:23.284: INFO: (11) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 9.780902ms)
Jul 19 05:33:23.284: INFO: (11) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 9.717349ms)
Jul 19 05:33:23.284: INFO: (11) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 9.635147ms)
Jul 19 05:33:23.284: INFO: (11) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 9.684918ms)
Jul 19 05:33:23.284: INFO: (11) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 9.714656ms)
Jul 19 05:33:23.285: INFO: (11) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 10.066274ms)
Jul 19 05:33:23.285: INFO: (11) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 10.194266ms)
Jul 19 05:33:23.286: INFO: (11) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 10.808243ms)
Jul 19 05:33:23.286: INFO: (11) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 10.868883ms)
Jul 19 05:33:23.286: INFO: (11) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 10.797859ms)
Jul 19 05:33:23.286: INFO: (11) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 10.84358ms)
Jul 19 05:33:23.286: INFO: (11) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 10.829274ms)
Jul 19 05:33:23.286: INFO: (11) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 11.46825ms)
Jul 19 05:33:23.286: INFO: (11) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 11.475669ms)
Jul 19 05:33:23.291: INFO: (12) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 4.928954ms)
Jul 19 05:33:23.291: INFO: (12) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 4.768759ms)
Jul 19 05:33:23.291: INFO: (12) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 5.107672ms)
Jul 19 05:33:23.291: INFO: (12) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 5.072144ms)
Jul 19 05:33:23.291: INFO: (12) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 5.192797ms)
Jul 19 05:33:23.294: INFO: (12) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 7.328867ms)
Jul 19 05:33:23.294: INFO: (12) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 7.361132ms)
Jul 19 05:33:23.294: INFO: (12) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 7.259819ms)
Jul 19 05:33:23.294: INFO: (12) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 7.950442ms)
Jul 19 05:33:23.294: INFO: (12) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 8.043061ms)
Jul 19 05:33:23.294: INFO: (12) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 8.047251ms)
Jul 19 05:33:23.295: INFO: (12) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 8.348969ms)
Jul 19 05:33:23.295: INFO: (12) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 8.62543ms)
Jul 19 05:33:23.295: INFO: (12) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 9.175939ms)
Jul 19 05:33:23.295: INFO: (12) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 9.191244ms)
Jul 19 05:33:23.297: INFO: (12) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 10.266981ms)
Jul 19 05:33:23.302: INFO: (13) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 5.248741ms)
Jul 19 05:33:23.302: INFO: (13) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 5.288104ms)
Jul 19 05:33:23.302: INFO: (13) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 5.35155ms)
Jul 19 05:33:23.303: INFO: (13) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 5.696897ms)
Jul 19 05:33:23.303: INFO: (13) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 5.783828ms)
Jul 19 05:33:23.303: INFO: (13) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 5.700565ms)
Jul 19 05:33:23.303: INFO: (13) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 5.683486ms)
Jul 19 05:33:23.303: INFO: (13) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 6.278127ms)
Jul 19 05:33:23.304: INFO: (13) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 7.264579ms)
Jul 19 05:33:23.304: INFO: (13) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 7.598636ms)
Jul 19 05:33:23.344: INFO: (13) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 47.052594ms)
Jul 19 05:33:23.344: INFO: (13) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 47.037135ms)
Jul 19 05:33:23.344: INFO: (13) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 47.162928ms)
Jul 19 05:33:23.344: INFO: (13) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 46.93606ms)
Jul 19 05:33:23.344: INFO: (13) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 47.479136ms)
Jul 19 05:33:23.346: INFO: (13) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 48.848365ms)
Jul 19 05:33:23.351: INFO: (14) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 5.008588ms)
Jul 19 05:33:23.352: INFO: (14) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 6.592131ms)
Jul 19 05:33:23.353: INFO: (14) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 6.782308ms)
Jul 19 05:33:23.353: INFO: (14) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 6.977331ms)
Jul 19 05:33:23.353: INFO: (14) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 6.983189ms)
Jul 19 05:33:23.353: INFO: (14) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 7.206057ms)
Jul 19 05:33:23.354: INFO: (14) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 7.817859ms)
Jul 19 05:33:23.354: INFO: (14) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 8.564321ms)
Jul 19 05:33:23.355: INFO: (14) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 9.114469ms)
Jul 19 05:33:23.360: INFO: (14) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 14.340794ms)
Jul 19 05:33:23.360: INFO: (14) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 14.209505ms)
Jul 19 05:33:23.362: INFO: (14) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 16.201307ms)
Jul 19 05:33:23.362: INFO: (14) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 16.117118ms)
Jul 19 05:33:23.362: INFO: (14) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 16.366422ms)
Jul 19 05:33:23.362: INFO: (14) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 16.203875ms)
Jul 19 05:33:23.365: INFO: (14) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 19.539468ms)
Jul 19 05:33:23.387: INFO: (15) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 21.942053ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 22.207947ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 21.928945ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 21.975396ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 21.909583ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 22.008182ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 21.96794ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 22.019959ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 22.272179ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 21.967031ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 21.989523ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 22.03646ms)
Jul 19 05:33:23.388: INFO: (15) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 22.302621ms)
Jul 19 05:33:23.390: INFO: (15) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 24.726104ms)
Jul 19 05:33:23.392: INFO: (15) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 26.828486ms)
Jul 19 05:33:23.392: INFO: (15) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 26.782137ms)
Jul 19 05:33:23.398: INFO: (16) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 5.490265ms)
Jul 19 05:33:23.401: INFO: (16) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 7.978163ms)
Jul 19 05:33:23.401: INFO: (16) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 8.275931ms)
Jul 19 05:33:23.401: INFO: (16) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 8.130251ms)
Jul 19 05:33:23.401: INFO: (16) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 8.002755ms)
Jul 19 05:33:23.401: INFO: (16) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 8.037608ms)
Jul 19 05:33:23.401: INFO: (16) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 8.174833ms)
Jul 19 05:33:23.401: INFO: (16) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 8.095192ms)
Jul 19 05:33:23.401: INFO: (16) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 8.145451ms)
Jul 19 05:33:23.402: INFO: (16) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 9.86492ms)
Jul 19 05:33:23.402: INFO: (16) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 9.635944ms)
Jul 19 05:33:23.404: INFO: (16) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 11.69108ms)
Jul 19 05:33:23.404: INFO: (16) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 11.62035ms)
Jul 19 05:33:23.404: INFO: (16) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 11.92672ms)
Jul 19 05:33:23.405: INFO: (16) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 11.937449ms)
Jul 19 05:33:23.442: INFO: (16) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 49.116925ms)
Jul 19 05:33:23.449: INFO: (17) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 7.294431ms)
Jul 19 05:33:23.449: INFO: (17) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 7.34185ms)
Jul 19 05:33:23.449: INFO: (17) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 7.36854ms)
Jul 19 05:33:23.449: INFO: (17) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 7.48405ms)
Jul 19 05:33:23.449: INFO: (17) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 7.520987ms)
Jul 19 05:33:23.453: INFO: (17) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 10.772447ms)
Jul 19 05:33:23.453: INFO: (17) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 10.758871ms)
Jul 19 05:33:23.453: INFO: (17) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 11.305552ms)
Jul 19 05:33:23.454: INFO: (17) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 11.562012ms)
Jul 19 05:33:23.454: INFO: (17) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 11.816409ms)
Jul 19 05:33:23.454: INFO: (17) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 11.856406ms)
Jul 19 05:33:23.454: INFO: (17) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 12.275045ms)
Jul 19 05:33:23.454: INFO: (17) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 12.164499ms)
Jul 19 05:33:23.455: INFO: (17) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 12.713079ms)
Jul 19 05:33:23.456: INFO: (17) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 13.482517ms)
Jul 19 05:33:23.457: INFO: (17) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 14.888788ms)
Jul 19 05:33:23.464: INFO: (18) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 6.86927ms)
Jul 19 05:33:23.464: INFO: (18) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 6.863558ms)
Jul 19 05:33:23.464: INFO: (18) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 6.71264ms)
Jul 19 05:33:23.464: INFO: (18) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 6.772396ms)
Jul 19 05:33:23.464: INFO: (18) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 7.36251ms)
Jul 19 05:33:23.466: INFO: (18) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 8.310888ms)
Jul 19 05:33:23.466: INFO: (18) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 8.387736ms)
Jul 19 05:33:23.467: INFO: (18) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 9.361344ms)
Jul 19 05:33:23.467: INFO: (18) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 9.249391ms)
Jul 19 05:33:23.467: INFO: (18) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 9.339355ms)
Jul 19 05:33:23.467: INFO: (18) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 9.20817ms)
Jul 19 05:33:23.467: INFO: (18) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 9.808995ms)
Jul 19 05:33:23.467: INFO: (18) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 10.002095ms)
Jul 19 05:33:23.468: INFO: (18) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 11.299132ms)
Jul 19 05:33:23.469: INFO: (18) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 11.145941ms)
Jul 19 05:33:23.469: INFO: (18) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 12.573514ms)
Jul 19 05:33:23.480: INFO: (19) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:1080/proxy/rewriteme">... (200; 10.899247ms)
Jul 19 05:33:23.480: INFO: (19) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:162/proxy/: bar (200; 10.694813ms)
Jul 19 05:33:23.480: INFO: (19) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz/proxy/rewriteme">test</a> (200; 10.679539ms)
Jul 19 05:33:23.480: INFO: (19) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:460/proxy/: tls baz (200; 10.79014ms)
Jul 19 05:33:23.480: INFO: (19) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:1080/proxy/rewriteme">test<... (200; 10.838932ms)
Jul 19 05:33:23.481: INFO: (19) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:162/proxy/: bar (200; 10.845078ms)
Jul 19 05:33:23.481: INFO: (19) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:462/proxy/: tls qux (200; 11.020583ms)
Jul 19 05:33:23.482: INFO: (19) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname1/proxy/: foo (200; 11.917091ms)
Jul 19 05:33:23.482: INFO: (19) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname1/proxy/: tls baz (200; 12.177716ms)
Jul 19 05:33:23.482: INFO: (19) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname1/proxy/: foo (200; 12.314441ms)
Jul 19 05:33:23.485: INFO: (19) /api/v1/namespaces/proxy-2020/pods/proxy-service-456w4-dz7lz:160/proxy/: foo (200; 15.362485ms)
Jul 19 05:33:23.485: INFO: (19) /api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/: <a href="/api/v1/namespaces/proxy-2020/pods/https:proxy-service-456w4-dz7lz:443/proxy/tlsrewritem... (200; 15.393583ms)
Jul 19 05:33:23.485: INFO: (19) /api/v1/namespaces/proxy-2020/pods/http:proxy-service-456w4-dz7lz:160/proxy/: foo (200; 15.605032ms)
Jul 19 05:33:23.485: INFO: (19) /api/v1/namespaces/proxy-2020/services/proxy-service-456w4:portname2/proxy/: bar (200; 15.430491ms)
Jul 19 05:33:23.486: INFO: (19) /api/v1/namespaces/proxy-2020/services/https:proxy-service-456w4:tlsportname2/proxy/: tls qux (200; 15.934542ms)
Jul 19 05:33:23.487: INFO: (19) /api/v1/namespaces/proxy-2020/services/http:proxy-service-456w4:portname2/proxy/: bar (200; 17.084873ms)
STEP: deleting ReplicationController proxy-service-456w4 in namespace proxy-2020, will wait for the garbage collector to delete the pods
Jul 19 05:33:23.550: INFO: Deleting ReplicationController proxy-service-456w4 took: 9.721082ms
Jul 19 05:33:23.650: INFO: Terminating ReplicationController proxy-service-456w4 pods took: 100.157619ms
[AfterEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:33:39.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2020" for this suite.

• [SLOW TEST:28.508 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":247,"skipped":3906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:33:39.463: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 19 05:33:39.545: INFO: Waiting up to 5m0s for pod "pod-bfe23516-cfb6-40ef-9adf-24f22c9065fa" in namespace "emptydir-9262" to be "Succeeded or Failed"
Jul 19 05:33:39.548: INFO: Pod "pod-bfe23516-cfb6-40ef-9adf-24f22c9065fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.932671ms
Jul 19 05:33:41.552: INFO: Pod "pod-bfe23516-cfb6-40ef-9adf-24f22c9065fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00676882s
Jul 19 05:33:43.556: INFO: Pod "pod-bfe23516-cfb6-40ef-9adf-24f22c9065fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011241974s
STEP: Saw pod success
Jul 19 05:33:43.557: INFO: Pod "pod-bfe23516-cfb6-40ef-9adf-24f22c9065fa" satisfied condition "Succeeded or Failed"
Jul 19 05:33:43.559: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-bfe23516-cfb6-40ef-9adf-24f22c9065fa container test-container: <nil>
STEP: delete the pod
Jul 19 05:33:43.619: INFO: Waiting for pod pod-bfe23516-cfb6-40ef-9adf-24f22c9065fa to disappear
Jul 19 05:33:43.622: INFO: Pod pod-bfe23516-cfb6-40ef-9adf-24f22c9065fa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:33:43.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9262" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":248,"skipped":3942,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:33:43.635: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-70209acc-1d9c-4320-9984-0e2918488ba2
STEP: Creating secret with name s-test-opt-upd-14b1850c-cd5f-4e2f-83a0-e77dc8e636ff
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-70209acc-1d9c-4320-9984-0e2918488ba2
STEP: Updating secret s-test-opt-upd-14b1850c-cd5f-4e2f-83a0-e77dc8e636ff
STEP: Creating secret with name s-test-opt-create-56e758fb-9a9a-42e9-8e77-01136b1d2c8b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:33:51.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1390" for this suite.

• [SLOW TEST:8.230 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":249,"skipped":3981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:33:51.865: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 19 05:33:51.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-3130'
Jul 19 05:33:52.378: INFO: stderr: ""
Jul 19 05:33:52.378: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jul 19 05:33:52.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 get pod e2e-test-httpd-pod -o json --namespace=kubectl-3130'
Jul 19 05:33:52.479: INFO: stderr: ""
Jul 19 05:33:52.479: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-07-19T05:33:52Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3130\",\n        \"resourceVersion\": \"2444503\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3130/pods/e2e-test-httpd-pod\",\n        \"uid\": \"162c765e-e402-44f5-b5b6-11b83bd3cd8b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBE_POD_NODE_NAME\",\n                        \"valueFrom\": {\n                            \"fieldRef\": {\n                                \"apiVersion\": \"v1\",\n                                \"fieldPath\": \"spec.nodeName\"\n                            }\n                        }\n                    },\n                    {\n                        \"name\": \"KUBE_POD_NAMESPACE\",\n                        \"valueFrom\": {\n                            \"fieldRef\": {\n                                \"apiVersion\": \"v1\",\n                                \"fieldPath\": \"metadata.namespace\"\n                            }\n                        }\n                    },\n                    {\n                        \"name\": \"KUBE_POD_CLUSTERID\"\n                    }\n                ],\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-fljlw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"paas-192-168-10-158\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-fljlw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-fljlw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-19T05:33:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-19T05:33:52Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-19T05:33:52Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-19T05:33:52Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-19T05:33:52Z\",\n                \"status\": \"True\",\n                \"type\": \"ResourceAllocated\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.10.158\",\n        \"managementIP\": \"192.168.10.158\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-07-19T05:33:52Z\"\n    }\n}\n"
Jul 19 05:33:52.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 replace -f - --dry-run server --namespace=kubectl-3130'
Jul 19 05:33:52.762: INFO: stderr: "W0719 05:33:52.545882    1431 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Jul 19 05:33:52.762: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Jul 19 05:33:52.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete pods e2e-test-httpd-pod --namespace=kubectl-3130'
Jul 19 05:34:00.062: INFO: stderr: ""
Jul 19 05:34:00.062: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:34:00.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3130" for this suite.

• [SLOW TEST:8.211 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:919
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":250,"skipped":4007,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:34:00.077: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-1114
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1114 to expose endpoints map[]
Jul 19 05:34:00.220: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jul 19 05:34:01.227: INFO: successfully validated that service multi-endpoint-test in namespace services-1114 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1114
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1114 to expose endpoints map[pod1:[100]]
Jul 19 05:34:05.265: INFO: successfully validated that service multi-endpoint-test in namespace services-1114 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1114
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1114 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 19 05:34:09.334: INFO: successfully validated that service multi-endpoint-test in namespace services-1114 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-1114
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1114 to expose endpoints map[pod2:[101]]
Jul 19 05:34:09.357: INFO: successfully validated that service multi-endpoint-test in namespace services-1114 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1114
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1114 to expose endpoints map[]
Jul 19 05:34:09.377: INFO: successfully validated that service multi-endpoint-test in namespace services-1114 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:34:09.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1114" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.344 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":251,"skipped":4017,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:34:09.422: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:34:09.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9307" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":252,"skipped":4034,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:34:09.530: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 05:34:09.627: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a432d50-5c0b-4c5a-a623-0d19175bcf6d" in namespace "downward-api-3622" to be "Succeeded or Failed"
Jul 19 05:34:09.631: INFO: Pod "downwardapi-volume-4a432d50-5c0b-4c5a-a623-0d19175bcf6d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.445689ms
Jul 19 05:34:11.635: INFO: Pod "downwardapi-volume-4a432d50-5c0b-4c5a-a623-0d19175bcf6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00795925s
Jul 19 05:34:13.640: INFO: Pod "downwardapi-volume-4a432d50-5c0b-4c5a-a623-0d19175bcf6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012478664s
STEP: Saw pod success
Jul 19 05:34:13.640: INFO: Pod "downwardapi-volume-4a432d50-5c0b-4c5a-a623-0d19175bcf6d" satisfied condition "Succeeded or Failed"
Jul 19 05:34:13.644: INFO: Trying to get logs from node paas-192-168-10-158 pod downwardapi-volume-4a432d50-5c0b-4c5a-a623-0d19175bcf6d container client-container: <nil>
STEP: delete the pod
Jul 19 05:34:13.673: INFO: Waiting for pod downwardapi-volume-4a432d50-5c0b-4c5a-a623-0d19175bcf6d to disappear
Jul 19 05:34:13.676: INFO: Pod downwardapi-volume-4a432d50-5c0b-4c5a-a623-0d19175bcf6d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:34:13.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3622" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":253,"skipped":4044,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:34:13.698: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2595
STEP: creating service affinity-nodeport in namespace services-2595
STEP: creating replication controller affinity-nodeport in namespace services-2595
I0719 05:34:13.800135      24 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2595, replica count: 3
I0719 05:34:16.850440      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 05:34:19.850616      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 05:34:19.863: INFO: Creating new exec pod
Jul 19 05:34:24.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2595 execpod-affinityndwhx -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jul 19 05:34:27.679: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul 19 05:34:27.679: INFO: stdout: ""
Jul 19 05:34:27.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2595 execpod-affinityndwhx -- /bin/sh -x -c nc -zv -t -w 2 10.247.238.144 80'
Jul 19 05:34:27.912: INFO: stderr: "+ nc -zv -t -w 2 10.247.238.144 80\nConnection to 10.247.238.144 80 port [tcp/http] succeeded!\n"
Jul 19 05:34:27.912: INFO: stdout: ""
Jul 19 05:34:27.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2595 execpod-affinityndwhx -- /bin/sh -x -c nc -zv -t -w 2 192.168.10.158 32486'
Jul 19 05:34:28.222: INFO: stderr: "+ nc -zv -t -w 2 192.168.10.158 32486\nConnection to 192.168.10.158 32486 port [tcp/32486] succeeded!\n"
Jul 19 05:34:28.222: INFO: stdout: ""
Jul 19 05:34:28.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2595 execpod-affinityndwhx -- /bin/sh -x -c nc -zv -t -w 2 192.168.10.183 32486'
Jul 19 05:34:28.555: INFO: stderr: "+ nc -zv -t -w 2 192.168.10.183 32486\nConnection to 192.168.10.183 32486 port [tcp/32486] succeeded!\n"
Jul 19 05:34:28.555: INFO: stdout: ""
Jul 19 05:34:28.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-2595 execpod-affinityndwhx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.10.158:32486/ ; done'
Jul 19 05:34:29.071: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.158:32486/\n"
Jul 19 05:34:29.071: INFO: stdout: "\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj\naffinity-nodeport-hc2qj"
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Received response from host: affinity-nodeport-hc2qj
Jul 19 05:34:29.071: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2595, will wait for the garbage collector to delete the pods
Jul 19 05:34:29.158: INFO: Deleting ReplicationController affinity-nodeport took: 17.560172ms
Jul 19 05:34:29.258: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.171699ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:34:49.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2595" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:35.822 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":254,"skipped":4047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:34:49.520: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 19 05:34:49.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7256'
Jul 19 05:34:49.700: INFO: stderr: ""
Jul 19 05:34:49.700: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Jul 19 05:34:49.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 delete pods e2e-test-httpd-pod --namespace=kubectl-7256'
Jul 19 05:35:14.258: INFO: stderr: ""
Jul 19 05:35:14.258: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:14.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7256" for this suite.

• [SLOW TEST:24.765 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":255,"skipped":4096,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:14.285: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:18.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2147" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":256,"skipped":4121,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:18.530: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Jul 19 05:35:18.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 api-versions'
Jul 19 05:35:18.676: INFO: stderr: ""
Jul 19 05:35:18.676: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nk8s.cni.cncf.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npaas/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:18.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-504" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":257,"skipped":4122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:18.688: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:35:18.766: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0df521a2-d6fb-4680-b666-580021119d7c" in namespace "security-context-test-9479" to be "Succeeded or Failed"
Jul 19 05:35:18.769: INFO: Pod "alpine-nnp-false-0df521a2-d6fb-4680-b666-580021119d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.080711ms
Jul 19 05:35:20.773: INFO: Pod "alpine-nnp-false-0df521a2-d6fb-4680-b666-580021119d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007279268s
Jul 19 05:35:22.778: INFO: Pod "alpine-nnp-false-0df521a2-d6fb-4680-b666-580021119d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012248055s
Jul 19 05:35:24.783: INFO: Pod "alpine-nnp-false-0df521a2-d6fb-4680-b666-580021119d7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017260064s
Jul 19 05:35:24.783: INFO: Pod "alpine-nnp-false-0df521a2-d6fb-4680-b666-580021119d7c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:24.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9479" for this suite.

• [SLOW TEST:6.147 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4146,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:24.835: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Jul 19 05:35:24.894: INFO: Major version: 1
STEP: Confirm minor version
Jul 19 05:35:24.895: INFO: cleanMinorVersion: 19
Jul 19 05:35:24.895: INFO: Minor version: 19+
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:24.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8113" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":259,"skipped":4194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:24.907: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 19 05:35:24.976: INFO: Waiting up to 5m0s for pod "pod-780a9c27-9a2b-4ff1-bd6e-fdcfb090d305" in namespace "emptydir-9380" to be "Succeeded or Failed"
Jul 19 05:35:24.980: INFO: Pod "pod-780a9c27-9a2b-4ff1-bd6e-fdcfb090d305": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022372ms
Jul 19 05:35:26.984: INFO: Pod "pod-780a9c27-9a2b-4ff1-bd6e-fdcfb090d305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008078739s
Jul 19 05:35:28.988: INFO: Pod "pod-780a9c27-9a2b-4ff1-bd6e-fdcfb090d305": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01207872s
STEP: Saw pod success
Jul 19 05:35:28.988: INFO: Pod "pod-780a9c27-9a2b-4ff1-bd6e-fdcfb090d305" satisfied condition "Succeeded or Failed"
Jul 19 05:35:28.996: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-780a9c27-9a2b-4ff1-bd6e-fdcfb090d305 container test-container: <nil>
STEP: delete the pod
Jul 19 05:35:29.033: INFO: Waiting for pod pod-780a9c27-9a2b-4ff1-bd6e-fdcfb090d305 to disappear
Jul 19 05:35:29.036: INFO: Pod pod-780a9c27-9a2b-4ff1-bd6e-fdcfb090d305 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:29.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9380" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":260,"skipped":4233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:29.048: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Jul 19 05:35:29.128: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-086781019 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:29.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7566" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":261,"skipped":4257,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:29.239: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-0dd3a061-b29e-4693-beea-47b2a55b7306
STEP: Creating a pod to test consume configMaps
Jul 19 05:35:29.407: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2d26ad95-32d6-4e8c-9c95-e55186902942" in namespace "projected-8703" to be "Succeeded or Failed"
Jul 19 05:35:29.413: INFO: Pod "pod-projected-configmaps-2d26ad95-32d6-4e8c-9c95-e55186902942": Phase="Pending", Reason="", readiness=false. Elapsed: 5.879732ms
Jul 19 05:35:31.417: INFO: Pod "pod-projected-configmaps-2d26ad95-32d6-4e8c-9c95-e55186902942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009669807s
Jul 19 05:35:33.422: INFO: Pod "pod-projected-configmaps-2d26ad95-32d6-4e8c-9c95-e55186902942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015194603s
STEP: Saw pod success
Jul 19 05:35:33.422: INFO: Pod "pod-projected-configmaps-2d26ad95-32d6-4e8c-9c95-e55186902942" satisfied condition "Succeeded or Failed"
Jul 19 05:35:33.425: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-projected-configmaps-2d26ad95-32d6-4e8c-9c95-e55186902942 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 05:35:33.452: INFO: Waiting for pod pod-projected-configmaps-2d26ad95-32d6-4e8c-9c95-e55186902942 to disappear
Jul 19 05:35:33.458: INFO: Pod pod-projected-configmaps-2d26ad95-32d6-4e8c-9c95-e55186902942 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:33.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8703" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":262,"skipped":4278,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:33.473: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:33.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5007" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":263,"skipped":4282,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:33.582: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:33.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5480" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":264,"skipped":4283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:33.699: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-923e0c40-b569-4933-a0c8-899acb7556f4
STEP: Creating a pod to test consume secrets
Jul 19 05:35:33.795: INFO: Waiting up to 5m0s for pod "pod-secrets-c6a8c23f-f502-4c34-9980-973f0513ae2a" in namespace "secrets-8202" to be "Succeeded or Failed"
Jul 19 05:35:33.798: INFO: Pod "pod-secrets-c6a8c23f-f502-4c34-9980-973f0513ae2a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.311313ms
Jul 19 05:35:35.802: INFO: Pod "pod-secrets-c6a8c23f-f502-4c34-9980-973f0513ae2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007133297s
Jul 19 05:35:37.806: INFO: Pod "pod-secrets-c6a8c23f-f502-4c34-9980-973f0513ae2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011492312s
STEP: Saw pod success
Jul 19 05:35:37.806: INFO: Pod "pod-secrets-c6a8c23f-f502-4c34-9980-973f0513ae2a" satisfied condition "Succeeded or Failed"
Jul 19 05:35:37.810: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-secrets-c6a8c23f-f502-4c34-9980-973f0513ae2a container secret-volume-test: <nil>
STEP: delete the pod
Jul 19 05:35:37.836: INFO: Waiting for pod pod-secrets-c6a8c23f-f502-4c34-9980-973f0513ae2a to disappear
Jul 19 05:35:37.842: INFO: Pod pod-secrets-c6a8c23f-f502-4c34-9980-973f0513ae2a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:37.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8202" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":265,"skipped":4315,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:37.856: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Jul 19 05:35:37.930: INFO: Waiting up to 5m0s for pod "var-expansion-ec507bb6-ee9d-4ab7-ac93-59fd69b4eaa9" in namespace "var-expansion-5852" to be "Succeeded or Failed"
Jul 19 05:35:37.935: INFO: Pod "var-expansion-ec507bb6-ee9d-4ab7-ac93-59fd69b4eaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.345674ms
Jul 19 05:35:40.005: INFO: Pod "var-expansion-ec507bb6-ee9d-4ab7-ac93-59fd69b4eaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075036328s
Jul 19 05:35:42.009: INFO: Pod "var-expansion-ec507bb6-ee9d-4ab7-ac93-59fd69b4eaa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079006147s
STEP: Saw pod success
Jul 19 05:35:42.009: INFO: Pod "var-expansion-ec507bb6-ee9d-4ab7-ac93-59fd69b4eaa9" satisfied condition "Succeeded or Failed"
Jul 19 05:35:42.013: INFO: Trying to get logs from node paas-192-168-10-183 pod var-expansion-ec507bb6-ee9d-4ab7-ac93-59fd69b4eaa9 container dapi-container: <nil>
STEP: delete the pod
Jul 19 05:35:42.040: INFO: Waiting for pod var-expansion-ec507bb6-ee9d-4ab7-ac93-59fd69b4eaa9 to disappear
Jul 19 05:35:42.044: INFO: Pod var-expansion-ec507bb6-ee9d-4ab7-ac93-59fd69b4eaa9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:35:42.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5852" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":266,"skipped":4315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:35:42.056: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-9358
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 19 05:35:42.110: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 19 05:35:42.219: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 05:35:44.223: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 05:35:46.223: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:35:48.224: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:35:50.223: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:35:52.224: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:35:54.223: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:35:56.223: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:35:58.224: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:36:00.223: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 19 05:36:02.223: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 19 05:36:02.230: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 19 05:36:06.284: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.0.25 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9358 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 05:36:06.284: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 05:36:07.538: INFO: Found all expected endpoints: [netserver-0]
Jul 19 05:36:07.542: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.0.75 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9358 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 19 05:36:07.542: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 05:36:08.712: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:36:08.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9358" for this suite.

• [SLOW TEST:26.669 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:36:08.726: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 05:36:08.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1519a98-5408-41ad-8b2f-38345aac134f" in namespace "projected-8446" to be "Succeeded or Failed"
Jul 19 05:36:08.799: INFO: Pod "downwardapi-volume-d1519a98-5408-41ad-8b2f-38345aac134f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83319ms
Jul 19 05:36:10.804: INFO: Pod "downwardapi-volume-d1519a98-5408-41ad-8b2f-38345aac134f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007388545s
Jul 19 05:36:12.808: INFO: Pod "downwardapi-volume-d1519a98-5408-41ad-8b2f-38345aac134f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011838172s
STEP: Saw pod success
Jul 19 05:36:12.808: INFO: Pod "downwardapi-volume-d1519a98-5408-41ad-8b2f-38345aac134f" satisfied condition "Succeeded or Failed"
Jul 19 05:36:12.811: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-d1519a98-5408-41ad-8b2f-38345aac134f container client-container: <nil>
STEP: delete the pod
Jul 19 05:36:12.837: INFO: Waiting for pod downwardapi-volume-d1519a98-5408-41ad-8b2f-38345aac134f to disappear
Jul 19 05:36:12.840: INFO: Pod downwardapi-volume-d1519a98-5408-41ad-8b2f-38345aac134f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:36:12.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8446" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":268,"skipped":4373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:36:12.853: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:36:12.997: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"77d47df3-f86b-4191-9efb-950a7672c0fc", Controller:(*bool)(0xc004c5b4a2), BlockOwnerDeletion:(*bool)(0xc004c5b4a3)}}
Jul 19 05:36:13.010: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"706a2700-ac18-4fb8-93f0-db83dfb02253", Controller:(*bool)(0xc005fb77fa), BlockOwnerDeletion:(*bool)(0xc005fb77fb)}}
Jul 19 05:36:13.021: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d2f772c9-afd1-4172-bc3b-a0029ec20276", Controller:(*bool)(0xc004c5b68a), BlockOwnerDeletion:(*bool)(0xc004c5b68b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:36:18.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6651" for this suite.

• [SLOW TEST:5.197 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":269,"skipped":4401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:36:18.050: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-4f84a092-6e3f-448c-aa7f-2679845c7ac6 in namespace container-probe-2477
Jul 19 05:36:22.141: INFO: Started pod liveness-4f84a092-6e3f-448c-aa7f-2679845c7ac6 in namespace container-probe-2477
STEP: checking the pod's current state and verifying that restartCount is present
Jul 19 05:36:22.144: INFO: Initial restart count of pod liveness-4f84a092-6e3f-448c-aa7f-2679845c7ac6 is 0
Jul 19 05:36:34.185: INFO: Restart count of pod container-probe-2477/liveness-4f84a092-6e3f-448c-aa7f-2679845c7ac6 is now 1 (12.041050307s elapsed)
Jul 19 05:36:54.229: INFO: Restart count of pod container-probe-2477/liveness-4f84a092-6e3f-448c-aa7f-2679845c7ac6 is now 2 (32.085240164s elapsed)
Jul 19 05:37:14.287: INFO: Restart count of pod container-probe-2477/liveness-4f84a092-6e3f-448c-aa7f-2679845c7ac6 is now 3 (52.143358775s elapsed)
Jul 19 05:37:34.336: INFO: Restart count of pod container-probe-2477/liveness-4f84a092-6e3f-448c-aa7f-2679845c7ac6 is now 4 (1m12.192625596s elapsed)
Jul 19 05:37:54.387: INFO: Restart count of pod container-probe-2477/liveness-4f84a092-6e3f-448c-aa7f-2679845c7ac6 is now 5 (1m32.243067605s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:37:54.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2477" for this suite.

• [SLOW TEST:96.367 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4459,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:37:54.418: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jul 19 05:37:54.489: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 05:38:54.595: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jul 19 05:38:54.620: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 19 05:38:54.638: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:39:24.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8488" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:90.331 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":271,"skipped":4476,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:39:24.749: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:39:24.842: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 19 05:39:24.849: INFO: Number of nodes with available pods: 0
Jul 19 05:39:24.849: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 19 05:39:24.873: INFO: Number of nodes with available pods: 0
Jul 19 05:39:24.873: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:25.878: INFO: Number of nodes with available pods: 0
Jul 19 05:39:25.878: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:26.878: INFO: Number of nodes with available pods: 0
Jul 19 05:39:26.878: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:27.877: INFO: Number of nodes with available pods: 0
Jul 19 05:39:27.877: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:28.878: INFO: Number of nodes with available pods: 1
Jul 19 05:39:28.878: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 19 05:39:28.898: INFO: Number of nodes with available pods: 1
Jul 19 05:39:28.899: INFO: Number of running nodes: 0, number of available pods: 1
Jul 19 05:39:29.906: INFO: Number of nodes with available pods: 0
Jul 19 05:39:29.906: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 19 05:39:29.926: INFO: Number of nodes with available pods: 0
Jul 19 05:39:29.926: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:30.930: INFO: Number of nodes with available pods: 0
Jul 19 05:39:30.930: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:31.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:31.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:32.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:32.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:33.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:33.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:34.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:34.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:35.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:35.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:36.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:36.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:37.930: INFO: Number of nodes with available pods: 0
Jul 19 05:39:37.930: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:38.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:38.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:39.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:39.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:40.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:40.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:41.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:41.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:42.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:42.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:43.932: INFO: Number of nodes with available pods: 0
Jul 19 05:39:43.932: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:44.930: INFO: Number of nodes with available pods: 0
Jul 19 05:39:44.930: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:45.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:45.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:46.930: INFO: Number of nodes with available pods: 0
Jul 19 05:39:46.930: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:47.934: INFO: Number of nodes with available pods: 0
Jul 19 05:39:47.934: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:48.933: INFO: Number of nodes with available pods: 0
Jul 19 05:39:48.933: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:49.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:49.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:50.931: INFO: Number of nodes with available pods: 0
Jul 19 05:39:50.931: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:51.932: INFO: Number of nodes with available pods: 0
Jul 19 05:39:51.932: INFO: Node paas-192-168-10-158 is running more than one daemon pod
Jul 19 05:39:52.948: INFO: Number of nodes with available pods: 1
Jul 19 05:39:52.948: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6453, will wait for the garbage collector to delete the pods
Jul 19 05:39:53.036: INFO: Deleting DaemonSet.extensions daemon-set took: 11.70052ms
Jul 19 05:39:53.136: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.227742ms
Jul 19 05:40:09.540: INFO: Number of nodes with available pods: 0
Jul 19 05:40:09.540: INFO: Number of running nodes: 0, number of available pods: 0
Jul 19 05:40:09.544: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6453/daemonsets","resourceVersion":"2447577"},"items":null}

Jul 19 05:40:09.547: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6453/pods","resourceVersion":"2447577"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:40:09.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6453" for this suite.

• [SLOW TEST:44.835 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":272,"skipped":4483,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:40:09.584: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:40:09.646: INFO: Creating ReplicaSet my-hostname-basic-db70979b-e189-4238-85b4-4041afabf406
Jul 19 05:40:09.655: INFO: Pod name my-hostname-basic-db70979b-e189-4238-85b4-4041afabf406: Found 0 pods out of 1
Jul 19 05:40:14.659: INFO: Pod name my-hostname-basic-db70979b-e189-4238-85b4-4041afabf406: Found 1 pods out of 1
Jul 19 05:40:14.659: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-db70979b-e189-4238-85b4-4041afabf406" is running
Jul 19 05:40:14.663: INFO: Pod "my-hostname-basic-db70979b-e189-4238-85b4-4041afabf406-qpg2j" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-19 05:40:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-19 05:40:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-19 05:40:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-19 05:40:09 +0000 UTC Reason: Message:} {Type:ResourceAllocated Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-19 05:40:09 +0000 UTC Reason: Message:}])
Jul 19 05:40:14.663: INFO: Trying to dial the pod
Jul 19 05:40:19.685: INFO: Controller my-hostname-basic-db70979b-e189-4238-85b4-4041afabf406: Got expected result from replica 1 [my-hostname-basic-db70979b-e189-4238-85b4-4041afabf406-qpg2j]: "my-hostname-basic-db70979b-e189-4238-85b4-4041afabf406-qpg2j", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:40:19.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1888" for this suite.

• [SLOW TEST:10.114 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":273,"skipped":4492,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:40:19.698: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6695
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6695
STEP: Creating statefulset with conflicting port in namespace statefulset-6695
STEP: Waiting until pod test-pod will start running in namespace statefulset-6695
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6695
Jul 19 05:40:25.805: INFO: Observed stateful pod in namespace: statefulset-6695, name: ss-0, uid: eae1a7e2-d6b7-4010-8208-b424ff6e64ba, status phase: Pending. Waiting for statefulset controller to delete.
Jul 19 05:40:26.219: INFO: Observed stateful pod in namespace: statefulset-6695, name: ss-0, uid: eae1a7e2-d6b7-4010-8208-b424ff6e64ba, status phase: Failed. Waiting for statefulset controller to delete.
Jul 19 05:40:26.229: INFO: Observed stateful pod in namespace: statefulset-6695, name: ss-0, uid: eae1a7e2-d6b7-4010-8208-b424ff6e64ba, status phase: Failed. Waiting for statefulset controller to delete.
Jul 19 05:40:26.236: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6695
STEP: Removing pod with conflicting port in namespace statefulset-6695
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6695 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 19 05:40:32.297: INFO: Deleting all statefulset in ns statefulset-6695
Jul 19 05:40:32.300: INFO: Scaling statefulset ss to 0
Jul 19 05:40:42.317: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 05:40:42.320: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:40:42.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6695" for this suite.

• [SLOW TEST:22.668 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":274,"skipped":4506,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:40:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7682
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-7682
Jul 19 05:40:42.475: INFO: Found 0 stateful pods, waiting for 1
Jul 19 05:40:52.480: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 19 05:40:52.545: INFO: Deleting all statefulset in ns statefulset-7682
Jul 19 05:40:52.548: INFO: Scaling statefulset ss to 0
Jul 19 05:41:32.590: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 05:41:32.593: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:41:32.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7682" for this suite.

• [SLOW TEST:50.261 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":275,"skipped":4527,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:41:32.628: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9870
STEP: creating service affinity-clusterip in namespace services-9870
STEP: creating replication controller affinity-clusterip in namespace services-9870
I0719 05:41:32.717139      24 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9870, replica count: 3
I0719 05:41:35.767456      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 05:41:38.767632      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 05:41:38.776: INFO: Creating new exec pod
Jul 19 05:41:43.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-9870 execpod-affinity2xjbn -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jul 19 05:41:44.082: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul 19 05:41:44.082: INFO: stdout: ""
Jul 19 05:41:44.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-9870 execpod-affinity2xjbn -- /bin/sh -x -c nc -zv -t -w 2 10.247.42.154 80'
Jul 19 05:41:44.453: INFO: stderr: "+ nc -zv -t -w 2 10.247.42.154 80\nConnection to 10.247.42.154 80 port [tcp/http] succeeded!\n"
Jul 19 05:41:44.453: INFO: stdout: ""
Jul 19 05:41:44.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 exec --namespace=services-9870 execpod-affinity2xjbn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.247.42.154:80/ ; done'
Jul 19 05:41:44.971: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.247.42.154:80/\n"
Jul 19 05:41:44.971: INFO: stdout: "\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h\naffinity-clusterip-sws2h"
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Received response from host: affinity-clusterip-sws2h
Jul 19 05:41:44.971: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9870, will wait for the garbage collector to delete the pods
Jul 19 05:41:45.055: INFO: Deleting ReplicationController affinity-clusterip took: 12.91425ms
Jul 19 05:41:45.155: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.181813ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:42:04.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9870" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:31.776 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":276,"skipped":4540,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:42:04.405: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 05:42:04.881: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 05:42:06.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270124, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270124, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270124, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270124, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 05:42:08.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270124, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270124, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270124, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270124, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 05:42:11.925: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:42:12.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8366" for this suite.
STEP: Destroying namespace "webhook-8366-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.750 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":277,"skipped":4565,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:42:12.155: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-72e53beb-2347-453e-b15c-d3b224e7d986
Jul 19 05:42:12.328: INFO: Pod name my-hostname-basic-72e53beb-2347-453e-b15c-d3b224e7d986: Found 0 pods out of 1
Jul 19 05:42:17.333: INFO: Pod name my-hostname-basic-72e53beb-2347-453e-b15c-d3b224e7d986: Found 1 pods out of 1
Jul 19 05:42:17.333: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-72e53beb-2347-453e-b15c-d3b224e7d986" are running
Jul 19 05:42:17.336: INFO: Pod "my-hostname-basic-72e53beb-2347-453e-b15c-d3b224e7d986-wl5pd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-19 05:42:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-19 05:42:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-19 05:42:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-19 05:42:12 +0000 UTC Reason: Message:} {Type:ResourceAllocated Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-19 05:42:12 +0000 UTC Reason: Message:}])
Jul 19 05:42:17.336: INFO: Trying to dial the pod
Jul 19 05:42:22.353: INFO: Controller my-hostname-basic-72e53beb-2347-453e-b15c-d3b224e7d986: Got expected result from replica 1 [my-hostname-basic-72e53beb-2347-453e-b15c-d3b224e7d986-wl5pd]: "my-hostname-basic-72e53beb-2347-453e-b15c-d3b224e7d986-wl5pd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:42:22.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2348" for this suite.

• [SLOW TEST:10.210 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":278,"skipped":4566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:42:22.366: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:42:38.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2558" for this suite.

• [SLOW TEST:16.212 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":279,"skipped":4601,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:42:38.578: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0719 05:42:39.707104      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0719 05:42:39.707123      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0719 05:42:39.707129      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 19 05:42:39.707: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:42:39.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4249" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":280,"skipped":4611,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:42:39.720: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7871.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7871.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 19 05:42:45.952: INFO: DNS probes using dns-7871/dns-test-e1d211d4-d0b6-4f32-a77c-034be31cecd2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:42:45.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7871" for this suite.

• [SLOW TEST:6.258 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":281,"skipped":4676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:42:45.979: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 19 05:42:46.059: INFO: Waiting up to 5m0s for pod "pod-7a756369-15d0-40f5-bf39-cfcfa41d7825" in namespace "emptydir-9402" to be "Succeeded or Failed"
Jul 19 05:42:46.064: INFO: Pod "pod-7a756369-15d0-40f5-bf39-cfcfa41d7825": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201978ms
Jul 19 05:42:48.068: INFO: Pod "pod-7a756369-15d0-40f5-bf39-cfcfa41d7825": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008354152s
Jul 19 05:42:50.072: INFO: Pod "pod-7a756369-15d0-40f5-bf39-cfcfa41d7825": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012847636s
STEP: Saw pod success
Jul 19 05:42:50.072: INFO: Pod "pod-7a756369-15d0-40f5-bf39-cfcfa41d7825" satisfied condition "Succeeded or Failed"
Jul 19 05:42:50.076: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-7a756369-15d0-40f5-bf39-cfcfa41d7825 container test-container: <nil>
STEP: delete the pod
Jul 19 05:42:50.154: INFO: Waiting for pod pod-7a756369-15d0-40f5-bf39-cfcfa41d7825 to disappear
Jul 19 05:42:50.157: INFO: Pod pod-7a756369-15d0-40f5-bf39-cfcfa41d7825 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:42:50.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9402" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":282,"skipped":4699,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:42:50.170: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-188b7948-f552-46c3-a208-6d0e39fc08d2
STEP: Creating a pod to test consume secrets
Jul 19 05:42:50.333: INFO: Waiting up to 5m0s for pod "pod-secrets-455abbb4-972c-4439-9fb1-4c1a9acf4423" in namespace "secrets-1382" to be "Succeeded or Failed"
Jul 19 05:42:50.340: INFO: Pod "pod-secrets-455abbb4-972c-4439-9fb1-4c1a9acf4423": Phase="Pending", Reason="", readiness=false. Elapsed: 7.03306ms
Jul 19 05:42:52.344: INFO: Pod "pod-secrets-455abbb4-972c-4439-9fb1-4c1a9acf4423": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010656511s
Jul 19 05:42:54.401: INFO: Pod "pod-secrets-455abbb4-972c-4439-9fb1-4c1a9acf4423": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067604085s
STEP: Saw pod success
Jul 19 05:42:54.401: INFO: Pod "pod-secrets-455abbb4-972c-4439-9fb1-4c1a9acf4423" satisfied condition "Succeeded or Failed"
Jul 19 05:42:54.405: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-secrets-455abbb4-972c-4439-9fb1-4c1a9acf4423 container secret-volume-test: <nil>
STEP: delete the pod
Jul 19 05:42:54.431: INFO: Waiting for pod pod-secrets-455abbb4-972c-4439-9fb1-4c1a9acf4423 to disappear
Jul 19 05:42:54.435: INFO: Pod pod-secrets-455abbb4-972c-4439-9fb1-4c1a9acf4423 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:42:54.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1382" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":283,"skipped":4705,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:42:54.446: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Jul 19 05:42:54.516: INFO: Waiting up to 5m0s for pod "client-containers-fb6ffb05-f1da-48df-9396-d083273fc312" in namespace "containers-1736" to be "Succeeded or Failed"
Jul 19 05:42:54.519: INFO: Pod "client-containers-fb6ffb05-f1da-48df-9396-d083273fc312": Phase="Pending", Reason="", readiness=false. Elapsed: 3.4883ms
Jul 19 05:42:56.523: INFO: Pod "client-containers-fb6ffb05-f1da-48df-9396-d083273fc312": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007237232s
Jul 19 05:42:58.528: INFO: Pod "client-containers-fb6ffb05-f1da-48df-9396-d083273fc312": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011701463s
STEP: Saw pod success
Jul 19 05:42:58.528: INFO: Pod "client-containers-fb6ffb05-f1da-48df-9396-d083273fc312" satisfied condition "Succeeded or Failed"
Jul 19 05:42:58.531: INFO: Trying to get logs from node paas-192-168-10-183 pod client-containers-fb6ffb05-f1da-48df-9396-d083273fc312 container test-container: <nil>
STEP: delete the pod
Jul 19 05:42:58.590: INFO: Waiting for pod client-containers-fb6ffb05-f1da-48df-9396-d083273fc312 to disappear
Jul 19 05:42:58.593: INFO: Pod client-containers-fb6ffb05-f1da-48df-9396-d083273fc312 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:42:58.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1736" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":284,"skipped":4708,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:42:58.607: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 19 05:42:58.679: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3de5ba4c-2aff-4a88-91a9-6cc23c63dd28" in namespace "downward-api-5752" to be "Succeeded or Failed"
Jul 19 05:42:58.682: INFO: Pod "downwardapi-volume-3de5ba4c-2aff-4a88-91a9-6cc23c63dd28": Phase="Pending", Reason="", readiness=false. Elapsed: 3.334172ms
Jul 19 05:43:00.686: INFO: Pod "downwardapi-volume-3de5ba4c-2aff-4a88-91a9-6cc23c63dd28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007266023s
Jul 19 05:43:02.690: INFO: Pod "downwardapi-volume-3de5ba4c-2aff-4a88-91a9-6cc23c63dd28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01182994s
STEP: Saw pod success
Jul 19 05:43:02.691: INFO: Pod "downwardapi-volume-3de5ba4c-2aff-4a88-91a9-6cc23c63dd28" satisfied condition "Succeeded or Failed"
Jul 19 05:43:02.694: INFO: Trying to get logs from node paas-192-168-10-183 pod downwardapi-volume-3de5ba4c-2aff-4a88-91a9-6cc23c63dd28 container client-container: <nil>
STEP: delete the pod
Jul 19 05:43:02.770: INFO: Waiting for pod downwardapi-volume-3de5ba4c-2aff-4a88-91a9-6cc23c63dd28 to disappear
Jul 19 05:43:02.773: INFO: Pod downwardapi-volume-3de5ba4c-2aff-4a88-91a9-6cc23c63dd28 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:43:02.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5752" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":285,"skipped":4709,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:43:02.791: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 19 05:43:07.898: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:43:08.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-822" for this suite.

• [SLOW TEST:6.175 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":286,"skipped":4741,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:43:08.966: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-79583456-c430-49cf-89bd-3b5605a2cab4
STEP: Creating a pod to test consume configMaps
Jul 19 05:43:09.070: INFO: Waiting up to 5m0s for pod "pod-configmaps-5aba563e-62ef-4484-bdef-0d4846178f46" in namespace "configmap-746" to be "Succeeded or Failed"
Jul 19 05:43:09.074: INFO: Pod "pod-configmaps-5aba563e-62ef-4484-bdef-0d4846178f46": Phase="Pending", Reason="", readiness=false. Elapsed: 3.953617ms
Jul 19 05:43:11.079: INFO: Pod "pod-configmaps-5aba563e-62ef-4484-bdef-0d4846178f46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008524525s
Jul 19 05:43:13.082: INFO: Pod "pod-configmaps-5aba563e-62ef-4484-bdef-0d4846178f46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012222194s
STEP: Saw pod success
Jul 19 05:43:13.082: INFO: Pod "pod-configmaps-5aba563e-62ef-4484-bdef-0d4846178f46" satisfied condition "Succeeded or Failed"
Jul 19 05:43:13.086: INFO: Trying to get logs from node paas-192-168-10-158 pod pod-configmaps-5aba563e-62ef-4484-bdef-0d4846178f46 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 05:43:13.160: INFO: Waiting for pod pod-configmaps-5aba563e-62ef-4484-bdef-0d4846178f46 to disappear
Jul 19 05:43:13.164: INFO: Pod pod-configmaps-5aba563e-62ef-4484-bdef-0d4846178f46 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:43:13.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-746" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":287,"skipped":4747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:43:13.176: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2504.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2504.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2504.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 19 05:43:19.412: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:19.418: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:19.422: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:19.426: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:19.466: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:19.471: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:19.476: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:19.481: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:19.490: INFO: Lookups using dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local]

Jul 19 05:43:24.496: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:24.503: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:24.509: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:24.512: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:24.524: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:24.528: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:24.532: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:24.536: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:24.544: INFO: Lookups using dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local]

Jul 19 05:43:29.494: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:29.499: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:29.504: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:29.508: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:29.521: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:29.524: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:29.528: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:29.532: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:29.543: INFO: Lookups using dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local]

Jul 19 05:43:34.495: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:34.499: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:34.525: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:34.529: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:34.541: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:34.545: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:34.549: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:34.553: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:34.561: INFO: Lookups using dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local]

Jul 19 05:43:39.495: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:39.499: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:39.504: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:39.508: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:39.520: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:39.524: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:39.528: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:39.532: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:39.540: INFO: Lookups using dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local]

Jul 19 05:43:44.496: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:44.502: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:44.506: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:44.511: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:44.528: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:44.533: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:44.538: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:44.543: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local from pod dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea: the server could not find the requested resource (get pods dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea)
Jul 19 05:43:44.555: INFO: Lookups using dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2504.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2504.svc.cluster.local jessie_udp@dns-test-service-2.dns-2504.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2504.svc.cluster.local]

Jul 19 05:43:49.534: INFO: DNS probes using dns-2504/dns-test-eb2bc477-fcf7-42a6-80f3-c6d2b283b4ea succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:43:49.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2504" for this suite.

• [SLOW TEST:36.432 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":288,"skipped":4772,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:43:49.609: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-7ac41cc9-8fbc-491e-99f7-e5cbbb221789
STEP: Creating a pod to test consume configMaps
Jul 19 05:43:49.694: INFO: Waiting up to 5m0s for pod "pod-configmaps-46d9a3c0-6553-4e48-a87b-ee6d87acd70e" in namespace "configmap-3247" to be "Succeeded or Failed"
Jul 19 05:43:49.698: INFO: Pod "pod-configmaps-46d9a3c0-6553-4e48-a87b-ee6d87acd70e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.918826ms
Jul 19 05:43:51.702: INFO: Pod "pod-configmaps-46d9a3c0-6553-4e48-a87b-ee6d87acd70e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007466186s
Jul 19 05:43:53.709: INFO: Pod "pod-configmaps-46d9a3c0-6553-4e48-a87b-ee6d87acd70e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015117033s
STEP: Saw pod success
Jul 19 05:43:53.709: INFO: Pod "pod-configmaps-46d9a3c0-6553-4e48-a87b-ee6d87acd70e" satisfied condition "Succeeded or Failed"
Jul 19 05:43:53.715: INFO: Trying to get logs from node paas-192-168-10-183 pod pod-configmaps-46d9a3c0-6553-4e48-a87b-ee6d87acd70e container configmap-volume-test: <nil>
STEP: delete the pod
Jul 19 05:43:53.742: INFO: Waiting for pod pod-configmaps-46d9a3c0-6553-4e48-a87b-ee6d87acd70e to disappear
Jul 19 05:43:53.745: INFO: Pod pod-configmaps-46d9a3c0-6553-4e48-a87b-ee6d87acd70e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:43:53.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3247" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":289,"skipped":4775,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:43:53.758: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:43:53.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5652" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":290,"skipped":4783,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:43:53.833: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-7afe8490-b35d-49ea-bda9-5fbf5d8074be
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:43:57.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-813" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":291,"skipped":4784,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:43:57.995: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Jul 19 05:44:02.108: INFO: Pod pod-hostip-bec8532e-0a8a-4932-8cc6-fe8ac0c6d609 has hostIP: 192.168.10.158
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:44:02.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4712" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":4789,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:44:02.122: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:44:02.179: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:44:08.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3784" for this suite.

• [SLOW TEST:6.581 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":293,"skipped":4802,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:44:08.702: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-57f30cf2-f37f-48f6-809f-81febf630b00 in namespace container-probe-364
Jul 19 05:44:12.897: INFO: Started pod liveness-57f30cf2-f37f-48f6-809f-81febf630b00 in namespace container-probe-364
STEP: checking the pod's current state and verifying that restartCount is present
Jul 19 05:44:12.902: INFO: Initial restart count of pod liveness-57f30cf2-f37f-48f6-809f-81febf630b00 is 0
Jul 19 05:44:28.940: INFO: Restart count of pod container-probe-364/liveness-57f30cf2-f37f-48f6-809f-81febf630b00 is now 1 (16.037191443s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:44:28.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-364" for this suite.

• [SLOW TEST:20.265 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":294,"skipped":4807,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:44:28.967: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-6585
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6585 to expose endpoints map[]
Jul 19 05:44:29.094: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jul 19 05:44:30.105: INFO: successfully validated that service endpoint-test2 in namespace services-6585 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6585
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6585 to expose endpoints map[pod1:[80]]
Jul 19 05:44:34.146: INFO: successfully validated that service endpoint-test2 in namespace services-6585 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-6585
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6585 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 19 05:44:38.181: INFO: successfully validated that service endpoint-test2 in namespace services-6585 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-6585
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6585 to expose endpoints map[pod2:[80]]
Jul 19 05:44:38.211: INFO: successfully validated that service endpoint-test2 in namespace services-6585 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-6585
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6585 to expose endpoints map[]
Jul 19 05:44:39.238: INFO: successfully validated that service endpoint-test2 in namespace services-6585 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:44:39.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6585" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.398 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":295,"skipped":4822,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:44:39.366: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 05:44:40.036: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 05:44:42.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270280, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270280, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270280, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270280, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 05:44:45.097: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:44:57.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7923" for this suite.
STEP: Destroying namespace "webhook-7923-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.021 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":296,"skipped":4838,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:44:57.387: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-cc27f519-3f13-4ead-b895-65d139da4730 in namespace container-probe-1008
Jul 19 05:45:01.467: INFO: Started pod test-webserver-cc27f519-3f13-4ead-b895-65d139da4730 in namespace container-probe-1008
STEP: checking the pod's current state and verifying that restartCount is present
Jul 19 05:45:01.470: INFO: Initial restart count of pod test-webserver-cc27f519-3f13-4ead-b895-65d139da4730 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:49:02.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1008" for this suite.

• [SLOW TEST:244.654 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":297,"skipped":4845,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:49:02.041: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul 19 05:49:02.100: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul 19 05:49:15.406: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
Jul 19 05:49:18.358: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:49:32.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2384" for this suite.

• [SLOW TEST:30.317 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":298,"skipped":4856,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:49:32.358: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 19 05:49:33.072: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 05:49:35.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270573, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270573, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270573, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762270573, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-85774c5cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 19 05:49:38.107: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:49:38.112: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3045-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:49:39.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9975" for this suite.
STEP: Destroying namespace "webhook-9975-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.385 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":299,"skipped":4857,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:49:39.743: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Jul 19 05:49:39.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 cluster-info'
Jul 19 05:49:40.517: INFO: stderr: ""
Jul 19 05:49:40.517: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.247.0.2:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.247.0.2:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:49:40.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5755" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":300,"skipped":4867,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:49:40.533: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:49:40.606: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul 19 05:49:44.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-9271 create -f -'
Jul 19 05:49:44.771: INFO: stderr: ""
Jul 19 05:49:44.771: INFO: stdout: "e2e-test-crd-publish-openapi-4767-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 19 05:49:44.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-9271 delete e2e-test-crd-publish-openapi-4767-crds test-foo'
Jul 19 05:49:44.878: INFO: stderr: ""
Jul 19 05:49:44.878: INFO: stdout: "e2e-test-crd-publish-openapi-4767-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 19 05:49:44.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-9271 apply -f -'
Jul 19 05:49:45.110: INFO: stderr: ""
Jul 19 05:49:45.110: INFO: stdout: "e2e-test-crd-publish-openapi-4767-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 19 05:49:45.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-9271 delete e2e-test-crd-publish-openapi-4767-crds test-foo'
Jul 19 05:49:45.230: INFO: stderr: ""
Jul 19 05:49:45.230: INFO: stdout: "e2e-test-crd-publish-openapi-4767-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul 19 05:49:45.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-9271 create -f -'
Jul 19 05:49:45.426: INFO: rc: 1
Jul 19 05:49:45.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-9271 apply -f -'
Jul 19 05:49:45.627: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul 19 05:49:45.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-9271 create -f -'
Jul 19 05:49:45.824: INFO: rc: 1
Jul 19 05:49:45.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 --namespace=crd-publish-openapi-9271 apply -f -'
Jul 19 05:49:46.042: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul 19 05:49:46.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 explain e2e-test-crd-publish-openapi-4767-crds'
Jul 19 05:49:46.242: INFO: stderr: ""
Jul 19 05:49:46.242: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4767-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul 19 05:49:46.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 explain e2e-test-crd-publish-openapi-4767-crds.metadata'
Jul 19 05:49:46.449: INFO: stderr: ""
Jul 19 05:49:46.449: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4767-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 19 05:49:46.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 explain e2e-test-crd-publish-openapi-4767-crds.spec'
Jul 19 05:49:46.661: INFO: stderr: ""
Jul 19 05:49:46.661: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4767-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 19 05:49:46.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 explain e2e-test-crd-publish-openapi-4767-crds.spec.bars'
Jul 19 05:49:46.846: INFO: stderr: ""
Jul 19 05:49:46.846: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4767-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul 19 05:49:46.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-086781019 explain e2e-test-crd-publish-openapi-4767-crds.spec.bars2'
Jul 19 05:49:47.059: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:49:50.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9271" for this suite.

• [SLOW TEST:9.511 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":301,"skipped":4867,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:49:50.045: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:49:50.103: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:49:50.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8394" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":302,"skipped":4875,"failed":0}
SSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:49:50.807: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 19 05:49:51.248: INFO: starting watch
STEP: patching
STEP: updating
Jul 19 05:49:51.278: INFO: waiting for watch events with expected annotations
Jul 19 05:49:51.278: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:49:51.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-9539" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":303,"skipped":4879,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:49:51.337: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Jul 19 05:49:51.964: INFO: created pod pod-service-account-defaultsa
Jul 19 05:49:51.964: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 19 05:49:51.971: INFO: created pod pod-service-account-mountsa
Jul 19 05:49:51.971: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 19 05:49:51.987: INFO: created pod pod-service-account-nomountsa
Jul 19 05:49:51.987: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 19 05:49:52.032: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 19 05:49:52.032: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 19 05:49:52.117: INFO: created pod pod-service-account-mountsa-mountspec
Jul 19 05:49:52.117: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 19 05:49:52.160: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 19 05:49:52.160: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 19 05:49:52.173: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 19 05:49:52.173: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 19 05:49:52.197: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 19 05:49:52.197: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 19 05:49:52.227: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 19 05:49:52.227: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:49:52.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2594" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":304,"skipped":4896,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 19 05:49:52.250: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 19 05:49:52.388: INFO: >>> kubeConfig: /tmp/kubeconfig-086781019
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 19 05:49:53.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3417" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":305,"skipped":4916,"failed":0}
SSSSSSSSSSSSSJul 19 05:49:53.687: INFO: Running AfterSuite actions on all nodes
Jul 19 05:49:53.700: INFO: Running AfterSuite actions on node 1
Jul 19 05:49:53.700: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4929,"failed":0}

Ran 305 of 5234 Specs in 6836.206 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4929 Skipped
PASS

Ginkgo ran 1 suite in 1h53m59.629713378s
Test Suite Passed
