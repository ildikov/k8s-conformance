I0520 12:42:59.794807      20 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-609854186
I0520 12:42:59.794916      20 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0520 12:42:59.795139      20 e2e.go:129] Starting e2e run "50d4a008-69ba-48a0-a676-f4b325cd4ec8" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1621514578 - Will randomize all specs
Will run 305 of 5484 specs

May 20 12:42:59.815: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 12:42:59.817: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0520 12:42:59.818197      20 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
May 20 12:42:59.829: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 20 12:42:59.860: INFO: 17 / 17 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 20 12:42:59.860: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
May 20 12:42:59.861: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 20 12:42:59.868: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel' (0 seconds elapsed)
May 20 12:42:59.868: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
May 20 12:42:59.868: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
May 20 12:42:59.868: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
May 20 12:42:59.868: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
May 20 12:42:59.868: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 20 12:42:59.868: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
May 20 12:42:59.869: INFO: e2e test version: v1.19.9
May 20 12:42:59.871: INFO: kube-apiserver version: v1.19.9
May 20 12:42:59.871: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 12:42:59.875: INFO: Cluster IP family: ipv4
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:42:59.876: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
May 20 12:42:59.905: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
May 20 12:42:59.911: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 12:42:59.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861" in namespace "downward-api-7709" to be "Succeeded or Failed"
May 20 12:42:59.926: INFO: Pod "downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861": Phase="Pending", Reason="", readiness=false. Elapsed: 4.611093ms
May 20 12:43:01.931: INFO: Pod "downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009560089s
May 20 12:43:03.935: INFO: Pod "downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012859567s
May 20 12:43:05.987: INFO: Pod "downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065380625s
May 20 12:43:08.045: INFO: Pod "downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861": Phase="Pending", Reason="", readiness=false. Elapsed: 8.123254216s
May 20 12:43:10.050: INFO: Pod "downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861": Phase="Pending", Reason="", readiness=false. Elapsed: 10.128646085s
May 20 12:43:12.056: INFO: Pod "downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861": Phase="Pending", Reason="", readiness=false. Elapsed: 12.134214675s
May 20 12:43:14.059: INFO: Pod "downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.137295239s
STEP: Saw pod success
May 20 12:43:14.059: INFO: Pod "downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861" satisfied condition "Succeeded or Failed"
May 20 12:43:14.061: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861 container client-container: <nil>
STEP: delete the pod
May 20 12:43:14.099: INFO: Waiting for pod downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861 to disappear
May 20 12:43:14.101: INFO: Pod downwardapi-volume-8e402af4-d354-4443-87bd-55a033d58861 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:43:14.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7709" for this suite.

• [SLOW TEST:14.233 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":1,"skipped":6,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:43:14.110: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-e9bc08f3-fd3b-46fd-9620-7aeab2c05078
STEP: Creating secret with name s-test-opt-upd-205ea29c-d588-439e-8aa3-ca2c5d7b2bfb
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e9bc08f3-fd3b-46fd-9620-7aeab2c05078
STEP: Updating secret s-test-opt-upd-205ea29c-d588-439e-8aa3-ca2c5d7b2bfb
STEP: Creating secret with name s-test-opt-create-66aefee7-0684-4e38-8242-64e709feafd8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:44:38.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4984" for this suite.

• [SLOW TEST:84.580 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":2,"skipped":17,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:44:38.691: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-9682
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 20 12:44:38.713: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 20 12:44:38.761: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:44:40.767: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:44:42.766: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:44:44.764: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:44:46.765: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:44:48.765: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:44:50.766: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:44:52.764: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:44:54.765: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:44:56.764: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 12:44:58.764: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 12:45:00.765: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 12:45:02.764: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 12:45:04.774: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 12:45:06.766: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 12:45:08.764: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 12:45:10.764: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 12:45:12.767: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 20 12:45:12.771: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 20 12:45:12.776: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 20 12:45:16.808: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9682 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 12:45:16.808: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 12:45:18.013: INFO: Found all expected endpoints: [netserver-0]
May 20 12:45:18.016: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9682 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 12:45:18.016: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 12:45:19.138: INFO: Found all expected endpoints: [netserver-1]
May 20 12:45:19.143: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.5 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9682 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 12:45:19.143: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 12:45:20.251: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:45:20.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9682" for this suite.

• [SLOW TEST:41.568 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":3,"skipped":42,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:45:20.262: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
May 20 12:45:20.290: INFO: Waiting up to 5m0s for pod "var-expansion-c2c02e4c-cc5d-4750-a135-5b78e0723e52" in namespace "var-expansion-1660" to be "Succeeded or Failed"
May 20 12:45:20.296: INFO: Pod "var-expansion-c2c02e4c-cc5d-4750-a135-5b78e0723e52": Phase="Pending", Reason="", readiness=false. Elapsed: 5.877247ms
May 20 12:45:22.300: INFO: Pod "var-expansion-c2c02e4c-cc5d-4750-a135-5b78e0723e52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010335425s
May 20 12:45:24.304: INFO: Pod "var-expansion-c2c02e4c-cc5d-4750-a135-5b78e0723e52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013798521s
May 20 12:45:26.308: INFO: Pod "var-expansion-c2c02e4c-cc5d-4750-a135-5b78e0723e52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017583651s
STEP: Saw pod success
May 20 12:45:26.308: INFO: Pod "var-expansion-c2c02e4c-cc5d-4750-a135-5b78e0723e52" satisfied condition "Succeeded or Failed"
May 20 12:45:26.312: INFO: Trying to get logs from node k8s-2 pod var-expansion-c2c02e4c-cc5d-4750-a135-5b78e0723e52 container dapi-container: <nil>
STEP: delete the pod
May 20 12:45:26.341: INFO: Waiting for pod var-expansion-c2c02e4c-cc5d-4750-a135-5b78e0723e52 to disappear
May 20 12:45:26.344: INFO: Pod var-expansion-c2c02e4c-cc5d-4750-a135-5b78e0723e52 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:45:26.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1660" for this suite.

• [SLOW TEST:6.091 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":4,"skipped":57,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:45:26.354: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
May 20 12:45:26.394: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8110 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:45:26.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8110" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":5,"skipped":64,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:45:26.474: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 20 12:45:26.497: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 20 12:45:26.504: INFO: Waiting for terminating namespaces to be deleted...
May 20 12:45:26.507: INFO: 
Logging pods the apiserver thinks is on node k8s-1 before test
May 20 12:45:26.514: INFO: coredns-7677f9bb54-24hll from kube-system started at 2021-05-20 12:39:45 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.514: INFO: 	Container coredns ready: true, restart count 0
May 20 12:45:26.515: INFO: dns-autoscaler-5b7b5c9b6f-xgl8n from kube-system started at 2021-05-20 12:39:49 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.515: INFO: 	Container autoscaler ready: true, restart count 0
May 20 12:45:26.515: INFO: kube-apiserver-k8s-1 from kube-system started at 2021-05-20 12:37:59 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.515: INFO: 	Container kube-apiserver ready: true, restart count 0
May 20 12:45:26.515: INFO: kube-controller-manager-k8s-1 from kube-system started at 2021-05-20 12:38:14 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.515: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 20 12:45:26.515: INFO: kube-flannel-xshzr from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.515: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 12:45:26.515: INFO: kube-proxy-r8x5s from kube-system started at 2021-05-20 12:39:03 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.515: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 12:45:26.515: INFO: kube-scheduler-k8s-1 from kube-system started at 2021-05-20 12:38:14 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.515: INFO: 	Container kube-scheduler ready: true, restart count 0
May 20 12:45:26.515: INFO: nodelocaldns-j9jjn from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.516: INFO: 	Container node-cache ready: true, restart count 0
May 20 12:45:26.516: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-nkrjs from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 12:45:26.516: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 12:45:26.516: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 12:45:26.516: INFO: 
Logging pods the apiserver thinks is on node k8s-2 before test
May 20 12:45:26.522: INFO: coredns-7677f9bb54-m6mpv from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.522: INFO: 	Container coredns ready: true, restart count 0
May 20 12:45:26.522: INFO: kube-flannel-ggvw9 from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.522: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 12:45:26.522: INFO: kube-proxy-hdfgh from kube-system started at 2021-05-20 12:39:03 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.522: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 12:45:26.522: INFO: nginx-proxy-k8s-2 from kube-system started at 2021-05-20 12:38:55 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.522: INFO: 	Container nginx-proxy ready: true, restart count 0
May 20 12:45:26.523: INFO: nodelocaldns-nl6xd from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.523: INFO: 	Container node-cache ready: true, restart count 0
May 20 12:45:26.523: INFO: sonobuoy-e2e-job-9cce46af7e3b479e from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 12:45:26.523: INFO: 	Container e2e ready: true, restart count 0
May 20 12:45:26.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 12:45:26.523: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-v8rq9 from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 12:45:26.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 12:45:26.523: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 12:45:26.523: INFO: 
Logging pods the apiserver thinks is on node k8s-3 before test
May 20 12:45:26.528: INFO: kube-flannel-hx4xl from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.528: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 12:45:26.528: INFO: kube-proxy-whdwz from kube-system started at 2021-05-20 12:39:04 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.528: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 12:45:26.529: INFO: nginx-proxy-k8s-3 from kube-system started at 2021-05-20 12:38:56 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.529: INFO: 	Container nginx-proxy ready: true, restart count 0
May 20 12:45:26.529: INFO: nodelocaldns-pzwcd from kube-system started at 2021-05-20 12:39:51 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.529: INFO: 	Container node-cache ready: true, restart count 0
May 20 12:45:26.529: INFO: sonobuoy from sonobuoy started at 2021-05-20 12:41:44 +0000 UTC (1 container statuses recorded)
May 20 12:45:26.529: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 20 12:45:26.529: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-lvpcw from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 12:45:26.529: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 12:45:26.529: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node k8s-1
STEP: verifying the node has the label node k8s-2
STEP: verifying the node has the label node k8s-3
May 20 12:45:26.590: INFO: Pod coredns-7677f9bb54-24hll requesting resource cpu=100m on Node k8s-1
May 20 12:45:26.590: INFO: Pod coredns-7677f9bb54-m6mpv requesting resource cpu=100m on Node k8s-2
May 20 12:45:26.590: INFO: Pod dns-autoscaler-5b7b5c9b6f-xgl8n requesting resource cpu=20m on Node k8s-1
May 20 12:45:26.590: INFO: Pod kube-apiserver-k8s-1 requesting resource cpu=250m on Node k8s-1
May 20 12:45:26.590: INFO: Pod kube-controller-manager-k8s-1 requesting resource cpu=200m on Node k8s-1
May 20 12:45:26.590: INFO: Pod kube-flannel-ggvw9 requesting resource cpu=150m on Node k8s-2
May 20 12:45:26.590: INFO: Pod kube-flannel-hx4xl requesting resource cpu=150m on Node k8s-3
May 20 12:45:26.590: INFO: Pod kube-flannel-xshzr requesting resource cpu=150m on Node k8s-1
May 20 12:45:26.590: INFO: Pod kube-proxy-hdfgh requesting resource cpu=0m on Node k8s-2
May 20 12:45:26.590: INFO: Pod kube-proxy-r8x5s requesting resource cpu=0m on Node k8s-1
May 20 12:45:26.590: INFO: Pod kube-proxy-whdwz requesting resource cpu=0m on Node k8s-3
May 20 12:45:26.590: INFO: Pod kube-scheduler-k8s-1 requesting resource cpu=100m on Node k8s-1
May 20 12:45:26.590: INFO: Pod nginx-proxy-k8s-2 requesting resource cpu=25m on Node k8s-2
May 20 12:45:26.590: INFO: Pod nginx-proxy-k8s-3 requesting resource cpu=25m on Node k8s-3
May 20 12:45:26.590: INFO: Pod nodelocaldns-j9jjn requesting resource cpu=100m on Node k8s-1
May 20 12:45:26.590: INFO: Pod nodelocaldns-nl6xd requesting resource cpu=100m on Node k8s-2
May 20 12:45:26.590: INFO: Pod nodelocaldns-pzwcd requesting resource cpu=100m on Node k8s-3
May 20 12:45:26.590: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-3
May 20 12:45:26.590: INFO: Pod sonobuoy-e2e-job-9cce46af7e3b479e requesting resource cpu=0m on Node k8s-2
May 20 12:45:26.590: INFO: Pod sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-lvpcw requesting resource cpu=0m on Node k8s-3
May 20 12:45:26.590: INFO: Pod sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-nkrjs requesting resource cpu=0m on Node k8s-1
May 20 12:45:26.590: INFO: Pod sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-v8rq9 requesting resource cpu=0m on Node k8s-2
STEP: Starting Pods to consume most of the cluster CPU.
May 20 12:45:26.590: INFO: Creating a pod which consumes cpu=616m on Node k8s-1
May 20 12:45:26.595: INFO: Creating a pod which consumes cpu=1067m on Node k8s-2
May 20 12:45:26.601: INFO: Creating a pod which consumes cpu=1137m on Node k8s-3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-83808946-445d-462d-a88f-777a570b2ed2.1680c70d94d08042], Reason = [Scheduled], Message = [Successfully assigned sched-pred-370/filler-pod-83808946-445d-462d-a88f-777a570b2ed2 to k8s-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-83808946-445d-462d-a88f-777a570b2ed2.1680c70dbab1054e], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-83808946-445d-462d-a88f-777a570b2ed2.1680c70e07b7cd8f], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 1.292278315s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-83808946-445d-462d-a88f-777a570b2ed2.1680c70e0edc6ed4], Reason = [Created], Message = [Created container filler-pod-83808946-445d-462d-a88f-777a570b2ed2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-83808946-445d-462d-a88f-777a570b2ed2.1680c70e190fb39b], Reason = [Started], Message = [Started container filler-pod-83808946-445d-462d-a88f-777a570b2ed2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9a0e4c9b-948e-4d79-b870-4ec8b253a8f4.1680c70d93dcd1a5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-370/filler-pod-9a0e4c9b-948e-4d79-b870-4ec8b253a8f4 to k8s-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9a0e4c9b-948e-4d79-b870-4ec8b253a8f4.1680c70dd8a1f70c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9a0e4c9b-948e-4d79-b870-4ec8b253a8f4.1680c70dddc718dc], Reason = [Created], Message = [Created container filler-pod-9a0e4c9b-948e-4d79-b870-4ec8b253a8f4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9a0e4c9b-948e-4d79-b870-4ec8b253a8f4.1680c70deca85618], Reason = [Started], Message = [Started container filler-pod-9a0e4c9b-948e-4d79-b870-4ec8b253a8f4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2120d7c-1942-4c36-bf30-7497f93fd594.1680c70d96140f0e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-370/filler-pod-f2120d7c-1942-4c36-bf30-7497f93fd594 to k8s-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2120d7c-1942-4c36-bf30-7497f93fd594.1680c70ddba137fe], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2120d7c-1942-4c36-bf30-7497f93fd594.1680c70e1c334fa9], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 1.083299554s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2120d7c-1942-4c36-bf30-7497f93fd594.1680c70e269230ac], Reason = [Created], Message = [Created container filler-pod-f2120d7c-1942-4c36-bf30-7497f93fd594]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2120d7c-1942-4c36-bf30-7497f93fd594.1680c70e32fe335a], Reason = [Started], Message = [Started container filler-pod-f2120d7c-1942-4c36-bf30-7497f93fd594]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1680c70e85bc52a9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1680c70e86fe41ad], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node k8s-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:45:31.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-370" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.362 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":6,"skipped":123,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:45:31.836: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
May 20 12:45:31.860: INFO: namespace kubectl-8389
May 20 12:45:31.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8389 create -f -'
May 20 12:45:32.605: INFO: stderr: ""
May 20 12:45:32.605: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 20 12:45:33.610: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 12:45:33.610: INFO: Found 0 / 1
May 20 12:45:34.608: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 12:45:34.608: INFO: Found 0 / 1
May 20 12:45:35.609: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 12:45:35.609: INFO: Found 1 / 1
May 20 12:45:35.609: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 20 12:45:35.619: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 12:45:35.619: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 20 12:45:35.619: INFO: wait on agnhost-primary startup in kubectl-8389 
May 20 12:45:35.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8389 logs agnhost-primary-rds82 agnhost-primary'
May 20 12:45:35.729: INFO: stderr: ""
May 20 12:45:35.729: INFO: stdout: "Paused\n"
STEP: exposing RC
May 20 12:45:35.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8389 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 20 12:45:35.848: INFO: stderr: ""
May 20 12:45:35.848: INFO: stdout: "service/rm2 exposed\n"
May 20 12:45:35.854: INFO: Service rm2 in namespace kubectl-8389 found.
STEP: exposing service
May 20 12:45:37.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8389 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 20 12:45:37.946: INFO: stderr: ""
May 20 12:45:37.946: INFO: stdout: "service/rm3 exposed\n"
May 20 12:45:37.951: INFO: Service rm3 in namespace kubectl-8389 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:45:39.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8389" for this suite.

• [SLOW TEST:8.129 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":7,"skipped":124,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:45:39.965: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 20 12:45:46.011: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9967 PodName:var-expansion-46d8598d-526d-4cf0-8008-fb13dbfda5d8 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 12:45:46.011: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: test for file in mounted path
May 20 12:45:46.121: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9967 PodName:var-expansion-46d8598d-526d-4cf0-8008-fb13dbfda5d8 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 12:45:46.121: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: updating the annotation value
May 20 12:45:46.757: INFO: Successfully updated pod "var-expansion-46d8598d-526d-4cf0-8008-fb13dbfda5d8"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 20 12:45:46.760: INFO: Deleting pod "var-expansion-46d8598d-526d-4cf0-8008-fb13dbfda5d8" in namespace "var-expansion-9967"
May 20 12:45:46.765: INFO: Wait up to 5m0s for pod "var-expansion-46d8598d-526d-4cf0-8008-fb13dbfda5d8" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:46:26.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9967" for this suite.

• [SLOW TEST:46.816 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":8,"skipped":144,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:46:26.781: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 20 12:46:41.829: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:46:41.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1508" for this suite.

• [SLOW TEST:15.073 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":9,"skipped":146,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:46:41.854: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4892
STEP: creating service affinity-clusterip in namespace services-4892
STEP: creating replication controller affinity-clusterip in namespace services-4892
I0520 12:46:41.925282      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4892, replica count: 3
I0520 12:46:44.976445      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 12:46:44.981: INFO: Creating new exec pod
May 20 12:46:47.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-4892 exec execpod-affinityz4zxl -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
May 20 12:46:48.190: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 20 12:46:48.190: INFO: stdout: ""
May 20 12:46:48.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-4892 exec execpod-affinityz4zxl -- /bin/sh -x -c nc -zv -t -w 2 10.233.58.144 80'
May 20 12:46:48.407: INFO: stderr: "+ nc -zv -t -w 2 10.233.58.144 80\nConnection to 10.233.58.144 80 port [tcp/http] succeeded!\n"
May 20 12:46:48.407: INFO: stdout: ""
May 20 12:46:48.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-4892 exec execpod-affinityz4zxl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.58.144:80/ ; done'
May 20 12:46:48.648: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.58.144:80/\n"
May 20 12:46:48.649: INFO: stdout: "\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx\naffinity-clusterip-689gx"
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Received response from host: affinity-clusterip-689gx
May 20 12:46:48.649: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4892, will wait for the garbage collector to delete the pods
May 20 12:46:48.720: INFO: Deleting ReplicationController affinity-clusterip took: 10.222273ms
May 20 12:46:48.821: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.617373ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:47:03.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4892" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:21.779 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":10,"skipped":147,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:47:03.633: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 12:47:04.324: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 12:47:06.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111624, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111624, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111624, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111624, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 12:47:09.349: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:47:09.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6283" for this suite.
STEP: Destroying namespace "webhook-6283-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.775 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":11,"skipped":156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:47:09.409: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0520 12:47:19.489353      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 20 12:48:21.508: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:48:21.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1946" for this suite.

• [SLOW TEST:72.107 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":12,"skipped":191,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:48:21.519: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 20 12:48:24.117: INFO: Successfully updated pod "labelsupdatee8b0321e-6981-4dab-ba11-f5dbe8b56887"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:48:26.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1911" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":209,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:48:26.149: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 20 12:48:28.215: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:48:28.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1864" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":14,"skipped":213,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:48:28.241: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
May 20 12:48:28.279: INFO: Waiting up to 5m0s for pod "client-containers-a28610cc-7631-48ef-9c85-90677bb7ba92" in namespace "containers-2227" to be "Succeeded or Failed"
May 20 12:48:28.283: INFO: Pod "client-containers-a28610cc-7631-48ef-9c85-90677bb7ba92": Phase="Pending", Reason="", readiness=false. Elapsed: 3.330196ms
May 20 12:48:30.286: INFO: Pod "client-containers-a28610cc-7631-48ef-9c85-90677bb7ba92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006613108s
STEP: Saw pod success
May 20 12:48:30.286: INFO: Pod "client-containers-a28610cc-7631-48ef-9c85-90677bb7ba92" satisfied condition "Succeeded or Failed"
May 20 12:48:30.289: INFO: Trying to get logs from node k8s-3 pod client-containers-a28610cc-7631-48ef-9c85-90677bb7ba92 container test-container: <nil>
STEP: delete the pod
May 20 12:48:30.307: INFO: Waiting for pod client-containers-a28610cc-7631-48ef-9c85-90677bb7ba92 to disappear
May 20 12:48:30.309: INFO: Pod client-containers-a28610cc-7631-48ef-9c85-90677bb7ba92 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:48:30.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2227" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":15,"skipped":226,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:48:30.318: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 12:48:30.367: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 20 12:48:30.375: INFO: Number of nodes with available pods: 0
May 20 12:48:30.375: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 20 12:48:30.408: INFO: Number of nodes with available pods: 0
May 20 12:48:30.408: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:31.412: INFO: Number of nodes with available pods: 0
May 20 12:48:31.412: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:32.412: INFO: Number of nodes with available pods: 0
May 20 12:48:32.412: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:33.464: INFO: Number of nodes with available pods: 1
May 20 12:48:33.464: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 20 12:48:33.487: INFO: Number of nodes with available pods: 1
May 20 12:48:33.487: INFO: Number of running nodes: 0, number of available pods: 1
May 20 12:48:34.490: INFO: Number of nodes with available pods: 0
May 20 12:48:34.490: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 20 12:48:34.500: INFO: Number of nodes with available pods: 0
May 20 12:48:34.500: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:35.504: INFO: Number of nodes with available pods: 0
May 20 12:48:35.504: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:36.504: INFO: Number of nodes with available pods: 0
May 20 12:48:36.504: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:37.505: INFO: Number of nodes with available pods: 0
May 20 12:48:37.505: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:38.503: INFO: Number of nodes with available pods: 0
May 20 12:48:38.503: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:39.510: INFO: Number of nodes with available pods: 0
May 20 12:48:39.510: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:40.504: INFO: Number of nodes with available pods: 0
May 20 12:48:40.504: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:41.504: INFO: Number of nodes with available pods: 0
May 20 12:48:41.504: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:42.509: INFO: Number of nodes with available pods: 0
May 20 12:48:42.509: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:43.506: INFO: Number of nodes with available pods: 0
May 20 12:48:43.506: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:44.504: INFO: Number of nodes with available pods: 0
May 20 12:48:44.504: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:45.504: INFO: Number of nodes with available pods: 0
May 20 12:48:45.505: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:46.513: INFO: Number of nodes with available pods: 0
May 20 12:48:46.513: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:48:47.504: INFO: Number of nodes with available pods: 1
May 20 12:48:47.504: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8987, will wait for the garbage collector to delete the pods
May 20 12:48:47.566: INFO: Deleting DaemonSet.extensions daemon-set took: 4.620773ms
May 20 12:48:47.967: INFO: Terminating DaemonSet.extensions daemon-set pods took: 401.217352ms
May 20 12:48:54.871: INFO: Number of nodes with available pods: 0
May 20 12:48:54.871: INFO: Number of running nodes: 0, number of available pods: 0
May 20 12:48:54.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8987/daemonsets","resourceVersion":"3413"},"items":null}

May 20 12:48:54.879: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8987/pods","resourceVersion":"3413"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:48:54.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8987" for this suite.

• [SLOW TEST:24.599 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":16,"skipped":235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:48:54.929: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-2366bb2c-5a92-442d-a6ce-6b68745027de
STEP: Creating a pod to test consume configMaps
May 20 12:48:54.979: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3902590d-c038-4c45-b7ab-c02dfd790f59" in namespace "projected-1141" to be "Succeeded or Failed"
May 20 12:48:54.983: INFO: Pod "pod-projected-configmaps-3902590d-c038-4c45-b7ab-c02dfd790f59": Phase="Pending", Reason="", readiness=false. Elapsed: 3.669746ms
May 20 12:48:56.987: INFO: Pod "pod-projected-configmaps-3902590d-c038-4c45-b7ab-c02dfd790f59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007338069s
May 20 12:48:58.992: INFO: Pod "pod-projected-configmaps-3902590d-c038-4c45-b7ab-c02dfd790f59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012439111s
STEP: Saw pod success
May 20 12:48:58.992: INFO: Pod "pod-projected-configmaps-3902590d-c038-4c45-b7ab-c02dfd790f59" satisfied condition "Succeeded or Failed"
May 20 12:48:58.995: INFO: Trying to get logs from node k8s-3 pod pod-projected-configmaps-3902590d-c038-4c45-b7ab-c02dfd790f59 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 20 12:48:59.020: INFO: Waiting for pod pod-projected-configmaps-3902590d-c038-4c45-b7ab-c02dfd790f59 to disappear
May 20 12:48:59.026: INFO: Pod pod-projected-configmaps-3902590d-c038-4c45-b7ab-c02dfd790f59 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:48:59.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1141" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":276,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:48:59.258: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0520 12:49:09.647737      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 20 12:50:11.688: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 20 12:50:11.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbcgc" in namespace "gc-1548"
May 20 12:50:11.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmpdl" in namespace "gc-1548"
May 20 12:50:11.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlplx" in namespace "gc-1548"
May 20 12:50:11.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-jl8n9" in namespace "gc-1548"
May 20 12:50:11.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-lltmr" in namespace "gc-1548"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:50:11.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1548" for this suite.

• [SLOW TEST:72.592 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":18,"skipped":300,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:50:11.851: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 20 12:50:12.494: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 20 12:50:14.514: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111812, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111812, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111812, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111812, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 12:50:17.534: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 12:50:17.538: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:50:18.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4668" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.999 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":19,"skipped":317,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:50:18.851: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5862.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5862.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5862.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5862.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5862.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5862.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 20 12:50:38.933: INFO: DNS probes using dns-5862/dns-test-0dec6513-5dc8-442c-91dd-e75b623debc3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:50:39.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5862" for this suite.

• [SLOW TEST:20.169 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":20,"skipped":319,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:50:39.021: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 12:50:39.053: INFO: Creating deployment "webserver-deployment"
May 20 12:50:39.057: INFO: Waiting for observed generation 1
May 20 12:50:41.068: INFO: Waiting for all required pods to come up
May 20 12:50:41.080: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 20 12:51:01.096: INFO: Waiting for deployment "webserver-deployment" to complete
May 20 12:51:01.101: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 20 12:51:01.109: INFO: Updating deployment webserver-deployment
May 20 12:51:01.109: INFO: Waiting for observed generation 2
May 20 12:51:03.116: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 20 12:51:03.119: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 20 12:51:03.121: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 20 12:51:03.138: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 20 12:51:03.138: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 20 12:51:03.141: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 20 12:51:03.146: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 20 12:51:03.146: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 20 12:51:03.154: INFO: Updating deployment webserver-deployment
May 20 12:51:03.155: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 20 12:51:03.200: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 20 12:51:05.228: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 20 12:51:05.254: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8903 /apis/apps/v1/namespaces/deployment-8903/deployments/webserver-deployment c90f316a-8668-4a96-9c74-9d692202fe65 4437 3 2021-05-20 12:50:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-20 12:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0019eb7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-20 12:51:03 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-20 12:51:03 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 20 12:51:05.260: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-8903 /apis/apps/v1/namespaces/deployment-8903/replicasets/webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 4420 3 2021-05-20 12:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c90f316a-8668-4a96-9c74-9d692202fe65 0xc0019ebc77 0xc0019ebc78}] []  [{kube-controller-manager Update apps/v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c90f316a-8668-4a96-9c74-9d692202fe65\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0019ebcf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 20 12:51:05.260: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 20 12:51:05.261: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-8903 /apis/apps/v1/namespaces/deployment-8903/replicasets/webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 4429 3 2021-05-20 12:50:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c90f316a-8668-4a96-9c74-9d692202fe65 0xc0019ebd87 0xc0019ebd88}] []  [{kube-controller-manager Update apps/v1 2021-05-20 12:50:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c90f316a-8668-4a96-9c74-9d692202fe65\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0019ebe18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 20 12:51:05.283: INFO: Pod "webserver-deployment-795d758f88-564mn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-564mn webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-564mn 0ed42b31-e4e6-43ae-84c0-e770a8ada74d 4443 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc000959830 0xc000959831}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.283: INFO: Pod "webserver-deployment-795d758f88-575fz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-575fz webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-575fz 2c60b7e5-4171-4edf-bfde-ef9d1b1078e4 4352 0 2021-05-20 12:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc0009599d7 0xc0009599d8}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:,StartTime:2021-05-20 12:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.283: INFO: Pod "webserver-deployment-795d758f88-5rtvw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5rtvw webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-5rtvw 473ffdd4-5168-494a-ba93-f5c80c0e37be 4427 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc000959b87 0xc000959b88}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.283: INFO: Pod "webserver-deployment-795d758f88-br2hm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-br2hm webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-br2hm 092803cc-d453-41c5-b968-5856e63bd15d 4334 0 2021-05-20 12:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc000959d47 0xc000959d48}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:,StartTime:2021-05-20 12:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.284: INFO: Pod "webserver-deployment-795d758f88-hs6n5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hs6n5 webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-hs6n5 79658f65-77bd-4007-8305-807acb531da6 4364 0 2021-05-20 12:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc000959ef7 0xc000959ef8}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:,StartTime:2021-05-20 12:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.284: INFO: Pod "webserver-deployment-795d758f88-lwf2t" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lwf2t webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-lwf2t b6e58ec9-4f1c-4512-baf4-38ddfcf426c4 4470 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc0032800b7 0xc0032800b8}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.284: INFO: Pod "webserver-deployment-795d758f88-p5n6l" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-p5n6l webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-p5n6l b0cb0168-53dd-4b4d-9e93-314aa5ba5c00 4477 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc003280267 0xc003280268}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:,StartTime:2021-05-20 12:51:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.285: INFO: Pod "webserver-deployment-795d758f88-qcwhp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qcwhp webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-qcwhp c2e9a3b4-fd68-4039-9785-82d8a2a594c0 4493 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc003280427 0xc003280428}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:,StartTime:2021-05-20 12:51:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.285: INFO: Pod "webserver-deployment-795d758f88-qj8bg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qj8bg webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-qj8bg 45afd683-d0e8-4702-8299-e04bbeb1afe9 4405 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc0032805d7 0xc0032805d8}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.285: INFO: Pod "webserver-deployment-795d758f88-sqznt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sqznt webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-sqznt aa07e420-22d2-4860-a41e-12742f13d824 4490 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc003280787 0xc003280788}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:,StartTime:2021-05-20 12:51:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.285: INFO: Pod "webserver-deployment-795d758f88-vb9sq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vb9sq webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-vb9sq 929ad6f7-7e36-4f9f-bed2-867d1ecd784c 4497 0 2021-05-20 12:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc003280937 0xc003280938}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:10.233.66.34,StartTime:2021-05-20 12:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.285: INFO: Pod "webserver-deployment-795d758f88-x62cr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-x62cr webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-x62cr 61ef47be-e2e3-4b44-b1cb-77dcc81da9a6 4365 0 2021-05-20 12:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc003280b10 0xc003280b11}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:,StartTime:2021-05-20 12:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.285: INFO: Pod "webserver-deployment-795d758f88-xfwgk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xfwgk webserver-deployment-795d758f88- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-795d758f88-xfwgk 077e09da-dc42-4476-88f8-b78642febe66 4458 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 47624b15-00f2-44c6-b77b-9ddfd933acb0 0xc003280cb7 0xc003280cb8}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47624b15-00f2-44c6-b77b-9ddfd933acb0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.285: INFO: Pod "webserver-deployment-dd94f59b7-2dvf2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2dvf2 webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-2dvf2 261aae6b-3d1e-4296-81e3-bdb88a682faf 4478 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003280e60 0xc003280e61}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.286: INFO: Pod "webserver-deployment-dd94f59b7-4x4w4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4x4w4 webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-4x4w4 a61128c0-c2ad-49bb-aa2d-e80f971a0003 4394 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003280fe0 0xc003280fe1}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.286: INFO: Pod "webserver-deployment-dd94f59b7-6hst2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6hst2 webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-6hst2 e3f31f95-0dc2-4fc0-ac36-5d68e9b2e782 4485 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003281180 0xc003281181}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:,StartTime:2021-05-20 12:51:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.286: INFO: Pod "webserver-deployment-dd94f59b7-7knq8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7knq8 webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-7knq8 334185b7-c073-4d0b-9a1b-a39c58305830 4273 0 2021-05-20 12:50:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003281310 0xc003281311}] []  [{kube-controller-manager Update v1 2021-05-20 12:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:50:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:10.233.64.10,StartTime:2021-05-20 12:50:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 12:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5d23fad9e36df8359e6211b1ac05ff2af36fa928f3d58f6d2522156f747a9f1b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.286: INFO: Pod "webserver-deployment-dd94f59b7-8nbvd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8nbvd webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-8nbvd d3c5572f-ce66-40af-b286-68ca5ebac194 4310 0 2021-05-20 12:50:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc0032814d0 0xc0032814d1}] []  [{kube-controller-manager Update v1 2021-05-20 12:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:10.233.64.12,StartTime:2021-05-20 12:50:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 12:51:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://63aded1d56732e5a39f381d1bd1a55d6cf86250fee308362b753cc9650c21e03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.287: INFO: Pod "webserver-deployment-dd94f59b7-944vh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-944vh webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-944vh d42958b2-55f7-4598-84eb-15281b844ac4 4467 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003281670 0xc003281671}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.287: INFO: Pod "webserver-deployment-dd94f59b7-9bwhc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9bwhc webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-9bwhc fe7edd33-1faf-4e8a-a839-e80b3e84c07c 4414 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc0032817f0 0xc0032817f1}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.287: INFO: Pod "webserver-deployment-dd94f59b7-9s8k9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9s8k9 webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-9s8k9 94db744b-3142-4f49-bd6f-4724c397a494 4499 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003281970 0xc003281971}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:,StartTime:2021-05-20 12:51:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.301: INFO: Pod "webserver-deployment-dd94f59b7-br4n5" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-br4n5 webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-br4n5 dcd655f3-225e-4f5e-a9a0-17a6139e4358 4495 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003281af0 0xc003281af1}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:,StartTime:2021-05-20 12:51:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.312: INFO: Pod "webserver-deployment-dd94f59b7-hthvv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hthvv webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-hthvv e74e11da-69c4-4714-baa7-b2b8043d3d95 4188 0 2021-05-20 12:50:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003281c70 0xc003281c71}] []  [{kube-controller-manager Update v1 2021-05-20 12:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:50:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:10.233.66.30,StartTime:2021-05-20 12:50:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 12:50:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://edc2926336f40833bb514fb56b49a6fbc11ea902707ea3a31737614d3319d2a0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.313: INFO: Pod "webserver-deployment-dd94f59b7-lpcvd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lpcvd webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-lpcvd 75c55c84-3644-4940-ae24-82e4ede089f5 4462 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003281e10 0xc003281e11}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.313: INFO: Pod "webserver-deployment-dd94f59b7-np4cs" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-np4cs webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-np4cs 6ac604a7-3d91-4b3a-9a9d-6a37971a8208 4270 0 2021-05-20 12:50:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003281f90 0xc003281f91}] []  [{kube-controller-manager Update v1 2021-05-20 12:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:50:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:10.233.65.13,StartTime:2021-05-20 12:50:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 12:50:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://edd315ae69f486303e2db83fd269020ef45f5bd9158c374160896f8855f1ee75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.313: INFO: Pod "webserver-deployment-dd94f59b7-nsgvm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nsgvm webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-nsgvm eb3635a8-fa2e-4278-bafb-45d0621d9e73 4472 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003310140 0xc003310141}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.313: INFO: Pod "webserver-deployment-dd94f59b7-nwvvd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nwvvd webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-nwvvd 103f54b8-a3b6-4162-b8d6-cdfe3633d835 4287 0 2021-05-20 12:50:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003310420 0xc003310421}] []  [{kube-controller-manager Update v1 2021-05-20 12:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:10.233.65.14,StartTime:2021-05-20 12:50:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 12:50:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d56c22a88ce5c8f897ec24cdb528e09f9ee64279e8890cca76aa3c38e751ac7c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.314: INFO: Pod "webserver-deployment-dd94f59b7-phsh9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-phsh9 webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-phsh9 12c2873e-b304-4d06-bdd7-e1f9e7671d76 4196 0 2021-05-20 12:50:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003310b80 0xc003310b81}] []  [{kube-controller-manager Update v1 2021-05-20 12:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:50:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:10.233.66.31,StartTime:2021-05-20 12:50:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 12:50:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2a78bf765e610e1b6cb808a780106185f813253579ae0800b97001ef351a4c04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.314: INFO: Pod "webserver-deployment-dd94f59b7-r8l2t" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-r8l2t webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-r8l2t 96affaf4-e03c-4500-9c9f-4cd674aa6ca3 4422 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003310d20 0xc003310d21}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.314: INFO: Pod "webserver-deployment-dd94f59b7-vj7r2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vj7r2 webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-vj7r2 9b850287-1080-484c-a17e-b36cfadafee5 4489 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003310ea0 0xc003310ea1}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.315: INFO: Pod "webserver-deployment-dd94f59b7-w9lfj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w9lfj webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-w9lfj d17a6494-13fd-472b-b279-3997b61f64d2 4307 0 2021-05-20 12:50:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003311020 0xc003311021}] []  [{kube-controller-manager Update v1 2021-05-20 12:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:10.233.65.15,StartTime:2021-05-20 12:50:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 12:51:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c6a83d8770ec39bcd8458399490e9decde4d06300d77b4a44bbbbb3f9f3d148d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.315: INFO: Pod "webserver-deployment-dd94f59b7-wnvdz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wnvdz webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-wnvdz 07e284f8-5d78-42af-bad3-8747573fac08 4459 0 2021-05-20 12:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc0033111c0 0xc0033111c1}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:,StartTime:2021-05-20 12:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:51:05.315: INFO: Pod "webserver-deployment-dd94f59b7-zwd2t" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zwd2t webserver-deployment-dd94f59b7- deployment-8903 /api/v1/namespaces/deployment-8903/pods/webserver-deployment-dd94f59b7-zwd2t 8316266e-b0de-407c-bfc5-1477396c7f1a 4291 0 2021-05-20 12:50:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 27a37043-30ec-40c7-9655-04b321d5dd41 0xc003311340 0xc003311341}] []  [{kube-controller-manager Update v1 2021-05-20 12:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a37043-30ec-40c7-9655-04b321d5dd41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmzw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmzw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmzw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:50:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.101,PodIP:10.233.64.11,StartTime:2021-05-20 12:50:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 12:50:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://768ea85b08ad91a95ede4a47cff43501e127f4c493bf06e4a65664f9d1464f2b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:51:05.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8903" for this suite.

• [SLOW TEST:26.329 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":21,"skipped":331,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:51:05.351: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-b9aeca2d-538e-4312-ae5c-9166702e7ef0
STEP: Creating a pod to test consume configMaps
May 20 12:51:05.512: INFO: Waiting up to 5m0s for pod "pod-configmaps-4219d53a-eafa-408d-9172-59fee8c3692d" in namespace "configmap-354" to be "Succeeded or Failed"
May 20 12:51:05.519: INFO: Pod "pod-configmaps-4219d53a-eafa-408d-9172-59fee8c3692d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.225072ms
May 20 12:51:07.524: INFO: Pod "pod-configmaps-4219d53a-eafa-408d-9172-59fee8c3692d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011721982s
May 20 12:51:09.529: INFO: Pod "pod-configmaps-4219d53a-eafa-408d-9172-59fee8c3692d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017062669s
STEP: Saw pod success
May 20 12:51:09.530: INFO: Pod "pod-configmaps-4219d53a-eafa-408d-9172-59fee8c3692d" satisfied condition "Succeeded or Failed"
May 20 12:51:09.533: INFO: Trying to get logs from node k8s-2 pod pod-configmaps-4219d53a-eafa-408d-9172-59fee8c3692d container configmap-volume-test: <nil>
STEP: delete the pod
May 20 12:51:09.829: INFO: Waiting for pod pod-configmaps-4219d53a-eafa-408d-9172-59fee8c3692d to disappear
May 20 12:51:09.832: INFO: Pod pod-configmaps-4219d53a-eafa-408d-9172-59fee8c3692d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:51:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-354" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":22,"skipped":341,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:51:09.846: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:51:21.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6556" for this suite.

• [SLOW TEST:11.221 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":23,"skipped":341,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:51:21.067: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 20 12:51:21.530: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 20 12:51:23.539: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111881, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111881, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111881, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111881, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 12:51:26.556: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 12:51:26.559: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:51:27.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9993" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.876 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":24,"skipped":350,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:51:27.944: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 20 12:51:28.023: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 20 12:51:28.035: INFO: Waiting for terminating namespaces to be deleted...
May 20 12:51:28.047: INFO: 
Logging pods the apiserver thinks is on node k8s-1 before test
May 20 12:51:28.063: INFO: coredns-7677f9bb54-24hll from kube-system started at 2021-05-20 12:39:45 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.064: INFO: 	Container coredns ready: true, restart count 0
May 20 12:51:28.064: INFO: dns-autoscaler-5b7b5c9b6f-xgl8n from kube-system started at 2021-05-20 12:39:49 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.064: INFO: 	Container autoscaler ready: true, restart count 0
May 20 12:51:28.064: INFO: kube-apiserver-k8s-1 from kube-system started at 2021-05-20 12:37:59 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.064: INFO: 	Container kube-apiserver ready: true, restart count 0
May 20 12:51:28.064: INFO: kube-controller-manager-k8s-1 from kube-system started at 2021-05-20 12:38:14 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.064: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 20 12:51:28.064: INFO: kube-flannel-xshzr from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.064: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 12:51:28.064: INFO: kube-proxy-r8x5s from kube-system started at 2021-05-20 12:39:03 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.064: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 12:51:28.064: INFO: kube-scheduler-k8s-1 from kube-system started at 2021-05-20 12:38:14 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.064: INFO: 	Container kube-scheduler ready: true, restart count 0
May 20 12:51:28.064: INFO: nodelocaldns-j9jjn from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.064: INFO: 	Container node-cache ready: true, restart count 0
May 20 12:51:28.064: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-nkrjs from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 12:51:28.064: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 12:51:28.064: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 12:51:28.064: INFO: 
Logging pods the apiserver thinks is on node k8s-2 before test
May 20 12:51:28.077: INFO: coredns-7677f9bb54-m6mpv from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.077: INFO: 	Container coredns ready: true, restart count 0
May 20 12:51:28.077: INFO: kube-flannel-ggvw9 from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.077: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 12:51:28.077: INFO: kube-proxy-hdfgh from kube-system started at 2021-05-20 12:39:03 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.077: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 12:51:28.077: INFO: nginx-proxy-k8s-2 from kube-system started at 2021-05-20 12:38:55 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.077: INFO: 	Container nginx-proxy ready: true, restart count 0
May 20 12:51:28.077: INFO: nodelocaldns-nl6xd from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.077: INFO: 	Container node-cache ready: true, restart count 0
May 20 12:51:28.077: INFO: sonobuoy-e2e-job-9cce46af7e3b479e from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 12:51:28.077: INFO: 	Container e2e ready: true, restart count 0
May 20 12:51:28.077: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 12:51:28.077: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-v8rq9 from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 12:51:28.077: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 12:51:28.077: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 12:51:28.077: INFO: 
Logging pods the apiserver thinks is on node k8s-3 before test
May 20 12:51:28.089: INFO: kube-flannel-hx4xl from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.091: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 12:51:28.091: INFO: kube-proxy-whdwz from kube-system started at 2021-05-20 12:39:04 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.091: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 12:51:28.091: INFO: nginx-proxy-k8s-3 from kube-system started at 2021-05-20 12:38:56 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.091: INFO: 	Container nginx-proxy ready: true, restart count 0
May 20 12:51:28.091: INFO: nodelocaldns-pzwcd from kube-system started at 2021-05-20 12:39:51 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.091: INFO: 	Container node-cache ready: true, restart count 0
May 20 12:51:28.092: INFO: sonobuoy from sonobuoy started at 2021-05-20 12:41:44 +0000 UTC (1 container statuses recorded)
May 20 12:51:28.092: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 20 12:51:28.092: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-lvpcw from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 12:51:28.092: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 12:51:28.092: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1680c761c2868212], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1680c761ca31b2a1], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:51:29.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-175" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":25,"skipped":354,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:51:29.164: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2086
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2086
I0520 12:51:29.255383      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2086, replica count: 2
I0520 12:51:32.306933      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 12:51:32.307: INFO: Creating new exec pod
May 20 12:51:37.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-2086 exec execpodl5kb6 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 20 12:51:37.691: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 20 12:51:37.691: INFO: stdout: ""
May 20 12:51:37.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-2086 exec execpodl5kb6 -- /bin/sh -x -c nc -zv -t -w 2 10.233.13.169 80'
May 20 12:51:38.008: INFO: stderr: "+ nc -zv -t -w 2 10.233.13.169 80\nConnection to 10.233.13.169 80 port [tcp/http] succeeded!\n"
May 20 12:51:38.008: INFO: stdout: ""
May 20 12:51:38.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-2086 exec execpodl5kb6 -- /bin/sh -x -c nc -zv -t -w 2 172.18.8.101 31916'
May 20 12:51:38.262: INFO: stderr: "+ nc -zv -t -w 2 172.18.8.101 31916\nConnection to 172.18.8.101 31916 port [tcp/31916] succeeded!\n"
May 20 12:51:38.262: INFO: stdout: ""
May 20 12:51:38.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-2086 exec execpodl5kb6 -- /bin/sh -x -c nc -zv -t -w 2 172.18.8.102 31916'
May 20 12:51:38.536: INFO: stderr: "+ nc -zv -t -w 2 172.18.8.102 31916\nConnection to 172.18.8.102 31916 port [tcp/31916] succeeded!\n"
May 20 12:51:38.536: INFO: stdout: ""
May 20 12:51:38.536: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:51:38.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2086" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.479 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":26,"skipped":367,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:51:38.644: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 12:51:39.081: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 20 12:51:41.099: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111899, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111899, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111899, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111899, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 12:51:44.120: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
May 20 12:51:44.200: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:51:44.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9325" for this suite.
STEP: Destroying namespace "webhook-9325-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.862 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":27,"skipped":384,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:51:44.511: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 12:51:44.562: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:51:45.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2049" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":28,"skipped":386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:51:45.165: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-522e02f6-0f35-4117-ba6d-5b6e58549114
STEP: Creating a pod to test consume secrets
May 20 12:51:45.199: INFO: Waiting up to 5m0s for pod "pod-secrets-f66228be-0c6d-4b96-af75-34fba0e203b1" in namespace "secrets-498" to be "Succeeded or Failed"
May 20 12:51:45.206: INFO: Pod "pod-secrets-f66228be-0c6d-4b96-af75-34fba0e203b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.328922ms
May 20 12:51:47.211: INFO: Pod "pod-secrets-f66228be-0c6d-4b96-af75-34fba0e203b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011950719s
May 20 12:51:49.216: INFO: Pod "pod-secrets-f66228be-0c6d-4b96-af75-34fba0e203b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016374546s
STEP: Saw pod success
May 20 12:51:49.216: INFO: Pod "pod-secrets-f66228be-0c6d-4b96-af75-34fba0e203b1" satisfied condition "Succeeded or Failed"
May 20 12:51:49.218: INFO: Trying to get logs from node k8s-3 pod pod-secrets-f66228be-0c6d-4b96-af75-34fba0e203b1 container secret-volume-test: <nil>
STEP: delete the pod
May 20 12:51:49.241: INFO: Waiting for pod pod-secrets-f66228be-0c6d-4b96-af75-34fba0e203b1 to disappear
May 20 12:51:49.243: INFO: Pod pod-secrets-f66228be-0c6d-4b96-af75-34fba0e203b1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:51:49.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-498" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":29,"skipped":413,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:51:49.251: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 12:51:49.277: INFO: Creating deployment "test-recreate-deployment"
May 20 12:51:49.280: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 20 12:51:49.290: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 20 12:51:51.297: INFO: Waiting deployment "test-recreate-deployment" to complete
May 20 12:51:51.299: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111909, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111909, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111909, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757111909, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 12:51:53.321: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 20 12:51:53.329: INFO: Updating deployment test-recreate-deployment
May 20 12:51:53.329: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 20 12:51:53.464: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3831 /apis/apps/v1/namespaces/deployment-3831/deployments/test-recreate-deployment 4246e153-2f12-49bd-aa4b-0be2964d1b7e 5275 2 2021-05-20 12:51:49 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-20 12:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-20 12:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dc9ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-20 12:51:53 +0000 UTC,LastTransitionTime:2021-05-20 12:51:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-05-20 12:51:53 +0000 UTC,LastTransitionTime:2021-05-20 12:51:49 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 20 12:51:53.480: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-3831 /apis/apps/v1/namespaces/deployment-3831/replicasets/test-recreate-deployment-f79dd4667 d4e01386-7028-457e-910b-57de4c2a3b2c 5272 1 2021-05-20 12:51:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4246e153-2f12-49bd-aa4b-0be2964d1b7e 0xc003dc9fc0 0xc003dc9fc1}] []  [{kube-controller-manager Update apps/v1 2021-05-20 12:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4246e153-2f12-49bd-aa4b-0be2964d1b7e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f20038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 20 12:51:53.480: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 20 12:51:53.481: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-3831 /apis/apps/v1/namespaces/deployment-3831/replicasets/test-recreate-deployment-c96cf48f e53b9ebc-57dc-495f-81c8-1b568e0943aa 5264 2 2021-05-20 12:51:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4246e153-2f12-49bd-aa4b-0be2964d1b7e 0xc003dc9edf 0xc003dc9ef0}] []  [{kube-controller-manager Update apps/v1 2021-05-20 12:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4246e153-2f12-49bd-aa4b-0be2964d1b7e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dc9f68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 20 12:51:53.492: INFO: Pod "test-recreate-deployment-f79dd4667-pv572" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-pv572 test-recreate-deployment-f79dd4667- deployment-3831 /api/v1/namespaces/deployment-3831/pods/test-recreate-deployment-f79dd4667-pv572 ce57abf1-1608-40c7-a0bb-3afcae76e7fd 5276 0 2021-05-20 12:51:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 d4e01386-7028-457e-910b-57de4c2a3b2c 0xc003f20500 0xc003f20501}] []  [{kube-controller-manager Update v1 2021-05-20 12:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4e01386-7028-457e-910b-57de4c2a3b2c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 12:51:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fzw8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fzw8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fzw8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 12:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:,StartTime:2021-05-20 12:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:51:53.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3831" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":30,"skipped":418,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:51:53.509: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 20 12:51:53.538: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 20 12:52:03.927: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 12:52:06.717: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:52:17.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7301" for this suite.

• [SLOW TEST:24.380 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":31,"skipped":420,"failed":0}
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:52:17.891: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 20 12:52:17.925: INFO: Created pod &Pod{ObjectMeta:{dns-288  dns-288 /api/v1/namespaces/dns-288/pods/dns-288 74973da5-83f7-4f02-b7c6-95dad8f505f4 5418 0 2021-05-20 12:52:18 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-05-20 12:52:18 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-txqjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-txqjx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-txqjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 20 12:52:17.929: INFO: The status of Pod dns-288 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:52:19.933: INFO: The status of Pod dns-288 is Pending, waiting for it to be Running (with Ready = true)
May 20 12:52:21.933: INFO: The status of Pod dns-288 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 20 12:52:21.933: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-288 PodName:dns-288 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 12:52:21.933: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Verifying customized DNS server is configured on pod...
May 20 12:52:22.065: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-288 PodName:dns-288 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 12:52:22.065: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 12:52:22.173: INFO: Deleting pod dns-288...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:52:22.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-288" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":32,"skipped":420,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:52:22.200: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 12:52:22.230: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:52:26.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2331" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":33,"skipped":467,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:52:26.378: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
May 20 12:52:26.407: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:52:41.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7591" for this suite.

• [SLOW TEST:14.955 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":34,"skipped":488,"failed":0}
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:52:41.333: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 12:52:41.512: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 20 12:52:41.537: INFO: Number of nodes with available pods: 0
May 20 12:52:41.537: INFO: Node k8s-1 is running more than one daemon pod
May 20 12:52:42.545: INFO: Number of nodes with available pods: 0
May 20 12:52:42.545: INFO: Node k8s-1 is running more than one daemon pod
May 20 12:52:43.543: INFO: Number of nodes with available pods: 0
May 20 12:52:43.543: INFO: Node k8s-1 is running more than one daemon pod
May 20 12:52:44.547: INFO: Number of nodes with available pods: 3
May 20 12:52:44.547: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 20 12:52:44.569: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:44.569: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:44.569: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:45.576: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:45.577: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:45.577: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:46.579: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:46.579: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:46.579: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:47.576: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:47.576: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:47.576: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:47.576: INFO: Pod daemon-set-xmvgc is not available
May 20 12:52:48.579: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:48.579: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:48.579: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:48.579: INFO: Pod daemon-set-xmvgc is not available
May 20 12:52:49.576: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:49.576: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:49.576: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:49.576: INFO: Pod daemon-set-xmvgc is not available
May 20 12:52:50.576: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:50.576: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:50.576: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:50.576: INFO: Pod daemon-set-xmvgc is not available
May 20 12:52:51.578: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:51.578: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:51.578: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:51.578: INFO: Pod daemon-set-xmvgc is not available
May 20 12:52:52.577: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:52.577: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:52.577: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:52.577: INFO: Pod daemon-set-xmvgc is not available
May 20 12:52:53.577: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:53.577: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:53.577: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:53.577: INFO: Pod daemon-set-xmvgc is not available
May 20 12:52:54.578: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:54.578: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:54.578: INFO: Wrong image for pod: daemon-set-xmvgc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:54.578: INFO: Pod daemon-set-xmvgc is not available
May 20 12:52:55.576: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:55.577: INFO: Pod daemon-set-qzr8j is not available
May 20 12:52:55.577: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:56.579: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:56.579: INFO: Pod daemon-set-qzr8j is not available
May 20 12:52:56.579: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:57.579: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:57.579: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:58.580: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:58.580: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:58.580: INFO: Pod daemon-set-v4l5c is not available
May 20 12:52:59.576: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:59.576: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:52:59.576: INFO: Pod daemon-set-v4l5c is not available
May 20 12:53:00.578: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:00.578: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:00.578: INFO: Pod daemon-set-v4l5c is not available
May 20 12:53:01.579: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:01.580: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:01.580: INFO: Pod daemon-set-v4l5c is not available
May 20 12:53:02.579: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:02.580: INFO: Wrong image for pod: daemon-set-v4l5c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:02.580: INFO: Pod daemon-set-v4l5c is not available
May 20 12:53:03.578: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:03.578: INFO: Pod daemon-set-rm5xc is not available
May 20 12:53:04.589: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:04.589: INFO: Pod daemon-set-rm5xc is not available
May 20 12:53:05.582: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:05.582: INFO: Pod daemon-set-rm5xc is not available
May 20 12:53:06.577: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:07.576: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:07.576: INFO: Pod daemon-set-d72mn is not available
May 20 12:53:08.577: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:08.577: INFO: Pod daemon-set-d72mn is not available
May 20 12:53:09.577: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:09.577: INFO: Pod daemon-set-d72mn is not available
May 20 12:53:10.578: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:10.578: INFO: Pod daemon-set-d72mn is not available
May 20 12:53:11.579: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:11.579: INFO: Pod daemon-set-d72mn is not available
May 20 12:53:12.576: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:12.576: INFO: Pod daemon-set-d72mn is not available
May 20 12:53:13.576: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:13.576: INFO: Pod daemon-set-d72mn is not available
May 20 12:53:14.580: INFO: Wrong image for pod: daemon-set-d72mn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 20 12:53:14.580: INFO: Pod daemon-set-d72mn is not available
May 20 12:53:15.579: INFO: Pod daemon-set-rbhp2 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 20 12:53:15.590: INFO: Number of nodes with available pods: 2
May 20 12:53:15.590: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:53:16.601: INFO: Number of nodes with available pods: 2
May 20 12:53:16.601: INFO: Node k8s-3 is running more than one daemon pod
May 20 12:53:17.596: INFO: Number of nodes with available pods: 3
May 20 12:53:17.596: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-74, will wait for the garbage collector to delete the pods
May 20 12:53:17.667: INFO: Deleting DaemonSet.extensions daemon-set took: 6.734807ms
May 20 12:53:18.068: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.297243ms
May 20 12:53:24.973: INFO: Number of nodes with available pods: 0
May 20 12:53:24.973: INFO: Number of running nodes: 0, number of available pods: 0
May 20 12:53:24.976: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-74/daemonsets","resourceVersion":"5799"},"items":null}

May 20 12:53:24.978: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-74/pods","resourceVersion":"5799"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:53:24.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-74" for this suite.

• [SLOW TEST:43.662 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":35,"skipped":488,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:53:24.997: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 12:53:25.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e450536-d7bb-4971-8a9d-2b0435dabf0e" in namespace "downward-api-7169" to be "Succeeded or Failed"
May 20 12:53:25.051: INFO: Pod "downwardapi-volume-9e450536-d7bb-4971-8a9d-2b0435dabf0e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.652519ms
May 20 12:53:27.055: INFO: Pod "downwardapi-volume-9e450536-d7bb-4971-8a9d-2b0435dabf0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013340435s
May 20 12:53:29.059: INFO: Pod "downwardapi-volume-9e450536-d7bb-4971-8a9d-2b0435dabf0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017390503s
STEP: Saw pod success
May 20 12:53:29.059: INFO: Pod "downwardapi-volume-9e450536-d7bb-4971-8a9d-2b0435dabf0e" satisfied condition "Succeeded or Failed"
May 20 12:53:29.061: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-9e450536-d7bb-4971-8a9d-2b0435dabf0e container client-container: <nil>
STEP: delete the pod
May 20 12:53:29.092: INFO: Waiting for pod downwardapi-volume-9e450536-d7bb-4971-8a9d-2b0435dabf0e to disappear
May 20 12:53:29.096: INFO: Pod downwardapi-volume-9e450536-d7bb-4971-8a9d-2b0435dabf0e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:53:29.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7169" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":36,"skipped":493,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:53:29.110: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 20 12:53:29.149: INFO: Waiting up to 1m0s for all nodes to be ready
May 20 12:54:29.171: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
May 20 12:54:29.202: INFO: Created pod: pod0-sched-preemption-low-priority
May 20 12:54:29.223: INFO: Created pod: pod1-sched-preemption-medium-priority
May 20 12:54:29.241: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:54:41.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2439" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:72.209 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":37,"skipped":497,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:54:41.319: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-9b6beebd-7ca7-415e-8029-1d70484df055
STEP: Creating a pod to test consume configMaps
May 20 12:54:41.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a66a512f-a154-47ad-b074-66fb9734e643" in namespace "projected-312" to be "Succeeded or Failed"
May 20 12:54:41.364: INFO: Pod "pod-projected-configmaps-a66a512f-a154-47ad-b074-66fb9734e643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.15858ms
May 20 12:54:43.367: INFO: Pod "pod-projected-configmaps-a66a512f-a154-47ad-b074-66fb9734e643": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005039137s
STEP: Saw pod success
May 20 12:54:43.367: INFO: Pod "pod-projected-configmaps-a66a512f-a154-47ad-b074-66fb9734e643" satisfied condition "Succeeded or Failed"
May 20 12:54:43.372: INFO: Trying to get logs from node k8s-3 pod pod-projected-configmaps-a66a512f-a154-47ad-b074-66fb9734e643 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 20 12:54:43.403: INFO: Waiting for pod pod-projected-configmaps-a66a512f-a154-47ad-b074-66fb9734e643 to disappear
May 20 12:54:43.408: INFO: Pod pod-projected-configmaps-a66a512f-a154-47ad-b074-66fb9734e643 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:54:43.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-312" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":38,"skipped":515,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:54:43.424: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 20 12:54:49.508: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 20 12:54:49.511: INFO: Pod pod-with-prestop-http-hook still exists
May 20 12:54:51.512: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 20 12:54:51.515: INFO: Pod pod-with-prestop-http-hook still exists
May 20 12:54:53.511: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 20 12:54:53.515: INFO: Pod pod-with-prestop-http-hook still exists
May 20 12:54:55.512: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 20 12:54:55.517: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:54:55.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9454" for this suite.

• [SLOW TEST:12.118 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":564,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:54:55.544: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 20 12:54:55.634: INFO: Waiting up to 5m0s for pod "downward-api-55e197ff-513b-43d3-bb98-577d45985444" in namespace "downward-api-25" to be "Succeeded or Failed"
May 20 12:54:55.640: INFO: Pod "downward-api-55e197ff-513b-43d3-bb98-577d45985444": Phase="Pending", Reason="", readiness=false. Elapsed: 4.759529ms
May 20 12:54:57.644: INFO: Pod "downward-api-55e197ff-513b-43d3-bb98-577d45985444": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00892457s
May 20 12:54:59.648: INFO: Pod "downward-api-55e197ff-513b-43d3-bb98-577d45985444": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013317469s
STEP: Saw pod success
May 20 12:54:59.649: INFO: Pod "downward-api-55e197ff-513b-43d3-bb98-577d45985444" satisfied condition "Succeeded or Failed"
May 20 12:54:59.652: INFO: Trying to get logs from node k8s-3 pod downward-api-55e197ff-513b-43d3-bb98-577d45985444 container dapi-container: <nil>
STEP: delete the pod
May 20 12:54:59.671: INFO: Waiting for pod downward-api-55e197ff-513b-43d3-bb98-577d45985444 to disappear
May 20 12:54:59.683: INFO: Pod downward-api-55e197ff-513b-43d3-bb98-577d45985444 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:54:59.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-25" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":40,"skipped":583,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:54:59.691: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
May 20 12:54:59.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-9231 cluster-info'
May 20 12:54:59.828: INFO: stderr: ""
May 20 12:54:59.828: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:54:59.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9231" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":41,"skipped":598,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:54:59.838: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-8548
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8548
STEP: Deleting pre-stop pod
May 20 12:55:12.940: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:55:12.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8548" for this suite.

• [SLOW TEST:13.127 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":42,"skipped":618,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:55:12.966: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-ba7cf539-253a-41e8-b3a2-43b748e61783 in namespace container-probe-9320
May 20 12:55:15.006: INFO: Started pod test-webserver-ba7cf539-253a-41e8-b3a2-43b748e61783 in namespace container-probe-9320
STEP: checking the pod's current state and verifying that restartCount is present
May 20 12:55:15.008: INFO: Initial restart count of pod test-webserver-ba7cf539-253a-41e8-b3a2-43b748e61783 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:59:15.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9320" for this suite.

• [SLOW TEST:242.785 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":43,"skipped":634,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:59:15.752: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-b48971db-c9d4-4ca7-945e-f0e6e10f3a4e
STEP: Creating a pod to test consume configMaps
May 20 12:59:15.821: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3946aee0-ff6f-4a89-818a-c46b9a9618e6" in namespace "projected-4252" to be "Succeeded or Failed"
May 20 12:59:15.826: INFO: Pod "pod-projected-configmaps-3946aee0-ff6f-4a89-818a-c46b9a9618e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.634175ms
May 20 12:59:17.840: INFO: Pod "pod-projected-configmaps-3946aee0-ff6f-4a89-818a-c46b9a9618e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019341909s
May 20 12:59:19.844: INFO: Pod "pod-projected-configmaps-3946aee0-ff6f-4a89-818a-c46b9a9618e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023396359s
STEP: Saw pod success
May 20 12:59:19.844: INFO: Pod "pod-projected-configmaps-3946aee0-ff6f-4a89-818a-c46b9a9618e6" satisfied condition "Succeeded or Failed"
May 20 12:59:19.847: INFO: Trying to get logs from node k8s-3 pod pod-projected-configmaps-3946aee0-ff6f-4a89-818a-c46b9a9618e6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 20 12:59:19.891: INFO: Waiting for pod pod-projected-configmaps-3946aee0-ff6f-4a89-818a-c46b9a9618e6 to disappear
May 20 12:59:19.895: INFO: Pod pod-projected-configmaps-3946aee0-ff6f-4a89-818a-c46b9a9618e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:59:19.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4252" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":44,"skipped":637,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:59:19.913: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
May 20 12:59:19.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 create -f -'
May 20 12:59:20.802: INFO: stderr: ""
May 20 12:59:20.802: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 20 12:59:20.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 20 12:59:20.940: INFO: stderr: ""
May 20 12:59:20.940: INFO: stdout: "update-demo-nautilus-hftwp update-demo-nautilus-mcf7z "
May 20 12:59:20.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get pods update-demo-nautilus-hftwp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 12:59:21.046: INFO: stderr: ""
May 20 12:59:21.046: INFO: stdout: ""
May 20 12:59:21.046: INFO: update-demo-nautilus-hftwp is created but not running
May 20 12:59:26.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 20 12:59:26.148: INFO: stderr: ""
May 20 12:59:26.148: INFO: stdout: "update-demo-nautilus-hftwp update-demo-nautilus-mcf7z "
May 20 12:59:26.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get pods update-demo-nautilus-hftwp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 12:59:26.228: INFO: stderr: ""
May 20 12:59:26.228: INFO: stdout: "true"
May 20 12:59:26.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get pods update-demo-nautilus-hftwp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 20 12:59:26.305: INFO: stderr: ""
May 20 12:59:26.305: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 20 12:59:26.305: INFO: validating pod update-demo-nautilus-hftwp
May 20 12:59:26.309: INFO: got data: {
  "image": "nautilus.jpg"
}

May 20 12:59:26.310: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 20 12:59:26.310: INFO: update-demo-nautilus-hftwp is verified up and running
May 20 12:59:26.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get pods update-demo-nautilus-mcf7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 12:59:26.390: INFO: stderr: ""
May 20 12:59:26.390: INFO: stdout: "true"
May 20 12:59:26.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get pods update-demo-nautilus-mcf7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 20 12:59:26.462: INFO: stderr: ""
May 20 12:59:26.463: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 20 12:59:26.463: INFO: validating pod update-demo-nautilus-mcf7z
May 20 12:59:26.467: INFO: got data: {
  "image": "nautilus.jpg"
}

May 20 12:59:26.467: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 20 12:59:26.467: INFO: update-demo-nautilus-mcf7z is verified up and running
STEP: using delete to clean up resources
May 20 12:59:26.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 delete --grace-period=0 --force -f -'
May 20 12:59:26.552: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 20 12:59:26.552: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 20 12:59:26.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get rc,svc -l name=update-demo --no-headers'
May 20 12:59:26.642: INFO: stderr: "No resources found in kubectl-6107 namespace.\n"
May 20 12:59:26.642: INFO: stdout: ""
May 20 12:59:26.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 20 12:59:26.722: INFO: stderr: ""
May 20 12:59:26.722: INFO: stdout: "update-demo-nautilus-hftwp\nupdate-demo-nautilus-mcf7z\n"
May 20 12:59:27.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get rc,svc -l name=update-demo --no-headers'
May 20 12:59:27.311: INFO: stderr: "No resources found in kubectl-6107 namespace.\n"
May 20 12:59:27.311: INFO: stdout: ""
May 20 12:59:27.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6107 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 20 12:59:27.397: INFO: stderr: ""
May 20 12:59:27.397: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:59:27.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6107" for this suite.

• [SLOW TEST:7.491 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":45,"skipped":659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:59:27.405: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 20 12:59:30.467: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:59:30.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6397" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":46,"skipped":685,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:59:30.490: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:59:37.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4185" for this suite.

• [SLOW TEST:7.146 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":47,"skipped":695,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:59:37.663: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:59:53.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9409" for this suite.

• [SLOW TEST:16.266 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":48,"skipped":746,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:59:53.931: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
May 20 12:59:53.964: INFO: created test-event-1
May 20 12:59:53.968: INFO: created test-event-2
May 20 12:59:53.972: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 20 12:59:53.977: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 20 12:59:53.993: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:59:53.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-465" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":49,"skipped":755,"failed":0}

------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:59:54.003: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 20 12:59:54.088: INFO: starting watch
STEP: patching
STEP: updating
May 20 12:59:54.096: INFO: waiting for watch events with expected annotations
May 20 12:59:54.096: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 12:59:54.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-983" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":50,"skipped":755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 12:59:54.134: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:02.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6495" for this suite.

• [SLOW TEST:8.053 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":51,"skipped":790,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:02.191: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 20 13:00:02.220: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 20 13:00:02.234: INFO: Waiting for terminating namespaces to be deleted...
May 20 13:00:02.237: INFO: 
Logging pods the apiserver thinks is on node k8s-1 before test
May 20 13:00:02.241: INFO: coredns-7677f9bb54-24hll from kube-system started at 2021-05-20 12:39:45 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.241: INFO: 	Container coredns ready: true, restart count 0
May 20 13:00:02.242: INFO: dns-autoscaler-5b7b5c9b6f-xgl8n from kube-system started at 2021-05-20 12:39:49 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.242: INFO: 	Container autoscaler ready: true, restart count 0
May 20 13:00:02.242: INFO: kube-apiserver-k8s-1 from kube-system started at 2021-05-20 12:37:59 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.242: INFO: 	Container kube-apiserver ready: true, restart count 0
May 20 13:00:02.242: INFO: kube-controller-manager-k8s-1 from kube-system started at 2021-05-20 12:38:14 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.242: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 20 13:00:02.242: INFO: kube-flannel-xshzr from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.243: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 13:00:02.243: INFO: kube-proxy-r8x5s from kube-system started at 2021-05-20 12:39:03 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.243: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 13:00:02.243: INFO: kube-scheduler-k8s-1 from kube-system started at 2021-05-20 12:38:14 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.243: INFO: 	Container kube-scheduler ready: true, restart count 0
May 20 13:00:02.243: INFO: nodelocaldns-j9jjn from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.243: INFO: 	Container node-cache ready: true, restart count 0
May 20 13:00:02.244: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-nkrjs from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 13:00:02.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 13:00:02.244: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 13:00:02.244: INFO: 
Logging pods the apiserver thinks is on node k8s-2 before test
May 20 13:00:02.248: INFO: coredns-7677f9bb54-m6mpv from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.248: INFO: 	Container coredns ready: true, restart count 0
May 20 13:00:02.249: INFO: kube-flannel-ggvw9 from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.249: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 13:00:02.249: INFO: kube-proxy-hdfgh from kube-system started at 2021-05-20 12:39:03 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.249: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 13:00:02.249: INFO: nginx-proxy-k8s-2 from kube-system started at 2021-05-20 12:38:55 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.249: INFO: 	Container nginx-proxy ready: true, restart count 0
May 20 13:00:02.249: INFO: nodelocaldns-nl6xd from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.249: INFO: 	Container node-cache ready: true, restart count 0
May 20 13:00:02.250: INFO: sonobuoy-e2e-job-9cce46af7e3b479e from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 13:00:02.250: INFO: 	Container e2e ready: true, restart count 0
May 20 13:00:02.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 13:00:02.250: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-v8rq9 from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 13:00:02.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 13:00:02.250: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 13:00:02.250: INFO: 
Logging pods the apiserver thinks is on node k8s-3 before test
May 20 13:00:02.255: INFO: fail-once-local-ctjz6 from job-6495 started at 2021-05-20 12:59:58 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.255: INFO: 	Container c ready: false, restart count 1
May 20 13:00:02.255: INFO: fail-once-local-ftwpd from job-6495 started at 2021-05-20 12:59:54 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.256: INFO: 	Container c ready: false, restart count 1
May 20 13:00:02.256: INFO: fail-once-local-srlbp from job-6495 started at 2021-05-20 12:59:54 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.256: INFO: 	Container c ready: false, restart count 1
May 20 13:00:02.256: INFO: fail-once-local-vmm7z from job-6495 started at 2021-05-20 12:59:58 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.256: INFO: 	Container c ready: false, restart count 1
May 20 13:00:02.256: INFO: kube-flannel-hx4xl from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.256: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 13:00:02.257: INFO: kube-proxy-whdwz from kube-system started at 2021-05-20 12:39:04 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.257: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 13:00:02.257: INFO: nginx-proxy-k8s-3 from kube-system started at 2021-05-20 12:38:56 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.257: INFO: 	Container nginx-proxy ready: true, restart count 0
May 20 13:00:02.257: INFO: nodelocaldns-pzwcd from kube-system started at 2021-05-20 12:39:51 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.257: INFO: 	Container node-cache ready: true, restart count 0
May 20 13:00:02.257: INFO: sonobuoy from sonobuoy started at 2021-05-20 12:41:44 +0000 UTC (1 container statuses recorded)
May 20 13:00:02.257: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 20 13:00:02.258: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-lvpcw from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 13:00:02.258: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 13:00:02.258: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-332e880d-2c5f-42c0-8f3d-9d6501e1e2e0 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-332e880d-2c5f-42c0-8f3d-9d6501e1e2e0 off the node k8s-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-332e880d-2c5f-42c0-8f3d-9d6501e1e2e0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:10.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3126" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.151 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":52,"skipped":797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:10.343: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8421
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8421
I0520 13:00:10.398158      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8421, replica count: 2
May 20 13:00:13.449: INFO: Creating new exec pod
I0520 13:00:13.449615      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 13:00:18.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-8421 exec execpod8vh2d -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 20 13:00:18.674: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 20 13:00:18.675: INFO: stdout: ""
May 20 13:00:18.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-8421 exec execpod8vh2d -- /bin/sh -x -c nc -zv -t -w 2 10.233.6.181 80'
May 20 13:00:18.870: INFO: stderr: "+ nc -zv -t -w 2 10.233.6.181 80\nConnection to 10.233.6.181 80 port [tcp/http] succeeded!\n"
May 20 13:00:18.870: INFO: stdout: ""
May 20 13:00:18.870: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:18.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8421" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:8.567 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":53,"skipped":821,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:18.910: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
May 20 13:00:18.959: INFO: created test-pod-1
May 20 13:00:18.971: INFO: created test-pod-2
May 20 13:00:18.986: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:19.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1682" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":54,"skipped":823,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:19.067: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 20 13:00:19.095: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 13:00:22.057: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:35.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7246" for this suite.

• [SLOW TEST:16.077 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":55,"skipped":824,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:35.146: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
May 20 13:00:35.185: INFO: Waiting up to 5m0s for pod "pod-6df1c561-231f-49f2-815e-2441048e0a58" in namespace "emptydir-7401" to be "Succeeded or Failed"
May 20 13:00:35.188: INFO: Pod "pod-6df1c561-231f-49f2-815e-2441048e0a58": Phase="Pending", Reason="", readiness=false. Elapsed: 3.398468ms
May 20 13:00:37.211: INFO: Pod "pod-6df1c561-231f-49f2-815e-2441048e0a58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026034764s
STEP: Saw pod success
May 20 13:00:37.211: INFO: Pod "pod-6df1c561-231f-49f2-815e-2441048e0a58" satisfied condition "Succeeded or Failed"
May 20 13:00:37.269: INFO: Trying to get logs from node k8s-3 pod pod-6df1c561-231f-49f2-815e-2441048e0a58 container test-container: <nil>
STEP: delete the pod
May 20 13:00:37.383: INFO: Waiting for pod pod-6df1c561-231f-49f2-815e-2441048e0a58 to disappear
May 20 13:00:37.397: INFO: Pod pod-6df1c561-231f-49f2-815e-2441048e0a58 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:37.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7401" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":56,"skipped":860,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:37.496: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:00:37.758: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df6ef25d-5508-494f-a9c4-687344667ab5" in namespace "projected-6075" to be "Succeeded or Failed"
May 20 13:00:37.811: INFO: Pod "downwardapi-volume-df6ef25d-5508-494f-a9c4-687344667ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.582362ms
May 20 13:00:39.817: INFO: Pod "downwardapi-volume-df6ef25d-5508-494f-a9c4-687344667ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058362661s
May 20 13:00:41.829: INFO: Pod "downwardapi-volume-df6ef25d-5508-494f-a9c4-687344667ab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071206697s
STEP: Saw pod success
May 20 13:00:41.829: INFO: Pod "downwardapi-volume-df6ef25d-5508-494f-a9c4-687344667ab5" satisfied condition "Succeeded or Failed"
May 20 13:00:41.833: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-df6ef25d-5508-494f-a9c4-687344667ab5 container client-container: <nil>
STEP: delete the pod
May 20 13:00:41.849: INFO: Waiting for pod downwardapi-volume-df6ef25d-5508-494f-a9c4-687344667ab5 to disappear
May 20 13:00:41.852: INFO: Pod downwardapi-volume-df6ef25d-5508-494f-a9c4-687344667ab5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:41.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6075" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":57,"skipped":866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:41.866: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-592a56f3-b96d-49e0-a416-f97c88e0bc90
STEP: Creating a pod to test consume secrets
May 20 13:00:41.899: INFO: Waiting up to 5m0s for pod "pod-secrets-b6906408-0798-4983-8ae4-a161d8f3bd13" in namespace "secrets-6842" to be "Succeeded or Failed"
May 20 13:00:41.902: INFO: Pod "pod-secrets-b6906408-0798-4983-8ae4-a161d8f3bd13": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214656ms
May 20 13:00:43.911: INFO: Pod "pod-secrets-b6906408-0798-4983-8ae4-a161d8f3bd13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012845819s
May 20 13:00:45.917: INFO: Pod "pod-secrets-b6906408-0798-4983-8ae4-a161d8f3bd13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018047665s
STEP: Saw pod success
May 20 13:00:45.917: INFO: Pod "pod-secrets-b6906408-0798-4983-8ae4-a161d8f3bd13" satisfied condition "Succeeded or Failed"
May 20 13:00:45.921: INFO: Trying to get logs from node k8s-3 pod pod-secrets-b6906408-0798-4983-8ae4-a161d8f3bd13 container secret-volume-test: <nil>
STEP: delete the pod
May 20 13:00:45.949: INFO: Waiting for pod pod-secrets-b6906408-0798-4983-8ae4-a161d8f3bd13 to disappear
May 20 13:00:45.952: INFO: Pod pod-secrets-b6906408-0798-4983-8ae4-a161d8f3bd13 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:45.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6842" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":58,"skipped":898,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:45.963: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:46.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3198" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":59,"skipped":899,"failed":0}

------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:46.032: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-9680/configmap-test-b51e3aab-484e-4433-8572-c5910c5cc97f
STEP: Creating a pod to test consume configMaps
May 20 13:00:46.083: INFO: Waiting up to 5m0s for pod "pod-configmaps-2a2e0214-5541-45b9-9e7f-908fcbc45f82" in namespace "configmap-9680" to be "Succeeded or Failed"
May 20 13:00:46.088: INFO: Pod "pod-configmaps-2a2e0214-5541-45b9-9e7f-908fcbc45f82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.407503ms
May 20 13:00:48.108: INFO: Pod "pod-configmaps-2a2e0214-5541-45b9-9e7f-908fcbc45f82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025117372s
May 20 13:00:50.113: INFO: Pod "pod-configmaps-2a2e0214-5541-45b9-9e7f-908fcbc45f82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029568322s
STEP: Saw pod success
May 20 13:00:50.113: INFO: Pod "pod-configmaps-2a2e0214-5541-45b9-9e7f-908fcbc45f82" satisfied condition "Succeeded or Failed"
May 20 13:00:50.115: INFO: Trying to get logs from node k8s-3 pod pod-configmaps-2a2e0214-5541-45b9-9e7f-908fcbc45f82 container env-test: <nil>
STEP: delete the pod
May 20 13:00:50.142: INFO: Waiting for pod pod-configmaps-2a2e0214-5541-45b9-9e7f-908fcbc45f82 to disappear
May 20 13:00:50.145: INFO: Pod pod-configmaps-2a2e0214-5541-45b9-9e7f-908fcbc45f82 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:50.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9680" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":60,"skipped":899,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:50.154: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:00:50.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1421 create -f -'
May 20 13:00:50.396: INFO: stderr: ""
May 20 13:00:50.396: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 20 13:00:50.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1421 create -f -'
May 20 13:00:50.593: INFO: stderr: ""
May 20 13:00:50.593: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 20 13:00:51.599: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 13:00:51.599: INFO: Found 0 / 1
May 20 13:00:52.596: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 13:00:52.596: INFO: Found 0 / 1
May 20 13:00:53.597: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 13:00:53.597: INFO: Found 1 / 1
May 20 13:00:53.597: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 20 13:00:53.600: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 13:00:53.600: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 20 13:00:53.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1421 describe pod agnhost-primary-pvb57'
May 20 13:00:53.690: INFO: stderr: ""
May 20 13:00:53.690: INFO: stdout: "Name:         agnhost-primary-pvb57\nNamespace:    kubectl-1421\nPriority:     0\nNode:         k8s-3/172.18.8.103\nStart Time:   Thu, 20 May 2021 13:00:50 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.233.66.78\nIPs:\n  IP:           10.233.66.78\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://798f9eec557e3eaea35e8a3791d9e5103405e4b180fd3fb0102a277488a6351b\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 20 May 2021 13:00:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lcdlp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-lcdlp:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-lcdlp\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1421/agnhost-primary-pvb57 to k8s-3\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May 20 13:00:53.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1421 describe rc agnhost-primary'
May 20 13:00:53.785: INFO: stderr: ""
May 20 13:00:53.785: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1421\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-pvb57\n"
May 20 13:00:53.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1421 describe service agnhost-primary'
May 20 13:00:53.864: INFO: stderr: ""
May 20 13:00:53.864: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1421\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.233.63.216\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.78:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 20 13:00:53.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1421 describe node k8s-1'
May 20 13:00:53.968: INFO: stderr: ""
May 20 13:00:53.968: INFO: stdout: "Name:               k8s-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"5e:75:d8:96:30:c7\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.18.8.101\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 20 May 2021 12:37:51 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 20 May 2021 13:00:45 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 20 May 2021 12:39:30 +0000   Thu, 20 May 2021 12:39:30 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Thu, 20 May 2021 13:00:52 +0000   Thu, 20 May 2021 12:37:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 20 May 2021 13:00:52 +0000   Thu, 20 May 2021 12:37:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 20 May 2021 13:00:52 +0000   Thu, 20 May 2021 12:37:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 20 May 2021 13:00:52 +0000   Thu, 20 May 2021 12:39:34 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.18.8.101\n  Hostname:    k8s-1\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      129125532Ki\n  hugepages-2Mi:          0\n  memory:                 2040792Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nAllocatable:\n  cpu:                    1800m\n  ephemeral-storage:      119002090095\n  hugepages-2Mi:          0\n  memory:                 1414104Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nSystem Info:\n  Machine ID:                 b02a85867873495a92c003650990e28b\n  System UUID:                8B5828D6-DAD9-4692-ABD3-97CCA70A6E7E\n  Boot ID:                    b5d85f96-822b-4aba-a457-a972403f7846\n  Kernel Version:             4.15.0-128-generic\n  OS Image:                   Ubuntu 18.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.14\n  Kubelet Version:            v1.19.9\n  Kube-Proxy Version:         v1.19.9\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-7677f9bb54-24hll                                   100m (5%)     0 (0%)      70Mi (5%)        170Mi (12%)    21m\n  kube-system                 dns-autoscaler-5b7b5c9b6f-xgl8n                            20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         21m\n  kube-system                 kube-apiserver-k8s-1                                       250m (13%)    0 (0%)      0 (0%)           0 (0%)         22m\n  kube-system                 kube-controller-manager-k8s-1                              200m (11%)    0 (0%)      0 (0%)           0 (0%)         22m\n  kube-system                 kube-flannel-xshzr                                         150m (8%)     300m (16%)  64M (4%)         500M (34%)     21m\n  kube-system                 kube-proxy-r8x5s                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m\n  kube-system                 kube-scheduler-k8s-1                                       100m (5%)     0 (0%)      0 (0%)           0 (0%)         22m\n  kube-system                 nodelocaldns-j9jjn                                         100m (5%)     0 (0%)      70Mi (5%)        170Mi (12%)    21m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-nkrjs    0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests        Limits\n  --------               --------        ------\n  cpu                    920m (51%)      300m (16%)\n  memory                 216100Ki (15%)  856515840 (59%)\n  ephemeral-storage      0 (0%)          0 (0%)\n  hugepages-2Mi          0 (0%)          0 (0%)\n  scheduling.k8s.io/foo  0               0\nEvents:\n  Type    Reason                   Age                From        Message\n  ----    ------                   ----               ----        -------\n  Normal  NodeHasSufficientMemory  23m (x5 over 23m)  kubelet     Node k8s-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    23m (x5 over 23m)  kubelet     Node k8s-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     23m (x5 over 23m)  kubelet     Node k8s-1 status is now: NodeHasSufficientPID\n  Normal  Starting                 22m                kubelet     Starting kubelet.\n  Normal  NodeHasSufficientMemory  22m                kubelet     Node k8s-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    22m                kubelet     Node k8s-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     22m                kubelet     Node k8s-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  22m                kubelet     Updated Node Allocatable limit across pods\n  Normal  Starting                 22m                kubelet     Starting kubelet.\n  Normal  NodeHasSufficientMemory  22m                kubelet     Node k8s-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    22m                kubelet     Node k8s-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     22m                kubelet     Node k8s-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  22m                kubelet     Updated Node Allocatable limit across pods\n  Normal  Starting                 22m                kube-proxy  Starting kube-proxy.\n  Normal  Starting                 21m                kube-proxy  Starting kube-proxy.\n  Normal  NodeReady                21m                kubelet     Node k8s-1 status is now: NodeReady\n"
May 20 13:00:53.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1421 describe namespace kubectl-1421'
May 20 13:00:54.047: INFO: stderr: ""
May 20 13:00:54.047: INFO: stdout: "Name:         kubectl-1421\nLabels:       e2e-framework=kubectl\n              e2e-run=50d4a008-69ba-48a0-a676-f4b325cd4ec8\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:54.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1421" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":61,"skipped":936,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:54.054: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:00:54.075: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:55.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-337" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":62,"skipped":951,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:55.224: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
May 20 13:00:55.278: INFO: Waiting up to 5m0s for pod "var-expansion-2a7e8447-616b-4ce7-8afb-b2fc0888f5a8" in namespace "var-expansion-4768" to be "Succeeded or Failed"
May 20 13:00:55.295: INFO: Pod "var-expansion-2a7e8447-616b-4ce7-8afb-b2fc0888f5a8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.377067ms
May 20 13:00:57.298: INFO: Pod "var-expansion-2a7e8447-616b-4ce7-8afb-b2fc0888f5a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019682559s
STEP: Saw pod success
May 20 13:00:57.299: INFO: Pod "var-expansion-2a7e8447-616b-4ce7-8afb-b2fc0888f5a8" satisfied condition "Succeeded or Failed"
May 20 13:00:57.301: INFO: Trying to get logs from node k8s-3 pod var-expansion-2a7e8447-616b-4ce7-8afb-b2fc0888f5a8 container dapi-container: <nil>
STEP: delete the pod
May 20 13:00:57.313: INFO: Waiting for pod var-expansion-2a7e8447-616b-4ce7-8afb-b2fc0888f5a8 to disappear
May 20 13:00:57.317: INFO: Pod var-expansion-2a7e8447-616b-4ce7-8afb-b2fc0888f5a8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:00:57.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4768" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":63,"skipped":966,"failed":0}

------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:00:57.331: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:00:57.410: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ec58de63-58df-441d-834f-613dbf80bf07" in namespace "security-context-test-7669" to be "Succeeded or Failed"
May 20 13:00:57.420: INFO: Pod "alpine-nnp-false-ec58de63-58df-441d-834f-613dbf80bf07": Phase="Pending", Reason="", readiness=false. Elapsed: 9.41983ms
May 20 13:00:59.434: INFO: Pod "alpine-nnp-false-ec58de63-58df-441d-834f-613dbf80bf07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023467761s
May 20 13:01:01.445: INFO: Pod "alpine-nnp-false-ec58de63-58df-441d-834f-613dbf80bf07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034757029s
May 20 13:01:03.450: INFO: Pod "alpine-nnp-false-ec58de63-58df-441d-834f-613dbf80bf07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039816024s
May 20 13:01:03.451: INFO: Pod "alpine-nnp-false-ec58de63-58df-441d-834f-613dbf80bf07" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:01:03.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7669" for this suite.

• [SLOW TEST:6.159 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:01:03.490: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:01:03.628: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f778a521-fd04-4e03-86cc-42108d4536e1" in namespace "projected-3382" to be "Succeeded or Failed"
May 20 13:01:03.635: INFO: Pod "downwardapi-volume-f778a521-fd04-4e03-86cc-42108d4536e1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.73901ms
May 20 13:01:05.657: INFO: Pod "downwardapi-volume-f778a521-fd04-4e03-86cc-42108d4536e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029201416s
May 20 13:01:07.661: INFO: Pod "downwardapi-volume-f778a521-fd04-4e03-86cc-42108d4536e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033175312s
STEP: Saw pod success
May 20 13:01:07.661: INFO: Pod "downwardapi-volume-f778a521-fd04-4e03-86cc-42108d4536e1" satisfied condition "Succeeded or Failed"
May 20 13:01:07.664: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-f778a521-fd04-4e03-86cc-42108d4536e1 container client-container: <nil>
STEP: delete the pod
May 20 13:01:07.688: INFO: Waiting for pod downwardapi-volume-f778a521-fd04-4e03-86cc-42108d4536e1 to disappear
May 20 13:01:07.690: INFO: Pod downwardapi-volume-f778a521-fd04-4e03-86cc-42108d4536e1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:01:07.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3382" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":65,"skipped":991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:01:07.700: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 20 13:01:07.729: INFO: Waiting up to 5m0s for pod "pod-d8362e26-99b3-4f7f-9f42-3d04a7e67b09" in namespace "emptydir-25" to be "Succeeded or Failed"
May 20 13:01:07.732: INFO: Pod "pod-d8362e26-99b3-4f7f-9f42-3d04a7e67b09": Phase="Pending", Reason="", readiness=false. Elapsed: 3.45804ms
May 20 13:01:09.735: INFO: Pod "pod-d8362e26-99b3-4f7f-9f42-3d04a7e67b09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005967861s
May 20 13:01:11.738: INFO: Pod "pod-d8362e26-99b3-4f7f-9f42-3d04a7e67b09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009500296s
STEP: Saw pod success
May 20 13:01:11.738: INFO: Pod "pod-d8362e26-99b3-4f7f-9f42-3d04a7e67b09" satisfied condition "Succeeded or Failed"
May 20 13:01:11.741: INFO: Trying to get logs from node k8s-3 pod pod-d8362e26-99b3-4f7f-9f42-3d04a7e67b09 container test-container: <nil>
STEP: delete the pod
May 20 13:01:11.754: INFO: Waiting for pod pod-d8362e26-99b3-4f7f-9f42-3d04a7e67b09 to disappear
May 20 13:01:11.759: INFO: Pod pod-d8362e26-99b3-4f7f-9f42-3d04a7e67b09 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:01:11.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-25" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":1032,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:01:11.769: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:01:11.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-483b3be0-6e34-4cdd-889b-739e1c1b8bac" in namespace "projected-7830" to be "Succeeded or Failed"
May 20 13:01:11.797: INFO: Pod "downwardapi-volume-483b3be0-6e34-4cdd-889b-739e1c1b8bac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611762ms
May 20 13:01:13.801: INFO: Pod "downwardapi-volume-483b3be0-6e34-4cdd-889b-739e1c1b8bac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006924143s
May 20 13:01:15.804: INFO: Pod "downwardapi-volume-483b3be0-6e34-4cdd-889b-739e1c1b8bac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009896365s
STEP: Saw pod success
May 20 13:01:15.804: INFO: Pod "downwardapi-volume-483b3be0-6e34-4cdd-889b-739e1c1b8bac" satisfied condition "Succeeded or Failed"
May 20 13:01:15.807: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-483b3be0-6e34-4cdd-889b-739e1c1b8bac container client-container: <nil>
STEP: delete the pod
May 20 13:01:15.918: INFO: Waiting for pod downwardapi-volume-483b3be0-6e34-4cdd-889b-739e1c1b8bac to disappear
May 20 13:01:15.922: INFO: Pod downwardapi-volume-483b3be0-6e34-4cdd-889b-739e1c1b8bac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:01:15.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7830" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":67,"skipped":1051,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:01:15.930: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-aff3e3a3-e09d-4db2-af65-2874b3dbb6f8
STEP: Creating a pod to test consume configMaps
May 20 13:01:15.964: INFO: Waiting up to 5m0s for pod "pod-configmaps-06630978-47c3-44fd-a478-43cb57fd1151" in namespace "configmap-6771" to be "Succeeded or Failed"
May 20 13:01:15.967: INFO: Pod "pod-configmaps-06630978-47c3-44fd-a478-43cb57fd1151": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282965ms
May 20 13:01:17.977: INFO: Pod "pod-configmaps-06630978-47c3-44fd-a478-43cb57fd1151": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012619824s
May 20 13:01:19.980: INFO: Pod "pod-configmaps-06630978-47c3-44fd-a478-43cb57fd1151": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015775157s
STEP: Saw pod success
May 20 13:01:19.980: INFO: Pod "pod-configmaps-06630978-47c3-44fd-a478-43cb57fd1151" satisfied condition "Succeeded or Failed"
May 20 13:01:19.983: INFO: Trying to get logs from node k8s-3 pod pod-configmaps-06630978-47c3-44fd-a478-43cb57fd1151 container configmap-volume-test: <nil>
STEP: delete the pod
May 20 13:01:20.004: INFO: Waiting for pod pod-configmaps-06630978-47c3-44fd-a478-43cb57fd1151 to disappear
May 20 13:01:20.008: INFO: Pod pod-configmaps-06630978-47c3-44fd-a478-43cb57fd1151 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:01:20.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6771" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":68,"skipped":1053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:01:20.019: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-918
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-918 to expose endpoints map[]
May 20 13:01:20.088: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May 20 13:01:21.093: INFO: successfully validated that service endpoint-test2 in namespace services-918 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-918
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-918 to expose endpoints map[pod1:[80]]
May 20 13:01:24.112: INFO: successfully validated that service endpoint-test2 in namespace services-918 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-918
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-918 to expose endpoints map[pod1:[80] pod2:[80]]
May 20 13:01:26.151: INFO: successfully validated that service endpoint-test2 in namespace services-918 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-918
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-918 to expose endpoints map[pod2:[80]]
May 20 13:01:26.195: INFO: successfully validated that service endpoint-test2 in namespace services-918 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-918
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-918 to expose endpoints map[]
May 20 13:01:26.239: INFO: successfully validated that service endpoint-test2 in namespace services-918 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:01:26.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-918" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.303 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":69,"skipped":1086,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:01:26.322: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 20 13:01:26.395: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:01:26.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2954" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":70,"skipped":1100,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:01:26.453: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 20 13:01:31.065: INFO: Successfully updated pod "pod-update-4680560e-44f4-427d-a3b6-b13e70c4a7f5"
STEP: verifying the updated pod is in kubernetes
May 20 13:01:31.084: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:01:31.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1822" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":71,"skipped":1112,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:01:31.114: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 20 13:01:37.299: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 20 13:01:37.302: INFO: Pod pod-with-poststart-http-hook still exists
May 20 13:01:39.303: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 20 13:01:39.306: INFO: Pod pod-with-poststart-http-hook still exists
May 20 13:01:41.302: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 20 13:01:41.306: INFO: Pod pod-with-poststart-http-hook still exists
May 20 13:01:43.303: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 20 13:01:43.308: INFO: Pod pod-with-poststart-http-hook still exists
May 20 13:01:45.303: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 20 13:01:45.307: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:01:45.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9533" for this suite.

• [SLOW TEST:14.213 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":72,"skipped":1116,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:01:45.328: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-kfdh
STEP: Creating a pod to test atomic-volume-subpath
May 20 13:01:45.411: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kfdh" in namespace "subpath-6410" to be "Succeeded or Failed"
May 20 13:01:45.414: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.240673ms
May 20 13:01:47.417: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005801729s
May 20 13:01:49.421: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Running", Reason="", readiness=true. Elapsed: 4.009532444s
May 20 13:01:51.424: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Running", Reason="", readiness=true. Elapsed: 6.012796308s
May 20 13:01:53.429: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Running", Reason="", readiness=true. Elapsed: 8.017277799s
May 20 13:01:55.434: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Running", Reason="", readiness=true. Elapsed: 10.022629983s
May 20 13:01:57.439: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Running", Reason="", readiness=true. Elapsed: 12.027294183s
May 20 13:01:59.442: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Running", Reason="", readiness=true. Elapsed: 14.030913046s
May 20 13:02:01.454: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Running", Reason="", readiness=true. Elapsed: 16.04262037s
May 20 13:02:03.458: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Running", Reason="", readiness=true. Elapsed: 18.046195726s
May 20 13:02:05.461: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Running", Reason="", readiness=true. Elapsed: 20.049322905s
May 20 13:02:07.465: INFO: Pod "pod-subpath-test-configmap-kfdh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.053314055s
STEP: Saw pod success
May 20 13:02:07.465: INFO: Pod "pod-subpath-test-configmap-kfdh" satisfied condition "Succeeded or Failed"
May 20 13:02:07.468: INFO: Trying to get logs from node k8s-3 pod pod-subpath-test-configmap-kfdh container test-container-subpath-configmap-kfdh: <nil>
STEP: delete the pod
May 20 13:02:07.488: INFO: Waiting for pod pod-subpath-test-configmap-kfdh to disappear
May 20 13:02:07.492: INFO: Pod pod-subpath-test-configmap-kfdh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kfdh
May 20 13:02:07.492: INFO: Deleting pod "pod-subpath-test-configmap-kfdh" in namespace "subpath-6410"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:02:07.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6410" for this suite.

• [SLOW TEST:22.177 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":73,"skipped":1143,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:02:07.507: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 13:02:08.265: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 13:02:10.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757112528, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757112528, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757112528, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757112528, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 13:02:13.294: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:02:13.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8456" for this suite.
STEP: Destroying namespace "webhook-8456-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.124 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":74,"skipped":1143,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:02:13.632: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1737
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-1737
May 20 13:02:13.691: INFO: Found 0 stateful pods, waiting for 1
May 20 13:02:23.694: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 20 13:02:23.714: INFO: Deleting all statefulset in ns statefulset-1737
May 20 13:02:23.724: INFO: Scaling statefulset ss to 0
May 20 13:02:43.751: INFO: Waiting for statefulset status.replicas updated to 0
May 20 13:02:43.754: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:02:43.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1737" for this suite.

• [SLOW TEST:30.149 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":75,"skipped":1157,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:02:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:02:44.616: INFO: Checking APIGroup: apiregistration.k8s.io
May 20 13:02:44.618: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 20 13:02:44.618: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.618: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 20 13:02:44.618: INFO: Checking APIGroup: extensions
May 20 13:02:44.619: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 20 13:02:44.619: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 20 13:02:44.619: INFO: extensions/v1beta1 matches extensions/v1beta1
May 20 13:02:44.619: INFO: Checking APIGroup: apps
May 20 13:02:44.621: INFO: PreferredVersion.GroupVersion: apps/v1
May 20 13:02:44.621: INFO: Versions found [{apps/v1 v1}]
May 20 13:02:44.621: INFO: apps/v1 matches apps/v1
May 20 13:02:44.621: INFO: Checking APIGroup: events.k8s.io
May 20 13:02:44.622: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 20 13:02:44.622: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.622: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 20 13:02:44.622: INFO: Checking APIGroup: authentication.k8s.io
May 20 13:02:44.623: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 20 13:02:44.623: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.623: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 20 13:02:44.623: INFO: Checking APIGroup: authorization.k8s.io
May 20 13:02:44.624: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 20 13:02:44.625: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.625: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 20 13:02:44.625: INFO: Checking APIGroup: autoscaling
May 20 13:02:44.626: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 20 13:02:44.626: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 20 13:02:44.626: INFO: autoscaling/v1 matches autoscaling/v1
May 20 13:02:44.626: INFO: Checking APIGroup: batch
May 20 13:02:44.627: INFO: PreferredVersion.GroupVersion: batch/v1
May 20 13:02:44.627: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 20 13:02:44.627: INFO: batch/v1 matches batch/v1
May 20 13:02:44.627: INFO: Checking APIGroup: certificates.k8s.io
May 20 13:02:44.629: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 20 13:02:44.629: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.629: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 20 13:02:44.629: INFO: Checking APIGroup: networking.k8s.io
May 20 13:02:44.630: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 20 13:02:44.631: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.631: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 20 13:02:44.631: INFO: Checking APIGroup: policy
May 20 13:02:44.632: INFO: PreferredVersion.GroupVersion: policy/v1beta1
May 20 13:02:44.632: INFO: Versions found [{policy/v1beta1 v1beta1}]
May 20 13:02:44.632: INFO: policy/v1beta1 matches policy/v1beta1
May 20 13:02:44.632: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 20 13:02:44.634: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 20 13:02:44.634: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.634: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 20 13:02:44.634: INFO: Checking APIGroup: storage.k8s.io
May 20 13:02:44.636: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 20 13:02:44.636: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.636: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 20 13:02:44.636: INFO: Checking APIGroup: admissionregistration.k8s.io
May 20 13:02:44.637: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 20 13:02:44.637: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.637: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 20 13:02:44.637: INFO: Checking APIGroup: apiextensions.k8s.io
May 20 13:02:44.639: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 20 13:02:44.639: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.639: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 20 13:02:44.639: INFO: Checking APIGroup: scheduling.k8s.io
May 20 13:02:44.640: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 20 13:02:44.640: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.640: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 20 13:02:44.640: INFO: Checking APIGroup: coordination.k8s.io
May 20 13:02:44.641: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 20 13:02:44.641: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.641: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 20 13:02:44.641: INFO: Checking APIGroup: node.k8s.io
May 20 13:02:44.642: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
May 20 13:02:44.642: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.642: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
May 20 13:02:44.642: INFO: Checking APIGroup: discovery.k8s.io
May 20 13:02:44.644: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
May 20 13:02:44.644: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
May 20 13:02:44.644: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:02:44.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7418" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":76,"skipped":1171,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:02:44.654: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-8e980ce4-dfe7-4d90-a775-0d25bb56ae7f in namespace container-probe-2847
May 20 13:02:46.690: INFO: Started pod liveness-8e980ce4-dfe7-4d90-a775-0d25bb56ae7f in namespace container-probe-2847
STEP: checking the pod's current state and verifying that restartCount is present
May 20 13:02:46.694: INFO: Initial restart count of pod liveness-8e980ce4-dfe7-4d90-a775-0d25bb56ae7f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:06:47.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2847" for this suite.

• [SLOW TEST:243.250 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":77,"skipped":1182,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:06:47.905: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
May 20 13:06:47.954: INFO: Waiting up to 5m0s for pod "pod-f0e168e4-3dec-43e0-b0c0-dabfb1ffe23e" in namespace "emptydir-1780" to be "Succeeded or Failed"
May 20 13:06:47.960: INFO: Pod "pod-f0e168e4-3dec-43e0-b0c0-dabfb1ffe23e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.863011ms
May 20 13:06:49.965: INFO: Pod "pod-f0e168e4-3dec-43e0-b0c0-dabfb1ffe23e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011319988s
May 20 13:06:51.969: INFO: Pod "pod-f0e168e4-3dec-43e0-b0c0-dabfb1ffe23e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01549508s
STEP: Saw pod success
May 20 13:06:51.969: INFO: Pod "pod-f0e168e4-3dec-43e0-b0c0-dabfb1ffe23e" satisfied condition "Succeeded or Failed"
May 20 13:06:51.972: INFO: Trying to get logs from node k8s-3 pod pod-f0e168e4-3dec-43e0-b0c0-dabfb1ffe23e container test-container: <nil>
STEP: delete the pod
May 20 13:06:52.005: INFO: Waiting for pod pod-f0e168e4-3dec-43e0-b0c0-dabfb1ffe23e to disappear
May 20 13:06:52.008: INFO: Pod pod-f0e168e4-3dec-43e0-b0c0-dabfb1ffe23e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:06:52.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1780" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":78,"skipped":1196,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:06:52.015: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-64129997-bc2e-416d-a1d5-be5fb425562b
STEP: Creating a pod to test consume secrets
May 20 13:06:52.045: INFO: Waiting up to 5m0s for pod "pod-secrets-9bb79e41-cef1-421d-b511-1d53d5be111b" in namespace "secrets-6262" to be "Succeeded or Failed"
May 20 13:06:52.055: INFO: Pod "pod-secrets-9bb79e41-cef1-421d-b511-1d53d5be111b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.970923ms
May 20 13:06:54.059: INFO: Pod "pod-secrets-9bb79e41-cef1-421d-b511-1d53d5be111b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013658295s
May 20 13:06:56.076: INFO: Pod "pod-secrets-9bb79e41-cef1-421d-b511-1d53d5be111b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03095423s
STEP: Saw pod success
May 20 13:06:56.076: INFO: Pod "pod-secrets-9bb79e41-cef1-421d-b511-1d53d5be111b" satisfied condition "Succeeded or Failed"
May 20 13:06:56.079: INFO: Trying to get logs from node k8s-3 pod pod-secrets-9bb79e41-cef1-421d-b511-1d53d5be111b container secret-volume-test: <nil>
STEP: delete the pod
May 20 13:06:56.093: INFO: Waiting for pod pod-secrets-9bb79e41-cef1-421d-b511-1d53d5be111b to disappear
May 20 13:06:56.095: INFO: Pod pod-secrets-9bb79e41-cef1-421d-b511-1d53d5be111b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:06:56.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6262" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":1211,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:06:56.106: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 20 13:06:56.151: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-207 /api/v1/namespaces/watch-207/configmaps/e2e-watch-test-resource-version 02e1f8a1-647e-419e-9c71-18eefdc42b26 9687 0 2021-05-20 13:06:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-20 13:06:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 13:06:56.153: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-207 /api/v1/namespaces/watch-207/configmaps/e2e-watch-test-resource-version 02e1f8a1-647e-419e-9c71-18eefdc42b26 9688 0 2021-05-20 13:06:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-20 13:06:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:06:56.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-207" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":80,"skipped":1224,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:06:56.162: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:06:56.198: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db7be30f-1f0f-4ffa-802c-654379728e44" in namespace "downward-api-5810" to be "Succeeded or Failed"
May 20 13:06:56.203: INFO: Pod "downwardapi-volume-db7be30f-1f0f-4ffa-802c-654379728e44": Phase="Pending", Reason="", readiness=false. Elapsed: 4.926067ms
May 20 13:06:58.209: INFO: Pod "downwardapi-volume-db7be30f-1f0f-4ffa-802c-654379728e44": Phase="Running", Reason="", readiness=true. Elapsed: 2.011433533s
May 20 13:07:00.215: INFO: Pod "downwardapi-volume-db7be30f-1f0f-4ffa-802c-654379728e44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016763193s
STEP: Saw pod success
May 20 13:07:00.215: INFO: Pod "downwardapi-volume-db7be30f-1f0f-4ffa-802c-654379728e44" satisfied condition "Succeeded or Failed"
May 20 13:07:00.217: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-db7be30f-1f0f-4ffa-802c-654379728e44 container client-container: <nil>
STEP: delete the pod
May 20 13:07:00.231: INFO: Waiting for pod downwardapi-volume-db7be30f-1f0f-4ffa-802c-654379728e44 to disappear
May 20 13:07:00.233: INFO: Pod downwardapi-volume-db7be30f-1f0f-4ffa-802c-654379728e44 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:07:00.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5810" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":81,"skipped":1225,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:07:00.243: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 20 13:07:00.272: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 20 13:07:00.278: INFO: Waiting for terminating namespaces to be deleted...
May 20 13:07:00.281: INFO: 
Logging pods the apiserver thinks is on node k8s-1 before test
May 20 13:07:00.286: INFO: coredns-7677f9bb54-24hll from kube-system started at 2021-05-20 12:39:45 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.286: INFO: 	Container coredns ready: true, restart count 0
May 20 13:07:00.287: INFO: dns-autoscaler-5b7b5c9b6f-xgl8n from kube-system started at 2021-05-20 12:39:49 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.287: INFO: 	Container autoscaler ready: true, restart count 0
May 20 13:07:00.287: INFO: kube-apiserver-k8s-1 from kube-system started at 2021-05-20 12:37:59 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.287: INFO: 	Container kube-apiserver ready: true, restart count 0
May 20 13:07:00.287: INFO: kube-controller-manager-k8s-1 from kube-system started at 2021-05-20 12:38:14 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.287: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 20 13:07:00.287: INFO: kube-flannel-xshzr from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.287: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 13:07:00.288: INFO: kube-proxy-r8x5s from kube-system started at 2021-05-20 12:39:03 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.288: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 13:07:00.288: INFO: kube-scheduler-k8s-1 from kube-system started at 2021-05-20 12:38:14 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.288: INFO: 	Container kube-scheduler ready: true, restart count 0
May 20 13:07:00.288: INFO: nodelocaldns-j9jjn from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.288: INFO: 	Container node-cache ready: true, restart count 0
May 20 13:07:00.288: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-nkrjs from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 13:07:00.289: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 13:07:00.289: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 13:07:00.289: INFO: 
Logging pods the apiserver thinks is on node k8s-2 before test
May 20 13:07:00.294: INFO: coredns-7677f9bb54-m6mpv from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.294: INFO: 	Container coredns ready: true, restart count 0
May 20 13:07:00.295: INFO: kube-flannel-ggvw9 from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.295: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 13:07:00.295: INFO: kube-proxy-hdfgh from kube-system started at 2021-05-20 12:39:03 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.295: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 13:07:00.295: INFO: nginx-proxy-k8s-2 from kube-system started at 2021-05-20 12:38:55 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.295: INFO: 	Container nginx-proxy ready: true, restart count 0
May 20 13:07:00.295: INFO: nodelocaldns-nl6xd from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.295: INFO: 	Container node-cache ready: true, restart count 0
May 20 13:07:00.295: INFO: sonobuoy-e2e-job-9cce46af7e3b479e from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 13:07:00.295: INFO: 	Container e2e ready: true, restart count 0
May 20 13:07:00.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 13:07:00.296: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-v8rq9 from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 13:07:00.296: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 13:07:00.296: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 13:07:00.296: INFO: 
Logging pods the apiserver thinks is on node k8s-3 before test
May 20 13:07:00.300: INFO: kube-flannel-hx4xl from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.300: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 13:07:00.300: INFO: kube-proxy-whdwz from kube-system started at 2021-05-20 12:39:04 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.300: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 13:07:00.300: INFO: nginx-proxy-k8s-3 from kube-system started at 2021-05-20 12:38:56 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.300: INFO: 	Container nginx-proxy ready: true, restart count 0
May 20 13:07:00.300: INFO: nodelocaldns-pzwcd from kube-system started at 2021-05-20 12:39:51 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.300: INFO: 	Container node-cache ready: true, restart count 0
May 20 13:07:00.300: INFO: sonobuoy from sonobuoy started at 2021-05-20 12:41:44 +0000 UTC (1 container statuses recorded)
May 20 13:07:00.300: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 20 13:07:00.301: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-lvpcw from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 13:07:00.301: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 13:07:00.301: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b4841d96-fa8f-42cd-8abb-90d28cb0b7c2 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-b4841d96-fa8f-42cd-8abb-90d28cb0b7c2 off the node k8s-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b4841d96-fa8f-42cd-8abb-90d28cb0b7c2
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:12:06.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8997" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:306.184 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":82,"skipped":1244,"failed":0}
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:12:06.429: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4562
STEP: creating service affinity-nodeport in namespace services-4562
STEP: creating replication controller affinity-nodeport in namespace services-4562
I0520 13:12:06.616698      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-4562, replica count: 3
I0520 13:12:09.668799      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 13:12:09.678: INFO: Creating new exec pod
May 20 13:12:14.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-4562 exec execpod-affinitytrqhl -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
May 20 13:12:16.093: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 20 13:12:16.093: INFO: stdout: ""
May 20 13:12:16.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-4562 exec execpod-affinitytrqhl -- /bin/sh -x -c nc -zv -t -w 2 10.233.22.45 80'
May 20 13:12:16.371: INFO: stderr: "+ nc -zv -t -w 2 10.233.22.45 80\nConnection to 10.233.22.45 80 port [tcp/http] succeeded!\n"
May 20 13:12:16.371: INFO: stdout: ""
May 20 13:12:16.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-4562 exec execpod-affinitytrqhl -- /bin/sh -x -c nc -zv -t -w 2 172.18.8.103 32172'
May 20 13:12:16.700: INFO: stderr: "+ nc -zv -t -w 2 172.18.8.103 32172\nConnection to 172.18.8.103 32172 port [tcp/32172] succeeded!\n"
May 20 13:12:16.700: INFO: stdout: ""
May 20 13:12:16.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-4562 exec execpod-affinitytrqhl -- /bin/sh -x -c nc -zv -t -w 2 172.18.8.102 32172'
May 20 13:12:16.894: INFO: stderr: "+ nc -zv -t -w 2 172.18.8.102 32172\nConnection to 172.18.8.102 32172 port [tcp/32172] succeeded!\n"
May 20 13:12:16.894: INFO: stdout: ""
May 20 13:12:16.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-4562 exec execpod-affinitytrqhl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.8.101:32172/ ; done'
May 20 13:12:17.197: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:32172/\n"
May 20 13:12:17.197: INFO: stdout: "\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw\naffinity-nodeport-ms2zw"
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Received response from host: affinity-nodeport-ms2zw
May 20 13:12:17.197: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4562, will wait for the garbage collector to delete the pods
May 20 13:12:17.296: INFO: Deleting ReplicationController affinity-nodeport took: 10.137372ms
May 20 13:12:17.697: INFO: Terminating ReplicationController affinity-nodeport pods took: 400.837024ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:12:25.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4562" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:18.625 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":83,"skipped":1244,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:12:25.053: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 20 13:12:25.090: INFO: Waiting up to 1m0s for all nodes to be ready
May 20 13:13:25.110: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:13:25.113: INFO: Starting informer...
STEP: Starting pod...
May 20 13:13:25.325: INFO: Pod is running on k8s-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 20 13:13:25.340: INFO: Pod wasn't evicted. Proceeding
May 20 13:13:25.340: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 20 13:14:40.352: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:14:40.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6177" for this suite.

• [SLOW TEST:135.308 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":84,"skipped":1252,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:14:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
May 20 13:14:40.390: INFO: Waiting up to 5m0s for pod "client-containers-57105e45-aff9-47af-975f-94b002fc8708" in namespace "containers-7106" to be "Succeeded or Failed"
May 20 13:14:40.402: INFO: Pod "client-containers-57105e45-aff9-47af-975f-94b002fc8708": Phase="Pending", Reason="", readiness=false. Elapsed: 11.459044ms
May 20 13:14:42.408: INFO: Pod "client-containers-57105e45-aff9-47af-975f-94b002fc8708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017589058s
STEP: Saw pod success
May 20 13:14:42.408: INFO: Pod "client-containers-57105e45-aff9-47af-975f-94b002fc8708" satisfied condition "Succeeded or Failed"
May 20 13:14:42.413: INFO: Trying to get logs from node k8s-3 pod client-containers-57105e45-aff9-47af-975f-94b002fc8708 container test-container: <nil>
STEP: delete the pod
May 20 13:14:42.436: INFO: Waiting for pod client-containers-57105e45-aff9-47af-975f-94b002fc8708 to disappear
May 20 13:14:42.440: INFO: Pod client-containers-57105e45-aff9-47af-975f-94b002fc8708 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:14:42.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7106" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":85,"skipped":1264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:14:42.457: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:14:42.483: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Creating first CR 
May 20 13:14:43.048: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-20T13:14:43Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-20T13:14:43Z]] name:name1 resourceVersion:11261 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:3a4743e8-86f7-4d46-89fe-23527c32129e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 20 13:14:53.053: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-20T13:14:53Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-20T13:14:53Z]] name:name2 resourceVersion:11312 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:0065f2ec-e8bf-4982-8c31-d387a656e196] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 20 13:15:03.075: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-20T13:14:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-20T13:15:03Z]] name:name1 resourceVersion:11341 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:3a4743e8-86f7-4d46-89fe-23527c32129e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 20 13:15:13.080: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-20T13:14:53Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-20T13:15:13Z]] name:name2 resourceVersion:11370 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:0065f2ec-e8bf-4982-8c31-d387a656e196] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 20 13:15:23.090: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-20T13:14:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-20T13:15:03Z]] name:name1 resourceVersion:11401 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:3a4743e8-86f7-4d46-89fe-23527c32129e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 20 13:15:33.099: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-20T13:14:53Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-20T13:15:13Z]] name:name2 resourceVersion:11427 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:0065f2ec-e8bf-4982-8c31-d387a656e196] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:15:43.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-522" for this suite.

• [SLOW TEST:61.174 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":86,"skipped":1331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:15:43.635: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 20 13:15:43.668: INFO: Waiting up to 5m0s for pod "pod-82796f30-6fc6-45e8-91d0-bb47960cd78e" in namespace "emptydir-1528" to be "Succeeded or Failed"
May 20 13:15:43.675: INFO: Pod "pod-82796f30-6fc6-45e8-91d0-bb47960cd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.946275ms
May 20 13:15:45.679: INFO: Pod "pod-82796f30-6fc6-45e8-91d0-bb47960cd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010433029s
May 20 13:15:47.683: INFO: Pod "pod-82796f30-6fc6-45e8-91d0-bb47960cd78e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014569758s
STEP: Saw pod success
May 20 13:15:47.683: INFO: Pod "pod-82796f30-6fc6-45e8-91d0-bb47960cd78e" satisfied condition "Succeeded or Failed"
May 20 13:15:47.685: INFO: Trying to get logs from node k8s-3 pod pod-82796f30-6fc6-45e8-91d0-bb47960cd78e container test-container: <nil>
STEP: delete the pod
May 20 13:15:47.699: INFO: Waiting for pod pod-82796f30-6fc6-45e8-91d0-bb47960cd78e to disappear
May 20 13:15:47.701: INFO: Pod pod-82796f30-6fc6-45e8-91d0-bb47960cd78e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:15:47.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1528" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1355,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:15:47.711: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8773
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8773
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8773
May 20 13:15:47.767: INFO: Found 0 stateful pods, waiting for 1
May 20 13:15:57.772: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 20 13:15:57.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-8773 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 20 13:15:58.012: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 20 13:15:58.012: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 20 13:15:58.012: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 20 13:15:58.015: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 20 13:16:08.019: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 20 13:16:08.019: INFO: Waiting for statefulset status.replicas updated to 0
May 20 13:16:08.030: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999709s
May 20 13:16:09.034: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997146878s
May 20 13:16:10.044: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98743154s
May 20 13:16:11.048: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983307242s
May 20 13:16:12.060: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.970429972s
May 20 13:16:13.066: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.9664258s
May 20 13:16:14.069: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.961011797s
May 20 13:16:15.103: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.928722464s
May 20 13:16:16.106: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.92449466s
May 20 13:16:17.110: INFO: Verifying statefulset ss doesn't scale past 1 for another 920.195191ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8773
May 20 13:16:18.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-8773 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 20 13:16:18.306: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 20 13:16:18.307: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 20 13:16:18.307: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 20 13:16:18.310: INFO: Found 1 stateful pods, waiting for 3
May 20 13:16:28.315: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 20 13:16:28.315: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 20 13:16:28.315: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 20 13:16:28.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-8773 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 20 13:16:28.507: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 20 13:16:28.507: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 20 13:16:28.507: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 20 13:16:28.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-8773 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 20 13:16:28.726: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 20 13:16:28.726: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 20 13:16:28.726: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 20 13:16:28.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-8773 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 20 13:16:29.128: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 20 13:16:29.128: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 20 13:16:29.128: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 20 13:16:29.128: INFO: Waiting for statefulset status.replicas updated to 0
May 20 13:16:29.130: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 20 13:16:39.138: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 20 13:16:39.138: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 20 13:16:39.139: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 20 13:16:39.151: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999665s
May 20 13:16:40.156: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995079587s
May 20 13:16:41.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990758383s
May 20 13:16:42.164: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987139263s
May 20 13:16:43.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982748812s
May 20 13:16:44.173: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977895674s
May 20 13:16:45.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970113637s
May 20 13:16:46.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.877365435s
May 20 13:16:47.282: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.868346456s
May 20 13:16:48.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 864.987287ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8773
May 20 13:16:49.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-8773 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 20 13:16:49.468: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 20 13:16:49.468: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 20 13:16:49.468: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 20 13:16:49.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-8773 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 20 13:16:49.654: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 20 13:16:49.654: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 20 13:16:49.654: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 20 13:16:49.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-8773 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 20 13:16:49.947: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 20 13:16:49.947: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 20 13:16:49.947: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 20 13:16:49.947: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 20 13:16:59.968: INFO: Deleting all statefulset in ns statefulset-8773
May 20 13:16:59.970: INFO: Scaling statefulset ss to 0
May 20 13:16:59.978: INFO: Waiting for statefulset status.replicas updated to 0
May 20 13:16:59.980: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:16:59.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8773" for this suite.

• [SLOW TEST:72.297 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":88,"skipped":1374,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:00.009: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:00.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1174" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":89,"skipped":1381,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:00.059: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
May 20 13:17:04.116: INFO: Pod pod-hostip-752d1156-b184-4377-895b-790cd42e4e8e has hostIP: 172.18.8.103
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:04.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7783" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":90,"skipped":1389,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:04.125: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 20 13:17:04.154: INFO: Waiting up to 5m0s for pod "pod-2d75bb42-34a2-49de-b39a-58ca1ff2e594" in namespace "emptydir-7185" to be "Succeeded or Failed"
May 20 13:17:04.162: INFO: Pod "pod-2d75bb42-34a2-49de-b39a-58ca1ff2e594": Phase="Pending", Reason="", readiness=false. Elapsed: 8.389669ms
May 20 13:17:06.167: INFO: Pod "pod-2d75bb42-34a2-49de-b39a-58ca1ff2e594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012914743s
STEP: Saw pod success
May 20 13:17:06.167: INFO: Pod "pod-2d75bb42-34a2-49de-b39a-58ca1ff2e594" satisfied condition "Succeeded or Failed"
May 20 13:17:06.170: INFO: Trying to get logs from node k8s-3 pod pod-2d75bb42-34a2-49de-b39a-58ca1ff2e594 container test-container: <nil>
STEP: delete the pod
May 20 13:17:06.192: INFO: Waiting for pod pod-2d75bb42-34a2-49de-b39a-58ca1ff2e594 to disappear
May 20 13:17:06.195: INFO: Pod pod-2d75bb42-34a2-49de-b39a-58ca1ff2e594 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:06.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7185" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":91,"skipped":1394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:06.207: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4465.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4465.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4465.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4465.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 20 13:17:10.270: INFO: DNS probes using dns-test-1a4b6cd9-f9bc-469a-b9d9-ee8b86306e6d succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4465.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4465.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4465.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4465.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 20 13:17:14.313: INFO: DNS probes using dns-test-cd2649f7-998a-4685-9595-e0c77f62dd72 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4465.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4465.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4465.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4465.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 20 13:17:16.383: INFO: DNS probes using dns-test-f114a84c-c333-4d49-8c46-db3ca5a5c410 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:16.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4465" for this suite.

• [SLOW TEST:10.227 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":92,"skipped":1444,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:16.434: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 20 13:17:16.468: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:19.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1333" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":93,"skipped":1462,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:19.677: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:19.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-625" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":94,"skipped":1466,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:19.739: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 13:17:20.293: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 13:17:22.316: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757113440, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757113440, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757113440, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757113440, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 13:17:25.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
May 20 13:17:25.354: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:25.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4872" for this suite.
STEP: Destroying namespace "webhook-4872-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.812 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":95,"skipped":1469,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:25.552: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-3a74c435-15e5-4c00-b4c8-ed0fd2358f6d
STEP: Creating a pod to test consume configMaps
May 20 13:17:25.608: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6f6e393c-20ad-4585-aa9d-ce3edf05f9a3" in namespace "projected-9143" to be "Succeeded or Failed"
May 20 13:17:25.612: INFO: Pod "pod-projected-configmaps-6f6e393c-20ad-4585-aa9d-ce3edf05f9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968987ms
May 20 13:17:27.615: INFO: Pod "pod-projected-configmaps-6f6e393c-20ad-4585-aa9d-ce3edf05f9a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0063139s
STEP: Saw pod success
May 20 13:17:27.615: INFO: Pod "pod-projected-configmaps-6f6e393c-20ad-4585-aa9d-ce3edf05f9a3" satisfied condition "Succeeded or Failed"
May 20 13:17:27.617: INFO: Trying to get logs from node k8s-3 pod pod-projected-configmaps-6f6e393c-20ad-4585-aa9d-ce3edf05f9a3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 20 13:17:27.634: INFO: Waiting for pod pod-projected-configmaps-6f6e393c-20ad-4585-aa9d-ce3edf05f9a3 to disappear
May 20 13:17:27.636: INFO: Pod pod-projected-configmaps-6f6e393c-20ad-4585-aa9d-ce3edf05f9a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:27.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9143" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":96,"skipped":1487,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:27.649: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1307
STEP: creating the pod
May 20 13:17:27.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1180 create -f -'
May 20 13:17:27.908: INFO: stderr: ""
May 20 13:17:27.908: INFO: stdout: "pod/pause created\n"
May 20 13:17:27.908: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 20 13:17:27.908: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1180" to be "running and ready"
May 20 13:17:27.913: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.833518ms
May 20 13:17:29.918: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009785153s
May 20 13:17:29.918: INFO: Pod "pause" satisfied condition "running and ready"
May 20 13:17:29.918: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
May 20 13:17:29.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1180 label pods pause testing-label=testing-label-value'
May 20 13:17:30.006: INFO: stderr: ""
May 20 13:17:30.007: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 20 13:17:30.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1180 get pod pause -L testing-label'
May 20 13:17:30.080: INFO: stderr: ""
May 20 13:17:30.080: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 20 13:17:30.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1180 label pods pause testing-label-'
May 20 13:17:30.163: INFO: stderr: ""
May 20 13:17:30.163: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 20 13:17:30.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1180 get pod pause -L testing-label'
May 20 13:17:30.242: INFO: stderr: ""
May 20 13:17:30.242: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1313
STEP: using delete to clean up resources
May 20 13:17:30.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1180 delete --grace-period=0 --force -f -'
May 20 13:17:30.348: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 20 13:17:30.348: INFO: stdout: "pod \"pause\" force deleted\n"
May 20 13:17:30.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1180 get rc,svc -l name=pause --no-headers'
May 20 13:17:30.450: INFO: stderr: "No resources found in kubectl-1180 namespace.\n"
May 20 13:17:30.450: INFO: stdout: ""
May 20 13:17:30.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-1180 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 20 13:17:30.602: INFO: stderr: ""
May 20 13:17:30.602: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:30.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1180" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":97,"skipped":1496,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:30.612: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
May 20 13:17:31.671: INFO: created pod pod-service-account-defaultsa
May 20 13:17:31.671: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 20 13:17:31.677: INFO: created pod pod-service-account-mountsa
May 20 13:17:31.677: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 20 13:17:31.686: INFO: created pod pod-service-account-nomountsa
May 20 13:17:31.687: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 20 13:17:31.697: INFO: created pod pod-service-account-defaultsa-mountspec
May 20 13:17:31.697: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 20 13:17:31.710: INFO: created pod pod-service-account-mountsa-mountspec
May 20 13:17:31.710: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 20 13:17:31.733: INFO: created pod pod-service-account-nomountsa-mountspec
May 20 13:17:31.734: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 20 13:17:31.748: INFO: created pod pod-service-account-defaultsa-nomountspec
May 20 13:17:31.748: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 20 13:17:31.759: INFO: created pod pod-service-account-mountsa-nomountspec
May 20 13:17:31.759: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 20 13:17:31.770: INFO: created pod pod-service-account-nomountsa-nomountspec
May 20 13:17:31.770: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:31.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9036" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":98,"skipped":1498,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:31.799: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-f6a6d575-6335-4bdc-a576-b9d3ff4c5809
STEP: Creating a pod to test consume configMaps
May 20 13:17:31.861: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0312f2b8-f1ab-4aa2-8352-5e3b11a14564" in namespace "projected-3035" to be "Succeeded or Failed"
May 20 13:17:31.865: INFO: Pod "pod-projected-configmaps-0312f2b8-f1ab-4aa2-8352-5e3b11a14564": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765098ms
May 20 13:17:33.871: INFO: Pod "pod-projected-configmaps-0312f2b8-f1ab-4aa2-8352-5e3b11a14564": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009509832s
May 20 13:17:35.881: INFO: Pod "pod-projected-configmaps-0312f2b8-f1ab-4aa2-8352-5e3b11a14564": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019467237s
STEP: Saw pod success
May 20 13:17:35.881: INFO: Pod "pod-projected-configmaps-0312f2b8-f1ab-4aa2-8352-5e3b11a14564" satisfied condition "Succeeded or Failed"
May 20 13:17:35.884: INFO: Trying to get logs from node k8s-3 pod pod-projected-configmaps-0312f2b8-f1ab-4aa2-8352-5e3b11a14564 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 20 13:17:35.911: INFO: Waiting for pod pod-projected-configmaps-0312f2b8-f1ab-4aa2-8352-5e3b11a14564 to disappear
May 20 13:17:35.913: INFO: Pod pod-projected-configmaps-0312f2b8-f1ab-4aa2-8352-5e3b11a14564 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:35.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3035" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":99,"skipped":1517,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:35.923: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 20 13:17:35.962: INFO: Pod name pod-release: Found 0 pods out of 1
May 20 13:17:40.966: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:17:41.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2035" for this suite.

• [SLOW TEST:6.083 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":100,"skipped":1517,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:17:42.007: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-xlp8
STEP: Creating a pod to test atomic-volume-subpath
May 20 13:17:42.099: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xlp8" in namespace "subpath-8465" to be "Succeeded or Failed"
May 20 13:17:42.106: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.774176ms
May 20 13:17:44.110: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011250103s
May 20 13:17:46.116: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Running", Reason="", readiness=true. Elapsed: 4.017028794s
May 20 13:17:48.120: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Running", Reason="", readiness=true. Elapsed: 6.020503013s
May 20 13:17:50.167: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Running", Reason="", readiness=true. Elapsed: 8.067560217s
May 20 13:17:52.173: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Running", Reason="", readiness=true. Elapsed: 10.073483005s
May 20 13:17:54.176: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Running", Reason="", readiness=true. Elapsed: 12.077093557s
May 20 13:17:56.181: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Running", Reason="", readiness=true. Elapsed: 14.08162049s
May 20 13:17:58.205: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Running", Reason="", readiness=true. Elapsed: 16.105783642s
May 20 13:18:00.261: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Running", Reason="", readiness=true. Elapsed: 18.16142145s
May 20 13:18:02.266: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Running", Reason="", readiness=true. Elapsed: 20.167018359s
May 20 13:18:04.269: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Running", Reason="", readiness=true. Elapsed: 22.170188679s
May 20 13:18:06.273: INFO: Pod "pod-subpath-test-downwardapi-xlp8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.173764783s
STEP: Saw pod success
May 20 13:18:06.273: INFO: Pod "pod-subpath-test-downwardapi-xlp8" satisfied condition "Succeeded or Failed"
May 20 13:18:06.276: INFO: Trying to get logs from node k8s-3 pod pod-subpath-test-downwardapi-xlp8 container test-container-subpath-downwardapi-xlp8: <nil>
STEP: delete the pod
May 20 13:18:06.296: INFO: Waiting for pod pod-subpath-test-downwardapi-xlp8 to disappear
May 20 13:18:06.300: INFO: Pod pod-subpath-test-downwardapi-xlp8 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-xlp8
May 20 13:18:06.300: INFO: Deleting pod "pod-subpath-test-downwardapi-xlp8" in namespace "subpath-8465"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:18:06.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8465" for this suite.

• [SLOW TEST:24.309 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":101,"skipped":1531,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:18:06.316: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-12953f27-10b5-4d68-b707-d95161e68be1
STEP: Creating a pod to test consume configMaps
May 20 13:18:06.347: INFO: Waiting up to 5m0s for pod "pod-configmaps-cda5e15c-dc45-4dcf-a936-634a90b12a12" in namespace "configmap-5775" to be "Succeeded or Failed"
May 20 13:18:06.351: INFO: Pod "pod-configmaps-cda5e15c-dc45-4dcf-a936-634a90b12a12": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568401ms
May 20 13:18:08.355: INFO: Pod "pod-configmaps-cda5e15c-dc45-4dcf-a936-634a90b12a12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007718265s
May 20 13:18:10.359: INFO: Pod "pod-configmaps-cda5e15c-dc45-4dcf-a936-634a90b12a12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012045296s
STEP: Saw pod success
May 20 13:18:10.359: INFO: Pod "pod-configmaps-cda5e15c-dc45-4dcf-a936-634a90b12a12" satisfied condition "Succeeded or Failed"
May 20 13:18:10.362: INFO: Trying to get logs from node k8s-3 pod pod-configmaps-cda5e15c-dc45-4dcf-a936-634a90b12a12 container configmap-volume-test: <nil>
STEP: delete the pod
May 20 13:18:10.383: INFO: Waiting for pod pod-configmaps-cda5e15c-dc45-4dcf-a936-634a90b12a12 to disappear
May 20 13:18:10.386: INFO: Pod pod-configmaps-cda5e15c-dc45-4dcf-a936-634a90b12a12 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:18:10.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5775" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":102,"skipped":1538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:18:10.396: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-4krx
STEP: Creating a pod to test atomic-volume-subpath
May 20 13:18:10.439: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-4krx" in namespace "subpath-7214" to be "Succeeded or Failed"
May 20 13:18:10.442: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.681283ms
May 20 13:18:12.446: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006645549s
May 20 13:18:14.450: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Running", Reason="", readiness=true. Elapsed: 4.011059319s
May 20 13:18:16.454: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Running", Reason="", readiness=true. Elapsed: 6.01495006s
May 20 13:18:18.458: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Running", Reason="", readiness=true. Elapsed: 8.019414937s
May 20 13:18:20.462: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Running", Reason="", readiness=true. Elapsed: 10.023054171s
May 20 13:18:22.465: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Running", Reason="", readiness=true. Elapsed: 12.026408908s
May 20 13:18:24.472: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Running", Reason="", readiness=true. Elapsed: 14.033184032s
May 20 13:18:26.475: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Running", Reason="", readiness=true. Elapsed: 16.036547203s
May 20 13:18:28.479: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Running", Reason="", readiness=true. Elapsed: 18.039683636s
May 20 13:18:30.483: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Running", Reason="", readiness=true. Elapsed: 20.043621137s
May 20 13:18:32.491: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Running", Reason="", readiness=true. Elapsed: 22.052250838s
May 20 13:18:34.495: INFO: Pod "pod-subpath-test-secret-4krx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.055955951s
STEP: Saw pod success
May 20 13:18:34.495: INFO: Pod "pod-subpath-test-secret-4krx" satisfied condition "Succeeded or Failed"
May 20 13:18:34.497: INFO: Trying to get logs from node k8s-3 pod pod-subpath-test-secret-4krx container test-container-subpath-secret-4krx: <nil>
STEP: delete the pod
May 20 13:18:34.513: INFO: Waiting for pod pod-subpath-test-secret-4krx to disappear
May 20 13:18:34.515: INFO: Pod pod-subpath-test-secret-4krx no longer exists
STEP: Deleting pod pod-subpath-test-secret-4krx
May 20 13:18:34.515: INFO: Deleting pod "pod-subpath-test-secret-4krx" in namespace "subpath-7214"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:18:34.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7214" for this suite.

• [SLOW TEST:24.135 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":103,"skipped":1590,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:18:34.531: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 13:18:34.792: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 13:18:36.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757113515, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757113515, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757113515, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757113515, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 13:18:39.817: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 20 13:18:41.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=webhook-5796 attach --namespace=webhook-5796 to-be-attached-pod -i -c=container1'
May 20 13:18:41.985: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:18:41.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5796" for this suite.
STEP: Destroying namespace "webhook-5796-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.545 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":104,"skipped":1593,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:18:42.090: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:18:42.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b7c8286a-7e85-4a09-98d6-4cd3c8d171af" in namespace "downward-api-1479" to be "Succeeded or Failed"
May 20 13:18:42.161: INFO: Pod "downwardapi-volume-b7c8286a-7e85-4a09-98d6-4cd3c8d171af": Phase="Pending", Reason="", readiness=false. Elapsed: 13.181177ms
May 20 13:18:44.165: INFO: Pod "downwardapi-volume-b7c8286a-7e85-4a09-98d6-4cd3c8d171af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016328846s
May 20 13:18:46.168: INFO: Pod "downwardapi-volume-b7c8286a-7e85-4a09-98d6-4cd3c8d171af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019400154s
STEP: Saw pod success
May 20 13:18:46.168: INFO: Pod "downwardapi-volume-b7c8286a-7e85-4a09-98d6-4cd3c8d171af" satisfied condition "Succeeded or Failed"
May 20 13:18:46.170: INFO: Trying to get logs from node k8s-2 pod downwardapi-volume-b7c8286a-7e85-4a09-98d6-4cd3c8d171af container client-container: <nil>
STEP: delete the pod
May 20 13:18:46.205: INFO: Waiting for pod downwardapi-volume-b7c8286a-7e85-4a09-98d6-4cd3c8d171af to disappear
May 20 13:18:46.208: INFO: Pod downwardapi-volume-b7c8286a-7e85-4a09-98d6-4cd3c8d171af no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:18:46.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1479" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":105,"skipped":1611,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:18:46.221: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-xp8h
STEP: Creating a pod to test atomic-volume-subpath
May 20 13:18:46.260: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xp8h" in namespace "subpath-3250" to be "Succeeded or Failed"
May 20 13:18:46.263: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980471ms
May 20 13:18:48.267: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 2.00623536s
May 20 13:18:50.271: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 4.010774368s
May 20 13:18:52.279: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 6.018595533s
May 20 13:18:54.286: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 8.025180997s
May 20 13:18:56.293: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 10.032089201s
May 20 13:18:58.296: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 12.03510895s
May 20 13:19:00.306: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 14.045617144s
May 20 13:19:02.350: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 16.089420734s
May 20 13:19:04.354: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 18.093707608s
May 20 13:19:06.359: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 20.098183796s
May 20 13:19:08.364: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Running", Reason="", readiness=true. Elapsed: 22.103118883s
May 20 13:19:10.367: INFO: Pod "pod-subpath-test-configmap-xp8h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.107039704s
STEP: Saw pod success
May 20 13:19:10.368: INFO: Pod "pod-subpath-test-configmap-xp8h" satisfied condition "Succeeded or Failed"
May 20 13:19:10.370: INFO: Trying to get logs from node k8s-3 pod pod-subpath-test-configmap-xp8h container test-container-subpath-configmap-xp8h: <nil>
STEP: delete the pod
May 20 13:19:10.387: INFO: Waiting for pod pod-subpath-test-configmap-xp8h to disappear
May 20 13:19:10.390: INFO: Pod pod-subpath-test-configmap-xp8h no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xp8h
May 20 13:19:10.390: INFO: Deleting pod "pod-subpath-test-configmap-xp8h" in namespace "subpath-3250"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:19:10.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3250" for this suite.

• [SLOW TEST:24.178 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":106,"skipped":1626,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:19:10.400: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
May 20 13:19:10.428: INFO: Waiting up to 5m0s for pod "pod-06a4cf95-2736-4475-9ac2-2b4994c142d8" in namespace "emptydir-6432" to be "Succeeded or Failed"
May 20 13:19:10.431: INFO: Pod "pod-06a4cf95-2736-4475-9ac2-2b4994c142d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138747ms
May 20 13:19:12.435: INFO: Pod "pod-06a4cf95-2736-4475-9ac2-2b4994c142d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006528132s
May 20 13:19:14.438: INFO: Pod "pod-06a4cf95-2736-4475-9ac2-2b4994c142d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009122078s
STEP: Saw pod success
May 20 13:19:14.438: INFO: Pod "pod-06a4cf95-2736-4475-9ac2-2b4994c142d8" satisfied condition "Succeeded or Failed"
May 20 13:19:14.440: INFO: Trying to get logs from node k8s-3 pod pod-06a4cf95-2736-4475-9ac2-2b4994c142d8 container test-container: <nil>
STEP: delete the pod
May 20 13:19:14.455: INFO: Waiting for pod pod-06a4cf95-2736-4475-9ac2-2b4994c142d8 to disappear
May 20 13:19:14.457: INFO: Pod pod-06a4cf95-2736-4475-9ac2-2b4994c142d8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:19:14.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6432" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:19:14.464: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
May 20 13:19:14.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-9145 api-versions'
May 20 13:19:14.559: INFO: stderr: ""
May 20 13:19:14.559: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:19:14.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9145" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":108,"skipped":1741,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:19:14.566: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:19:14.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1774" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":109,"skipped":1749,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:19:14.606: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 20 13:19:14.663: INFO: Number of nodes with available pods: 0
May 20 13:19:14.663: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:15.674: INFO: Number of nodes with available pods: 0
May 20 13:19:15.674: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:16.670: INFO: Number of nodes with available pods: 0
May 20 13:19:16.670: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:17.670: INFO: Number of nodes with available pods: 3
May 20 13:19:17.670: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 20 13:19:17.686: INFO: Number of nodes with available pods: 2
May 20 13:19:17.686: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:18.692: INFO: Number of nodes with available pods: 2
May 20 13:19:18.692: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:19.694: INFO: Number of nodes with available pods: 2
May 20 13:19:19.694: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:20.695: INFO: Number of nodes with available pods: 2
May 20 13:19:20.695: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:21.694: INFO: Number of nodes with available pods: 2
May 20 13:19:21.695: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:22.696: INFO: Number of nodes with available pods: 2
May 20 13:19:22.696: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:23.711: INFO: Number of nodes with available pods: 2
May 20 13:19:23.711: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:24.694: INFO: Number of nodes with available pods: 2
May 20 13:19:24.694: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:19:25.765: INFO: Number of nodes with available pods: 3
May 20 13:19:25.765: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4589, will wait for the garbage collector to delete the pods
May 20 13:19:25.863: INFO: Deleting DaemonSet.extensions daemon-set took: 7.407209ms
May 20 13:19:26.264: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.474238ms
May 20 13:19:35.166: INFO: Number of nodes with available pods: 0
May 20 13:19:35.167: INFO: Number of running nodes: 0, number of available pods: 0
May 20 13:19:35.169: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4589/daemonsets","resourceVersion":"13206"},"items":null}

May 20 13:19:35.171: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4589/pods","resourceVersion":"13206"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:19:35.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4589" for this suite.

• [SLOW TEST:20.586 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":110,"skipped":1760,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:19:35.197: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:19:41.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7239" for this suite.
STEP: Destroying namespace "nsdeletetest-4139" for this suite.
May 20 13:19:41.423: INFO: Namespace nsdeletetest-4139 was already deleted
STEP: Destroying namespace "nsdeletetest-7221" for this suite.

• [SLOW TEST:6.231 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":111,"skipped":1766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:19:41.434: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0520 13:19:42.516199      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 20 13:20:44.529: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:20:44.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1289" for this suite.

• [SLOW TEST:63.102 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":112,"skipped":1794,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:20:44.536: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3039.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3039.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3039.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3039.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3039.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3039.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 20 13:20:46.601: INFO: DNS probes using dns-3039/dns-test-051d97ad-59a2-45b3-99e4-665a6a03c38d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:20:46.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3039" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":113,"skipped":1810,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:20:46.620: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7896
May 20 13:20:48.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 20 13:20:48.903: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 20 13:20:48.903: INFO: stdout: "ipvs"
May 20 13:20:48.903: INFO: proxyMode: ipvs
May 20 13:20:48.908: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 20 13:20:48.913: INFO: Pod kube-proxy-mode-detector still exists
May 20 13:20:50.914: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 20 13:20:50.916: INFO: Pod kube-proxy-mode-detector still exists
May 20 13:20:52.914: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 20 13:20:52.917: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7896
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7896
I0520 13:20:52.934843      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7896, replica count: 3
I0520 13:20:55.986386      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 13:20:55.996: INFO: Creating new exec pod
May 20 13:20:59.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec execpod-affinity9gg9p -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
May 20 13:20:59.259: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 20 13:20:59.259: INFO: stdout: ""
May 20 13:20:59.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec execpod-affinity9gg9p -- /bin/sh -x -c nc -zv -t -w 2 10.233.59.56 80'
May 20 13:20:59.455: INFO: stderr: "+ nc -zv -t -w 2 10.233.59.56 80\nConnection to 10.233.59.56 80 port [tcp/http] succeeded!\n"
May 20 13:20:59.456: INFO: stdout: ""
May 20 13:20:59.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec execpod-affinity9gg9p -- /bin/sh -x -c nc -zv -t -w 2 172.18.8.103 31749'
May 20 13:20:59.636: INFO: stderr: "+ nc -zv -t -w 2 172.18.8.103 31749\nConnection to 172.18.8.103 31749 port [tcp/31749] succeeded!\n"
May 20 13:20:59.636: INFO: stdout: ""
May 20 13:20:59.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec execpod-affinity9gg9p -- /bin/sh -x -c nc -zv -t -w 2 172.18.8.101 31749'
May 20 13:20:59.827: INFO: stderr: "+ nc -zv -t -w 2 172.18.8.101 31749\nConnection to 172.18.8.101 31749 port [tcp/31749] succeeded!\n"
May 20 13:20:59.827: INFO: stdout: ""
May 20 13:20:59.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec execpod-affinity9gg9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.8.101:31749/ ; done'
May 20 13:21:00.092: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n"
May 20 13:21:00.092: INFO: stdout: "\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6\naffinity-nodeport-timeout-d8bn6"
May 20 13:21:00.092: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.092: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.092: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.092: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.092: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.092: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.092: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.093: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.093: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.093: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.093: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.093: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.093: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.093: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.093: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.093: INFO: Received response from host: affinity-nodeport-timeout-d8bn6
May 20 13:21:00.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec execpod-affinity9gg9p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.18.8.101:31749/'
May 20 13:21:00.273: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n"
May 20 13:21:00.273: INFO: stdout: "affinity-nodeport-timeout-d8bn6"
May 20 13:23:05.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec execpod-affinity9gg9p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.18.8.101:31749/'
May 20 13:23:06.016: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n"
May 20 13:23:06.016: INFO: stdout: "affinity-nodeport-timeout-d8bn6"
May 20 13:25:11.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec execpod-affinity9gg9p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.18.8.101:31749/'
May 20 13:25:11.208: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n"
May 20 13:25:11.208: INFO: stdout: "affinity-nodeport-timeout-d8bn6"
May 20 13:27:16.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec execpod-affinity9gg9p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.18.8.101:31749/'
May 20 13:27:16.503: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n"
May 20 13:27:16.503: INFO: stdout: "affinity-nodeport-timeout-d8bn6"
May 20 13:29:21.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7896 exec execpod-affinity9gg9p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.18.8.101:31749/'
May 20 13:29:21.735: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.18.8.101:31749/\n"
May 20 13:29:21.735: INFO: stdout: "affinity-nodeport-timeout-pdg9n"
May 20 13:29:21.735: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7896, will wait for the garbage collector to delete the pods
May 20 13:29:21.811: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.915276ms
May 20 13:29:22.212: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 400.491731ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:29:35.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7896" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:528.839 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":114,"skipped":1812,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:29:35.463: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-77bb1b36-ca3d-478f-bc0c-89bff09a296c
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:29:35.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5946" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":115,"skipped":1826,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:29:35.521: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
May 20 13:29:35.549: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
May 20 13:29:36.410: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 20 13:29:38.458: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:29:40.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:29:42.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:29:44.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:29:46.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:29:48.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:29:50.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:29:52.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:29:54.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:29:56.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:29:58.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:30:00.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114176, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:30:03.192: INFO: Waited 723.317855ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:03.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2452" for this suite.

• [SLOW TEST:28.330 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":116,"skipped":1834,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:03.851: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-a6b8fc1b-ad90-47db-b115-81d5bf0484a0
STEP: Creating a pod to test consume secrets
May 20 13:30:04.053: INFO: Waiting up to 5m0s for pod "pod-secrets-e9b3c467-459b-4afa-be1e-0da6e2899498" in namespace "secrets-4649" to be "Succeeded or Failed"
May 20 13:30:04.058: INFO: Pod "pod-secrets-e9b3c467-459b-4afa-be1e-0da6e2899498": Phase="Pending", Reason="", readiness=false. Elapsed: 4.935891ms
May 20 13:30:06.061: INFO: Pod "pod-secrets-e9b3c467-459b-4afa-be1e-0da6e2899498": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008771774s
May 20 13:30:08.065: INFO: Pod "pod-secrets-e9b3c467-459b-4afa-be1e-0da6e2899498": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011978224s
STEP: Saw pod success
May 20 13:30:08.065: INFO: Pod "pod-secrets-e9b3c467-459b-4afa-be1e-0da6e2899498" satisfied condition "Succeeded or Failed"
May 20 13:30:08.067: INFO: Trying to get logs from node k8s-3 pod pod-secrets-e9b3c467-459b-4afa-be1e-0da6e2899498 container secret-volume-test: <nil>
STEP: delete the pod
May 20 13:30:08.089: INFO: Waiting for pod pod-secrets-e9b3c467-459b-4afa-be1e-0da6e2899498 to disappear
May 20 13:30:08.092: INFO: Pod pod-secrets-e9b3c467-459b-4afa-be1e-0da6e2899498 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:08.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4649" for this suite.
STEP: Destroying namespace "secret-namespace-8605" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":117,"skipped":1849,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:08.105: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-b06320f0-dbe9-4645-b21f-8150973b1dbc
STEP: Creating a pod to test consume configMaps
May 20 13:30:08.139: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0d9102a9-1ca7-448e-bc8d-7d219e2dc035" in namespace "projected-2012" to be "Succeeded or Failed"
May 20 13:30:08.146: INFO: Pod "pod-projected-configmaps-0d9102a9-1ca7-448e-bc8d-7d219e2dc035": Phase="Pending", Reason="", readiness=false. Elapsed: 6.871105ms
May 20 13:30:10.153: INFO: Pod "pod-projected-configmaps-0d9102a9-1ca7-448e-bc8d-7d219e2dc035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014274816s
May 20 13:30:12.158: INFO: Pod "pod-projected-configmaps-0d9102a9-1ca7-448e-bc8d-7d219e2dc035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018844442s
STEP: Saw pod success
May 20 13:30:12.158: INFO: Pod "pod-projected-configmaps-0d9102a9-1ca7-448e-bc8d-7d219e2dc035" satisfied condition "Succeeded or Failed"
May 20 13:30:12.160: INFO: Trying to get logs from node k8s-3 pod pod-projected-configmaps-0d9102a9-1ca7-448e-bc8d-7d219e2dc035 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 20 13:30:12.175: INFO: Waiting for pod pod-projected-configmaps-0d9102a9-1ca7-448e-bc8d-7d219e2dc035 to disappear
May 20 13:30:12.178: INFO: Pod pod-projected-configmaps-0d9102a9-1ca7-448e-bc8d-7d219e2dc035 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:12.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2012" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":1849,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:12.189: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 20 13:30:12.220: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:19.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8628" for this suite.

• [SLOW TEST:7.252 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":119,"skipped":1861,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:19.442: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-8984b776-649a-48ea-bb69-c19ea9d2886b
STEP: Creating a pod to test consume configMaps
May 20 13:30:19.476: INFO: Waiting up to 5m0s for pod "pod-configmaps-95e54a29-807d-40c3-9bc6-0f4a3ce7cf5b" in namespace "configmap-3069" to be "Succeeded or Failed"
May 20 13:30:19.478: INFO: Pod "pod-configmaps-95e54a29-807d-40c3-9bc6-0f4a3ce7cf5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242827ms
May 20 13:30:21.519: INFO: Pod "pod-configmaps-95e54a29-807d-40c3-9bc6-0f4a3ce7cf5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043168533s
STEP: Saw pod success
May 20 13:30:21.519: INFO: Pod "pod-configmaps-95e54a29-807d-40c3-9bc6-0f4a3ce7cf5b" satisfied condition "Succeeded or Failed"
May 20 13:30:21.522: INFO: Trying to get logs from node k8s-3 pod pod-configmaps-95e54a29-807d-40c3-9bc6-0f4a3ce7cf5b container configmap-volume-test: <nil>
STEP: delete the pod
May 20 13:30:21.540: INFO: Waiting for pod pod-configmaps-95e54a29-807d-40c3-9bc6-0f4a3ce7cf5b to disappear
May 20 13:30:21.543: INFO: Pod pod-configmaps-95e54a29-807d-40c3-9bc6-0f4a3ce7cf5b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:21.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3069" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":120,"skipped":1881,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:21.555: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-aee0d3e1-2689-4241-91e7-0f4ed24b4d14
STEP: Creating a pod to test consume secrets
May 20 13:30:21.600: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0d4b3454-4330-4266-9f52-8c8ebbbe2209" in namespace "projected-961" to be "Succeeded or Failed"
May 20 13:30:21.605: INFO: Pod "pod-projected-secrets-0d4b3454-4330-4266-9f52-8c8ebbbe2209": Phase="Pending", Reason="", readiness=false. Elapsed: 4.652183ms
May 20 13:30:23.607: INFO: Pod "pod-projected-secrets-0d4b3454-4330-4266-9f52-8c8ebbbe2209": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007318659s
STEP: Saw pod success
May 20 13:30:23.608: INFO: Pod "pod-projected-secrets-0d4b3454-4330-4266-9f52-8c8ebbbe2209" satisfied condition "Succeeded or Failed"
May 20 13:30:23.612: INFO: Trying to get logs from node k8s-3 pod pod-projected-secrets-0d4b3454-4330-4266-9f52-8c8ebbbe2209 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 20 13:30:23.627: INFO: Waiting for pod pod-projected-secrets-0d4b3454-4330-4266-9f52-8c8ebbbe2209 to disappear
May 20 13:30:23.630: INFO: Pod pod-projected-secrets-0d4b3454-4330-4266-9f52-8c8ebbbe2209 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:23.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-961" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":1895,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:23.642: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-7662/configmap-test-d4173014-fe94-4cca-8d6e-f55b02e27099
STEP: Creating a pod to test consume configMaps
May 20 13:30:23.691: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed24664b-8990-4b23-b61e-f25590a07cad" in namespace "configmap-7662" to be "Succeeded or Failed"
May 20 13:30:23.696: INFO: Pod "pod-configmaps-ed24664b-8990-4b23-b61e-f25590a07cad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.751668ms
May 20 13:30:25.719: INFO: Pod "pod-configmaps-ed24664b-8990-4b23-b61e-f25590a07cad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028458562s
May 20 13:30:27.723: INFO: Pod "pod-configmaps-ed24664b-8990-4b23-b61e-f25590a07cad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032360438s
STEP: Saw pod success
May 20 13:30:27.724: INFO: Pod "pod-configmaps-ed24664b-8990-4b23-b61e-f25590a07cad" satisfied condition "Succeeded or Failed"
May 20 13:30:27.727: INFO: Trying to get logs from node k8s-3 pod pod-configmaps-ed24664b-8990-4b23-b61e-f25590a07cad container env-test: <nil>
STEP: delete the pod
May 20 13:30:27.746: INFO: Waiting for pod pod-configmaps-ed24664b-8990-4b23-b61e-f25590a07cad to disappear
May 20 13:30:27.749: INFO: Pod pod-configmaps-ed24664b-8990-4b23-b61e-f25590a07cad no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7662" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":122,"skipped":1908,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:27.755: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1385
STEP: creating an pod
May 20 13:30:27.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7032 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 20 13:30:27.867: INFO: stderr: ""
May 20 13:30:27.867: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
May 20 13:30:27.867: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 20 13:30:27.867: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7032" to be "running and ready, or succeeded"
May 20 13:30:27.870: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.666921ms
May 20 13:30:29.877: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010208233s
May 20 13:30:31.881: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.013862821s
May 20 13:30:31.881: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 20 13:30:31.881: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 20 13:30:31.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7032 logs logs-generator logs-generator'
May 20 13:30:31.974: INFO: stderr: ""
May 20 13:30:31.974: INFO: stdout: "I0520 13:30:29.508328       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/pwv 224\nI0520 13:30:29.725781       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/h6gv 450\nI0520 13:30:29.908363       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/rlz 474\nI0520 13:30:30.109336       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/2m65 499\nI0520 13:30:30.309446       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/5rdk 544\nI0520 13:30:30.511949       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/4wt 218\nI0520 13:30:30.747483       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/jmqw 368\nI0520 13:30:30.908508       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/7npq 561\nI0520 13:30:31.109292       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/c4rj 465\nI0520 13:30:31.309439       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/kqpg 323\nI0520 13:30:31.518080       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/d28j 550\nI0520 13:30:31.712760       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/jr45 385\nI0520 13:30:31.910209       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/dw7v 375\n"
STEP: limiting log lines
May 20 13:30:31.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7032 logs logs-generator logs-generator --tail=1'
May 20 13:30:32.061: INFO: stderr: ""
May 20 13:30:32.061: INFO: stdout: "I0520 13:30:31.910209       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/dw7v 375\n"
May 20 13:30:32.061: INFO: got output "I0520 13:30:31.910209       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/dw7v 375\n"
STEP: limiting log bytes
May 20 13:30:32.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7032 logs logs-generator logs-generator --limit-bytes=1'
May 20 13:30:32.164: INFO: stderr: ""
May 20 13:30:32.164: INFO: stdout: "I"
May 20 13:30:32.164: INFO: got output "I"
STEP: exposing timestamps
May 20 13:30:32.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7032 logs logs-generator logs-generator --tail=1 --timestamps'
May 20 13:30:32.249: INFO: stderr: ""
May 20 13:30:32.249: INFO: stdout: "2021-05-20T13:30:32.108854176Z I0520 13:30:32.108529       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/dvvn 569\n"
May 20 13:30:32.249: INFO: got output "2021-05-20T13:30:32.108854176Z I0520 13:30:32.108529       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/dvvn 569\n"
STEP: restricting to a time range
May 20 13:30:34.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7032 logs logs-generator logs-generator --since=1s'
May 20 13:30:34.841: INFO: stderr: ""
May 20 13:30:34.841: INFO: stdout: "I0520 13:30:33.908491       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/cmtl 291\nI0520 13:30:34.114240       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/6q4 245\nI0520 13:30:34.308598       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/27z 446\nI0520 13:30:34.508604       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/77gs 494\nI0520 13:30:34.708573       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/5tf 575\n"
May 20 13:30:34.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7032 logs logs-generator logs-generator --since=24h'
May 20 13:30:34.937: INFO: stderr: ""
May 20 13:30:34.937: INFO: stdout: "I0520 13:30:29.508328       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/pwv 224\nI0520 13:30:29.725781       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/h6gv 450\nI0520 13:30:29.908363       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/rlz 474\nI0520 13:30:30.109336       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/2m65 499\nI0520 13:30:30.309446       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/5rdk 544\nI0520 13:30:30.511949       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/4wt 218\nI0520 13:30:30.747483       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/jmqw 368\nI0520 13:30:30.908508       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/7npq 561\nI0520 13:30:31.109292       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/c4rj 465\nI0520 13:30:31.309439       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/kqpg 323\nI0520 13:30:31.518080       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/d28j 550\nI0520 13:30:31.712760       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/jr45 385\nI0520 13:30:31.910209       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/dw7v 375\nI0520 13:30:32.108529       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/dvvn 569\nI0520 13:30:32.308969       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/fq7 597\nI0520 13:30:32.509167       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/wxvw 259\nI0520 13:30:32.708806       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/lvc 546\nI0520 13:30:32.956021       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/24n 216\nI0520 13:30:33.117743       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/l2v 554\nI0520 13:30:33.314971       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/mjs7 435\nI0520 13:30:33.508759       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/86c 387\nI0520 13:30:33.709028       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/tj4t 343\nI0520 13:30:33.908491       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/cmtl 291\nI0520 13:30:34.114240       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/6q4 245\nI0520 13:30:34.308598       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/27z 446\nI0520 13:30:34.508604       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/77gs 494\nI0520 13:30:34.708573       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/5tf 575\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
May 20 13:30:34.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7032 delete pod logs-generator'
May 20 13:30:45.399: INFO: stderr: ""
May 20 13:30:45.399: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:45.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7032" for this suite.

• [SLOW TEST:17.651 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":123,"skipped":1913,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:45.407: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
May 20 13:30:45.439: INFO: Waiting up to 5m0s for pod "pod-7e909f50-2d0f-4ce7-a08f-38455d3eb29c" in namespace "emptydir-5767" to be "Succeeded or Failed"
May 20 13:30:45.442: INFO: Pod "pod-7e909f50-2d0f-4ce7-a08f-38455d3eb29c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.251253ms
May 20 13:30:47.445: INFO: Pod "pod-7e909f50-2d0f-4ce7-a08f-38455d3eb29c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006826404s
May 20 13:30:49.454: INFO: Pod "pod-7e909f50-2d0f-4ce7-a08f-38455d3eb29c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015455233s
STEP: Saw pod success
May 20 13:30:49.454: INFO: Pod "pod-7e909f50-2d0f-4ce7-a08f-38455d3eb29c" satisfied condition "Succeeded or Failed"
May 20 13:30:49.456: INFO: Trying to get logs from node k8s-3 pod pod-7e909f50-2d0f-4ce7-a08f-38455d3eb29c container test-container: <nil>
STEP: delete the pod
May 20 13:30:49.472: INFO: Waiting for pod pod-7e909f50-2d0f-4ce7-a08f-38455d3eb29c to disappear
May 20 13:30:49.475: INFO: Pod pod-7e909f50-2d0f-4ce7-a08f-38455d3eb29c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:49.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5767" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":124,"skipped":1971,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:49.484: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
May 20 13:30:49.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-9824 create -f -'
May 20 13:30:49.751: INFO: stderr: ""
May 20 13:30:49.751: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 20 13:30:50.758: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 13:30:50.758: INFO: Found 0 / 1
May 20 13:30:51.762: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 13:30:51.762: INFO: Found 1 / 1
May 20 13:30:51.762: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 20 13:30:51.764: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 13:30:51.764: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 20 13:30:51.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-9824 patch pod agnhost-primary-2gspw -p {"metadata":{"annotations":{"x":"y"}}}'
May 20 13:30:51.949: INFO: stderr: ""
May 20 13:30:51.949: INFO: stdout: "pod/agnhost-primary-2gspw patched\n"
STEP: checking annotations
May 20 13:30:51.952: INFO: Selector matched 1 pods for map[app:agnhost]
May 20 13:30:51.952: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:51.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9824" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":125,"skipped":1971,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:51.961: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:30:52.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2dfd769e-c354-4f47-bd1c-467f5286e376" in namespace "downward-api-8147" to be "Succeeded or Failed"
May 20 13:30:52.028: INFO: Pod "downwardapi-volume-2dfd769e-c354-4f47-bd1c-467f5286e376": Phase="Pending", Reason="", readiness=false. Elapsed: 13.780312ms
May 20 13:30:54.031: INFO: Pod "downwardapi-volume-2dfd769e-c354-4f47-bd1c-467f5286e376": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016718015s
May 20 13:30:56.034: INFO: Pod "downwardapi-volume-2dfd769e-c354-4f47-bd1c-467f5286e376": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019716621s
STEP: Saw pod success
May 20 13:30:56.034: INFO: Pod "downwardapi-volume-2dfd769e-c354-4f47-bd1c-467f5286e376" satisfied condition "Succeeded or Failed"
May 20 13:30:56.036: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-2dfd769e-c354-4f47-bd1c-467f5286e376 container client-container: <nil>
STEP: delete the pod
May 20 13:30:56.054: INFO: Waiting for pod downwardapi-volume-2dfd769e-c354-4f47-bd1c-467f5286e376 to disappear
May 20 13:30:56.057: INFO: Pod downwardapi-volume-2dfd769e-c354-4f47-bd1c-467f5286e376 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:56.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8147" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":126,"skipped":1974,"failed":0}
SS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:56.065: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
May 20 13:30:56.098: INFO: Major version: 1
STEP: Confirm minor version
May 20 13:30:56.098: INFO: cleanMinorVersion: 19
May 20 13:30:56.098: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:30:56.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9549" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":127,"skipped":1976,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:30:56.109: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 20 13:30:56.437: INFO: Pod name wrapped-volume-race-efb2c170-75d3-4d88-9392-8e03d7a17100: Found 1 pods out of 5
May 20 13:31:01.456: INFO: Pod name wrapped-volume-race-efb2c170-75d3-4d88-9392-8e03d7a17100: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-efb2c170-75d3-4d88-9392-8e03d7a17100 in namespace emptydir-wrapper-3084, will wait for the garbage collector to delete the pods
May 20 13:31:15.650: INFO: Deleting ReplicationController wrapped-volume-race-efb2c170-75d3-4d88-9392-8e03d7a17100 took: 10.856478ms
May 20 13:31:16.150: INFO: Terminating ReplicationController wrapped-volume-race-efb2c170-75d3-4d88-9392-8e03d7a17100 pods took: 500.264912ms
STEP: Creating RC which spawns configmap-volume pods
May 20 13:31:24.073: INFO: Pod name wrapped-volume-race-f4989cec-fb6f-4092-8d43-0079cc6ec5e1: Found 0 pods out of 5
May 20 13:31:29.080: INFO: Pod name wrapped-volume-race-f4989cec-fb6f-4092-8d43-0079cc6ec5e1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f4989cec-fb6f-4092-8d43-0079cc6ec5e1 in namespace emptydir-wrapper-3084, will wait for the garbage collector to delete the pods
May 20 13:31:39.167: INFO: Deleting ReplicationController wrapped-volume-race-f4989cec-fb6f-4092-8d43-0079cc6ec5e1 took: 8.48506ms
May 20 13:31:39.568: INFO: Terminating ReplicationController wrapped-volume-race-f4989cec-fb6f-4092-8d43-0079cc6ec5e1 pods took: 400.281755ms
STEP: Creating RC which spawns configmap-volume pods
May 20 13:31:55.489: INFO: Pod name wrapped-volume-race-bfb34d3f-8803-4357-965a-a212e89a962f: Found 0 pods out of 5
May 20 13:32:00.496: INFO: Pod name wrapped-volume-race-bfb34d3f-8803-4357-965a-a212e89a962f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bfb34d3f-8803-4357-965a-a212e89a962f in namespace emptydir-wrapper-3084, will wait for the garbage collector to delete the pods
May 20 13:32:12.637: INFO: Deleting ReplicationController wrapped-volume-race-bfb34d3f-8803-4357-965a-a212e89a962f took: 5.319114ms
May 20 13:32:13.160: INFO: Terminating ReplicationController wrapped-volume-race-bfb34d3f-8803-4357-965a-a212e89a962f pods took: 523.689407ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:32:25.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3084" for this suite.

• [SLOW TEST:89.729 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":128,"skipped":1987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:32:25.839: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 20 13:32:25.878: INFO: Waiting up to 5m0s for pod "downward-api-43279f15-5258-49d9-9c47-5b3afac853d5" in namespace "downward-api-4312" to be "Succeeded or Failed"
May 20 13:32:25.883: INFO: Pod "downward-api-43279f15-5258-49d9-9c47-5b3afac853d5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.291957ms
May 20 13:32:27.886: INFO: Pod "downward-api-43279f15-5258-49d9-9c47-5b3afac853d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008269658s
May 20 13:32:29.889: INFO: Pod "downward-api-43279f15-5258-49d9-9c47-5b3afac853d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011468502s
STEP: Saw pod success
May 20 13:32:29.889: INFO: Pod "downward-api-43279f15-5258-49d9-9c47-5b3afac853d5" satisfied condition "Succeeded or Failed"
May 20 13:32:29.892: INFO: Trying to get logs from node k8s-3 pod downward-api-43279f15-5258-49d9-9c47-5b3afac853d5 container dapi-container: <nil>
STEP: delete the pod
May 20 13:32:29.916: INFO: Waiting for pod downward-api-43279f15-5258-49d9-9c47-5b3afac853d5 to disappear
May 20 13:32:29.920: INFO: Pod downward-api-43279f15-5258-49d9-9c47-5b3afac853d5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:32:29.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4312" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":129,"skipped":2027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:32:29.934: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 20 13:32:29.982: INFO: Waiting up to 1m0s for all nodes to be ready
May 20 13:33:30.022: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
May 20 13:33:30.040: INFO: Created pod: pod0-sched-preemption-low-priority
May 20 13:33:30.065: INFO: Created pod: pod1-sched-preemption-medium-priority
May 20 13:33:30.094: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:33:46.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7142" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:76.304 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":130,"skipped":2049,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:33:46.242: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:33:46.269: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 20 13:33:46.283: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 20 13:33:50.294: INFO: Creating deployment "test-rolling-update-deployment"
May 20 13:33:50.300: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 20 13:33:50.323: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
May 20 13:33:52.331: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 20 13:33:52.336: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114430, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114430, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114430, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114430, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:33:54.340: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 20 13:33:54.348: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-938 /apis/apps/v1/namespaces/deployment-938/deployments/test-rolling-update-deployment a8dbdebf-ed3f-4497-9cfc-b51c8e4a7e8f 17241 1 2021-05-20 13:33:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-20 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-20 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059b5c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-20 13:33:50 +0000 UTC,LastTransitionTime:2021-05-20 13:33:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-05-20 13:33:52 +0000 UTC,LastTransitionTime:2021-05-20 13:33:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 20 13:33:54.350: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-938 /apis/apps/v1/namespaces/deployment-938/replicasets/test-rolling-update-deployment-c4cb8d6d9 ff0e3b8f-f7f1-4fb3-959c-676a342faf57 17230 1 2021-05-20 13:33:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a8dbdebf-ed3f-4497-9cfc-b51c8e4a7e8f 0xc0046e4190 0xc0046e4191}] []  [{kube-controller-manager Update apps/v1 2021-05-20 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8dbdebf-ed3f-4497-9cfc-b51c8e4a7e8f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046e4208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 20 13:33:54.350: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 20 13:33:54.350: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-938 /apis/apps/v1/namespaces/deployment-938/replicasets/test-rolling-update-controller a03ec597-d426-41c1-a956-2d3dea320949 17240 2 2021-05-20 13:33:46 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a8dbdebf-ed3f-4497-9cfc-b51c8e4a7e8f 0xc0046e4087 0xc0046e4088}] []  [{e2e.test Update apps/v1 2021-05-20 13:33:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-20 13:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8dbdebf-ed3f-4497-9cfc-b51c8e4a7e8f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046e4128 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 20 13:33:54.353: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-78mck" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-78mck test-rolling-update-deployment-c4cb8d6d9- deployment-938 /api/v1/namespaces/deployment-938/pods/test-rolling-update-deployment-c4cb8d6d9-78mck 38e2b3b1-c10e-4196-93d6-8079898ffca1 17229 0 2021-05-20 13:33:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 ff0e3b8f-f7f1-4fb3-959c-676a342faf57 0xc0046e46d0 0xc0046e46d1}] []  [{kube-controller-manager Update v1 2021-05-20 13:33:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff0e3b8f-f7f1-4fb3-959c-676a342faf57\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 13:33:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-62fk2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-62fk2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-62fk2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:33:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:10.233.66.156,StartTime:2021-05-20 13:33:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 13:33:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://e8f315946fec347142a011ba6731f1642529f914a23eb6076c1b4ffb51abc21e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:33:54.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-938" for this suite.

• [SLOW TEST:8.119 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":131,"skipped":2071,"failed":0}
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:33:54.361: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-884
May 20 13:33:56.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-884 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 20 13:33:57.256: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 20 13:33:57.256: INFO: stdout: "ipvs"
May 20 13:33:57.256: INFO: proxyMode: ipvs
May 20 13:33:57.266: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 20 13:33:57.271: INFO: Pod kube-proxy-mode-detector still exists
May 20 13:33:59.271: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 20 13:33:59.277: INFO: Pod kube-proxy-mode-detector still exists
May 20 13:34:01.271: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 20 13:34:01.275: INFO: Pod kube-proxy-mode-detector still exists
May 20 13:34:03.273: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 20 13:34:03.277: INFO: Pod kube-proxy-mode-detector still exists
May 20 13:34:05.273: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 20 13:34:05.275: INFO: Pod kube-proxy-mode-detector still exists
May 20 13:34:07.271: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 20 13:34:07.274: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-884
STEP: creating replication controller affinity-clusterip-timeout in namespace services-884
I0520 13:34:07.294160      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-884, replica count: 3
I0520 13:34:10.344937      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 13:34:10.350: INFO: Creating new exec pod
May 20 13:34:15.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-884 exec execpod-affinity2485n -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
May 20 13:34:15.565: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 20 13:34:15.565: INFO: stdout: ""
May 20 13:34:15.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-884 exec execpod-affinity2485n -- /bin/sh -x -c nc -zv -t -w 2 10.233.47.169 80'
May 20 13:34:15.766: INFO: stderr: "+ nc -zv -t -w 2 10.233.47.169 80\nConnection to 10.233.47.169 80 port [tcp/http] succeeded!\n"
May 20 13:34:15.766: INFO: stdout: ""
May 20 13:34:15.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-884 exec execpod-affinity2485n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.47.169:80/ ; done'
May 20 13:34:16.106: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n"
May 20 13:34:16.106: INFO: stdout: "\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc\naffinity-clusterip-timeout-cdcvc"
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Received response from host: affinity-clusterip-timeout-cdcvc
May 20 13:34:16.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-884 exec execpod-affinity2485n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.47.169:80/'
May 20 13:34:16.315: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n"
May 20 13:34:16.315: INFO: stdout: "affinity-clusterip-timeout-cdcvc"
May 20 13:36:21.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-884 exec execpod-affinity2485n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.47.169:80/'
May 20 13:36:21.608: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n"
May 20 13:36:21.608: INFO: stdout: "affinity-clusterip-timeout-cdcvc"
May 20 13:38:26.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-884 exec execpod-affinity2485n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.47.169:80/'
May 20 13:38:26.813: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n"
May 20 13:38:26.813: INFO: stdout: "affinity-clusterip-timeout-cdcvc"
May 20 13:40:31.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-884 exec execpod-affinity2485n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.47.169:80/'
May 20 13:40:32.429: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.47.169:80/\n"
May 20 13:40:32.429: INFO: stdout: "affinity-clusterip-timeout-q99xk"
May 20 13:40:32.429: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-884, will wait for the garbage collector to delete the pods
May 20 13:40:32.518: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.08861ms
May 20 13:40:32.918: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 400.46033ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:40:45.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-884" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:411.212 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":132,"skipped":2071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:40:45.573: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:40:45.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ef35dab-e1ff-4b8b-b1b3-ddde27a764da" in namespace "projected-2516" to be "Succeeded or Failed"
May 20 13:40:45.635: INFO: Pod "downwardapi-volume-0ef35dab-e1ff-4b8b-b1b3-ddde27a764da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.826309ms
May 20 13:40:47.638: INFO: Pod "downwardapi-volume-0ef35dab-e1ff-4b8b-b1b3-ddde27a764da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009323917s
May 20 13:40:49.642: INFO: Pod "downwardapi-volume-0ef35dab-e1ff-4b8b-b1b3-ddde27a764da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013205332s
STEP: Saw pod success
May 20 13:40:49.642: INFO: Pod "downwardapi-volume-0ef35dab-e1ff-4b8b-b1b3-ddde27a764da" satisfied condition "Succeeded or Failed"
May 20 13:40:49.645: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-0ef35dab-e1ff-4b8b-b1b3-ddde27a764da container client-container: <nil>
STEP: delete the pod
May 20 13:40:49.700: INFO: Waiting for pod downwardapi-volume-0ef35dab-e1ff-4b8b-b1b3-ddde27a764da to disappear
May 20 13:40:49.706: INFO: Pod downwardapi-volume-0ef35dab-e1ff-4b8b-b1b3-ddde27a764da no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:40:49.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2516" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2113,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:40:49.731: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:40:49.782: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 20 13:40:53.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-2746 --namespace=crd-publish-openapi-2746 create -f -'
May 20 13:40:54.396: INFO: stderr: ""
May 20 13:40:54.396: INFO: stdout: "e2e-test-crd-publish-openapi-7045-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 20 13:40:54.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-2746 --namespace=crd-publish-openapi-2746 delete e2e-test-crd-publish-openapi-7045-crds test-cr'
May 20 13:40:54.498: INFO: stderr: ""
May 20 13:40:54.498: INFO: stdout: "e2e-test-crd-publish-openapi-7045-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 20 13:40:54.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-2746 --namespace=crd-publish-openapi-2746 apply -f -'
May 20 13:40:54.791: INFO: stderr: ""
May 20 13:40:54.791: INFO: stdout: "e2e-test-crd-publish-openapi-7045-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 20 13:40:54.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-2746 --namespace=crd-publish-openapi-2746 delete e2e-test-crd-publish-openapi-7045-crds test-cr'
May 20 13:40:55.012: INFO: stderr: ""
May 20 13:40:55.012: INFO: stdout: "e2e-test-crd-publish-openapi-7045-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 20 13:40:55.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-2746 explain e2e-test-crd-publish-openapi-7045-crds'
May 20 13:40:55.202: INFO: stderr: ""
May 20 13:40:55.202: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7045-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:40:58.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2746" for this suite.

• [SLOW TEST:8.817 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":134,"skipped":2120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:40:58.548: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:40:58.594: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da41a50c-1e90-4496-896f-7a4566069e67" in namespace "downward-api-3377" to be "Succeeded or Failed"
May 20 13:40:58.597: INFO: Pod "downwardapi-volume-da41a50c-1e90-4496-896f-7a4566069e67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.510585ms
May 20 13:41:00.602: INFO: Pod "downwardapi-volume-da41a50c-1e90-4496-896f-7a4566069e67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007663137s
May 20 13:41:02.607: INFO: Pod "downwardapi-volume-da41a50c-1e90-4496-896f-7a4566069e67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01325923s
STEP: Saw pod success
May 20 13:41:02.608: INFO: Pod "downwardapi-volume-da41a50c-1e90-4496-896f-7a4566069e67" satisfied condition "Succeeded or Failed"
May 20 13:41:02.611: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-da41a50c-1e90-4496-896f-7a4566069e67 container client-container: <nil>
STEP: delete the pod
May 20 13:41:02.635: INFO: Waiting for pod downwardapi-volume-da41a50c-1e90-4496-896f-7a4566069e67 to disappear
May 20 13:41:02.638: INFO: Pod downwardapi-volume-da41a50c-1e90-4496-896f-7a4566069e67 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:41:02.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3377" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2181,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:41:02.656: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-d3e65fc1-3412-402f-9aaa-d4508e0252c6
STEP: Creating a pod to test consume secrets
May 20 13:41:02.711: INFO: Waiting up to 5m0s for pod "pod-secrets-fb5a7163-9c88-4a39-ad8b-a08fc04b0c8f" in namespace "secrets-4562" to be "Succeeded or Failed"
May 20 13:41:02.718: INFO: Pod "pod-secrets-fb5a7163-9c88-4a39-ad8b-a08fc04b0c8f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.604314ms
May 20 13:41:04.722: INFO: Pod "pod-secrets-fb5a7163-9c88-4a39-ad8b-a08fc04b0c8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01070127s
May 20 13:41:06.727: INFO: Pod "pod-secrets-fb5a7163-9c88-4a39-ad8b-a08fc04b0c8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01543926s
STEP: Saw pod success
May 20 13:41:06.727: INFO: Pod "pod-secrets-fb5a7163-9c88-4a39-ad8b-a08fc04b0c8f" satisfied condition "Succeeded or Failed"
May 20 13:41:06.729: INFO: Trying to get logs from node k8s-3 pod pod-secrets-fb5a7163-9c88-4a39-ad8b-a08fc04b0c8f container secret-volume-test: <nil>
STEP: delete the pod
May 20 13:41:06.747: INFO: Waiting for pod pod-secrets-fb5a7163-9c88-4a39-ad8b-a08fc04b0c8f to disappear
May 20 13:41:06.750: INFO: Pod pod-secrets-fb5a7163-9c88-4a39-ad8b-a08fc04b0c8f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:41:06.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4562" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":136,"skipped":2196,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:41:06.760: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:41:11.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2444" for this suite.

• [SLOW TEST:5.059 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":137,"skipped":2196,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:41:11.820: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 20 13:41:14.397: INFO: Successfully updated pod "annotationupdate15b5b64f-1a9c-4706-85cb-9b806142880e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:41:16.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-589" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":138,"skipped":2211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:41:16.427: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
May 20 13:41:16.475: INFO: Waiting up to 5m0s for pod "client-containers-5f25b71f-a52f-4e5e-8b04-4631ab9b696d" in namespace "containers-6093" to be "Succeeded or Failed"
May 20 13:41:16.479: INFO: Pod "client-containers-5f25b71f-a52f-4e5e-8b04-4631ab9b696d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.119261ms
May 20 13:41:18.482: INFO: Pod "client-containers-5f25b71f-a52f-4e5e-8b04-4631ab9b696d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006768078s
STEP: Saw pod success
May 20 13:41:18.482: INFO: Pod "client-containers-5f25b71f-a52f-4e5e-8b04-4631ab9b696d" satisfied condition "Succeeded or Failed"
May 20 13:41:18.485: INFO: Trying to get logs from node k8s-3 pod client-containers-5f25b71f-a52f-4e5e-8b04-4631ab9b696d container test-container: <nil>
STEP: delete the pod
May 20 13:41:18.505: INFO: Waiting for pod client-containers-5f25b71f-a52f-4e5e-8b04-4631ab9b696d to disappear
May 20 13:41:18.509: INFO: Pod client-containers-5f25b71f-a52f-4e5e-8b04-4631ab9b696d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:41:18.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6093" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":139,"skipped":2255,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:41:18.520: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 20 13:41:19.255: INFO: starting watch
STEP: patching
STEP: updating
May 20 13:41:19.265: INFO: waiting for watch events with expected annotations
May 20 13:41:19.265: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:41:19.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7595" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":140,"skipped":2269,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:41:19.319: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2528 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2528;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2528 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2528;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2528.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2528.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2528.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2528.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2528.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2528.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2528.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2528.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2528.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2528.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2528.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2528.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2528.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 243.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.243_udp@PTR;check="$$(dig +tcp +noall +answer +search 243.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.243_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2528 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2528;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2528 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2528;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2528.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2528.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2528.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2528.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2528.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2528.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2528.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2528.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2528.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2528.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2528.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2528.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2528.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 243.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.243_udp@PTR;check="$$(dig +tcp +noall +answer +search 243.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.243_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 20 13:41:23.416: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.426: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.446: INFO: Unable to read wheezy_udp@dns-test-service.dns-2528.svc from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.453: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2528.svc from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.460: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2528.svc from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.467: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2528.svc from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.506: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.514: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.540: INFO: Unable to read jessie_udp@dns-test-service.dns-2528.svc from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.546: INFO: Unable to read jessie_tcp@dns-test-service.dns-2528.svc from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.552: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2528.svc from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.562: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2528.svc from pod dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed: the server could not find the requested resource (get pods dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed)
May 20 13:41:23.592: INFO: Lookups using dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2528.svc wheezy_tcp@dns-test-service.dns-2528.svc wheezy_udp@_http._tcp.dns-test-service.dns-2528.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2528.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2528.svc jessie_tcp@dns-test-service.dns-2528.svc jessie_udp@_http._tcp.dns-test-service.dns-2528.svc jessie_tcp@_http._tcp.dns-test-service.dns-2528.svc]

May 20 13:41:28.718: INFO: DNS probes using dns-2528/dns-test-67951356-bff6-4ff1-b444-ddb0ee5791ed succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:41:28.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2528" for this suite.

• [SLOW TEST:9.501 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":141,"skipped":2282,"failed":0}
SSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:41:28.827: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 20 13:41:28.931: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 20 13:41:28.937: INFO: starting watch
STEP: patching
STEP: updating
May 20 13:41:28.968: INFO: waiting for watch events with expected annotations
May 20 13:41:28.969: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:41:29.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-610" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":142,"skipped":2286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:41:29.043: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 13:41:29.887: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 13:41:31.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114889, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114889, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114889, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757114889, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 13:41:34.919: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:41:47.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2334" for this suite.
STEP: Destroying namespace "webhook-2334-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.068 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":143,"skipped":2321,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:41:47.115: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-9f1fd800-0b78-49c8-83ab-4f3e598e895e
STEP: Creating secret with name secret-projected-all-test-volume-8d76542c-5303-469a-aaef-ce05f25dc743
STEP: Creating a pod to test Check all projections for projected volume plugin
May 20 13:41:47.186: INFO: Waiting up to 5m0s for pod "projected-volume-2ad06ebf-f91e-4bed-bb60-18b2ac78d03c" in namespace "projected-3928" to be "Succeeded or Failed"
May 20 13:41:47.198: INFO: Pod "projected-volume-2ad06ebf-f91e-4bed-bb60-18b2ac78d03c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.026711ms
May 20 13:41:49.201: INFO: Pod "projected-volume-2ad06ebf-f91e-4bed-bb60-18b2ac78d03c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014658091s
May 20 13:41:51.223: INFO: Pod "projected-volume-2ad06ebf-f91e-4bed-bb60-18b2ac78d03c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036458381s
STEP: Saw pod success
May 20 13:41:51.223: INFO: Pod "projected-volume-2ad06ebf-f91e-4bed-bb60-18b2ac78d03c" satisfied condition "Succeeded or Failed"
May 20 13:41:51.226: INFO: Trying to get logs from node k8s-3 pod projected-volume-2ad06ebf-f91e-4bed-bb60-18b2ac78d03c container projected-all-volume-test: <nil>
STEP: delete the pod
May 20 13:41:51.244: INFO: Waiting for pod projected-volume-2ad06ebf-f91e-4bed-bb60-18b2ac78d03c to disappear
May 20 13:41:51.247: INFO: Pod projected-volume-2ad06ebf-f91e-4bed-bb60-18b2ac78d03c no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:41:51.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3928" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":144,"skipped":2326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:41:51.262: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 20 13:41:51.326: INFO: Number of nodes with available pods: 0
May 20 13:41:51.326: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:41:52.349: INFO: Number of nodes with available pods: 0
May 20 13:41:52.349: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:41:53.334: INFO: Number of nodes with available pods: 0
May 20 13:41:53.334: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:41:54.335: INFO: Number of nodes with available pods: 3
May 20 13:41:54.336: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 20 13:41:54.400: INFO: Number of nodes with available pods: 2
May 20 13:41:54.400: INFO: Node k8s-3 is running more than one daemon pod
May 20 13:41:55.408: INFO: Number of nodes with available pods: 2
May 20 13:41:55.408: INFO: Node k8s-3 is running more than one daemon pod
May 20 13:41:56.406: INFO: Number of nodes with available pods: 2
May 20 13:41:56.406: INFO: Node k8s-3 is running more than one daemon pod
May 20 13:41:57.410: INFO: Number of nodes with available pods: 3
May 20 13:41:57.410: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2695, will wait for the garbage collector to delete the pods
May 20 13:41:57.475: INFO: Deleting DaemonSet.extensions daemon-set took: 6.865079ms
May 20 13:41:57.575: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.546243ms
May 20 13:42:05.484: INFO: Number of nodes with available pods: 0
May 20 13:42:05.485: INFO: Number of running nodes: 0, number of available pods: 0
May 20 13:42:05.489: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2695/daemonsets","resourceVersion":"19347"},"items":null}

May 20 13:42:05.491: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2695/pods","resourceVersion":"19347"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:42:05.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2695" for this suite.

• [SLOW TEST:14.252 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":145,"skipped":2355,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:42:05.514: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-f445dde0-a27a-417c-bf96-4e73f6f0ba95
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:42:09.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-558" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":146,"skipped":2359,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:42:09.588: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1512
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 20 13:42:09.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3843 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
May 20 13:42:09.697: INFO: stderr: ""
May 20 13:42:09.697: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
May 20 13:42:09.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3843 delete pods e2e-test-httpd-pod'
May 20 13:42:27.185: INFO: stderr: ""
May 20 13:42:27.185: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:42:27.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3843" for this suite.

• [SLOW TEST:17.606 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1509
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":147,"skipped":2368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:42:27.194: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8359
STEP: creating service affinity-clusterip-transition in namespace services-8359
STEP: creating replication controller affinity-clusterip-transition in namespace services-8359
I0520 13:42:27.237043      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-8359, replica count: 3
I0520 13:42:30.289189      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 13:42:30.295: INFO: Creating new exec pod
May 20 13:42:35.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-8359 exec execpod-affinity2sb9v -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
May 20 13:42:35.641: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 20 13:42:35.641: INFO: stdout: ""
May 20 13:42:35.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-8359 exec execpod-affinity2sb9v -- /bin/sh -x -c nc -zv -t -w 2 10.233.14.58 80'
May 20 13:42:35.878: INFO: stderr: "+ nc -zv -t -w 2 10.233.14.58 80\nConnection to 10.233.14.58 80 port [tcp/http] succeeded!\n"
May 20 13:42:35.878: INFO: stdout: ""
May 20 13:42:35.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-8359 exec execpod-affinity2sb9v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.14.58:80/ ; done'
May 20 13:42:36.254: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n"
May 20 13:42:36.254: INFO: stdout: "\naffinity-clusterip-transition-kt4nk\naffinity-clusterip-transition-55fhn\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-kt4nk\naffinity-clusterip-transition-55fhn\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-kt4nk\naffinity-clusterip-transition-55fhn\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-kt4nk\naffinity-clusterip-transition-55fhn\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-kt4nk\naffinity-clusterip-transition-55fhn\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-kt4nk"
May 20 13:42:36.254: INFO: Received response from host: affinity-clusterip-transition-kt4nk
May 20 13:42:36.254: INFO: Received response from host: affinity-clusterip-transition-55fhn
May 20 13:42:36.254: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.254: INFO: Received response from host: affinity-clusterip-transition-kt4nk
May 20 13:42:36.254: INFO: Received response from host: affinity-clusterip-transition-55fhn
May 20 13:42:36.254: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.254: INFO: Received response from host: affinity-clusterip-transition-kt4nk
May 20 13:42:36.254: INFO: Received response from host: affinity-clusterip-transition-55fhn
May 20 13:42:36.254: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.255: INFO: Received response from host: affinity-clusterip-transition-kt4nk
May 20 13:42:36.255: INFO: Received response from host: affinity-clusterip-transition-55fhn
May 20 13:42:36.255: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.255: INFO: Received response from host: affinity-clusterip-transition-kt4nk
May 20 13:42:36.255: INFO: Received response from host: affinity-clusterip-transition-55fhn
May 20 13:42:36.255: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.255: INFO: Received response from host: affinity-clusterip-transition-kt4nk
May 20 13:42:36.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-8359 exec execpod-affinity2sb9v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.14.58:80/ ; done'
May 20 13:42:36.655: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.58:80/\n"
May 20 13:42:36.656: INFO: stdout: "\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b\naffinity-clusterip-transition-x987b"
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Received response from host: affinity-clusterip-transition-x987b
May 20 13:42:36.656: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8359, will wait for the garbage collector to delete the pods
May 20 13:42:36.731: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.089063ms
May 20 13:42:37.131: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 400.122951ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:42:45.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8359" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:18.285 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":148,"skipped":2390,"failed":0}
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:42:45.480: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
May 20 13:44:46.074: INFO: Successfully updated pod "var-expansion-4ca951d3-7263-46af-bffd-1b01f1707742"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 20 13:44:48.080: INFO: Deleting pod "var-expansion-4ca951d3-7263-46af-bffd-1b01f1707742" in namespace "var-expansion-7202"
May 20 13:44:48.089: INFO: Wait up to 5m0s for pod "var-expansion-4ca951d3-7263-46af-bffd-1b01f1707742" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:45:26.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7202" for this suite.

• [SLOW TEST:160.629 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":149,"skipped":2390,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:45:26.110: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:45:26.134: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:45:27.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":150,"skipped":2401,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:45:27.163: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:45:38.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1607" for this suite.

• [SLOW TEST:11.140 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":151,"skipped":2407,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:45:38.303: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 20 13:45:38.338: INFO: Waiting up to 5m0s for pod "pod-907096bf-f7f8-4c56-9e92-2ea1e8aa5c1e" in namespace "emptydir-3206" to be "Succeeded or Failed"
May 20 13:45:38.344: INFO: Pod "pod-907096bf-f7f8-4c56-9e92-2ea1e8aa5c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03905ms
May 20 13:45:40.349: INFO: Pod "pod-907096bf-f7f8-4c56-9e92-2ea1e8aa5c1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010393143s
STEP: Saw pod success
May 20 13:45:40.349: INFO: Pod "pod-907096bf-f7f8-4c56-9e92-2ea1e8aa5c1e" satisfied condition "Succeeded or Failed"
May 20 13:45:40.351: INFO: Trying to get logs from node k8s-3 pod pod-907096bf-f7f8-4c56-9e92-2ea1e8aa5c1e container test-container: <nil>
STEP: delete the pod
May 20 13:45:40.385: INFO: Waiting for pod pod-907096bf-f7f8-4c56-9e92-2ea1e8aa5c1e to disappear
May 20 13:45:40.389: INFO: Pod pod-907096bf-f7f8-4c56-9e92-2ea1e8aa5c1e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:45:40.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3206" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":152,"skipped":2423,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:45:40.405: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-6bc5fda6-8b23-4f53-9390-1585d4baf319
STEP: Creating a pod to test consume secrets
May 20 13:45:40.447: INFO: Waiting up to 5m0s for pod "pod-secrets-27baef58-d8c4-4e91-b88a-5e86435f4705" in namespace "secrets-4879" to be "Succeeded or Failed"
May 20 13:45:40.450: INFO: Pod "pod-secrets-27baef58-d8c4-4e91-b88a-5e86435f4705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243186ms
May 20 13:45:42.454: INFO: Pod "pod-secrets-27baef58-d8c4-4e91-b88a-5e86435f4705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006431941s
May 20 13:45:44.472: INFO: Pod "pod-secrets-27baef58-d8c4-4e91-b88a-5e86435f4705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024175162s
STEP: Saw pod success
May 20 13:45:44.472: INFO: Pod "pod-secrets-27baef58-d8c4-4e91-b88a-5e86435f4705" satisfied condition "Succeeded or Failed"
May 20 13:45:44.474: INFO: Trying to get logs from node k8s-3 pod pod-secrets-27baef58-d8c4-4e91-b88a-5e86435f4705 container secret-volume-test: <nil>
STEP: delete the pod
May 20 13:45:44.487: INFO: Waiting for pod pod-secrets-27baef58-d8c4-4e91-b88a-5e86435f4705 to disappear
May 20 13:45:44.489: INFO: Pod pod-secrets-27baef58-d8c4-4e91-b88a-5e86435f4705 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:45:44.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4879" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":153,"skipped":2448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:45:44.501: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:45:48.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3714" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":154,"skipped":2470,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:45:48.559: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:45:48.629: INFO: Create a RollingUpdate DaemonSet
May 20 13:45:48.635: INFO: Check that daemon pods launch on every node of the cluster
May 20 13:45:48.647: INFO: Number of nodes with available pods: 0
May 20 13:45:48.648: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:45:49.659: INFO: Number of nodes with available pods: 0
May 20 13:45:49.659: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:45:50.656: INFO: Number of nodes with available pods: 0
May 20 13:45:50.656: INFO: Node k8s-1 is running more than one daemon pod
May 20 13:45:51.658: INFO: Number of nodes with available pods: 3
May 20 13:45:51.658: INFO: Number of running nodes: 3, number of available pods: 3
May 20 13:45:51.658: INFO: Update the DaemonSet to trigger a rollout
May 20 13:45:51.666: INFO: Updating DaemonSet daemon-set
May 20 13:46:04.688: INFO: Roll back the DaemonSet before rollout is complete
May 20 13:46:04.696: INFO: Updating DaemonSet daemon-set
May 20 13:46:04.696: INFO: Make sure DaemonSet rollback is complete
May 20 13:46:04.701: INFO: Wrong image for pod: daemon-set-lvvc7. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 20 13:46:04.701: INFO: Pod daemon-set-lvvc7 is not available
May 20 13:46:05.716: INFO: Wrong image for pod: daemon-set-lvvc7. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 20 13:46:05.716: INFO: Pod daemon-set-lvvc7 is not available
May 20 13:46:06.712: INFO: Pod daemon-set-q72gz is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3397, will wait for the garbage collector to delete the pods
May 20 13:46:06.784: INFO: Deleting DaemonSet.extensions daemon-set took: 6.928349ms
May 20 13:46:07.185: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.4691ms
May 20 13:46:13.887: INFO: Number of nodes with available pods: 0
May 20 13:46:13.887: INFO: Number of running nodes: 0, number of available pods: 0
May 20 13:46:13.889: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3397/daemonsets","resourceVersion":"20661"},"items":null}

May 20 13:46:13.891: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3397/pods","resourceVersion":"20661"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:46:13.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3397" for this suite.

• [SLOW TEST:25.352 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":155,"skipped":2488,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:46:13.913: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
May 20 13:46:17.967: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4786 PodName:pod-sharedvolume-ed20f8ad-b7e9-4c07-b595-27bcf9118c02 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 13:46:17.967: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 13:46:18.105: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:46:18.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4786" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":156,"skipped":2495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:46:18.130: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 20 13:46:18.176: INFO: Waiting up to 5m0s for pod "downward-api-a692c885-54a6-4bbc-bf7b-cfb8be180426" in namespace "downward-api-8250" to be "Succeeded or Failed"
May 20 13:46:18.180: INFO: Pod "downward-api-a692c885-54a6-4bbc-bf7b-cfb8be180426": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27384ms
May 20 13:46:20.184: INFO: Pod "downward-api-a692c885-54a6-4bbc-bf7b-cfb8be180426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008545849s
May 20 13:46:22.196: INFO: Pod "downward-api-a692c885-54a6-4bbc-bf7b-cfb8be180426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020774764s
STEP: Saw pod success
May 20 13:46:22.196: INFO: Pod "downward-api-a692c885-54a6-4bbc-bf7b-cfb8be180426" satisfied condition "Succeeded or Failed"
May 20 13:46:22.214: INFO: Trying to get logs from node k8s-2 pod downward-api-a692c885-54a6-4bbc-bf7b-cfb8be180426 container dapi-container: <nil>
STEP: delete the pod
May 20 13:46:22.246: INFO: Waiting for pod downward-api-a692c885-54a6-4bbc-bf7b-cfb8be180426 to disappear
May 20 13:46:22.248: INFO: Pod downward-api-a692c885-54a6-4bbc-bf7b-cfb8be180426 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:46:22.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8250" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2539,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:46:22.257: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-f413159b-201b-40c6-a36d-fdeb1f1eae45
STEP: Creating a pod to test consume secrets
May 20 13:46:22.302: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4fee2365-b344-4dd5-a65e-b43ae0c14608" in namespace "projected-9333" to be "Succeeded or Failed"
May 20 13:46:22.305: INFO: Pod "pod-projected-secrets-4fee2365-b344-4dd5-a65e-b43ae0c14608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802309ms
May 20 13:46:24.309: INFO: Pod "pod-projected-secrets-4fee2365-b344-4dd5-a65e-b43ae0c14608": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00686827s
STEP: Saw pod success
May 20 13:46:24.309: INFO: Pod "pod-projected-secrets-4fee2365-b344-4dd5-a65e-b43ae0c14608" satisfied condition "Succeeded or Failed"
May 20 13:46:24.312: INFO: Trying to get logs from node k8s-2 pod pod-projected-secrets-4fee2365-b344-4dd5-a65e-b43ae0c14608 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 20 13:46:24.333: INFO: Waiting for pod pod-projected-secrets-4fee2365-b344-4dd5-a65e-b43ae0c14608 to disappear
May 20 13:46:24.349: INFO: Pod pod-projected-secrets-4fee2365-b344-4dd5-a65e-b43ae0c14608 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:46:24.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9333" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":158,"skipped":2556,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:46:24.359: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 13:46:25.037: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 13:46:27.047: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115185, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115185, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115185, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115185, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 13:46:30.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:46:30.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7047" for this suite.
STEP: Destroying namespace "webhook-7047-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.934 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":159,"skipped":2560,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:46:30.299: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:46:30.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6796" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":160,"skipped":2567,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:46:30.398: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
May 20 13:46:30.454: INFO: Waiting up to 5m0s for pod "pod-9ba9caec-2b73-4c32-8ed1-507def34c882" in namespace "emptydir-5555" to be "Succeeded or Failed"
May 20 13:46:30.458: INFO: Pod "pod-9ba9caec-2b73-4c32-8ed1-507def34c882": Phase="Pending", Reason="", readiness=false. Elapsed: 3.701739ms
May 20 13:46:32.462: INFO: Pod "pod-9ba9caec-2b73-4c32-8ed1-507def34c882": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007864149s
May 20 13:46:34.469: INFO: Pod "pod-9ba9caec-2b73-4c32-8ed1-507def34c882": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014818805s
STEP: Saw pod success
May 20 13:46:34.469: INFO: Pod "pod-9ba9caec-2b73-4c32-8ed1-507def34c882" satisfied condition "Succeeded or Failed"
May 20 13:46:34.471: INFO: Trying to get logs from node k8s-3 pod pod-9ba9caec-2b73-4c32-8ed1-507def34c882 container test-container: <nil>
STEP: delete the pod
May 20 13:46:34.494: INFO: Waiting for pod pod-9ba9caec-2b73-4c32-8ed1-507def34c882 to disappear
May 20 13:46:34.497: INFO: Pod pod-9ba9caec-2b73-4c32-8ed1-507def34c882 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:46:34.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5555" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":161,"skipped":2568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:46:34.527: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:48:34.624: INFO: Deleting pod "var-expansion-9d14ef84-5998-42e2-a9ba-3e8c39b3e478" in namespace "var-expansion-3385"
May 20 13:48:34.628: INFO: Wait up to 5m0s for pod "var-expansion-9d14ef84-5998-42e2-a9ba-3e8c39b3e478" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:48:36.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3385" for this suite.

• [SLOW TEST:122.114 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":162,"skipped":2615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:48:36.642: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
May 20 13:48:36.669: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 20 13:48:36.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 create -f -'
May 20 13:48:36.979: INFO: stderr: ""
May 20 13:48:36.979: INFO: stdout: "service/agnhost-replica created\n"
May 20 13:48:36.979: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 20 13:48:36.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 create -f -'
May 20 13:48:37.265: INFO: stderr: ""
May 20 13:48:37.265: INFO: stdout: "service/agnhost-primary created\n"
May 20 13:48:37.265: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 20 13:48:37.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 create -f -'
May 20 13:48:37.489: INFO: stderr: ""
May 20 13:48:37.489: INFO: stdout: "service/frontend created\n"
May 20 13:48:37.489: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 20 13:48:37.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 create -f -'
May 20 13:48:37.709: INFO: stderr: ""
May 20 13:48:37.709: INFO: stdout: "deployment.apps/frontend created\n"
May 20 13:48:37.710: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 20 13:48:37.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 create -f -'
May 20 13:48:38.390: INFO: stderr: ""
May 20 13:48:38.390: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 20 13:48:38.391: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 20 13:48:38.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 create -f -'
May 20 13:48:38.986: INFO: stderr: ""
May 20 13:48:38.986: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 20 13:48:38.986: INFO: Waiting for all frontend pods to be Running.
May 20 13:48:44.040: INFO: Waiting for frontend to serve content.
May 20 13:48:44.052: INFO: Trying to add a new entry to the guestbook.
May 20 13:48:44.143: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 20 13:48:44.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 delete --grace-period=0 --force -f -'
May 20 13:48:44.291: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 20 13:48:44.291: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 20 13:48:44.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 delete --grace-period=0 --force -f -'
May 20 13:48:44.446: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 20 13:48:44.446: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 20 13:48:44.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 delete --grace-period=0 --force -f -'
May 20 13:48:44.567: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 20 13:48:44.567: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 20 13:48:44.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 delete --grace-period=0 --force -f -'
May 20 13:48:44.711: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 20 13:48:44.711: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 20 13:48:44.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 delete --grace-period=0 --force -f -'
May 20 13:48:44.923: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 20 13:48:44.923: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 20 13:48:44.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-8284 delete --grace-period=0 --force -f -'
May 20 13:48:45.085: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 20 13:48:45.085: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:48:45.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8284" for this suite.

• [SLOW TEST:8.478 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":163,"skipped":2637,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:48:45.120: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 13:48:45.848: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 13:48:47.857: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115325, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115325, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115325, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115325, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 13:48:50.868: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:48:50.872: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:48:52.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8847" for this suite.
STEP: Destroying namespace "webhook-8847-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.109 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":164,"skipped":2640,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:48:52.238: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
May 20 13:48:52.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 create -f -'
May 20 13:48:52.601: INFO: stderr: ""
May 20 13:48:52.601: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 20 13:48:52.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 20 13:48:52.717: INFO: stderr: ""
May 20 13:48:52.717: INFO: stdout: "update-demo-nautilus-8kwlg update-demo-nautilus-l46p6 "
May 20 13:48:52.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-8kwlg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 13:48:52.791: INFO: stderr: ""
May 20 13:48:52.791: INFO: stdout: ""
May 20 13:48:52.791: INFO: update-demo-nautilus-8kwlg is created but not running
May 20 13:48:57.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 20 13:48:57.874: INFO: stderr: ""
May 20 13:48:57.874: INFO: stdout: "update-demo-nautilus-8kwlg update-demo-nautilus-l46p6 "
May 20 13:48:57.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-8kwlg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 13:48:57.954: INFO: stderr: ""
May 20 13:48:57.954: INFO: stdout: "true"
May 20 13:48:57.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-8kwlg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 20 13:48:58.034: INFO: stderr: ""
May 20 13:48:58.034: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 20 13:48:58.034: INFO: validating pod update-demo-nautilus-8kwlg
May 20 13:48:58.039: INFO: got data: {
  "image": "nautilus.jpg"
}

May 20 13:48:58.039: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 20 13:48:58.039: INFO: update-demo-nautilus-8kwlg is verified up and running
May 20 13:48:58.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-l46p6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 13:48:58.120: INFO: stderr: ""
May 20 13:48:58.120: INFO: stdout: "true"
May 20 13:48:58.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-l46p6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 20 13:48:58.212: INFO: stderr: ""
May 20 13:48:58.212: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 20 13:48:58.212: INFO: validating pod update-demo-nautilus-l46p6
May 20 13:48:58.222: INFO: got data: {
  "image": "nautilus.jpg"
}

May 20 13:48:58.222: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 20 13:48:58.222: INFO: update-demo-nautilus-l46p6 is verified up and running
STEP: scaling down the replication controller
May 20 13:48:58.228: INFO: scanned /root for discovery docs: <nil>
May 20 13:48:58.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 20 13:48:59.385: INFO: stderr: ""
May 20 13:48:59.385: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 20 13:48:59.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 20 13:48:59.475: INFO: stderr: ""
May 20 13:48:59.475: INFO: stdout: "update-demo-nautilus-8kwlg update-demo-nautilus-l46p6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 20 13:49:04.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 20 13:49:04.576: INFO: stderr: ""
May 20 13:49:04.576: INFO: stdout: "update-demo-nautilus-8kwlg update-demo-nautilus-l46p6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 20 13:49:09.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 20 13:49:09.681: INFO: stderr: ""
May 20 13:49:09.681: INFO: stdout: "update-demo-nautilus-l46p6 "
May 20 13:49:09.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-l46p6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 13:49:09.753: INFO: stderr: ""
May 20 13:49:09.753: INFO: stdout: "true"
May 20 13:49:09.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-l46p6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 20 13:49:09.830: INFO: stderr: ""
May 20 13:49:09.830: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 20 13:49:09.830: INFO: validating pod update-demo-nautilus-l46p6
May 20 13:49:09.834: INFO: got data: {
  "image": "nautilus.jpg"
}

May 20 13:49:09.834: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 20 13:49:09.834: INFO: update-demo-nautilus-l46p6 is verified up and running
STEP: scaling up the replication controller
May 20 13:49:09.836: INFO: scanned /root for discovery docs: <nil>
May 20 13:49:09.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 20 13:49:10.936: INFO: stderr: ""
May 20 13:49:10.936: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 20 13:49:10.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 20 13:49:11.100: INFO: stderr: ""
May 20 13:49:11.100: INFO: stdout: "update-demo-nautilus-l46p6 update-demo-nautilus-qngtf "
May 20 13:49:11.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-l46p6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 13:49:11.185: INFO: stderr: ""
May 20 13:49:11.185: INFO: stdout: "true"
May 20 13:49:11.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-l46p6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 20 13:49:11.288: INFO: stderr: ""
May 20 13:49:11.288: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 20 13:49:11.288: INFO: validating pod update-demo-nautilus-l46p6
May 20 13:49:11.293: INFO: got data: {
  "image": "nautilus.jpg"
}

May 20 13:49:11.293: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 20 13:49:11.293: INFO: update-demo-nautilus-l46p6 is verified up and running
May 20 13:49:11.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-qngtf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 13:49:11.379: INFO: stderr: ""
May 20 13:49:11.379: INFO: stdout: ""
May 20 13:49:11.379: INFO: update-demo-nautilus-qngtf is created but not running
May 20 13:49:16.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 20 13:49:16.469: INFO: stderr: ""
May 20 13:49:16.469: INFO: stdout: "update-demo-nautilus-l46p6 update-demo-nautilus-qngtf "
May 20 13:49:16.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-l46p6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 13:49:16.542: INFO: stderr: ""
May 20 13:49:16.542: INFO: stdout: "true"
May 20 13:49:16.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-l46p6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 20 13:49:16.615: INFO: stderr: ""
May 20 13:49:16.615: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 20 13:49:16.615: INFO: validating pod update-demo-nautilus-l46p6
May 20 13:49:16.619: INFO: got data: {
  "image": "nautilus.jpg"
}

May 20 13:49:16.619: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 20 13:49:16.619: INFO: update-demo-nautilus-l46p6 is verified up and running
May 20 13:49:16.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-qngtf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 20 13:49:16.689: INFO: stderr: ""
May 20 13:49:16.689: INFO: stdout: "true"
May 20 13:49:16.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods update-demo-nautilus-qngtf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 20 13:49:16.759: INFO: stderr: ""
May 20 13:49:16.760: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 20 13:49:16.760: INFO: validating pod update-demo-nautilus-qngtf
May 20 13:49:16.763: INFO: got data: {
  "image": "nautilus.jpg"
}

May 20 13:49:16.763: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 20 13:49:16.763: INFO: update-demo-nautilus-qngtf is verified up and running
STEP: using delete to clean up resources
May 20 13:49:16.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 delete --grace-period=0 --force -f -'
May 20 13:49:16.841: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 20 13:49:16.841: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 20 13:49:16.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get rc,svc -l name=update-demo --no-headers'
May 20 13:49:16.933: INFO: stderr: "No resources found in kubectl-3520 namespace.\n"
May 20 13:49:16.933: INFO: stdout: ""
May 20 13:49:16.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 20 13:49:17.009: INFO: stderr: ""
May 20 13:49:17.009: INFO: stdout: "update-demo-nautilus-l46p6\nupdate-demo-nautilus-qngtf\n"
May 20 13:49:17.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get rc,svc -l name=update-demo --no-headers'
May 20 13:49:17.626: INFO: stderr: "No resources found in kubectl-3520 namespace.\n"
May 20 13:49:17.626: INFO: stdout: ""
May 20 13:49:17.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-3520 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 20 13:49:17.731: INFO: stderr: ""
May 20 13:49:17.731: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:49:17.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3520" for this suite.

• [SLOW TEST:25.501 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":165,"skipped":2643,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:49:17.739: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:49:17.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-947 version'
May 20 13:49:17.847: INFO: stderr: ""
May 20 13:49:17.847: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.9\", GitCommit:\"9dd794e454ac32d97cde41ae10be801ae98f75df\", GitTreeState:\"clean\", BuildDate:\"2021-03-18T01:09:28Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.9\", GitCommit:\"9dd794e454ac32d97cde41ae10be801ae98f75df\", GitTreeState:\"clean\", BuildDate:\"2021-03-18T01:00:06Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:49:17.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-947" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":166,"skipped":2655,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:49:17.856: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:49:17.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9876" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":167,"skipped":2661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:49:17.934: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
May 20 13:49:17.959: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:49:32.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-833" for this suite.

• [SLOW TEST:14.965 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":168,"skipped":2692,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:49:32.903: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5140.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5140.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 6.1.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.1.6_udp@PTR;check="$$(dig +tcp +noall +answer +search 6.1.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.1.6_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5140.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5140.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 6.1.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.1.6_udp@PTR;check="$$(dig +tcp +noall +answer +search 6.1.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.1.6_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 20 13:49:37.004: INFO: Unable to read wheezy_udp@dns-test-service.dns-5140.svc.cluster.local from pod dns-5140/dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20: the server could not find the requested resource (get pods dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20)
May 20 13:49:37.008: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5140.svc.cluster.local from pod dns-5140/dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20: the server could not find the requested resource (get pods dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20)
May 20 13:49:37.011: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local from pod dns-5140/dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20: the server could not find the requested resource (get pods dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20)
May 20 13:49:37.014: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local from pod dns-5140/dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20: the server could not find the requested resource (get pods dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20)
May 20 13:49:37.042: INFO: Unable to read jessie_udp@dns-test-service.dns-5140.svc.cluster.local from pod dns-5140/dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20: the server could not find the requested resource (get pods dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20)
May 20 13:49:37.045: INFO: Unable to read jessie_tcp@dns-test-service.dns-5140.svc.cluster.local from pod dns-5140/dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20: the server could not find the requested resource (get pods dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20)
May 20 13:49:37.048: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local from pod dns-5140/dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20: the server could not find the requested resource (get pods dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20)
May 20 13:49:37.052: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local from pod dns-5140/dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20: the server could not find the requested resource (get pods dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20)
May 20 13:49:37.073: INFO: Lookups using dns-5140/dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20 failed for: [wheezy_udp@dns-test-service.dns-5140.svc.cluster.local wheezy_tcp@dns-test-service.dns-5140.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local jessie_udp@dns-test-service.dns-5140.svc.cluster.local jessie_tcp@dns-test-service.dns-5140.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5140.svc.cluster.local]

May 20 13:49:42.135: INFO: DNS probes using dns-5140/dns-test-d23cf709-6fa8-45fd-898f-83a2593faa20 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:49:42.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5140" for this suite.

• [SLOW TEST:9.340 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":169,"skipped":2736,"failed":0}
SS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:49:42.249: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
May 20 13:49:42.308: INFO: created test-podtemplate-1
May 20 13:49:42.318: INFO: created test-podtemplate-2
May 20 13:49:42.328: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 20 13:49:42.335: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 20 13:49:42.356: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:49:42.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-909" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":170,"skipped":2738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:49:42.379: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:49:42.444: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Pending, waiting for it to be Running (with Ready = true)
May 20 13:49:44.448: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = false)
May 20 13:49:46.450: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = false)
May 20 13:49:48.449: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = false)
May 20 13:49:50.463: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = false)
May 20 13:49:52.447: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = false)
May 20 13:49:54.448: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = false)
May 20 13:49:56.449: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = false)
May 20 13:49:58.447: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = false)
May 20 13:50:00.449: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = false)
May 20 13:50:02.448: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = false)
May 20 13:50:04.450: INFO: The status of Pod test-webserver-f747e7ce-8a52-49b8-851a-2d7e0262da96 is Running (Ready = true)
May 20 13:50:04.452: INFO: Container started at 2021-05-20 13:49:43 +0000 UTC, pod became ready at 2021-05-20 13:50:03 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:50:04.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-662" for this suite.

• [SLOW TEST:22.081 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":171,"skipped":2760,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:50:04.464: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:50:04.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8347" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":172,"skipped":2773,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:50:04.518: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-65e19902-fb3d-4571-ae52-e1f6ba23c323 in namespace container-probe-5214
May 20 13:50:08.572: INFO: Started pod liveness-65e19902-fb3d-4571-ae52-e1f6ba23c323 in namespace container-probe-5214
STEP: checking the pod's current state and verifying that restartCount is present
May 20 13:50:08.575: INFO: Initial restart count of pod liveness-65e19902-fb3d-4571-ae52-e1f6ba23c323 is 0
May 20 13:50:26.624: INFO: Restart count of pod container-probe-5214/liveness-65e19902-fb3d-4571-ae52-e1f6ba23c323 is now 1 (18.049159756s elapsed)
May 20 13:50:44.687: INFO: Restart count of pod container-probe-5214/liveness-65e19902-fb3d-4571-ae52-e1f6ba23c323 is now 2 (36.112523575s elapsed)
May 20 13:51:04.745: INFO: Restart count of pod container-probe-5214/liveness-65e19902-fb3d-4571-ae52-e1f6ba23c323 is now 3 (56.170050795s elapsed)
May 20 13:51:24.815: INFO: Restart count of pod container-probe-5214/liveness-65e19902-fb3d-4571-ae52-e1f6ba23c323 is now 4 (1m16.239881356s elapsed)
May 20 13:52:35.391: INFO: Restart count of pod container-probe-5214/liveness-65e19902-fb3d-4571-ae52-e1f6ba23c323 is now 5 (2m26.816059211s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:52:35.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5214" for this suite.

• [SLOW TEST:150.895 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:52:35.414: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:52:35.444: INFO: Creating ReplicaSet my-hostname-basic-0b6cb8f1-d1a9-4819-ae0d-6f428bdefe7a
May 20 13:52:35.451: INFO: Pod name my-hostname-basic-0b6cb8f1-d1a9-4819-ae0d-6f428bdefe7a: Found 0 pods out of 1
May 20 13:52:40.456: INFO: Pod name my-hostname-basic-0b6cb8f1-d1a9-4819-ae0d-6f428bdefe7a: Found 1 pods out of 1
May 20 13:52:40.456: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0b6cb8f1-d1a9-4819-ae0d-6f428bdefe7a" is running
May 20 13:52:40.461: INFO: Pod "my-hostname-basic-0b6cb8f1-d1a9-4819-ae0d-6f428bdefe7a-xfxzk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-20 13:52:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-20 13:52:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-20 13:52:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-20 13:52:35 +0000 UTC Reason: Message:}])
May 20 13:52:40.462: INFO: Trying to dial the pod
May 20 13:52:45.471: INFO: Controller my-hostname-basic-0b6cb8f1-d1a9-4819-ae0d-6f428bdefe7a: Got expected result from replica 1 [my-hostname-basic-0b6cb8f1-d1a9-4819-ae0d-6f428bdefe7a-xfxzk]: "my-hostname-basic-0b6cb8f1-d1a9-4819-ae0d-6f428bdefe7a-xfxzk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:52:45.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6414" for this suite.

• [SLOW TEST:10.065 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":174,"skipped":2808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:52:45.485: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-19678dda-082e-4a7e-8f7a-e5516558f0d2
STEP: Creating a pod to test consume secrets
May 20 13:52:45.519: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d08e514c-5ef2-46e8-bdff-382cb04ca82c" in namespace "projected-2353" to be "Succeeded or Failed"
May 20 13:52:45.524: INFO: Pod "pod-projected-secrets-d08e514c-5ef2-46e8-bdff-382cb04ca82c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.785894ms
May 20 13:52:47.527: INFO: Pod "pod-projected-secrets-d08e514c-5ef2-46e8-bdff-382cb04ca82c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007819975s
May 20 13:52:49.531: INFO: Pod "pod-projected-secrets-d08e514c-5ef2-46e8-bdff-382cb04ca82c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011631436s
STEP: Saw pod success
May 20 13:52:49.531: INFO: Pod "pod-projected-secrets-d08e514c-5ef2-46e8-bdff-382cb04ca82c" satisfied condition "Succeeded or Failed"
May 20 13:52:49.534: INFO: Trying to get logs from node k8s-3 pod pod-projected-secrets-d08e514c-5ef2-46e8-bdff-382cb04ca82c container projected-secret-volume-test: <nil>
STEP: delete the pod
May 20 13:52:49.576: INFO: Waiting for pod pod-projected-secrets-d08e514c-5ef2-46e8-bdff-382cb04ca82c to disappear
May 20 13:52:49.578: INFO: Pod pod-projected-secrets-d08e514c-5ef2-46e8-bdff-382cb04ca82c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:52:49.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2353" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":175,"skipped":2854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:52:49.591: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 20 13:52:53.664: INFO: &Pod{ObjectMeta:{send-events-0ad3f9e2-da29-4487-a7f7-dbd13add43c2  events-3258 /api/v1/namespaces/events-3258/pods/send-events-0ad3f9e2-da29-4487-a7f7-dbd13add43c2 62cc6970-8954-4b05-9127-787e089d757c 22725 0 2021-05-20 13:52:49 +0000 UTC <nil> <nil> map[name:foo time:636473289] map[] [] []  [{e2e.test Update v1 2021-05-20 13:52:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 13:52:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ngfvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ngfvv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ngfvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:52:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:52:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:52:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:52:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:10.233.66.191,StartTime:2021-05-20 13:52:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 13:52:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://a84878f26f0087bb674a77e7374ec57fbd9d29122ba282d03a5ce2ea3d2b6b02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 20 13:52:55.668: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 20 13:52:57.673: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:52:57.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3258" for this suite.

• [SLOW TEST:8.099 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":176,"skipped":2876,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:52:57.697: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:52:57.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1816a858-a8f0-43b3-8e72-0fbd9cb247d4" in namespace "projected-5297" to be "Succeeded or Failed"
May 20 13:52:57.734: INFO: Pod "downwardapi-volume-1816a858-a8f0-43b3-8e72-0fbd9cb247d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949616ms
May 20 13:52:59.738: INFO: Pod "downwardapi-volume-1816a858-a8f0-43b3-8e72-0fbd9cb247d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005411426s
May 20 13:53:01.742: INFO: Pod "downwardapi-volume-1816a858-a8f0-43b3-8e72-0fbd9cb247d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009424475s
STEP: Saw pod success
May 20 13:53:01.742: INFO: Pod "downwardapi-volume-1816a858-a8f0-43b3-8e72-0fbd9cb247d4" satisfied condition "Succeeded or Failed"
May 20 13:53:01.744: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-1816a858-a8f0-43b3-8e72-0fbd9cb247d4 container client-container: <nil>
STEP: delete the pod
May 20 13:53:01.789: INFO: Waiting for pod downwardapi-volume-1816a858-a8f0-43b3-8e72-0fbd9cb247d4 to disappear
May 20 13:53:01.792: INFO: Pod downwardapi-volume-1816a858-a8f0-43b3-8e72-0fbd9cb247d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:53:01.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5297" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":177,"skipped":2878,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:53:01.802: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:53:31.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9514" for this suite.
STEP: Destroying namespace "nsdeletetest-9457" for this suite.
May 20 13:53:31.040: INFO: Namespace nsdeletetest-9457 was already deleted
STEP: Destroying namespace "nsdeletetest-2912" for this suite.

• [SLOW TEST:29.246 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":178,"skipped":2891,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:53:31.048: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 13:53:31.546: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 13:53:33.572: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115611, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115611, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115611, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115611, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 13:53:36.582: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:53:36.585: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1987-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:53:37.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9167" for this suite.
STEP: Destroying namespace "webhook-9167-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.726 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":179,"skipped":2904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:53:37.775: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 13:53:38.546: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 13:53:40.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115618, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115618, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115618, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115618, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 13:53:43.587: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:53:43.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6553" for this suite.
STEP: Destroying namespace "webhook-6553-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.187 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":180,"skipped":2929,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:53:43.992: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:53:44.048: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5d0ce09f-77b0-49f6-88a9-791b8e7a226f" in namespace "security-context-test-2793" to be "Succeeded or Failed"
May 20 13:53:44.058: INFO: Pod "busybox-privileged-false-5d0ce09f-77b0-49f6-88a9-791b8e7a226f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.190655ms
May 20 13:53:46.066: INFO: Pod "busybox-privileged-false-5d0ce09f-77b0-49f6-88a9-791b8e7a226f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017263703s
May 20 13:53:48.071: INFO: Pod "busybox-privileged-false-5d0ce09f-77b0-49f6-88a9-791b8e7a226f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022571111s
May 20 13:53:48.071: INFO: Pod "busybox-privileged-false-5d0ce09f-77b0-49f6-88a9-791b8e7a226f" satisfied condition "Succeeded or Failed"
May 20 13:53:48.083: INFO: Got logs for pod "busybox-privileged-false-5d0ce09f-77b0-49f6-88a9-791b8e7a226f": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:53:48.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2793" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":181,"skipped":3006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:53:48.105: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-e753ed33-86e6-4ca7-bc41-13d3372cbb9f
STEP: Creating configMap with name cm-test-opt-upd-88643736-da5b-47ae-9891-3b3bd689089f
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e753ed33-86e6-4ca7-bc41-13d3372cbb9f
STEP: Updating configmap cm-test-opt-upd-88643736-da5b-47ae-9891-3b3bd689089f
STEP: Creating configMap with name cm-test-opt-create-e6e057d8-eea5-4706-bc62-6a0de488e720
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:53:52.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3078" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":182,"skipped":3046,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:53:52.243: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:53:56.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6689" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":183,"skipped":3056,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:53:56.300: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8813
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8813
STEP: creating replication controller externalsvc in namespace services-8813
I0520 13:53:56.415203      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8813, replica count: 2
I0520 13:53:59.467212      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 20 13:53:59.479: INFO: Creating new exec pod
May 20 13:54:03.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-8813 exec execpodtv6hq -- /bin/sh -x -c nslookup clusterip-service.services-8813.svc.cluster.local'
May 20 13:54:04.286: INFO: stderr: "+ nslookup clusterip-service.services-8813.svc.cluster.local\n"
May 20 13:54:04.287: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-8813.svc.cluster.local\tcanonical name = externalsvc.services-8813.svc.cluster.local.\nName:\texternalsvc.services-8813.svc.cluster.local\nAddress: 10.233.48.32\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8813, will wait for the garbage collector to delete the pods
May 20 13:54:04.372: INFO: Deleting ReplicationController externalsvc took: 9.284569ms
May 20 13:54:04.773: INFO: Terminating ReplicationController externalsvc pods took: 400.148557ms
May 20 13:54:15.489: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:54:15.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8813" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:19.240 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":184,"skipped":3059,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:54:15.540: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:54:19.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7358" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":185,"skipped":3064,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:54:19.638: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:54:19.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2955" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":186,"skipped":3092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:54:19.732: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-b1d251c6-d259-4e5e-bdaa-c5189201a19e in namespace container-probe-6281
May 20 13:54:23.771: INFO: Started pod busybox-b1d251c6-d259-4e5e-bdaa-c5189201a19e in namespace container-probe-6281
STEP: checking the pod's current state and verifying that restartCount is present
May 20 13:54:23.773: INFO: Initial restart count of pod busybox-b1d251c6-d259-4e5e-bdaa-c5189201a19e is 0
May 20 13:55:16.231: INFO: Restart count of pod container-probe-6281/busybox-b1d251c6-d259-4e5e-bdaa-c5189201a19e is now 1 (52.457782545s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:55:16.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6281" for this suite.

• [SLOW TEST:56.524 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":187,"skipped":3135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:55:16.257: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7397
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
May 20 13:55:16.314: INFO: Found 0 stateful pods, waiting for 3
May 20 13:55:26.318: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 20 13:55:26.318: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 20 13:55:26.318: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 20 13:55:26.342: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 20 13:55:36.374: INFO: Updating stateful set ss2
May 20 13:55:36.380: INFO: Waiting for Pod statefulset-7397/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May 20 13:55:46.499: INFO: Found 2 stateful pods, waiting for 3
May 20 13:55:56.513: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 20 13:55:56.513: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 20 13:55:56.513: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 20 13:56:06.509: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 20 13:56:06.509: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 20 13:56:06.509: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 20 13:56:06.535: INFO: Updating stateful set ss2
May 20 13:56:06.561: INFO: Waiting for Pod statefulset-7397/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 20 13:56:16.589: INFO: Updating stateful set ss2
May 20 13:56:16.598: INFO: Waiting for StatefulSet statefulset-7397/ss2 to complete update
May 20 13:56:16.598: INFO: Waiting for Pod statefulset-7397/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 20 13:56:26.606: INFO: Waiting for StatefulSet statefulset-7397/ss2 to complete update
May 20 13:56:26.606: INFO: Waiting for Pod statefulset-7397/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 20 13:56:36.604: INFO: Waiting for StatefulSet statefulset-7397/ss2 to complete update
May 20 13:56:46.607: INFO: Waiting for StatefulSet statefulset-7397/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 20 13:56:56.607: INFO: Deleting all statefulset in ns statefulset-7397
May 20 13:56:56.611: INFO: Scaling statefulset ss2 to 0
May 20 13:57:16.628: INFO: Waiting for statefulset status.replicas updated to 0
May 20 13:57:16.630: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:57:16.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7397" for this suite.

• [SLOW TEST:120.410 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":188,"skipped":3172,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:57:16.676: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
May 20 13:57:16.730: INFO: Waiting up to 5m0s for pod "pod-df4f9b79-21dd-4110-afef-6341e53565c4" in namespace "emptydir-9670" to be "Succeeded or Failed"
May 20 13:57:16.732: INFO: Pod "pod-df4f9b79-21dd-4110-afef-6341e53565c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.507393ms
May 20 13:57:18.735: INFO: Pod "pod-df4f9b79-21dd-4110-afef-6341e53565c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005809703s
May 20 13:57:20.740: INFO: Pod "pod-df4f9b79-21dd-4110-afef-6341e53565c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010569237s
STEP: Saw pod success
May 20 13:57:20.740: INFO: Pod "pod-df4f9b79-21dd-4110-afef-6341e53565c4" satisfied condition "Succeeded or Failed"
May 20 13:57:20.742: INFO: Trying to get logs from node k8s-3 pod pod-df4f9b79-21dd-4110-afef-6341e53565c4 container test-container: <nil>
STEP: delete the pod
May 20 13:57:20.766: INFO: Waiting for pod pod-df4f9b79-21dd-4110-afef-6341e53565c4 to disappear
May 20 13:57:20.769: INFO: Pod pod-df4f9b79-21dd-4110-afef-6341e53565c4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:57:20.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9670" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":3184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:57:20.781: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
May 20 13:57:20.810: INFO: Waiting up to 5m0s for pod "var-expansion-5c818e39-0870-46e5-bd61-221e22c58628" in namespace "var-expansion-4651" to be "Succeeded or Failed"
May 20 13:57:20.813: INFO: Pod "var-expansion-5c818e39-0870-46e5-bd61-221e22c58628": Phase="Pending", Reason="", readiness=false. Elapsed: 3.178115ms
May 20 13:57:22.823: INFO: Pod "var-expansion-5c818e39-0870-46e5-bd61-221e22c58628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012930109s
May 20 13:57:24.827: INFO: Pod "var-expansion-5c818e39-0870-46e5-bd61-221e22c58628": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017060682s
STEP: Saw pod success
May 20 13:57:24.827: INFO: Pod "var-expansion-5c818e39-0870-46e5-bd61-221e22c58628" satisfied condition "Succeeded or Failed"
May 20 13:57:24.829: INFO: Trying to get logs from node k8s-3 pod var-expansion-5c818e39-0870-46e5-bd61-221e22c58628 container dapi-container: <nil>
STEP: delete the pod
May 20 13:57:24.843: INFO: Waiting for pod var-expansion-5c818e39-0870-46e5-bd61-221e22c58628 to disappear
May 20 13:57:24.846: INFO: Pod var-expansion-5c818e39-0870-46e5-bd61-221e22c58628 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:57:24.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4651" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":190,"skipped":3221,"failed":0}
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:57:24.854: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-mz4lq in namespace proxy-6956
I0520 13:57:24.897670      20 runners.go:190] Created replication controller with name: proxy-service-mz4lq, namespace: proxy-6956, replica count: 1
I0520 13:57:25.948929      20 runners.go:190] proxy-service-mz4lq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0520 13:57:26.949236      20 runners.go:190] proxy-service-mz4lq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0520 13:57:27.949517      20 runners.go:190] proxy-service-mz4lq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 13:57:27.953: INFO: setup took 3.076580158s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 20 13:57:27.975: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 21.442871ms)
May 20 13:57:27.975: INFO: (0) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 21.960182ms)
May 20 13:57:27.975: INFO: (0) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 22.334263ms)
May 20 13:57:27.975: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 22.253992ms)
May 20 13:57:27.975: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 22.456089ms)
May 20 13:57:27.975: INFO: (0) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 22.308772ms)
May 20 13:57:27.976: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 22.91774ms)
May 20 13:57:27.983: INFO: (0) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 29.594868ms)
May 20 13:57:27.983: INFO: (0) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 29.632862ms)
May 20 13:57:27.983: INFO: (0) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 30.303953ms)
May 20 13:57:27.988: INFO: (0) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 35.14521ms)
May 20 13:57:27.991: INFO: (0) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 37.769933ms)
May 20 13:57:27.991: INFO: (0) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 38.459171ms)
May 20 13:57:27.992: INFO: (0) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 38.826488ms)
May 20 13:57:27.993: INFO: (0) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 39.800842ms)
May 20 13:57:27.993: INFO: (0) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 40.039104ms)
May 20 13:57:28.004: INFO: (1) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 10.873977ms)
May 20 13:57:28.004: INFO: (1) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 10.808744ms)
May 20 13:57:28.005: INFO: (1) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 11.034475ms)
May 20 13:57:28.005: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 11.674567ms)
May 20 13:57:28.005: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 11.68577ms)
May 20 13:57:28.007: INFO: (1) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 13.494881ms)
May 20 13:57:28.011: INFO: (1) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 17.28097ms)
May 20 13:57:28.012: INFO: (1) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 18.159018ms)
May 20 13:57:28.012: INFO: (1) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 18.353939ms)
May 20 13:57:28.012: INFO: (1) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 18.197211ms)
May 20 13:57:28.015: INFO: (1) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 21.24281ms)
May 20 13:57:28.015: INFO: (1) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 22.137345ms)
May 20 13:57:28.018: INFO: (1) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 24.011227ms)
May 20 13:57:28.018: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 23.928697ms)
May 20 13:57:28.019: INFO: (1) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 25.182811ms)
May 20 13:57:28.020: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 26.08591ms)
May 20 13:57:28.030: INFO: (2) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 9.611533ms)
May 20 13:57:28.033: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 11.759751ms)
May 20 13:57:28.036: INFO: (2) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 14.169169ms)
May 20 13:57:28.037: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 16.597955ms)
May 20 13:57:28.037: INFO: (2) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 16.891605ms)
May 20 13:57:28.038: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 16.707123ms)
May 20 13:57:28.038: INFO: (2) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 17.64824ms)
May 20 13:57:28.038: INFO: (2) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 16.60247ms)
May 20 13:57:28.038: INFO: (2) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 16.476387ms)
May 20 13:57:28.038: INFO: (2) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 17.555202ms)
May 20 13:57:28.038: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 17.875307ms)
May 20 13:57:28.041: INFO: (2) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 19.842095ms)
May 20 13:57:28.041: INFO: (2) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 19.652502ms)
May 20 13:57:28.041: INFO: (2) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 19.604897ms)
May 20 13:57:28.041: INFO: (2) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 19.745044ms)
May 20 13:57:28.041: INFO: (2) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 19.436367ms)
May 20 13:57:28.049: INFO: (3) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 7.625515ms)
May 20 13:57:28.050: INFO: (3) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 8.143088ms)
May 20 13:57:28.050: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 8.465447ms)
May 20 13:57:28.053: INFO: (3) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 11.239533ms)
May 20 13:57:28.054: INFO: (3) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 11.877217ms)
May 20 13:57:28.054: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 12.314044ms)
May 20 13:57:28.054: INFO: (3) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 12.469947ms)
May 20 13:57:28.054: INFO: (3) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 12.375922ms)
May 20 13:57:28.055: INFO: (3) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 12.635335ms)
May 20 13:57:28.055: INFO: (3) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 12.856962ms)
May 20 13:57:28.058: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 15.644568ms)
May 20 13:57:28.058: INFO: (3) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 16.257554ms)
May 20 13:57:28.059: INFO: (3) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 16.896629ms)
May 20 13:57:28.059: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 17.112496ms)
May 20 13:57:28.059: INFO: (3) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 17.32581ms)
May 20 13:57:28.059: INFO: (3) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 17.459871ms)
May 20 13:57:28.066: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 5.475337ms)
May 20 13:57:28.066: INFO: (4) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 5.766574ms)
May 20 13:57:28.072: INFO: (4) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 12.17898ms)
May 20 13:57:28.072: INFO: (4) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 12.27448ms)
May 20 13:57:28.072: INFO: (4) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 12.335957ms)
May 20 13:57:28.072: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 12.285247ms)
May 20 13:57:28.073: INFO: (4) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 12.766199ms)
May 20 13:57:28.073: INFO: (4) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 12.6148ms)
May 20 13:57:28.073: INFO: (4) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 12.857366ms)
May 20 13:57:28.073: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 12.796986ms)
May 20 13:57:28.073: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 13.432966ms)
May 20 13:57:28.079: INFO: (4) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 19.16447ms)
May 20 13:57:28.079: INFO: (4) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 19.172081ms)
May 20 13:57:28.080: INFO: (4) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 19.979417ms)
May 20 13:57:28.083: INFO: (4) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 23.250739ms)
May 20 13:57:28.083: INFO: (4) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 23.266835ms)
May 20 13:57:28.100: INFO: (5) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 15.49377ms)
May 20 13:57:28.104: INFO: (5) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 19.343023ms)
May 20 13:57:28.105: INFO: (5) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 19.033956ms)
May 20 13:57:28.105: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 19.332477ms)
May 20 13:57:28.105: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 20.448011ms)
May 20 13:57:28.105: INFO: (5) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 20.056264ms)
May 20 13:57:28.105: INFO: (5) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 19.413469ms)
May 20 13:57:28.105: INFO: (5) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 19.227774ms)
May 20 13:57:28.105: INFO: (5) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 19.552462ms)
May 20 13:57:28.108: INFO: (5) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 21.794467ms)
May 20 13:57:28.108: INFO: (5) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 21.886887ms)
May 20 13:57:28.108: INFO: (5) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 22.110168ms)
May 20 13:57:28.108: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 22.025308ms)
May 20 13:57:28.108: INFO: (5) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 22.075174ms)
May 20 13:57:28.108: INFO: (5) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 22.991379ms)
May 20 13:57:28.108: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 22.207292ms)
May 20 13:57:28.122: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 11.78245ms)
May 20 13:57:28.125: INFO: (6) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 14.29555ms)
May 20 13:57:28.125: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 14.723271ms)
May 20 13:57:28.125: INFO: (6) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 15.203084ms)
May 20 13:57:28.125: INFO: (6) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 13.908698ms)
May 20 13:57:28.126: INFO: (6) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 16.143698ms)
May 20 13:57:28.128: INFO: (6) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 17.421193ms)
May 20 13:57:28.129: INFO: (6) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 19.458589ms)
May 20 13:57:28.130: INFO: (6) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 18.905069ms)
May 20 13:57:28.130: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 18.637874ms)
May 20 13:57:28.131: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 19.820146ms)
May 20 13:57:28.132: INFO: (6) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 21.947388ms)
May 20 13:57:28.133: INFO: (6) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 22.990109ms)
May 20 13:57:28.134: INFO: (6) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 23.71053ms)
May 20 13:57:28.135: INFO: (6) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 24.278493ms)
May 20 13:57:28.136: INFO: (6) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 25.033517ms)
May 20 13:57:28.145: INFO: (7) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 9.545042ms)
May 20 13:57:28.145: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 9.336076ms)
May 20 13:57:28.145: INFO: (7) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 9.367524ms)
May 20 13:57:28.146: INFO: (7) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 10.336369ms)
May 20 13:57:28.146: INFO: (7) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 10.371743ms)
May 20 13:57:28.146: INFO: (7) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 10.03084ms)
May 20 13:57:28.146: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 9.856663ms)
May 20 13:57:28.146: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 9.838415ms)
May 20 13:57:28.146: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 10.350927ms)
May 20 13:57:28.146: INFO: (7) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 9.936683ms)
May 20 13:57:28.152: INFO: (7) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 15.89046ms)
May 20 13:57:28.152: INFO: (7) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 15.685484ms)
May 20 13:57:28.153: INFO: (7) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 16.159496ms)
May 20 13:57:28.153: INFO: (7) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 16.539326ms)
May 20 13:57:28.153: INFO: (7) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 16.616984ms)
May 20 13:57:28.157: INFO: (7) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 21.164922ms)
May 20 13:57:28.166: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 8.492142ms)
May 20 13:57:28.166: INFO: (8) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 8.409341ms)
May 20 13:57:28.166: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 8.517007ms)
May 20 13:57:28.166: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 8.887652ms)
May 20 13:57:28.167: INFO: (8) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 9.638526ms)
May 20 13:57:28.167: INFO: (8) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 9.650373ms)
May 20 13:57:28.167: INFO: (8) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 9.906784ms)
May 20 13:57:28.167: INFO: (8) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 9.799931ms)
May 20 13:57:28.167: INFO: (8) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 9.963763ms)
May 20 13:57:28.168: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 10.24134ms)
May 20 13:57:28.173: INFO: (8) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 15.460287ms)
May 20 13:57:28.173: INFO: (8) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 15.877504ms)
May 20 13:57:28.173: INFO: (8) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 15.617246ms)
May 20 13:57:28.173: INFO: (8) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 15.858425ms)
May 20 13:57:28.173: INFO: (8) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 15.634526ms)
May 20 13:57:28.174: INFO: (8) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 15.982292ms)
May 20 13:57:28.188: INFO: (9) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 14.232147ms)
May 20 13:57:28.188: INFO: (9) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 14.394405ms)
May 20 13:57:28.188: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 14.415388ms)
May 20 13:57:28.189: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 14.334408ms)
May 20 13:57:28.189: INFO: (9) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 14.464854ms)
May 20 13:57:28.189: INFO: (9) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 14.939465ms)
May 20 13:57:28.189: INFO: (9) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 14.910499ms)
May 20 13:57:28.189: INFO: (9) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 15.361231ms)
May 20 13:57:28.189: INFO: (9) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 15.345873ms)
May 20 13:57:28.189: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 15.526161ms)
May 20 13:57:28.189: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 15.306096ms)
May 20 13:57:28.191: INFO: (9) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 16.935738ms)
May 20 13:57:28.194: INFO: (9) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 20.198092ms)
May 20 13:57:28.195: INFO: (9) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 20.851693ms)
May 20 13:57:28.195: INFO: (9) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 20.993815ms)
May 20 13:57:28.195: INFO: (9) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 20.958748ms)
May 20 13:57:28.203: INFO: (10) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 7.824114ms)
May 20 13:57:28.203: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 7.630963ms)
May 20 13:57:28.204: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 8.503366ms)
May 20 13:57:28.205: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 9.375843ms)
May 20 13:57:28.205: INFO: (10) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 9.990668ms)
May 20 13:57:28.205: INFO: (10) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 9.370808ms)
May 20 13:57:28.206: INFO: (10) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 10.329576ms)
May 20 13:57:28.206: INFO: (10) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 10.442381ms)
May 20 13:57:28.206: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 10.241866ms)
May 20 13:57:28.206: INFO: (10) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 10.874924ms)
May 20 13:57:28.208: INFO: (10) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 12.693308ms)
May 20 13:57:28.210: INFO: (10) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 14.758999ms)
May 20 13:57:28.211: INFO: (10) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 15.053517ms)
May 20 13:57:28.212: INFO: (10) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 15.717448ms)
May 20 13:57:28.212: INFO: (10) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 16.374381ms)
May 20 13:57:28.212: INFO: (10) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 15.867169ms)
May 20 13:57:28.219: INFO: (11) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 7.068049ms)
May 20 13:57:28.219: INFO: (11) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 7.189815ms)
May 20 13:57:28.220: INFO: (11) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 7.952534ms)
May 20 13:57:28.220: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 7.799315ms)
May 20 13:57:28.219: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 6.973719ms)
May 20 13:57:28.221: INFO: (11) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 9.170078ms)
May 20 13:57:28.221: INFO: (11) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 8.825523ms)
May 20 13:57:28.224: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 11.374566ms)
May 20 13:57:28.224: INFO: (11) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 11.332684ms)
May 20 13:57:28.224: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 11.700223ms)
May 20 13:57:28.225: INFO: (11) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 13.149874ms)
May 20 13:57:28.225: INFO: (11) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 13.007125ms)
May 20 13:57:28.226: INFO: (11) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 14.22534ms)
May 20 13:57:28.228: INFO: (11) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 15.757901ms)
May 20 13:57:28.228: INFO: (11) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 15.600407ms)
May 20 13:57:28.228: INFO: (11) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 15.866987ms)
May 20 13:57:28.233: INFO: (12) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 4.385258ms)
May 20 13:57:28.239: INFO: (12) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 10.701742ms)
May 20 13:57:28.239: INFO: (12) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 10.20276ms)
May 20 13:57:28.239: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 10.334965ms)
May 20 13:57:28.240: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 11.954117ms)
May 20 13:57:28.241: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 11.954216ms)
May 20 13:57:28.241: INFO: (12) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 12.619912ms)
May 20 13:57:28.241: INFO: (12) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 13.094244ms)
May 20 13:57:28.242: INFO: (12) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 12.460696ms)
May 20 13:57:28.242: INFO: (12) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 13.031149ms)
May 20 13:57:28.242: INFO: (12) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 12.318322ms)
May 20 13:57:28.242: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 13.432075ms)
May 20 13:57:28.242: INFO: (12) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 12.753733ms)
May 20 13:57:28.242: INFO: (12) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 12.720429ms)
May 20 13:57:28.243: INFO: (12) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 13.308107ms)
May 20 13:57:28.243: INFO: (12) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 14.257379ms)
May 20 13:57:28.249: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 6.063752ms)
May 20 13:57:28.249: INFO: (13) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 6.051936ms)
May 20 13:57:28.251: INFO: (13) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 7.849659ms)
May 20 13:57:28.251: INFO: (13) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 7.868579ms)
May 20 13:57:28.251: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 7.917482ms)
May 20 13:57:28.251: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 7.932288ms)
May 20 13:57:28.254: INFO: (13) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 10.270012ms)
May 20 13:57:28.254: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 10.676467ms)
May 20 13:57:28.254: INFO: (13) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 10.239385ms)
May 20 13:57:28.255: INFO: (13) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 12.105741ms)
May 20 13:57:28.255: INFO: (13) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 11.576232ms)
May 20 13:57:28.255: INFO: (13) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 11.796388ms)
May 20 13:57:28.255: INFO: (13) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 12.062277ms)
May 20 13:57:28.256: INFO: (13) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 12.19221ms)
May 20 13:57:28.256: INFO: (13) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 12.698131ms)
May 20 13:57:28.256: INFO: (13) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 12.842312ms)
May 20 13:57:28.263: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 5.737602ms)
May 20 13:57:28.265: INFO: (14) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 7.864998ms)
May 20 13:57:28.265: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 7.814557ms)
May 20 13:57:28.265: INFO: (14) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 7.889689ms)
May 20 13:57:28.268: INFO: (14) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 10.532978ms)
May 20 13:57:28.268: INFO: (14) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 10.53526ms)
May 20 13:57:28.267: INFO: (14) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 10.052558ms)
May 20 13:57:28.270: INFO: (14) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 12.784073ms)
May 20 13:57:28.271: INFO: (14) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 13.399679ms)
May 20 13:57:28.271: INFO: (14) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 13.845899ms)
May 20 13:57:28.272: INFO: (14) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 14.890154ms)
May 20 13:57:28.274: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 16.397644ms)
May 20 13:57:28.274: INFO: (14) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 17.05539ms)
May 20 13:57:28.274: INFO: (14) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 17.000566ms)
May 20 13:57:28.275: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 17.278369ms)
May 20 13:57:28.275: INFO: (14) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 17.372972ms)
May 20 13:57:28.280: INFO: (15) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 4.911569ms)
May 20 13:57:28.284: INFO: (15) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 7.949366ms)
May 20 13:57:28.284: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 8.236134ms)
May 20 13:57:28.286: INFO: (15) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 10.192273ms)
May 20 13:57:28.286: INFO: (15) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 10.979372ms)
May 20 13:57:28.286: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 10.437121ms)
May 20 13:57:28.286: INFO: (15) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 10.460909ms)
May 20 13:57:28.286: INFO: (15) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 9.812027ms)
May 20 13:57:28.286: INFO: (15) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 11.320171ms)
May 20 13:57:28.286: INFO: (15) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 11.069295ms)
May 20 13:57:28.287: INFO: (15) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 11.289789ms)
May 20 13:57:28.287: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 10.392823ms)
May 20 13:57:28.287: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 11.472018ms)
May 20 13:57:28.288: INFO: (15) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 11.186185ms)
May 20 13:57:28.288: INFO: (15) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 13.15252ms)
May 20 13:57:28.289: INFO: (15) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 13.642301ms)
May 20 13:57:28.298: INFO: (16) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 7.67936ms)
May 20 13:57:28.298: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 6.925093ms)
May 20 13:57:28.299: INFO: (16) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 7.881145ms)
May 20 13:57:28.300: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 9.374138ms)
May 20 13:57:28.301: INFO: (16) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 11.084971ms)
May 20 13:57:28.301: INFO: (16) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 9.846455ms)
May 20 13:57:28.301: INFO: (16) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 10.006384ms)
May 20 13:57:28.301: INFO: (16) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 9.893374ms)
May 20 13:57:28.301: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 10.803161ms)
May 20 13:57:28.301: INFO: (16) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 10.269685ms)
May 20 13:57:28.301: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 10.391225ms)
May 20 13:57:28.302: INFO: (16) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 11.323559ms)
May 20 13:57:28.303: INFO: (16) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 11.989868ms)
May 20 13:57:28.303: INFO: (16) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 13.339299ms)
May 20 13:57:28.305: INFO: (16) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 13.907367ms)
May 20 13:57:28.305: INFO: (16) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 14.603655ms)
May 20 13:57:28.309: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 3.711865ms)
May 20 13:57:28.317: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 9.825809ms)
May 20 13:57:28.317: INFO: (17) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 9.791507ms)
May 20 13:57:28.317: INFO: (17) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 9.878828ms)
May 20 13:57:28.317: INFO: (17) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 10.696942ms)
May 20 13:57:28.317: INFO: (17) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 9.872632ms)
May 20 13:57:28.317: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 9.829686ms)
May 20 13:57:28.317: INFO: (17) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 9.94897ms)
May 20 13:57:28.317: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 9.886118ms)
May 20 13:57:28.317: INFO: (17) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 10.913065ms)
May 20 13:57:28.321: INFO: (17) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 14.673844ms)
May 20 13:57:28.321: INFO: (17) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 15.332825ms)
May 20 13:57:28.321: INFO: (17) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 14.356396ms)
May 20 13:57:28.321: INFO: (17) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 14.835389ms)
May 20 13:57:28.321: INFO: (17) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 15.532401ms)
May 20 13:57:28.321: INFO: (17) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 14.992033ms)
May 20 13:57:28.332: INFO: (18) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 8.70486ms)
May 20 13:57:28.332: INFO: (18) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 9.472487ms)
May 20 13:57:28.334: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 11.188733ms)
May 20 13:57:28.334: INFO: (18) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 11.375712ms)
May 20 13:57:28.335: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 12.263318ms)
May 20 13:57:28.335: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 12.481268ms)
May 20 13:57:28.337: INFO: (18) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 14.223222ms)
May 20 13:57:28.337: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 14.707897ms)
May 20 13:57:28.337: INFO: (18) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 14.710177ms)
May 20 13:57:28.337: INFO: (18) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 14.810767ms)
May 20 13:57:28.337: INFO: (18) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 15.014926ms)
May 20 13:57:28.337: INFO: (18) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 14.848163ms)
May 20 13:57:28.337: INFO: (18) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 14.890521ms)
May 20 13:57:28.337: INFO: (18) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 14.686073ms)
May 20 13:57:28.337: INFO: (18) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 14.530452ms)
May 20 13:57:28.337: INFO: (18) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 14.511739ms)
May 20 13:57:28.351: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 5.342772ms)
May 20 13:57:28.357: INFO: (19) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">... (200; 10.937561ms)
May 20 13:57:28.358: INFO: (19) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:443/proxy/tlsrewritem... (200; 11.88939ms)
May 20 13:57:28.359: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 13.324389ms)
May 20 13:57:28.360: INFO: (19) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname2/proxy/: tls qux (200; 14.32596ms)
May 20 13:57:28.360: INFO: (19) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:462/proxy/: tls qux (200; 14.383885ms)
May 20 13:57:28.360: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw:1080/proxy/rewriteme">test<... (200; 14.071145ms)
May 20 13:57:28.360: INFO: (19) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname1/proxy/: foo (200; 14.338477ms)
May 20 13:57:28.360: INFO: (19) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname2/proxy/: bar (200; 14.898357ms)
May 20 13:57:28.363: INFO: (19) /api/v1/namespaces/proxy-6956/services/proxy-service-mz4lq:portname2/proxy/: bar (200; 17.015419ms)
May 20 13:57:28.363: INFO: (19) /api/v1/namespaces/proxy-6956/services/https:proxy-service-mz4lq:tlsportname1/proxy/: tls baz (200; 17.000751ms)
May 20 13:57:28.363: INFO: (19) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:162/proxy/: bar (200; 17.278009ms)
May 20 13:57:28.363: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-mz4lq-gg4jw/proxy/rewriteme">test</a> (200; 16.802063ms)
May 20 13:57:28.364: INFO: (19) /api/v1/namespaces/proxy-6956/services/http:proxy-service-mz4lq:portname1/proxy/: foo (200; 17.689703ms)
May 20 13:57:28.364: INFO: (19) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-mz4lq-gg4jw:160/proxy/: foo (200; 18.719699ms)
May 20 13:57:28.364: INFO: (19) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-mz4lq-gg4jw:460/proxy/: tls baz (200; 18.641651ms)
STEP: deleting ReplicationController proxy-service-mz4lq in namespace proxy-6956, will wait for the garbage collector to delete the pods
May 20 13:57:28.446: INFO: Deleting ReplicationController proxy-service-mz4lq took: 6.009952ms
May 20 13:57:28.547: INFO: Terminating ReplicationController proxy-service-mz4lq pods took: 101.091357ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:57:35.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6956" for this suite.

• [SLOW TEST:10.601 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":191,"skipped":3228,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:57:35.455: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 13:57:36.224: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 20 13:57:38.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115856, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115856, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115856, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115856, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 13:57:41.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 20 13:57:41.283: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:57:41.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5334" for this suite.
STEP: Destroying namespace "webhook-5334-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.899 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":192,"skipped":3233,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:57:41.354: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:57:45.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9767" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":193,"skipped":3235,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:57:45.452: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:57:45.498: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77d64bea-83d3-4ea9-83ca-744b6aa72881" in namespace "projected-9118" to be "Succeeded or Failed"
May 20 13:57:45.502: INFO: Pod "downwardapi-volume-77d64bea-83d3-4ea9-83ca-744b6aa72881": Phase="Pending", Reason="", readiness=false. Elapsed: 4.350806ms
May 20 13:57:47.507: INFO: Pod "downwardapi-volume-77d64bea-83d3-4ea9-83ca-744b6aa72881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009210661s
STEP: Saw pod success
May 20 13:57:47.507: INFO: Pod "downwardapi-volume-77d64bea-83d3-4ea9-83ca-744b6aa72881" satisfied condition "Succeeded or Failed"
May 20 13:57:47.510: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-77d64bea-83d3-4ea9-83ca-744b6aa72881 container client-container: <nil>
STEP: delete the pod
May 20 13:57:47.533: INFO: Waiting for pod downwardapi-volume-77d64bea-83d3-4ea9-83ca-744b6aa72881 to disappear
May 20 13:57:47.536: INFO: Pod downwardapi-volume-77d64bea-83d3-4ea9-83ca-744b6aa72881 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:57:47.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9118" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":194,"skipped":3242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:57:47.553: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-7994
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 20 13:57:47.597: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 20 13:57:47.664: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 13:57:49.668: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 13:57:51.667: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 13:57:53.668: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 13:57:55.668: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 13:57:57.667: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 13:57:59.669: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 13:58:01.667: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 13:58:03.669: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 13:58:05.669: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 20 13:58:05.674: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 20 13:58:05.679: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 20 13:58:07.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.48:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7994 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 13:58:07.713: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 13:58:07.839: INFO: Found all expected endpoints: [netserver-0]
May 20 13:58:07.844: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.63:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7994 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 13:58:07.844: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 13:58:07.972: INFO: Found all expected endpoints: [netserver-1]
May 20 13:58:07.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.210:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7994 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 13:58:07.977: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 13:58:08.097: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:58:08.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7994" for this suite.

• [SLOW TEST:20.554 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":195,"skipped":3264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:58:08.113: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-0161dfcf-a9a4-4ee2-873c-4d118c03f0c3
STEP: Creating a pod to test consume configMaps
May 20 13:58:08.148: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6454fee-fe11-486f-8dcf-96e495cddb24" in namespace "configmap-8020" to be "Succeeded or Failed"
May 20 13:58:08.154: INFO: Pod "pod-configmaps-c6454fee-fe11-486f-8dcf-96e495cddb24": Phase="Pending", Reason="", readiness=false. Elapsed: 5.871366ms
May 20 13:58:10.159: INFO: Pod "pod-configmaps-c6454fee-fe11-486f-8dcf-96e495cddb24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01050882s
May 20 13:58:12.163: INFO: Pod "pod-configmaps-c6454fee-fe11-486f-8dcf-96e495cddb24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014717871s
STEP: Saw pod success
May 20 13:58:12.164: INFO: Pod "pod-configmaps-c6454fee-fe11-486f-8dcf-96e495cddb24" satisfied condition "Succeeded or Failed"
May 20 13:58:12.167: INFO: Trying to get logs from node k8s-3 pod pod-configmaps-c6454fee-fe11-486f-8dcf-96e495cddb24 container configmap-volume-test: <nil>
STEP: delete the pod
May 20 13:58:12.194: INFO: Waiting for pod pod-configmaps-c6454fee-fe11-486f-8dcf-96e495cddb24 to disappear
May 20 13:58:12.197: INFO: Pod pod-configmaps-c6454fee-fe11-486f-8dcf-96e495cddb24 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:58:12.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8020" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":196,"skipped":3307,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:58:12.209: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-ba8f3729-8cd4-4a30-9cf0-1ac7e61f2ba0
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:58:12.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2206" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":197,"skipped":3310,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:58:12.249: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:58:12.326: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 20 13:58:14.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 create -f -'
May 20 13:58:15.171: INFO: stderr: ""
May 20 13:58:15.171: INFO: stdout: "e2e-test-crd-publish-openapi-325-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 20 13:58:15.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 delete e2e-test-crd-publish-openapi-325-crds test-cr'
May 20 13:58:15.253: INFO: stderr: ""
May 20 13:58:15.253: INFO: stdout: "e2e-test-crd-publish-openapi-325-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 20 13:58:15.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 apply -f -'
May 20 13:58:15.455: INFO: stderr: ""
May 20 13:58:15.455: INFO: stdout: "e2e-test-crd-publish-openapi-325-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 20 13:58:15.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 delete e2e-test-crd-publish-openapi-325-crds test-cr'
May 20 13:58:15.541: INFO: stderr: ""
May 20 13:58:15.541: INFO: stdout: "e2e-test-crd-publish-openapi-325-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 20 13:58:15.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-5482 explain e2e-test-crd-publish-openapi-325-crds'
May 20 13:58:15.760: INFO: stderr: ""
May 20 13:58:15.760: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-325-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:58:18.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5482" for this suite.

• [SLOW TEST:6.389 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":198,"skipped":3321,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:58:18.638: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:58:18.664: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:58:25.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-408" for this suite.

• [SLOW TEST:6.693 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":199,"skipped":3337,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:58:25.336: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 20 13:58:25.380: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1920 /api/v1/namespaces/watch-1920/configmaps/e2e-watch-test-label-changed 1422162d-0204-42c6-a5ed-2bed3603cc5f 24896 0 2021-05-20 13:58:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-20 13:58:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 13:58:25.381: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1920 /api/v1/namespaces/watch-1920/configmaps/e2e-watch-test-label-changed 1422162d-0204-42c6-a5ed-2bed3603cc5f 24897 0 2021-05-20 13:58:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-20 13:58:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 13:58:25.381: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1920 /api/v1/namespaces/watch-1920/configmaps/e2e-watch-test-label-changed 1422162d-0204-42c6-a5ed-2bed3603cc5f 24898 0 2021-05-20 13:58:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-20 13:58:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 20 13:58:35.409: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1920 /api/v1/namespaces/watch-1920/configmaps/e2e-watch-test-label-changed 1422162d-0204-42c6-a5ed-2bed3603cc5f 24932 0 2021-05-20 13:58:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-20 13:58:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 13:58:35.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1920 /api/v1/namespaces/watch-1920/configmaps/e2e-watch-test-label-changed 1422162d-0204-42c6-a5ed-2bed3603cc5f 24933 0 2021-05-20 13:58:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-20 13:58:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 13:58:35.409: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1920 /api/v1/namespaces/watch-1920/configmaps/e2e-watch-test-label-changed 1422162d-0204-42c6-a5ed-2bed3603cc5f 24934 0 2021-05-20 13:58:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-20 13:58:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:58:35.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1920" for this suite.

• [SLOW TEST:10.082 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":200,"skipped":3351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:58:35.418: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-9d7bc718-bc6b-44d4-964b-43fe9ff385e4
STEP: Creating a pod to test consume configMaps
May 20 13:58:35.458: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff146f45-6d6b-4d69-93a3-ebaedd4399de" in namespace "configmap-7903" to be "Succeeded or Failed"
May 20 13:58:35.463: INFO: Pod "pod-configmaps-ff146f45-6d6b-4d69-93a3-ebaedd4399de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138221ms
May 20 13:58:37.467: INFO: Pod "pod-configmaps-ff146f45-6d6b-4d69-93a3-ebaedd4399de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009490092s
May 20 13:58:39.471: INFO: Pod "pod-configmaps-ff146f45-6d6b-4d69-93a3-ebaedd4399de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01299661s
STEP: Saw pod success
May 20 13:58:39.471: INFO: Pod "pod-configmaps-ff146f45-6d6b-4d69-93a3-ebaedd4399de" satisfied condition "Succeeded or Failed"
May 20 13:58:39.473: INFO: Trying to get logs from node k8s-3 pod pod-configmaps-ff146f45-6d6b-4d69-93a3-ebaedd4399de container configmap-volume-test: <nil>
STEP: delete the pod
May 20 13:58:39.487: INFO: Waiting for pod pod-configmaps-ff146f45-6d6b-4d69-93a3-ebaedd4399de to disappear
May 20 13:58:39.490: INFO: Pod pod-configmaps-ff146f45-6d6b-4d69-93a3-ebaedd4399de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:58:39.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7903" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":201,"skipped":3376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:58:39.497: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 13:58:39.533: INFO: Waiting up to 5m0s for pod "downwardapi-volume-daf7b498-0eb7-4e5e-bbad-cd6eacb018f5" in namespace "downward-api-6033" to be "Succeeded or Failed"
May 20 13:58:39.536: INFO: Pod "downwardapi-volume-daf7b498-0eb7-4e5e-bbad-cd6eacb018f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.834563ms
May 20 13:58:41.539: INFO: Pod "downwardapi-volume-daf7b498-0eb7-4e5e-bbad-cd6eacb018f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005711413s
May 20 13:58:43.546: INFO: Pod "downwardapi-volume-daf7b498-0eb7-4e5e-bbad-cd6eacb018f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013303998s
STEP: Saw pod success
May 20 13:58:43.546: INFO: Pod "downwardapi-volume-daf7b498-0eb7-4e5e-bbad-cd6eacb018f5" satisfied condition "Succeeded or Failed"
May 20 13:58:43.549: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-daf7b498-0eb7-4e5e-bbad-cd6eacb018f5 container client-container: <nil>
STEP: delete the pod
May 20 13:58:43.566: INFO: Waiting for pod downwardapi-volume-daf7b498-0eb7-4e5e-bbad-cd6eacb018f5 to disappear
May 20 13:58:43.568: INFO: Pod downwardapi-volume-daf7b498-0eb7-4e5e-bbad-cd6eacb018f5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:58:43.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6033" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":202,"skipped":3421,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:58:43.575: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5479
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5479
STEP: Creating statefulset with conflicting port in namespace statefulset-5479
STEP: Waiting until pod test-pod will start running in namespace statefulset-5479
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5479
May 20 13:58:47.650: INFO: Observed stateful pod in namespace: statefulset-5479, name: ss-0, uid: d9198060-b94c-4c53-ab44-c3f0dac44e2a, status phase: Pending. Waiting for statefulset controller to delete.
May 20 13:58:48.037: INFO: Observed stateful pod in namespace: statefulset-5479, name: ss-0, uid: d9198060-b94c-4c53-ab44-c3f0dac44e2a, status phase: Failed. Waiting for statefulset controller to delete.
May 20 13:58:48.044: INFO: Observed stateful pod in namespace: statefulset-5479, name: ss-0, uid: d9198060-b94c-4c53-ab44-c3f0dac44e2a, status phase: Failed. Waiting for statefulset controller to delete.
May 20 13:58:48.047: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5479
STEP: Removing pod with conflicting port in namespace statefulset-5479
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5479 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 20 13:58:52.072: INFO: Deleting all statefulset in ns statefulset-5479
May 20 13:58:52.075: INFO: Scaling statefulset ss to 0
May 20 13:59:02.087: INFO: Waiting for statefulset status.replicas updated to 0
May 20 13:59:02.089: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:59:02.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5479" for this suite.

• [SLOW TEST:18.547 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":203,"skipped":3435,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:59:02.122: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-3424379a-e536-4da3-ad0f-260f59667d94
STEP: Creating secret with name s-test-opt-upd-fa919e0c-1de4-414b-9804-4e107c4f932f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3424379a-e536-4da3-ad0f-260f59667d94
STEP: Updating secret s-test-opt-upd-fa919e0c-1de4-414b-9804-4e107c4f932f
STEP: Creating secret with name s-test-opt-create-6b307d90-681b-46f4-8802-3f14fe80db98
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:59:08.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2492" for this suite.

• [SLOW TEST:6.121 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":204,"skipped":3441,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:59:08.243: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9885
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9885
STEP: creating replication controller externalsvc in namespace services-9885
I0520 13:59:08.310480      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9885, replica count: 2
I0520 13:59:11.361577      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 20 13:59:11.387: INFO: Creating new exec pod
May 20 13:59:15.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-9885 exec execpodnqg7v -- /bin/sh -x -c nslookup nodeport-service.services-9885.svc.cluster.local'
May 20 13:59:15.673: INFO: stderr: "+ nslookup nodeport-service.services-9885.svc.cluster.local\n"
May 20 13:59:15.673: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-9885.svc.cluster.local\tcanonical name = externalsvc.services-9885.svc.cluster.local.\nName:\texternalsvc.services-9885.svc.cluster.local\nAddress: 10.233.12.9\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9885, will wait for the garbage collector to delete the pods
May 20 13:59:15.731: INFO: Deleting ReplicationController externalsvc took: 4.785437ms
May 20 13:59:16.131: INFO: Terminating ReplicationController externalsvc pods took: 400.203934ms
May 20 13:59:25.445: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:59:25.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9885" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:17.236 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":205,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:59:25.480: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-dda15633-d818-4493-bea4-c10f46b9f466
STEP: Creating a pod to test consume configMaps
May 20 13:59:25.534: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1959379c-5881-4655-ad1b-1f17d0ea27ab" in namespace "projected-6550" to be "Succeeded or Failed"
May 20 13:59:25.540: INFO: Pod "pod-projected-configmaps-1959379c-5881-4655-ad1b-1f17d0ea27ab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.66788ms
May 20 13:59:27.544: INFO: Pod "pod-projected-configmaps-1959379c-5881-4655-ad1b-1f17d0ea27ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01021282s
May 20 13:59:29.559: INFO: Pod "pod-projected-configmaps-1959379c-5881-4655-ad1b-1f17d0ea27ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024860511s
STEP: Saw pod success
May 20 13:59:29.559: INFO: Pod "pod-projected-configmaps-1959379c-5881-4655-ad1b-1f17d0ea27ab" satisfied condition "Succeeded or Failed"
May 20 13:59:29.563: INFO: Trying to get logs from node k8s-3 pod pod-projected-configmaps-1959379c-5881-4655-ad1b-1f17d0ea27ab container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 20 13:59:29.578: INFO: Waiting for pod pod-projected-configmaps-1959379c-5881-4655-ad1b-1f17d0ea27ab to disappear
May 20 13:59:29.581: INFO: Pod pod-projected-configmaps-1959379c-5881-4655-ad1b-1f17d0ea27ab no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:59:29.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6550" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:59:29.588: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 13:59:29.618: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 20 13:59:34.667: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 20 13:59:34.667: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 20 13:59:36.670: INFO: Creating deployment "test-rollover-deployment"
May 20 13:59:36.678: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 20 13:59:38.691: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 20 13:59:38.697: INFO: Ensure that both replica sets have 1 created replica
May 20 13:59:38.703: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 20 13:59:38.710: INFO: Updating deployment test-rollover-deployment
May 20 13:59:38.710: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 20 13:59:40.718: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 20 13:59:40.724: INFO: Make sure deployment "test-rollover-deployment" is complete
May 20 13:59:40.729: INFO: all replica sets need to contain the pod-template-hash label
May 20 13:59:40.729: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115978, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:59:42.738: INFO: all replica sets need to contain the pod-template-hash label
May 20 13:59:42.738: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115980, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:59:44.736: INFO: all replica sets need to contain the pod-template-hash label
May 20 13:59:44.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115980, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:59:46.752: INFO: all replica sets need to contain the pod-template-hash label
May 20 13:59:46.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115980, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:59:48.737: INFO: all replica sets need to contain the pod-template-hash label
May 20 13:59:48.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115980, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:59:50.736: INFO: all replica sets need to contain the pod-template-hash label
May 20 13:59:50.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115980, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115976, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 20 13:59:52.736: INFO: 
May 20 13:59:52.737: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 20 13:59:52.745: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4375 /apis/apps/v1/namespaces/deployment-4375/deployments/test-rollover-deployment 3c58f01f-5970-415c-8593-23fe7b7dfe2a 25618 2 2021-05-20 13:59:36 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-20 13:59:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-20 13:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d5fcd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-20 13:59:36 +0000 UTC,LastTransitionTime:2021-05-20 13:59:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-05-20 13:59:51 +0000 UTC,LastTransitionTime:2021-05-20 13:59:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 20 13:59:52.752: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-4375 /apis/apps/v1/namespaces/deployment-4375/replicasets/test-rollover-deployment-5797c7764 790a7e0e-0da3-471c-8783-90d13369c376 25608 2 2021-05-20 13:59:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3c58f01f-5970-415c-8593-23fe7b7dfe2a 0xc005dba510 0xc005dba511}] []  [{kube-controller-manager Update apps/v1 2021-05-20 13:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c58f01f-5970-415c-8593-23fe7b7dfe2a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dba598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 20 13:59:52.752: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 20 13:59:52.753: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4375 /apis/apps/v1/namespaces/deployment-4375/replicasets/test-rollover-controller 4637e5c1-2a5a-455d-9a2f-25564748d2f3 25617 2 2021-05-20 13:59:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3c58f01f-5970-415c-8593-23fe7b7dfe2a 0xc005dba3a7 0xc005dba3a8}] []  [{e2e.test Update apps/v1 2021-05-20 13:59:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-20 13:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c58f01f-5970-415c-8593-23fe7b7dfe2a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005dba498 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 20 13:59:52.753: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4375 /apis/apps/v1/namespaces/deployment-4375/replicasets/test-rollover-deployment-78bc8b888c 3db29cd0-7fa4-447a-9771-dd7ff8435d29 25565 2 2021-05-20 13:59:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3c58f01f-5970-415c-8593-23fe7b7dfe2a 0xc005dba607 0xc005dba608}] []  [{kube-controller-manager Update apps/v1 2021-05-20 13:59:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c58f01f-5970-415c-8593-23fe7b7dfe2a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dba698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 20 13:59:52.756: INFO: Pod "test-rollover-deployment-5797c7764-5w8tf" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-5w8tf test-rollover-deployment-5797c7764- deployment-4375 /api/v1/namespaces/deployment-4375/pods/test-rollover-deployment-5797c7764-5w8tf 5f9c1b10-e1ef-4f6c-abf9-11ac8e23e006 25577 0 2021-05-20 13:59:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 790a7e0e-0da3-471c-8783-90d13369c376 0xc005dbad60 0xc005dbad61}] []  [{kube-controller-manager Update v1 2021-05-20 13:59:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"790a7e0e-0da3-471c-8783-90d13369c376\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 13:59:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pbd7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pbd7h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pbd7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:59:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:59:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:59:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 13:59:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.102,PodIP:10.233.65.68,StartTime:2021-05-20 13:59:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 13:59:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://2179b1092294569d5c5ad9f5f2aef4a8e415d6f4ff9319892752acfefa29325d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:59:52.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4375" for this suite.

• [SLOW TEST:23.180 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":207,"skipped":3498,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:59:52.775: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 20 13:59:52.814: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-58 /api/v1/namespaces/watch-58/configmaps/e2e-watch-test-watch-closed 7af2107b-d9a5-40d5-bb09-28e9c4eeb582 25632 0 2021-05-20 13:59:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-20 13:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 13:59:52.815: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-58 /api/v1/namespaces/watch-58/configmaps/e2e-watch-test-watch-closed 7af2107b-d9a5-40d5-bb09-28e9c4eeb582 25633 0 2021-05-20 13:59:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-20 13:59:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 20 13:59:52.827: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-58 /api/v1/namespaces/watch-58/configmaps/e2e-watch-test-watch-closed 7af2107b-d9a5-40d5-bb09-28e9c4eeb582 25634 0 2021-05-20 13:59:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-20 13:59:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 13:59:52.827: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-58 /api/v1/namespaces/watch-58/configmaps/e2e-watch-test-watch-closed 7af2107b-d9a5-40d5-bb09-28e9c4eeb582 25635 0 2021-05-20 13:59:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-20 13:59:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 13:59:52.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-58" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":208,"skipped":3520,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 13:59:52.836: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 20 13:59:52.864: INFO: PodSpec: initContainers in spec.initContainers
May 20 14:00:38.696: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7f3bcc47-ae29-4e63-aa75-740c9105d76c", GenerateName:"", Namespace:"init-container-4753", SelfLink:"/api/v1/namespaces/init-container-4753/pods/pod-init-7f3bcc47-ae29-4e63-aa75-740c9105d76c", UID:"89a28934-1ae6-4956-b13e-566f500e9cff", ResourceVersion:"25833", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63757115992, loc:(*time.Location)(0x77148e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"863934844"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0047781c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004778220)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004778240), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004778280)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-kmq5b", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc007926100), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kmq5b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kmq5b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kmq5b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005f342c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002ef8150), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005f34350)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005f34370)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005f34378), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005f3437c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005476070), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115992, loc:(*time.Location)(0x77148e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115992, loc:(*time.Location)(0x77148e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115992, loc:(*time.Location)(0x77148e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757115992, loc:(*time.Location)(0x77148e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.18.8.103", PodIP:"10.233.66.220", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.220"}}, StartTime:(*v1.Time)(0xc0047782a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0047782e0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ef8230)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://56efe3034f9b79eecc70379fa56dda104bb298275e5f02670ea2f3f3dca47c36", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004778300), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0047782c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc005f343ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:00:38.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4753" for this suite.

• [SLOW TEST:45.871 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":209,"skipped":3529,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:00:38.707: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
May 20 14:00:38.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-5532 create -f -'
May 20 14:00:38.974: INFO: stderr: ""
May 20 14:00:38.974: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 20 14:00:38.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-5532 diff -f -'
May 20 14:00:39.259: INFO: rc: 1
May 20 14:00:39.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-5532 delete -f -'
May 20 14:00:39.394: INFO: stderr: ""
May 20 14:00:39.394: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:00:39.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5532" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":210,"skipped":3545,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:00:39.415: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:00:41.504: INFO: Waiting up to 5m0s for pod "client-envvars-6b87cd6a-9bf9-4879-8e55-39a3fff71135" in namespace "pods-5044" to be "Succeeded or Failed"
May 20 14:00:41.507: INFO: Pod "client-envvars-6b87cd6a-9bf9-4879-8e55-39a3fff71135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.589279ms
May 20 14:00:43.513: INFO: Pod "client-envvars-6b87cd6a-9bf9-4879-8e55-39a3fff71135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008702244s
May 20 14:00:45.520: INFO: Pod "client-envvars-6b87cd6a-9bf9-4879-8e55-39a3fff71135": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015648268s
STEP: Saw pod success
May 20 14:00:45.520: INFO: Pod "client-envvars-6b87cd6a-9bf9-4879-8e55-39a3fff71135" satisfied condition "Succeeded or Failed"
May 20 14:00:45.523: INFO: Trying to get logs from node k8s-3 pod client-envvars-6b87cd6a-9bf9-4879-8e55-39a3fff71135 container env3cont: <nil>
STEP: delete the pod
May 20 14:00:45.552: INFO: Waiting for pod client-envvars-6b87cd6a-9bf9-4879-8e55-39a3fff71135 to disappear
May 20 14:00:45.555: INFO: Pod client-envvars-6b87cd6a-9bf9-4879-8e55-39a3fff71135 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:00:45.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5044" for this suite.

• [SLOW TEST:6.153 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":211,"skipped":3548,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:00:45.568: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4570.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4570.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4570.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4570.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 20 14:00:49.675: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local from pod dns-4570/dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3: the server could not find the requested resource (get pods dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3)
May 20 14:00:49.678: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local from pod dns-4570/dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3: the server could not find the requested resource (get pods dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3)
May 20 14:00:49.682: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4570.svc.cluster.local from pod dns-4570/dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3: the server could not find the requested resource (get pods dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3)
May 20 14:00:49.685: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4570.svc.cluster.local from pod dns-4570/dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3: the server could not find the requested resource (get pods dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3)
May 20 14:00:49.696: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local from pod dns-4570/dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3: the server could not find the requested resource (get pods dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3)
May 20 14:00:49.705: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local from pod dns-4570/dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3: the server could not find the requested resource (get pods dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3)
May 20 14:00:49.709: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4570.svc.cluster.local from pod dns-4570/dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3: the server could not find the requested resource (get pods dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3)
May 20 14:00:49.712: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4570.svc.cluster.local from pod dns-4570/dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3: the server could not find the requested resource (get pods dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3)
May 20 14:00:49.718: INFO: Lookups using dns-4570/dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4570.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4570.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4570.svc.cluster.local jessie_udp@dns-test-service-2.dns-4570.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4570.svc.cluster.local]

May 20 14:00:54.760: INFO: DNS probes using dns-4570/dns-test-e9738f37-b950-4f68-b14f-e37dda3bf5e3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:00:54.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4570" for this suite.

• [SLOW TEST:9.312 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":212,"skipped":3549,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:00:54.879: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-j5k5
STEP: Creating a pod to test atomic-volume-subpath
May 20 14:00:54.945: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-j5k5" in namespace "subpath-299" to be "Succeeded or Failed"
May 20 14:00:54.960: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Pending", Reason="", readiness=false. Elapsed: 15.305082ms
May 20 14:00:56.974: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029472939s
May 20 14:00:58.978: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Running", Reason="", readiness=true. Elapsed: 4.033109861s
May 20 14:01:00.981: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Running", Reason="", readiness=true. Elapsed: 6.036496627s
May 20 14:01:02.988: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Running", Reason="", readiness=true. Elapsed: 8.042971485s
May 20 14:01:04.994: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Running", Reason="", readiness=true. Elapsed: 10.049497562s
May 20 14:01:06.998: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Running", Reason="", readiness=true. Elapsed: 12.052724403s
May 20 14:01:09.001: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Running", Reason="", readiness=true. Elapsed: 14.055750834s
May 20 14:01:11.004: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Running", Reason="", readiness=true. Elapsed: 16.058642173s
May 20 14:01:13.007: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Running", Reason="", readiness=true. Elapsed: 18.062015033s
May 20 14:01:15.010: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Running", Reason="", readiness=true. Elapsed: 20.065025199s
May 20 14:01:17.013: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Running", Reason="", readiness=true. Elapsed: 22.067978775s
May 20 14:01:19.016: INFO: Pod "pod-subpath-test-projected-j5k5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.071105293s
STEP: Saw pod success
May 20 14:01:19.016: INFO: Pod "pod-subpath-test-projected-j5k5" satisfied condition "Succeeded or Failed"
May 20 14:01:19.018: INFO: Trying to get logs from node k8s-3 pod pod-subpath-test-projected-j5k5 container test-container-subpath-projected-j5k5: <nil>
STEP: delete the pod
May 20 14:01:19.034: INFO: Waiting for pod pod-subpath-test-projected-j5k5 to disappear
May 20 14:01:19.036: INFO: Pod pod-subpath-test-projected-j5k5 no longer exists
STEP: Deleting pod pod-subpath-test-projected-j5k5
May 20 14:01:19.036: INFO: Deleting pod "pod-subpath-test-projected-j5k5" in namespace "subpath-299"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:01:19.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-299" for this suite.

• [SLOW TEST:24.166 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":213,"skipped":3553,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:01:19.046: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-9616ffaf-9fd0-4a02-8e66-a6a92801056a in namespace container-probe-4083
May 20 14:01:23.083: INFO: Started pod busybox-9616ffaf-9fd0-4a02-8e66-a6a92801056a in namespace container-probe-4083
STEP: checking the pod's current state and verifying that restartCount is present
May 20 14:01:23.086: INFO: Initial restart count of pod busybox-9616ffaf-9fd0-4a02-8e66-a6a92801056a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:05:24.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4083" for this suite.

• [SLOW TEST:245.209 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":214,"skipped":3568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:05:24.255: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 20 14:05:24.286: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:05:28.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1450" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":215,"skipped":3591,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:05:28.346: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-4822/secret-test-e4e0ccd8-e5da-4ff3-8ba7-320a4460de31
STEP: Creating a pod to test consume secrets
May 20 14:05:28.430: INFO: Waiting up to 5m0s for pod "pod-configmaps-99f804a5-7906-43c7-a4ad-e7dfb1bed5ad" in namespace "secrets-4822" to be "Succeeded or Failed"
May 20 14:05:28.438: INFO: Pod "pod-configmaps-99f804a5-7906-43c7-a4ad-e7dfb1bed5ad": Phase="Pending", Reason="", readiness=false. Elapsed: 7.414246ms
May 20 14:05:30.456: INFO: Pod "pod-configmaps-99f804a5-7906-43c7-a4ad-e7dfb1bed5ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025494737s
May 20 14:05:32.460: INFO: Pod "pod-configmaps-99f804a5-7906-43c7-a4ad-e7dfb1bed5ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030015111s
STEP: Saw pod success
May 20 14:05:32.461: INFO: Pod "pod-configmaps-99f804a5-7906-43c7-a4ad-e7dfb1bed5ad" satisfied condition "Succeeded or Failed"
May 20 14:05:32.463: INFO: Trying to get logs from node k8s-3 pod pod-configmaps-99f804a5-7906-43c7-a4ad-e7dfb1bed5ad container env-test: <nil>
STEP: delete the pod
May 20 14:05:32.493: INFO: Waiting for pod pod-configmaps-99f804a5-7906-43c7-a4ad-e7dfb1bed5ad to disappear
May 20 14:05:32.498: INFO: Pod pod-configmaps-99f804a5-7906-43c7-a4ad-e7dfb1bed5ad no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:05:32.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4822" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":216,"skipped":3601,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:05:32.511: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-a3c3e399-9fc7-4957-9b77-03a83362ebe4
STEP: Creating a pod to test consume secrets
May 20 14:05:32.554: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-34decfdd-bc81-4822-8bfa-473d574d9536" in namespace "projected-6725" to be "Succeeded or Failed"
May 20 14:05:32.558: INFO: Pod "pod-projected-secrets-34decfdd-bc81-4822-8bfa-473d574d9536": Phase="Pending", Reason="", readiness=false. Elapsed: 3.726641ms
May 20 14:05:34.562: INFO: Pod "pod-projected-secrets-34decfdd-bc81-4822-8bfa-473d574d9536": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007287936s
May 20 14:05:36.566: INFO: Pod "pod-projected-secrets-34decfdd-bc81-4822-8bfa-473d574d9536": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010995884s
STEP: Saw pod success
May 20 14:05:36.566: INFO: Pod "pod-projected-secrets-34decfdd-bc81-4822-8bfa-473d574d9536" satisfied condition "Succeeded or Failed"
May 20 14:05:36.568: INFO: Trying to get logs from node k8s-3 pod pod-projected-secrets-34decfdd-bc81-4822-8bfa-473d574d9536 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 20 14:05:36.585: INFO: Waiting for pod pod-projected-secrets-34decfdd-bc81-4822-8bfa-473d574d9536 to disappear
May 20 14:05:36.589: INFO: Pod pod-projected-secrets-34decfdd-bc81-4822-8bfa-473d574d9536 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:05:36.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6725" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":217,"skipped":3607,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:05:36.597: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 20 14:05:44.655: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5430 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:05:44.655: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:05:44.814: INFO: Exec stderr: ""
May 20 14:05:44.814: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5430 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:05:44.814: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:05:44.975: INFO: Exec stderr: ""
May 20 14:05:44.975: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5430 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:05:44.975: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:05:45.093: INFO: Exec stderr: ""
May 20 14:05:45.093: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5430 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:05:45.093: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:05:45.212: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 20 14:05:45.212: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5430 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:05:45.212: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:05:45.333: INFO: Exec stderr: ""
May 20 14:05:45.333: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5430 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:05:45.333: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:05:45.442: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 20 14:05:45.442: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5430 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:05:45.442: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:05:45.547: INFO: Exec stderr: ""
May 20 14:05:45.548: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5430 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:05:45.548: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:05:45.653: INFO: Exec stderr: ""
May 20 14:05:45.653: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5430 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:05:45.653: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:05:45.761: INFO: Exec stderr: ""
May 20 14:05:45.761: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5430 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:05:45.761: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:05:45.872: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:05:45.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5430" for this suite.

• [SLOW TEST:9.285 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":218,"skipped":3627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:05:45.883: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:05:45.919: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c8ac9b38-09d7-4838-ac67-6869cd7adea4" in namespace "security-context-test-5796" to be "Succeeded or Failed"
May 20 14:05:45.923: INFO: Pod "busybox-readonly-false-c8ac9b38-09d7-4838-ac67-6869cd7adea4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054055ms
May 20 14:05:47.927: INFO: Pod "busybox-readonly-false-c8ac9b38-09d7-4838-ac67-6869cd7adea4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00834753s
May 20 14:05:47.927: INFO: Pod "busybox-readonly-false-c8ac9b38-09d7-4838-ac67-6869cd7adea4" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:05:47.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5796" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":219,"skipped":3654,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:05:47.938: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 20 14:05:47.973: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 20 14:05:47.987: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 20 14:05:47.987: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 20 14:05:48.014: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 20 14:05:48.014: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 20 14:05:48.045: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 20 14:05:48.045: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 20 14:05:55.101: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:05:55.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9286" for this suite.

• [SLOW TEST:7.182 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":220,"skipped":3660,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:05:55.121: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 14:05:55.663: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 14:05:57.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757116355, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757116355, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757116355, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757116355, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 14:06:00.718: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:06:00.724: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-757-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:06:01.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5375" for this suite.
STEP: Destroying namespace "webhook-5375-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.845 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":221,"skipped":3663,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:06:01.966: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:07:02.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-100" for this suite.

• [SLOW TEST:60.087 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":222,"skipped":3683,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:07:02.056: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:07:02.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1937" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":223,"skipped":3704,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:07:02.144: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6018
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-6018
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6018
May 20 14:07:02.199: INFO: Found 0 stateful pods, waiting for 1
May 20 14:07:12.207: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 20 14:07:12.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-6018 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 20 14:07:12.448: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 20 14:07:12.448: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 20 14:07:12.448: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 20 14:07:12.450: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 20 14:07:22.455: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 20 14:07:22.455: INFO: Waiting for statefulset status.replicas updated to 0
May 20 14:07:22.471: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:22.471: INFO: ss-0  k8s-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:02 +0000 UTC  }]
May 20 14:07:22.471: INFO: ss-1         Pending         []
May 20 14:07:22.471: INFO: 
May 20 14:07:22.471: INFO: StatefulSet ss has not reached scale 3, at 2
May 20 14:07:23.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992770697s
May 20 14:07:24.491: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98307317s
May 20 14:07:25.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973101035s
May 20 14:07:26.500: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967872282s
May 20 14:07:27.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96389455s
May 20 14:07:28.508: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960252357s
May 20 14:07:29.512: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955477749s
May 20 14:07:30.518: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950466709s
May 20 14:07:31.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.62438ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6018
May 20 14:07:32.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-6018 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 20 14:07:32.709: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 20 14:07:32.709: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 20 14:07:32.709: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 20 14:07:32.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-6018 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 20 14:07:32.905: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 20 14:07:32.905: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 20 14:07:32.905: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 20 14:07:32.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-6018 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 20 14:07:33.128: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 20 14:07:33.128: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 20 14:07:33.128: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 20 14:07:33.132: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 20 14:07:33.132: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 20 14:07:33.132: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 20 14:07:33.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-6018 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 20 14:07:33.333: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 20 14:07:33.333: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 20 14:07:33.333: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 20 14:07:33.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-6018 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 20 14:07:33.533: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 20 14:07:33.533: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 20 14:07:33.533: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 20 14:07:33.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-6018 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 20 14:07:33.752: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 20 14:07:33.752: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 20 14:07:33.752: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 20 14:07:33.752: INFO: Waiting for statefulset status.replicas updated to 0
May 20 14:07:33.756: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 20 14:07:43.763: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 20 14:07:43.763: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 20 14:07:43.763: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 20 14:07:43.774: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:43.774: INFO: ss-0  k8s-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:02 +0000 UTC  }]
May 20 14:07:43.774: INFO: ss-1  k8s-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:43.774: INFO: ss-2  k8s-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:43.774: INFO: 
May 20 14:07:43.774: INFO: StatefulSet ss has not reached scale 0, at 3
May 20 14:07:44.778: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:44.778: INFO: ss-0  k8s-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:02 +0000 UTC  }]
May 20 14:07:44.778: INFO: ss-1  k8s-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:44.778: INFO: ss-2  k8s-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:44.778: INFO: 
May 20 14:07:44.778: INFO: StatefulSet ss has not reached scale 0, at 3
May 20 14:07:45.783: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:45.783: INFO: ss-2  k8s-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:45.783: INFO: 
May 20 14:07:45.783: INFO: StatefulSet ss has not reached scale 0, at 1
May 20 14:07:46.788: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:46.788: INFO: ss-2  k8s-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:46.788: INFO: 
May 20 14:07:46.788: INFO: StatefulSet ss has not reached scale 0, at 1
May 20 14:07:47.791: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:47.791: INFO: ss-2  k8s-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:47.791: INFO: 
May 20 14:07:47.791: INFO: StatefulSet ss has not reached scale 0, at 1
May 20 14:07:48.811: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:48.811: INFO: ss-2  k8s-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:48.811: INFO: 
May 20 14:07:48.811: INFO: StatefulSet ss has not reached scale 0, at 1
May 20 14:07:49.814: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:49.814: INFO: ss-2  k8s-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:49.814: INFO: 
May 20 14:07:49.814: INFO: StatefulSet ss has not reached scale 0, at 1
May 20 14:07:50.819: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:50.819: INFO: ss-2  k8s-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:50.819: INFO: 
May 20 14:07:50.819: INFO: StatefulSet ss has not reached scale 0, at 1
May 20 14:07:51.912: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:51.912: INFO: ss-2  k8s-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:51.912: INFO: 
May 20 14:07:51.912: INFO: StatefulSet ss has not reached scale 0, at 1
May 20 14:07:52.932: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 20 14:07:52.932: INFO: ss-2  k8s-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-20 14:07:22 +0000 UTC  }]
May 20 14:07:52.932: INFO: 
May 20 14:07:52.932: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6018
May 20 14:07:53.937: INFO: Scaling statefulset ss to 0
May 20 14:07:53.945: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 20 14:07:53.948: INFO: Deleting all statefulset in ns statefulset-6018
May 20 14:07:53.950: INFO: Scaling statefulset ss to 0
May 20 14:07:53.960: INFO: Waiting for statefulset status.replicas updated to 0
May 20 14:07:53.962: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:07:53.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6018" for this suite.

• [SLOW TEST:51.846 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":224,"skipped":3728,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:07:53.991: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:07:54.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1001" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":225,"skipped":3757,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:07:54.053: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 14:07:54.082: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65ac3b32-64de-4cdb-ab6d-e10830307a7e" in namespace "downward-api-7560" to be "Succeeded or Failed"
May 20 14:07:54.099: INFO: Pod "downwardapi-volume-65ac3b32-64de-4cdb-ab6d-e10830307a7e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.364809ms
May 20 14:07:56.102: INFO: Pod "downwardapi-volume-65ac3b32-64de-4cdb-ab6d-e10830307a7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01981364s
STEP: Saw pod success
May 20 14:07:56.102: INFO: Pod "downwardapi-volume-65ac3b32-64de-4cdb-ab6d-e10830307a7e" satisfied condition "Succeeded or Failed"
May 20 14:07:56.105: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-65ac3b32-64de-4cdb-ab6d-e10830307a7e container client-container: <nil>
STEP: delete the pod
May 20 14:07:56.127: INFO: Waiting for pod downwardapi-volume-65ac3b32-64de-4cdb-ab6d-e10830307a7e to disappear
May 20 14:07:56.129: INFO: Pod downwardapi-volume-65ac3b32-64de-4cdb-ab6d-e10830307a7e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:07:56.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7560" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":226,"skipped":3765,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:07:56.143: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1774.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1774.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 20 14:07:58.242: INFO: DNS probes using dns-1774/dns-test-b0e3e569-3757-45b1-98fa-1026bd07fa82 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:07:58.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1774" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":227,"skipped":3814,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:07:58.261: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 20 14:08:00.828: INFO: Successfully updated pod "annotationupdatef41dcce8-cb73-482a-af78-426e57145e00"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:08:02.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5517" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":228,"skipped":3820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:08:02.852: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:08:02.872: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 20 14:08:05.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 --namespace=crd-publish-openapi-6046 create -f -'
May 20 14:08:06.358: INFO: stderr: ""
May 20 14:08:06.358: INFO: stdout: "e2e-test-crd-publish-openapi-4705-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 20 14:08:06.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 --namespace=crd-publish-openapi-6046 delete e2e-test-crd-publish-openapi-4705-crds test-foo'
May 20 14:08:06.436: INFO: stderr: ""
May 20 14:08:06.436: INFO: stdout: "e2e-test-crd-publish-openapi-4705-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 20 14:08:06.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 --namespace=crd-publish-openapi-6046 apply -f -'
May 20 14:08:06.599: INFO: stderr: ""
May 20 14:08:06.599: INFO: stdout: "e2e-test-crd-publish-openapi-4705-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 20 14:08:06.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 --namespace=crd-publish-openapi-6046 delete e2e-test-crd-publish-openapi-4705-crds test-foo'
May 20 14:08:06.674: INFO: stderr: ""
May 20 14:08:06.674: INFO: stdout: "e2e-test-crd-publish-openapi-4705-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 20 14:08:06.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 --namespace=crd-publish-openapi-6046 create -f -'
May 20 14:08:06.827: INFO: rc: 1
May 20 14:08:06.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 --namespace=crd-publish-openapi-6046 apply -f -'
May 20 14:08:06.992: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 20 14:08:06.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 --namespace=crd-publish-openapi-6046 create -f -'
May 20 14:08:07.146: INFO: rc: 1
May 20 14:08:07.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 --namespace=crd-publish-openapi-6046 apply -f -'
May 20 14:08:07.289: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 20 14:08:07.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 explain e2e-test-crd-publish-openapi-4705-crds'
May 20 14:08:07.453: INFO: stderr: ""
May 20 14:08:07.453: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4705-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 20 14:08:07.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 explain e2e-test-crd-publish-openapi-4705-crds.metadata'
May 20 14:08:07.606: INFO: stderr: ""
May 20 14:08:07.606: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4705-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 20 14:08:07.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 explain e2e-test-crd-publish-openapi-4705-crds.spec'
May 20 14:08:07.770: INFO: stderr: ""
May 20 14:08:07.770: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4705-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 20 14:08:07.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 explain e2e-test-crd-publish-openapi-4705-crds.spec.bars'
May 20 14:08:07.919: INFO: stderr: ""
May 20 14:08:07.919: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4705-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 20 14:08:07.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-6046 explain e2e-test-crd-publish-openapi-4705-crds.spec.bars2'
May 20 14:08:08.077: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:08:09.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6046" for this suite.

• [SLOW TEST:6.970 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":229,"skipped":3871,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:08:09.823: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:08:09.858: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 20 14:08:10.903: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:08:11.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5721" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":230,"skipped":3880,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:08:11.926: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:08:11.955: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 20 14:08:13.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-1762 --namespace=crd-publish-openapi-1762 create -f -'
May 20 14:08:14.479: INFO: stderr: ""
May 20 14:08:14.479: INFO: stdout: "e2e-test-crd-publish-openapi-5284-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 20 14:08:14.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-1762 --namespace=crd-publish-openapi-1762 delete e2e-test-crd-publish-openapi-5284-crds test-cr'
May 20 14:08:14.557: INFO: stderr: ""
May 20 14:08:14.557: INFO: stdout: "e2e-test-crd-publish-openapi-5284-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 20 14:08:14.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-1762 --namespace=crd-publish-openapi-1762 apply -f -'
May 20 14:08:14.735: INFO: stderr: ""
May 20 14:08:14.735: INFO: stdout: "e2e-test-crd-publish-openapi-5284-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 20 14:08:14.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-1762 --namespace=crd-publish-openapi-1762 delete e2e-test-crd-publish-openapi-5284-crds test-cr'
May 20 14:08:14.821: INFO: stderr: ""
May 20 14:08:14.821: INFO: stdout: "e2e-test-crd-publish-openapi-5284-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 20 14:08:14.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=crd-publish-openapi-1762 explain e2e-test-crd-publish-openapi-5284-crds'
May 20 14:08:14.995: INFO: stderr: ""
May 20 14:08:14.995: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5284-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:08:17.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1762" for this suite.

• [SLOW TEST:6.042 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":231,"skipped":3891,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:08:17.972: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:08:18.009: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 20 14:08:23.043: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 20 14:08:23.044: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 20 14:08:25.095: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7874 /apis/apps/v1/namespaces/deployment-7874/deployments/test-cleanup-deployment 67215316-7d75-478e-aacd-486f98913abc 28144 1 2021-05-20 14:08:23 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-05-20 14:08:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-20 14:08:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006ca0dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-20 14:08:23 +0000 UTC,LastTransitionTime:2021-05-20 14:08:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5d446bdd47" has successfully progressed.,LastUpdateTime:2021-05-20 14:08:24 +0000 UTC,LastTransitionTime:2021-05-20 14:08:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 20 14:08:25.100: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-7874 /apis/apps/v1/namespaces/deployment-7874/replicasets/test-cleanup-deployment-5d446bdd47 e639704e-943c-483c-a5a7-3a6aaa987ca9 28134 1 2021-05-20 14:08:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 67215316-7d75-478e-aacd-486f98913abc 0xc006ca1227 0xc006ca1228}] []  [{kube-controller-manager Update apps/v1 2021-05-20 14:08:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67215316-7d75-478e-aacd-486f98913abc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006ca12b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 20 14:08:25.102: INFO: Pod "test-cleanup-deployment-5d446bdd47-6gpqf" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-6gpqf test-cleanup-deployment-5d446bdd47- deployment-7874 /api/v1/namespaces/deployment-7874/pods/test-cleanup-deployment-5d446bdd47-6gpqf d7cd1389-532d-4ba8-bfdf-d9028b5103f2 28133 0 2021-05-20 14:08:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 e639704e-943c-483c-a5a7-3a6aaa987ca9 0xc006ccef57 0xc006ccef58}] []  [{kube-controller-manager Update v1 2021-05-20 14:08:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e639704e-943c-483c-a5a7-3a6aaa987ca9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-20 14:08:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lmjsj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lmjsj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lmjsj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 14:08:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 14:08:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 14:08:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-20 14:08:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.8.103,PodIP:10.233.66.239,StartTime:2021-05-20 14:08:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-20 14:08:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://e4c5bf0d8ab272a311b6873c4d24f7b6838d4c72963fa5870b53a661b4a3a366,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:08:25.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7874" for this suite.

• [SLOW TEST:7.139 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":232,"skipped":3900,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:08:25.112: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:08:41.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2968" for this suite.

• [SLOW TEST:16.092 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":233,"skipped":3905,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:08:41.204: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 14:08:41.978: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 14:08:43.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757116521, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757116521, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757116521, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757116521, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 14:08:46.999: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:08:47.002: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9768-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:08:48.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1476" for this suite.
STEP: Destroying namespace "webhook-1476-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.986 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":234,"skipped":3962,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:08:48.194: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 20 14:08:48.225: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 20 14:08:48.232: INFO: Waiting for terminating namespaces to be deleted...
May 20 14:08:48.236: INFO: 
Logging pods the apiserver thinks is on node k8s-1 before test
May 20 14:08:48.240: INFO: coredns-7677f9bb54-24hll from kube-system started at 2021-05-20 12:39:45 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.240: INFO: 	Container coredns ready: true, restart count 0
May 20 14:08:48.240: INFO: dns-autoscaler-5b7b5c9b6f-xgl8n from kube-system started at 2021-05-20 12:39:49 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.240: INFO: 	Container autoscaler ready: true, restart count 0
May 20 14:08:48.240: INFO: kube-apiserver-k8s-1 from kube-system started at 2021-05-20 12:37:59 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.240: INFO: 	Container kube-apiserver ready: true, restart count 0
May 20 14:08:48.240: INFO: kube-controller-manager-k8s-1 from kube-system started at 2021-05-20 12:38:14 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.240: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 20 14:08:48.240: INFO: kube-flannel-xshzr from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.240: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 14:08:48.240: INFO: kube-proxy-r8x5s from kube-system started at 2021-05-20 12:39:03 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.240: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 14:08:48.240: INFO: kube-scheduler-k8s-1 from kube-system started at 2021-05-20 12:38:14 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.240: INFO: 	Container kube-scheduler ready: true, restart count 0
May 20 14:08:48.240: INFO: nodelocaldns-j9jjn from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.240: INFO: 	Container node-cache ready: true, restart count 0
May 20 14:08:48.240: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-nkrjs from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 14:08:48.240: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 20 14:08:48.240: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 14:08:48.240: INFO: 
Logging pods the apiserver thinks is on node k8s-2 before test
May 20 14:08:48.245: INFO: coredns-7677f9bb54-m6mpv from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.245: INFO: 	Container coredns ready: true, restart count 0
May 20 14:08:48.245: INFO: kube-flannel-ggvw9 from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.245: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 14:08:48.245: INFO: kube-proxy-hdfgh from kube-system started at 2021-05-20 12:39:03 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.245: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 14:08:48.245: INFO: nginx-proxy-k8s-2 from kube-system started at 2021-05-20 12:38:55 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.245: INFO: 	Container nginx-proxy ready: true, restart count 0
May 20 14:08:48.245: INFO: nodelocaldns-nl6xd from kube-system started at 2021-05-20 12:39:50 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.245: INFO: 	Container node-cache ready: true, restart count 0
May 20 14:08:48.245: INFO: sonobuoy-e2e-job-9cce46af7e3b479e from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 14:08:48.246: INFO: 	Container e2e ready: true, restart count 0
May 20 14:08:48.246: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 20 14:08:48.246: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-v8rq9 from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 14:08:48.246: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 20 14:08:48.246: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 14:08:48.246: INFO: 
Logging pods the apiserver thinks is on node k8s-3 before test
May 20 14:08:48.250: INFO: kube-flannel-hx4xl from kube-system started at 2021-05-20 12:39:25 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.250: INFO: 	Container kube-flannel ready: true, restart count 0
May 20 14:08:48.250: INFO: kube-proxy-whdwz from kube-system started at 2021-05-20 12:39:04 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.250: INFO: 	Container kube-proxy ready: true, restart count 0
May 20 14:08:48.250: INFO: nginx-proxy-k8s-3 from kube-system started at 2021-05-20 12:38:56 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.250: INFO: 	Container nginx-proxy ready: true, restart count 0
May 20 14:08:48.250: INFO: nodelocaldns-pzwcd from kube-system started at 2021-05-20 12:39:51 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.250: INFO: 	Container node-cache ready: true, restart count 0
May 20 14:08:48.250: INFO: sonobuoy from sonobuoy started at 2021-05-20 12:41:44 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.250: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 20 14:08:48.250: INFO: sonobuoy-systemd-logs-daemon-set-09ff829b88904f17-lvpcw from sonobuoy started at 2021-05-20 12:41:49 +0000 UTC (2 container statuses recorded)
May 20 14:08:48.251: INFO: 	Container sonobuoy-worker ready: false, restart count 9
May 20 14:08:48.251: INFO: 	Container systemd-logs ready: true, restart count 0
May 20 14:08:48.251: INFO: sample-webhook-deployment-cbccbf6bb-c9mfw from webhook-1476 started at 2021-05-20 14:08:41 +0000 UTC (1 container statuses recorded)
May 20 14:08:48.251: INFO: 	Container sample-webhook ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9db01d36-ab4e-43e6-b12f-857f3ec1dbea 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-9db01d36-ab4e-43e6-b12f-857f3ec1dbea off the node k8s-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9db01d36-ab4e-43e6-b12f-857f3ec1dbea
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:09:04.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8163" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:16.176 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":235,"skipped":3970,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:09:04.376: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
May 20 14:09:04.402: INFO: Waiting up to 5m0s for pod "var-expansion-c464ab41-65bd-43ef-ba3b-7644f9f0c558" in namespace "var-expansion-4692" to be "Succeeded or Failed"
May 20 14:09:04.408: INFO: Pod "var-expansion-c464ab41-65bd-43ef-ba3b-7644f9f0c558": Phase="Pending", Reason="", readiness=false. Elapsed: 5.462243ms
May 20 14:09:06.457: INFO: Pod "var-expansion-c464ab41-65bd-43ef-ba3b-7644f9f0c558": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054104347s
May 20 14:09:08.462: INFO: Pod "var-expansion-c464ab41-65bd-43ef-ba3b-7644f9f0c558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05985899s
STEP: Saw pod success
May 20 14:09:08.463: INFO: Pod "var-expansion-c464ab41-65bd-43ef-ba3b-7644f9f0c558" satisfied condition "Succeeded or Failed"
May 20 14:09:08.466: INFO: Trying to get logs from node k8s-2 pod var-expansion-c464ab41-65bd-43ef-ba3b-7644f9f0c558 container dapi-container: <nil>
STEP: delete the pod
May 20 14:09:08.490: INFO: Waiting for pod var-expansion-c464ab41-65bd-43ef-ba3b-7644f9f0c558 to disappear
May 20 14:09:08.493: INFO: Pod var-expansion-c464ab41-65bd-43ef-ba3b-7644f9f0c558 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:09:08.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4692" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":236,"skipped":4016,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:09:08.500: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:11:08.550: INFO: Deleting pod "var-expansion-5770763a-dc89-4148-bf81-1078eb6261d2" in namespace "var-expansion-8656"
May 20 14:11:08.555: INFO: Wait up to 5m0s for pod "var-expansion-5770763a-dc89-4148-bf81-1078eb6261d2" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:11:10.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8656" for this suite.

• [SLOW TEST:122.069 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":237,"skipped":4030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:11:10.572: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:11:10.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2605" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":238,"skipped":4057,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:11:10.625: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 20 14:11:13.681: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:11:13.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1279" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":239,"skipped":4065,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:11:13.706: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 14:11:14.088: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 14:11:17.109: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:11:17.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9503" for this suite.
STEP: Destroying namespace "webhook-9503-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":240,"skipped":4093,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:11:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-4616
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 20 14:11:17.264: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 20 14:11:17.295: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 14:11:19.302: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 14:11:21.299: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:11:23.301: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:11:25.298: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:11:27.353: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:11:29.298: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:11:31.361: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:11:33.301: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 20 14:11:33.309: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 20 14:11:35.334: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 20 14:11:37.314: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 20 14:11:39.315: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 20 14:11:39.320: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 20 14:11:41.342: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.248:8080/dial?request=hostname&protocol=http&host=10.233.64.50&port=8080&tries=1'] Namespace:pod-network-test-4616 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:11:41.342: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:11:41.466: INFO: Waiting for responses: map[]
May 20 14:11:41.469: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.248:8080/dial?request=hostname&protocol=http&host=10.233.65.75&port=8080&tries=1'] Namespace:pod-network-test-4616 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:11:41.470: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:11:41.597: INFO: Waiting for responses: map[]
May 20 14:11:41.600: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.248:8080/dial?request=hostname&protocol=http&host=10.233.66.247&port=8080&tries=1'] Namespace:pod-network-test-4616 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:11:41.600: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:11:41.696: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:11:41.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4616" for this suite.

• [SLOW TEST:24.471 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":4105,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:11:41.706: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1546
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 20 14:11:41.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6930 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 20 14:11:41.826: INFO: stderr: ""
May 20 14:11:41.826: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 20 14:11:46.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6930 get pod e2e-test-httpd-pod -o json'
May 20 14:11:47.029: INFO: stderr: ""
May 20 14:11:47.029: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-05-20T14:11:41Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-20T14:11:41Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.233.66.249\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-20T14:11:43Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6930\",\n        \"resourceVersion\": \"29131\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6930/pods/e2e-test-httpd-pod\",\n        \"uid\": \"64771468-2c2f-4e2a-8cb1-19b000764893\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-k4dxm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-k4dxm\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-k4dxm\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-20T14:11:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-20T14:11:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-20T14:11:43Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-20T14:11:41Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://08dc4f201196e938d8de2ce34eb7485386d4ca5240c95cd53873ba31e9c82b4f\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-20T14:11:42Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.18.8.103\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.249\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.249\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-20T14:11:41Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 20 14:11:47.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6930 replace -f -'
May 20 14:11:47.460: INFO: stderr: ""
May 20 14:11:47.460: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
May 20 14:11:47.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-6930 delete pods e2e-test-httpd-pod'
May 20 14:11:55.413: INFO: stderr: ""
May 20 14:11:55.413: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:11:55.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6930" for this suite.

• [SLOW TEST:13.715 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":242,"skipped":4124,"failed":0}
S
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:11:55.422: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 20 14:11:58.010: INFO: Successfully updated pod "adopt-release-h5j94"
STEP: Checking that the Job readopts the Pod
May 20 14:11:58.010: INFO: Waiting up to 15m0s for pod "adopt-release-h5j94" in namespace "job-4169" to be "adopted"
May 20 14:11:58.013: INFO: Pod "adopt-release-h5j94": Phase="Running", Reason="", readiness=true. Elapsed: 2.51963ms
May 20 14:12:00.018: INFO: Pod "adopt-release-h5j94": Phase="Running", Reason="", readiness=true. Elapsed: 2.007422816s
May 20 14:12:00.018: INFO: Pod "adopt-release-h5j94" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 20 14:12:00.530: INFO: Successfully updated pod "adopt-release-h5j94"
STEP: Checking that the Job releases the Pod
May 20 14:12:00.530: INFO: Waiting up to 15m0s for pod "adopt-release-h5j94" in namespace "job-4169" to be "released"
May 20 14:12:00.532: INFO: Pod "adopt-release-h5j94": Phase="Running", Reason="", readiness=true. Elapsed: 2.503944ms
May 20 14:12:02.536: INFO: Pod "adopt-release-h5j94": Phase="Running", Reason="", readiness=true. Elapsed: 2.006147753s
May 20 14:12:02.536: INFO: Pod "adopt-release-h5j94" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:12:02.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4169" for this suite.

• [SLOW TEST:7.122 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":243,"skipped":4125,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:12:02.545: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5598
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
May 20 14:12:02.581: INFO: Found 0 stateful pods, waiting for 3
May 20 14:12:12.584: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 20 14:12:12.584: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 20 14:12:12.584: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 20 14:12:12.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-5598 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 20 14:12:12.809: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 20 14:12:12.809: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 20 14:12:12.809: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 20 14:12:22.870: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 20 14:12:32.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-5598 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 20 14:12:33.085: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 20 14:12:33.085: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 20 14:12:33.085: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 20 14:12:43.109: INFO: Waiting for StatefulSet statefulset-5598/ss2 to complete update
May 20 14:12:43.109: INFO: Waiting for Pod statefulset-5598/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 20 14:12:43.109: INFO: Waiting for Pod statefulset-5598/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 20 14:12:43.110: INFO: Waiting for Pod statefulset-5598/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 20 14:12:53.123: INFO: Waiting for StatefulSet statefulset-5598/ss2 to complete update
May 20 14:12:53.123: INFO: Waiting for Pod statefulset-5598/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 20 14:13:03.129: INFO: Waiting for StatefulSet statefulset-5598/ss2 to complete update
May 20 14:13:03.129: INFO: Waiting for Pod statefulset-5598/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
May 20 14:13:13.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-5598 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 20 14:13:13.318: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 20 14:13:13.318: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 20 14:13:13.318: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 20 14:13:23.349: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 20 14:13:33.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=statefulset-5598 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 20 14:13:33.585: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 20 14:13:33.585: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 20 14:13:33.585: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 20 14:13:43.603: INFO: Waiting for StatefulSet statefulset-5598/ss2 to complete update
May 20 14:13:43.603: INFO: Waiting for Pod statefulset-5598/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 20 14:13:43.603: INFO: Waiting for Pod statefulset-5598/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 20 14:13:43.603: INFO: Waiting for Pod statefulset-5598/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 20 14:13:53.613: INFO: Waiting for StatefulSet statefulset-5598/ss2 to complete update
May 20 14:13:53.613: INFO: Waiting for Pod statefulset-5598/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 20 14:14:03.611: INFO: Deleting all statefulset in ns statefulset-5598
May 20 14:14:03.616: INFO: Scaling statefulset ss2 to 0
May 20 14:14:43.641: INFO: Waiting for statefulset status.replicas updated to 0
May 20 14:14:43.647: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:14:43.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5598" for this suite.

• [SLOW TEST:161.177 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":244,"skipped":4139,"failed":0}
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:14:43.724: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:14:43.770: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7157
I0520 14:14:43.790908      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7157, replica count: 1
I0520 14:14:44.842304      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0520 14:14:45.842844      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0520 14:14:46.843320      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 14:14:46.955: INFO: Created: latency-svc-tkmkv
May 20 14:14:46.962: INFO: Got endpoints: latency-svc-tkmkv [18.678086ms]
May 20 14:14:46.976: INFO: Created: latency-svc-9xbjj
May 20 14:14:46.986: INFO: Got endpoints: latency-svc-9xbjj [23.675047ms]
May 20 14:14:47.005: INFO: Created: latency-svc-m59rm
May 20 14:14:47.019: INFO: Created: latency-svc-jrgc6
May 20 14:14:47.034: INFO: Got endpoints: latency-svc-m59rm [70.81002ms]
May 20 14:14:47.042: INFO: Got endpoints: latency-svc-jrgc6 [78.646967ms]
May 20 14:14:47.046: INFO: Created: latency-svc-4444t
May 20 14:14:47.067: INFO: Got endpoints: latency-svc-4444t [104.357175ms]
May 20 14:14:47.103: INFO: Created: latency-svc-kt9q5
May 20 14:14:47.104: INFO: Got endpoints: latency-svc-kt9q5 [140.447225ms]
May 20 14:14:47.111: INFO: Created: latency-svc-xxhfn
May 20 14:14:47.130: INFO: Created: latency-svc-nkmqr
May 20 14:14:47.133: INFO: Got endpoints: latency-svc-xxhfn [169.68447ms]
May 20 14:14:47.150: INFO: Created: latency-svc-hk4fk
May 20 14:14:47.154: INFO: Created: latency-svc-2lbvt
May 20 14:14:47.159: INFO: Got endpoints: latency-svc-nkmqr [172.52887ms]
May 20 14:14:47.179: INFO: Got endpoints: latency-svc-hk4fk [216.030154ms]
May 20 14:14:47.187: INFO: Got endpoints: latency-svc-2lbvt [223.906136ms]
May 20 14:14:47.254: INFO: Created: latency-svc-crjpn
May 20 14:14:47.267: INFO: Got endpoints: latency-svc-crjpn [303.665505ms]
May 20 14:14:47.294: INFO: Created: latency-svc-f4ql6
May 20 14:14:47.312: INFO: Created: latency-svc-4nhnn
May 20 14:14:47.318: INFO: Got endpoints: latency-svc-f4ql6 [354.296349ms]
May 20 14:14:47.333: INFO: Created: latency-svc-7lhw2
May 20 14:14:47.335: INFO: Got endpoints: latency-svc-4nhnn [371.313666ms]
May 20 14:14:47.346: INFO: Got endpoints: latency-svc-7lhw2 [382.253505ms]
May 20 14:14:47.364: INFO: Created: latency-svc-xnn4c
May 20 14:14:47.387: INFO: Got endpoints: latency-svc-xnn4c [423.321437ms]
May 20 14:14:47.396: INFO: Created: latency-svc-xsxxb
May 20 14:14:47.435: INFO: Created: latency-svc-2mg6b
May 20 14:14:47.436: INFO: Created: latency-svc-4jrvp
May 20 14:14:47.436: INFO: Got endpoints: latency-svc-xsxxb [472.533965ms]
May 20 14:14:47.444: INFO: Created: latency-svc-65ttf
May 20 14:14:47.444: INFO: Got endpoints: latency-svc-2mg6b [410.221867ms]
May 20 14:14:47.444: INFO: Got endpoints: latency-svc-4jrvp [480.816417ms]
May 20 14:14:47.473: INFO: Created: latency-svc-tn7n7
May 20 14:14:47.474: INFO: Got endpoints: latency-svc-65ttf [430.948162ms]
May 20 14:14:47.476: INFO: Created: latency-svc-sqz7l
May 20 14:14:47.486: INFO: Created: latency-svc-7ks48
May 20 14:14:47.497: INFO: Got endpoints: latency-svc-7ks48 [363.277073ms]
May 20 14:14:47.499: INFO: Got endpoints: latency-svc-sqz7l [431.516818ms]
May 20 14:14:47.501: INFO: Created: latency-svc-5crnd
May 20 14:14:47.504: INFO: Got endpoints: latency-svc-tn7n7 [400.05555ms]
May 20 14:14:47.513: INFO: Created: latency-svc-qdsxt
May 20 14:14:47.518: INFO: Created: latency-svc-mp8q2
May 20 14:14:47.521: INFO: Got endpoints: latency-svc-5crnd [362.176477ms]
May 20 14:14:47.538: INFO: Got endpoints: latency-svc-qdsxt [358.086512ms]
May 20 14:14:47.545: INFO: Got endpoints: latency-svc-mp8q2 [357.141663ms]
May 20 14:14:47.555: INFO: Created: latency-svc-2h22p
May 20 14:14:47.556: INFO: Created: latency-svc-2r686
May 20 14:14:47.569: INFO: Got endpoints: latency-svc-2h22p [302.788742ms]
May 20 14:14:47.572: INFO: Got endpoints: latency-svc-2r686 [253.697686ms]
May 20 14:14:47.589: INFO: Created: latency-svc-ncp7v
May 20 14:14:47.603: INFO: Got endpoints: latency-svc-ncp7v [268.013097ms]
May 20 14:14:47.610: INFO: Created: latency-svc-hf2v2
May 20 14:14:47.612: INFO: Got endpoints: latency-svc-hf2v2 [266.046189ms]
May 20 14:14:47.624: INFO: Created: latency-svc-bpsb2
May 20 14:14:47.639: INFO: Created: latency-svc-tb6tt
May 20 14:14:47.656: INFO: Got endpoints: latency-svc-tb6tt [219.500822ms]
May 20 14:14:47.663: INFO: Created: latency-svc-8tck7
May 20 14:14:47.679: INFO: Got endpoints: latency-svc-bpsb2 [292.05121ms]
May 20 14:14:47.686: INFO: Got endpoints: latency-svc-8tck7 [241.157106ms]
May 20 14:14:47.690: INFO: Created: latency-svc-jbvpx
May 20 14:14:47.709: INFO: Created: latency-svc-lpbmg
May 20 14:14:47.719: INFO: Got endpoints: latency-svc-jbvpx [273.603529ms]
May 20 14:14:47.733: INFO: Got endpoints: latency-svc-lpbmg [259.367467ms]
May 20 14:14:47.734: INFO: Created: latency-svc-fldvl
May 20 14:14:47.755: INFO: Got endpoints: latency-svc-fldvl [255.145929ms]
May 20 14:14:47.762: INFO: Created: latency-svc-7znkl
May 20 14:14:47.768: INFO: Created: latency-svc-grg27
May 20 14:14:47.776: INFO: Got endpoints: latency-svc-7znkl [276.09799ms]
May 20 14:14:47.789: INFO: Created: latency-svc-pjss9
May 20 14:14:47.800: INFO: Got endpoints: latency-svc-grg27 [296.754948ms]
May 20 14:14:47.835: INFO: Got endpoints: latency-svc-pjss9 [313.749261ms]
May 20 14:14:47.844: INFO: Created: latency-svc-vwztr
May 20 14:14:47.874: INFO: Got endpoints: latency-svc-vwztr [334.24463ms]
May 20 14:14:47.894: INFO: Created: latency-svc-ssvxb
May 20 14:14:47.917: INFO: Created: latency-svc-vznkd
May 20 14:14:47.922: INFO: Got endpoints: latency-svc-ssvxb [377.073156ms]
May 20 14:14:47.924: INFO: Created: latency-svc-b2xch
May 20 14:14:47.928: INFO: Got endpoints: latency-svc-vznkd [356.078227ms]
May 20 14:14:47.941: INFO: Created: latency-svc-npjv6
May 20 14:14:47.956: INFO: Got endpoints: latency-svc-npjv6 [352.504738ms]
May 20 14:14:47.956: INFO: Got endpoints: latency-svc-b2xch [386.240863ms]
May 20 14:14:47.966: INFO: Created: latency-svc-dsdpd
May 20 14:14:47.974: INFO: Got endpoints: latency-svc-dsdpd [360.968456ms]
May 20 14:14:47.989: INFO: Created: latency-svc-5gpc6
May 20 14:14:47.989: INFO: Created: latency-svc-zx69t
May 20 14:14:48.011: INFO: Created: latency-svc-cl8tx
May 20 14:14:48.012: INFO: Got endpoints: latency-svc-zx69t [355.711024ms]
May 20 14:14:48.019: INFO: Got endpoints: latency-svc-5gpc6 [339.332945ms]
May 20 14:14:48.032: INFO: Got endpoints: latency-svc-cl8tx [346.3214ms]
May 20 14:14:48.033: INFO: Created: latency-svc-pkm75
May 20 14:14:48.044: INFO: Created: latency-svc-xrhvf
May 20 14:14:48.053: INFO: Got endpoints: latency-svc-pkm75 [333.586355ms]
May 20 14:14:48.057: INFO: Created: latency-svc-8kk62
May 20 14:14:48.069: INFO: Created: latency-svc-mzp79
May 20 14:14:48.083: INFO: Got endpoints: latency-svc-xrhvf [349.942385ms]
May 20 14:14:48.089: INFO: Created: latency-svc-zzvqj
May 20 14:14:48.092: INFO: Got endpoints: latency-svc-mzp79 [315.762854ms]
May 20 14:14:48.093: INFO: Got endpoints: latency-svc-8kk62 [337.521518ms]
May 20 14:14:48.097: INFO: Created: latency-svc-sngs6
May 20 14:14:48.112: INFO: Created: latency-svc-8gpz6
May 20 14:14:48.113: INFO: Got endpoints: latency-svc-zzvqj [311.664903ms]
May 20 14:14:48.123: INFO: Got endpoints: latency-svc-sngs6 [287.408513ms]
May 20 14:14:48.127: INFO: Created: latency-svc-lxk7x
May 20 14:14:48.137: INFO: Created: latency-svc-tp7x5
May 20 14:14:48.143: INFO: Created: latency-svc-mlmsj
May 20 14:14:48.170: INFO: Got endpoints: latency-svc-8gpz6 [295.115191ms]
May 20 14:14:48.170: INFO: Created: latency-svc-b42f9
May 20 14:14:48.171: INFO: Created: latency-svc-mgb2b
May 20 14:14:48.178: INFO: Created: latency-svc-nklgc
May 20 14:14:48.189: INFO: Created: latency-svc-mz52k
May 20 14:14:48.192: INFO: Created: latency-svc-gxpfl
May 20 14:14:48.199: INFO: Created: latency-svc-455cr
May 20 14:14:48.209: INFO: Created: latency-svc-hx496
May 20 14:14:48.213: INFO: Got endpoints: latency-svc-lxk7x [291.074973ms]
May 20 14:14:48.229: INFO: Created: latency-svc-jvfzt
May 20 14:14:48.236: INFO: Created: latency-svc-j2cv4
May 20 14:14:48.302: INFO: Got endpoints: latency-svc-tp7x5 [374.121156ms]
May 20 14:14:48.303: INFO: Created: latency-svc-g5z48
May 20 14:14:48.308: INFO: Created: latency-svc-q4dvk
May 20 14:14:48.324: INFO: Created: latency-svc-z84tw
May 20 14:14:48.330: INFO: Created: latency-svc-q5gkt
May 20 14:14:48.336: INFO: Got endpoints: latency-svc-mlmsj [379.399508ms]
May 20 14:14:48.346: INFO: Created: latency-svc-b2tfc
May 20 14:14:48.381: INFO: Got endpoints: latency-svc-b42f9 [423.700336ms]
May 20 14:14:48.395: INFO: Created: latency-svc-x8qjx
May 20 14:14:48.428: INFO: Got endpoints: latency-svc-mgb2b [454.477046ms]
May 20 14:14:48.436: INFO: Created: latency-svc-2fm9l
May 20 14:14:48.449: INFO: Created: latency-svc-fn2sc
May 20 14:14:48.466: INFO: Got endpoints: latency-svc-nklgc [454.457741ms]
May 20 14:14:48.506: INFO: Created: latency-svc-9j9zh
May 20 14:14:48.516: INFO: Got endpoints: latency-svc-mz52k [496.734557ms]
May 20 14:14:48.529: INFO: Created: latency-svc-qzxlx
May 20 14:14:48.565: INFO: Got endpoints: latency-svc-gxpfl [531.962044ms]
May 20 14:14:48.580: INFO: Created: latency-svc-wkcrf
May 20 14:14:48.610: INFO: Got endpoints: latency-svc-455cr [556.417592ms]
May 20 14:14:48.625: INFO: Created: latency-svc-6ckfd
May 20 14:14:48.658: INFO: Got endpoints: latency-svc-hx496 [564.31045ms]
May 20 14:14:48.672: INFO: Created: latency-svc-c8vst
May 20 14:14:48.711: INFO: Got endpoints: latency-svc-jvfzt [617.567385ms]
May 20 14:14:48.725: INFO: Created: latency-svc-w5lfd
May 20 14:14:48.760: INFO: Got endpoints: latency-svc-j2cv4 [677.038522ms]
May 20 14:14:48.791: INFO: Created: latency-svc-md95w
May 20 14:14:48.815: INFO: Got endpoints: latency-svc-g5z48 [701.976658ms]
May 20 14:14:48.835: INFO: Created: latency-svc-t84fj
May 20 14:14:48.871: INFO: Got endpoints: latency-svc-q4dvk [748.044855ms]
May 20 14:14:48.892: INFO: Created: latency-svc-4bqzc
May 20 14:14:48.913: INFO: Got endpoints: latency-svc-z84tw [743.126322ms]
May 20 14:14:48.939: INFO: Created: latency-svc-9ltgx
May 20 14:14:48.973: INFO: Got endpoints: latency-svc-q5gkt [759.519161ms]
May 20 14:14:49.033: INFO: Got endpoints: latency-svc-b2tfc [730.628126ms]
May 20 14:14:49.036: INFO: Created: latency-svc-tpchk
May 20 14:14:49.058: INFO: Created: latency-svc-2wmnn
May 20 14:14:49.069: INFO: Got endpoints: latency-svc-x8qjx [733.26445ms]
May 20 14:14:49.114: INFO: Created: latency-svc-5pfrp
May 20 14:14:49.120: INFO: Got endpoints: latency-svc-2fm9l [738.267102ms]
May 20 14:14:49.153: INFO: Created: latency-svc-gbh8d
May 20 14:14:49.170: INFO: Got endpoints: latency-svc-fn2sc [741.273492ms]
May 20 14:14:49.192: INFO: Created: latency-svc-9k4rv
May 20 14:14:49.271: INFO: Got endpoints: latency-svc-9j9zh [804.917378ms]
May 20 14:14:49.282: INFO: Got endpoints: latency-svc-qzxlx [765.964323ms]
May 20 14:14:49.324: INFO: Created: latency-svc-p9wz4
May 20 14:14:49.336: INFO: Got endpoints: latency-svc-wkcrf [770.96168ms]
May 20 14:14:49.385: INFO: Created: latency-svc-d588z
May 20 14:14:49.407: INFO: Got endpoints: latency-svc-6ckfd [797.229785ms]
May 20 14:14:49.424: INFO: Created: latency-svc-v5b4n
May 20 14:14:49.454: INFO: Got endpoints: latency-svc-c8vst [796.114564ms]
May 20 14:14:49.479: INFO: Got endpoints: latency-svc-w5lfd [768.339222ms]
May 20 14:14:49.486: INFO: Created: latency-svc-vzgdm
May 20 14:14:49.521: INFO: Created: latency-svc-dtqlr
May 20 14:14:49.536: INFO: Created: latency-svc-sxlql
May 20 14:14:49.548: INFO: Got endpoints: latency-svc-md95w [787.227555ms]
May 20 14:14:49.563: INFO: Got endpoints: latency-svc-t84fj [747.366352ms]
May 20 14:14:49.576: INFO: Created: latency-svc-bnvzb
May 20 14:14:49.598: INFO: Created: latency-svc-pjjr6
May 20 14:14:49.623: INFO: Got endpoints: latency-svc-4bqzc [751.627437ms]
May 20 14:14:49.642: INFO: Created: latency-svc-xdvsw
May 20 14:14:49.666: INFO: Got endpoints: latency-svc-9ltgx [752.518849ms]
May 20 14:14:49.687: INFO: Created: latency-svc-hqplz
May 20 14:14:49.713: INFO: Got endpoints: latency-svc-tpchk [740.579589ms]
May 20 14:14:49.744: INFO: Created: latency-svc-vvxq4
May 20 14:14:49.764: INFO: Got endpoints: latency-svc-2wmnn [731.009227ms]
May 20 14:14:49.797: INFO: Created: latency-svc-bsxmg
May 20 14:14:49.811: INFO: Got endpoints: latency-svc-5pfrp [740.905601ms]
May 20 14:14:49.851: INFO: Created: latency-svc-4c46v
May 20 14:14:49.867: INFO: Got endpoints: latency-svc-gbh8d [747.210721ms]
May 20 14:14:49.912: INFO: Got endpoints: latency-svc-9k4rv [742.54315ms]
May 20 14:14:49.930: INFO: Created: latency-svc-bzfbt
May 20 14:14:49.948: INFO: Created: latency-svc-6gtbt
May 20 14:14:50.001: INFO: Got endpoints: latency-svc-p9wz4 [718.212251ms]
May 20 14:14:50.017: INFO: Got endpoints: latency-svc-d588z [742.354255ms]
May 20 14:14:50.027: INFO: Created: latency-svc-kxtph
May 20 14:14:50.047: INFO: Created: latency-svc-9vb2l
May 20 14:14:50.063: INFO: Got endpoints: latency-svc-v5b4n [726.908516ms]
May 20 14:14:50.080: INFO: Created: latency-svc-xtvj7
May 20 14:14:50.118: INFO: Got endpoints: latency-svc-vzgdm [710.270928ms]
May 20 14:14:50.139: INFO: Created: latency-svc-nc8g4
May 20 14:14:50.170: INFO: Got endpoints: latency-svc-dtqlr [715.632294ms]
May 20 14:14:50.194: INFO: Created: latency-svc-n2lkv
May 20 14:14:50.209: INFO: Got endpoints: latency-svc-sxlql [730.099337ms]
May 20 14:14:50.226: INFO: Created: latency-svc-2b2xb
May 20 14:14:50.262: INFO: Got endpoints: latency-svc-bnvzb [714.393928ms]
May 20 14:14:50.277: INFO: Created: latency-svc-g2ttn
May 20 14:14:50.321: INFO: Got endpoints: latency-svc-pjjr6 [758.22858ms]
May 20 14:14:50.344: INFO: Created: latency-svc-5txcs
May 20 14:14:50.358: INFO: Got endpoints: latency-svc-xdvsw [734.564613ms]
May 20 14:14:50.371: INFO: Created: latency-svc-gpj7b
May 20 14:14:50.426: INFO: Got endpoints: latency-svc-hqplz [759.831209ms]
May 20 14:14:50.437: INFO: Created: latency-svc-4lcsh
May 20 14:14:50.461: INFO: Got endpoints: latency-svc-vvxq4 [747.4363ms]
May 20 14:14:50.474: INFO: Created: latency-svc-7dcdl
May 20 14:14:50.512: INFO: Got endpoints: latency-svc-bsxmg [746.893816ms]
May 20 14:14:50.527: INFO: Created: latency-svc-spvgh
May 20 14:14:50.559: INFO: Got endpoints: latency-svc-4c46v [748.384899ms]
May 20 14:14:50.572: INFO: Created: latency-svc-sfb9j
May 20 14:14:50.615: INFO: Got endpoints: latency-svc-bzfbt [747.411236ms]
May 20 14:14:50.627: INFO: Created: latency-svc-qjmxh
May 20 14:14:50.660: INFO: Got endpoints: latency-svc-6gtbt [747.329714ms]
May 20 14:14:50.672: INFO: Created: latency-svc-84p8n
May 20 14:14:50.708: INFO: Got endpoints: latency-svc-kxtph [706.579744ms]
May 20 14:14:50.716: INFO: Created: latency-svc-8hd2h
May 20 14:14:50.758: INFO: Got endpoints: latency-svc-9vb2l [739.675871ms]
May 20 14:14:50.769: INFO: Created: latency-svc-q4hjl
May 20 14:14:50.810: INFO: Got endpoints: latency-svc-xtvj7 [745.499007ms]
May 20 14:14:50.818: INFO: Created: latency-svc-zkh9n
May 20 14:14:50.908: INFO: Got endpoints: latency-svc-nc8g4 [790.124614ms]
May 20 14:14:50.921: INFO: Created: latency-svc-r2gtf
May 20 14:14:50.959: INFO: Got endpoints: latency-svc-n2lkv [788.180099ms]
May 20 14:14:50.978: INFO: Created: latency-svc-spf4d
May 20 14:14:51.010: INFO: Got endpoints: latency-svc-2b2xb [799.994962ms]
May 20 14:14:51.022: INFO: Created: latency-svc-q5xgn
May 20 14:14:51.060: INFO: Got endpoints: latency-svc-g2ttn [797.512488ms]
May 20 14:14:51.069: INFO: Created: latency-svc-2jb5g
May 20 14:14:51.113: INFO: Got endpoints: latency-svc-5txcs [792.335922ms]
May 20 14:14:51.124: INFO: Created: latency-svc-vptnb
May 20 14:14:51.162: INFO: Got endpoints: latency-svc-gpj7b [804.140257ms]
May 20 14:14:51.174: INFO: Created: latency-svc-488kq
May 20 14:14:51.210: INFO: Got endpoints: latency-svc-4lcsh [783.449644ms]
May 20 14:14:51.222: INFO: Created: latency-svc-v495w
May 20 14:14:51.280: INFO: Got endpoints: latency-svc-7dcdl [819.22715ms]
May 20 14:14:51.289: INFO: Created: latency-svc-sdxs5
May 20 14:14:51.307: INFO: Got endpoints: latency-svc-spvgh [795.106599ms]
May 20 14:14:51.318: INFO: Created: latency-svc-vtdhz
May 20 14:14:51.367: INFO: Got endpoints: latency-svc-sfb9j [807.482245ms]
May 20 14:14:51.380: INFO: Created: latency-svc-pmzvk
May 20 14:14:51.409: INFO: Got endpoints: latency-svc-qjmxh [793.712181ms]
May 20 14:14:51.421: INFO: Created: latency-svc-jxqn6
May 20 14:14:51.460: INFO: Got endpoints: latency-svc-84p8n [799.976411ms]
May 20 14:14:51.471: INFO: Created: latency-svc-m4drz
May 20 14:14:51.509: INFO: Got endpoints: latency-svc-8hd2h [801.273247ms]
May 20 14:14:51.519: INFO: Created: latency-svc-lqzb6
May 20 14:14:51.564: INFO: Got endpoints: latency-svc-q4hjl [806.520873ms]
May 20 14:14:51.575: INFO: Created: latency-svc-27ng2
May 20 14:14:51.613: INFO: Got endpoints: latency-svc-zkh9n [803.756799ms]
May 20 14:14:51.626: INFO: Created: latency-svc-4x4wx
May 20 14:14:51.662: INFO: Got endpoints: latency-svc-r2gtf [754.284395ms]
May 20 14:14:51.674: INFO: Created: latency-svc-whvsn
May 20 14:14:51.707: INFO: Got endpoints: latency-svc-spf4d [748.29504ms]
May 20 14:14:51.716: INFO: Created: latency-svc-74kdh
May 20 14:14:51.760: INFO: Got endpoints: latency-svc-q5xgn [750.387599ms]
May 20 14:14:51.773: INFO: Created: latency-svc-rqzgp
May 20 14:14:51.806: INFO: Got endpoints: latency-svc-2jb5g [746.312486ms]
May 20 14:14:51.818: INFO: Created: latency-svc-tk6zl
May 20 14:14:51.862: INFO: Got endpoints: latency-svc-vptnb [748.816912ms]
May 20 14:14:51.876: INFO: Created: latency-svc-sjmd8
May 20 14:14:51.910: INFO: Got endpoints: latency-svc-488kq [746.883975ms]
May 20 14:14:51.919: INFO: Created: latency-svc-rw5n9
May 20 14:14:51.965: INFO: Got endpoints: latency-svc-v495w [754.839312ms]
May 20 14:14:51.978: INFO: Created: latency-svc-pq4fb
May 20 14:14:52.011: INFO: Got endpoints: latency-svc-sdxs5 [730.502379ms]
May 20 14:14:52.022: INFO: Created: latency-svc-76677
May 20 14:14:52.061: INFO: Got endpoints: latency-svc-vtdhz [753.505671ms]
May 20 14:14:52.068: INFO: Created: latency-svc-qrjct
May 20 14:14:52.115: INFO: Got endpoints: latency-svc-pmzvk [747.728284ms]
May 20 14:14:52.123: INFO: Created: latency-svc-gphjz
May 20 14:14:52.159: INFO: Got endpoints: latency-svc-jxqn6 [750.536834ms]
May 20 14:14:52.183: INFO: Created: latency-svc-7p5jz
May 20 14:14:52.213: INFO: Got endpoints: latency-svc-m4drz [752.35711ms]
May 20 14:14:52.235: INFO: Created: latency-svc-cbqmr
May 20 14:14:52.263: INFO: Got endpoints: latency-svc-lqzb6 [753.955588ms]
May 20 14:14:52.273: INFO: Created: latency-svc-kv6x7
May 20 14:14:52.314: INFO: Got endpoints: latency-svc-27ng2 [749.283113ms]
May 20 14:14:52.325: INFO: Created: latency-svc-xmpvc
May 20 14:14:52.372: INFO: Got endpoints: latency-svc-4x4wx [758.152254ms]
May 20 14:14:52.383: INFO: Created: latency-svc-vrs4v
May 20 14:14:52.408: INFO: Got endpoints: latency-svc-whvsn [745.492473ms]
May 20 14:14:52.425: INFO: Created: latency-svc-jfzlc
May 20 14:14:52.460: INFO: Got endpoints: latency-svc-74kdh [752.975629ms]
May 20 14:14:52.471: INFO: Created: latency-svc-mpbxv
May 20 14:14:52.511: INFO: Got endpoints: latency-svc-rqzgp [749.91079ms]
May 20 14:14:52.543: INFO: Created: latency-svc-r6phc
May 20 14:14:52.564: INFO: Got endpoints: latency-svc-tk6zl [757.489306ms]
May 20 14:14:52.587: INFO: Created: latency-svc-trjck
May 20 14:14:52.631: INFO: Got endpoints: latency-svc-sjmd8 [769.135478ms]
May 20 14:14:52.707: INFO: Created: latency-svc-67xcz
May 20 14:14:52.752: INFO: Got endpoints: latency-svc-rw5n9 [842.261461ms]
May 20 14:14:52.756: INFO: Got endpoints: latency-svc-pq4fb [790.579735ms]
May 20 14:14:52.778: INFO: Got endpoints: latency-svc-76677 [767.436044ms]
May 20 14:14:52.802: INFO: Created: latency-svc-6gpg5
May 20 14:14:52.820: INFO: Got endpoints: latency-svc-qrjct [759.283131ms]
May 20 14:14:52.891: INFO: Created: latency-svc-592lq
May 20 14:14:52.896: INFO: Created: latency-svc-8zb97
May 20 14:14:52.921: INFO: Got endpoints: latency-svc-gphjz [805.989448ms]
May 20 14:14:52.981: INFO: Got endpoints: latency-svc-7p5jz [821.429246ms]
May 20 14:14:52.984: INFO: Created: latency-svc-lnzkr
May 20 14:14:52.989: INFO: Got endpoints: latency-svc-cbqmr [776.119901ms]
May 20 14:14:53.107: INFO: Created: latency-svc-x954x
May 20 14:14:53.115: INFO: Got endpoints: latency-svc-kv6x7 [852.180417ms]
May 20 14:14:53.140: INFO: Created: latency-svc-xcbc5
May 20 14:14:53.141: INFO: Got endpoints: latency-svc-vrs4v [769.14921ms]
May 20 14:14:53.141: INFO: Got endpoints: latency-svc-xmpvc [826.729397ms]
May 20 14:14:53.186: INFO: Created: latency-svc-kk7tv
May 20 14:14:53.199: INFO: Got endpoints: latency-svc-jfzlc [790.787487ms]
May 20 14:14:53.215: INFO: Created: latency-svc-lztqt
May 20 14:14:53.221: INFO: Created: latency-svc-bmfls
May 20 14:14:53.228: INFO: Got endpoints: latency-svc-mpbxv [768.062536ms]
May 20 14:14:53.236: INFO: Created: latency-svc-knbvb
May 20 14:14:53.254: INFO: Created: latency-svc-9spf7
May 20 14:14:53.260: INFO: Created: latency-svc-88gmv
May 20 14:14:53.275: INFO: Got endpoints: latency-svc-r6phc [763.79938ms]
May 20 14:14:53.297: INFO: Created: latency-svc-fmqhv
May 20 14:14:53.314: INFO: Got endpoints: latency-svc-trjck [749.984966ms]
May 20 14:14:53.335: INFO: Created: latency-svc-xj7gx
May 20 14:14:53.369: INFO: Got endpoints: latency-svc-67xcz [737.552726ms]
May 20 14:14:53.389: INFO: Created: latency-svc-s5fvs
May 20 14:14:53.417: INFO: Got endpoints: latency-svc-6gpg5 [663.710571ms]
May 20 14:14:53.434: INFO: Created: latency-svc-mj9h9
May 20 14:14:53.471: INFO: Got endpoints: latency-svc-592lq [692.457464ms]
May 20 14:14:53.491: INFO: Created: latency-svc-cw6xm
May 20 14:14:53.520: INFO: Got endpoints: latency-svc-8zb97 [763.663445ms]
May 20 14:14:53.540: INFO: Created: latency-svc-7nb2w
May 20 14:14:53.557: INFO: Got endpoints: latency-svc-lnzkr [736.847923ms]
May 20 14:14:53.576: INFO: Created: latency-svc-sr8qm
May 20 14:14:53.616: INFO: Got endpoints: latency-svc-x954x [694.946352ms]
May 20 14:14:53.645: INFO: Created: latency-svc-mpbsq
May 20 14:14:53.663: INFO: Got endpoints: latency-svc-xcbc5 [680.330498ms]
May 20 14:14:53.718: INFO: Got endpoints: latency-svc-kk7tv [728.992468ms]
May 20 14:14:53.720: INFO: Created: latency-svc-pvf47
May 20 14:14:53.753: INFO: Created: latency-svc-d75rn
May 20 14:14:53.771: INFO: Got endpoints: latency-svc-lztqt [655.284656ms]
May 20 14:14:53.798: INFO: Created: latency-svc-gpvzs
May 20 14:14:53.827: INFO: Got endpoints: latency-svc-bmfls [685.51482ms]
May 20 14:14:53.870: INFO: Got endpoints: latency-svc-knbvb [729.079277ms]
May 20 14:14:53.894: INFO: Created: latency-svc-4g4dj
May 20 14:14:53.900: INFO: Created: latency-svc-h2zmt
May 20 14:14:53.926: INFO: Got endpoints: latency-svc-9spf7 [726.964951ms]
May 20 14:14:53.943: INFO: Created: latency-svc-dglm5
May 20 14:14:53.977: INFO: Got endpoints: latency-svc-88gmv [748.612817ms]
May 20 14:14:54.012: INFO: Created: latency-svc-dl5p7
May 20 14:14:54.018: INFO: Got endpoints: latency-svc-fmqhv [743.341628ms]
May 20 14:14:54.039: INFO: Created: latency-svc-5w4c5
May 20 14:14:54.063: INFO: Got endpoints: latency-svc-xj7gx [748.290393ms]
May 20 14:14:54.088: INFO: Created: latency-svc-rwrtx
May 20 14:14:54.127: INFO: Got endpoints: latency-svc-s5fvs [757.926183ms]
May 20 14:14:54.150: INFO: Created: latency-svc-rxfqf
May 20 14:14:54.175: INFO: Got endpoints: latency-svc-mj9h9 [757.771547ms]
May 20 14:14:54.190: INFO: Created: latency-svc-wp24k
May 20 14:14:54.212: INFO: Got endpoints: latency-svc-cw6xm [740.829314ms]
May 20 14:14:54.227: INFO: Created: latency-svc-6cs8k
May 20 14:14:54.258: INFO: Got endpoints: latency-svc-7nb2w [738.442566ms]
May 20 14:14:54.267: INFO: Created: latency-svc-fqlxc
May 20 14:14:54.308: INFO: Got endpoints: latency-svc-sr8qm [750.510404ms]
May 20 14:14:54.322: INFO: Created: latency-svc-87mfp
May 20 14:14:54.360: INFO: Got endpoints: latency-svc-mpbsq [743.444554ms]
May 20 14:14:54.380: INFO: Created: latency-svc-pvbnv
May 20 14:14:54.418: INFO: Got endpoints: latency-svc-pvf47 [755.025942ms]
May 20 14:14:54.440: INFO: Created: latency-svc-6zzjt
May 20 14:14:54.467: INFO: Got endpoints: latency-svc-d75rn [748.831343ms]
May 20 14:14:54.492: INFO: Created: latency-svc-wg7c2
May 20 14:14:54.509: INFO: Got endpoints: latency-svc-gpvzs [737.869719ms]
May 20 14:14:54.529: INFO: Created: latency-svc-plwd9
May 20 14:14:54.570: INFO: Got endpoints: latency-svc-4g4dj [742.876019ms]
May 20 14:14:54.585: INFO: Created: latency-svc-lkwwl
May 20 14:14:54.623: INFO: Got endpoints: latency-svc-h2zmt [753.026653ms]
May 20 14:14:54.639: INFO: Created: latency-svc-ch9xw
May 20 14:14:54.664: INFO: Got endpoints: latency-svc-dglm5 [737.743588ms]
May 20 14:14:54.683: INFO: Created: latency-svc-vhfcv
May 20 14:14:54.717: INFO: Got endpoints: latency-svc-dl5p7 [739.622834ms]
May 20 14:14:54.729: INFO: Created: latency-svc-2w7pq
May 20 14:14:54.759: INFO: Got endpoints: latency-svc-5w4c5 [740.080341ms]
May 20 14:14:54.774: INFO: Created: latency-svc-dhfvk
May 20 14:14:54.817: INFO: Got endpoints: latency-svc-rwrtx [754.369759ms]
May 20 14:14:54.831: INFO: Created: latency-svc-xvf58
May 20 14:14:54.862: INFO: Got endpoints: latency-svc-rxfqf [733.540744ms]
May 20 14:14:54.917: INFO: Got endpoints: latency-svc-wp24k [742.02159ms]
May 20 14:14:55.000: INFO: Got endpoints: latency-svc-6cs8k [786.918371ms]
May 20 14:14:55.022: INFO: Got endpoints: latency-svc-fqlxc [763.963168ms]
May 20 14:14:55.065: INFO: Got endpoints: latency-svc-87mfp [757.454345ms]
May 20 14:14:55.114: INFO: Got endpoints: latency-svc-pvbnv [754.508051ms]
May 20 14:14:55.187: INFO: Got endpoints: latency-svc-6zzjt [768.892129ms]
May 20 14:14:55.272: INFO: Got endpoints: latency-svc-plwd9 [762.776614ms]
May 20 14:14:55.348: INFO: Got endpoints: latency-svc-wg7c2 [880.938458ms]
May 20 14:14:55.439: INFO: Got endpoints: latency-svc-lkwwl [868.991778ms]
May 20 14:14:55.443: INFO: Got endpoints: latency-svc-ch9xw [819.55193ms]
May 20 14:14:55.443: INFO: Got endpoints: latency-svc-vhfcv [779.146808ms]
May 20 14:14:55.466: INFO: Got endpoints: latency-svc-2w7pq [749.25866ms]
May 20 14:14:55.517: INFO: Got endpoints: latency-svc-dhfvk [758.699588ms]
May 20 14:14:55.565: INFO: Got endpoints: latency-svc-xvf58 [747.414226ms]
May 20 14:14:55.565: INFO: Latencies: [23.675047ms 70.81002ms 78.646967ms 104.357175ms 140.447225ms 169.68447ms 172.52887ms 216.030154ms 219.500822ms 223.906136ms 241.157106ms 253.697686ms 255.145929ms 259.367467ms 266.046189ms 268.013097ms 273.603529ms 276.09799ms 287.408513ms 291.074973ms 292.05121ms 295.115191ms 296.754948ms 302.788742ms 303.665505ms 311.664903ms 313.749261ms 315.762854ms 333.586355ms 334.24463ms 337.521518ms 339.332945ms 346.3214ms 349.942385ms 352.504738ms 354.296349ms 355.711024ms 356.078227ms 357.141663ms 358.086512ms 360.968456ms 362.176477ms 363.277073ms 371.313666ms 374.121156ms 377.073156ms 379.399508ms 382.253505ms 386.240863ms 400.05555ms 410.221867ms 423.321437ms 423.700336ms 430.948162ms 431.516818ms 454.457741ms 454.477046ms 472.533965ms 480.816417ms 496.734557ms 531.962044ms 556.417592ms 564.31045ms 617.567385ms 655.284656ms 663.710571ms 677.038522ms 680.330498ms 685.51482ms 692.457464ms 694.946352ms 701.976658ms 706.579744ms 710.270928ms 714.393928ms 715.632294ms 718.212251ms 726.908516ms 726.964951ms 728.992468ms 729.079277ms 730.099337ms 730.502379ms 730.628126ms 731.009227ms 733.26445ms 733.540744ms 734.564613ms 736.847923ms 737.552726ms 737.743588ms 737.869719ms 738.267102ms 738.442566ms 739.622834ms 739.675871ms 740.080341ms 740.579589ms 740.829314ms 740.905601ms 741.273492ms 742.02159ms 742.354255ms 742.54315ms 742.876019ms 743.126322ms 743.341628ms 743.444554ms 745.492473ms 745.499007ms 746.312486ms 746.883975ms 746.893816ms 747.210721ms 747.329714ms 747.366352ms 747.411236ms 747.414226ms 747.4363ms 747.728284ms 748.044855ms 748.290393ms 748.29504ms 748.384899ms 748.612817ms 748.816912ms 748.831343ms 749.25866ms 749.283113ms 749.91079ms 749.984966ms 750.387599ms 750.510404ms 750.536834ms 751.627437ms 752.35711ms 752.518849ms 752.975629ms 753.026653ms 753.505671ms 753.955588ms 754.284395ms 754.369759ms 754.508051ms 754.839312ms 755.025942ms 757.454345ms 757.489306ms 757.771547ms 757.926183ms 758.152254ms 758.22858ms 758.699588ms 759.283131ms 759.519161ms 759.831209ms 762.776614ms 763.663445ms 763.79938ms 763.963168ms 765.964323ms 767.436044ms 768.062536ms 768.339222ms 768.892129ms 769.135478ms 769.14921ms 770.96168ms 776.119901ms 779.146808ms 783.449644ms 786.918371ms 787.227555ms 788.180099ms 790.124614ms 790.579735ms 790.787487ms 792.335922ms 793.712181ms 795.106599ms 796.114564ms 797.229785ms 797.512488ms 799.976411ms 799.994962ms 801.273247ms 803.756799ms 804.140257ms 804.917378ms 805.989448ms 806.520873ms 807.482245ms 819.22715ms 819.55193ms 821.429246ms 826.729397ms 842.261461ms 852.180417ms 868.991778ms 880.938458ms]
May 20 14:14:55.565: INFO: 50 %ile: 741.273492ms
May 20 14:14:55.565: INFO: 90 %ile: 796.114564ms
May 20 14:14:55.566: INFO: 99 %ile: 868.991778ms
May 20 14:14:55.566: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:14:55.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7157" for this suite.

• [SLOW TEST:11.883 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":245,"skipped":4143,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:14:55.607: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 20 14:14:55.699: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-a f5f9d5f0-cdc3-469f-b10f-bc0bc67c063c 31021 0 2021-05-20 14:14:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-20 14:14:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 14:14:55.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-a f5f9d5f0-cdc3-469f-b10f-bc0bc67c063c 31021 0 2021-05-20 14:14:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-20 14:14:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 20 14:15:05.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-a f5f9d5f0-cdc3-469f-b10f-bc0bc67c063c 31411 0 2021-05-20 14:14:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-20 14:15:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 14:15:05.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-a f5f9d5f0-cdc3-469f-b10f-bc0bc67c063c 31411 0 2021-05-20 14:14:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-20 14:15:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 20 14:15:15.764: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-a f5f9d5f0-cdc3-469f-b10f-bc0bc67c063c 31895 0 2021-05-20 14:14:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-20 14:15:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 14:15:15.765: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-a f5f9d5f0-cdc3-469f-b10f-bc0bc67c063c 31895 0 2021-05-20 14:14:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-20 14:15:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 20 14:15:25.772: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-a f5f9d5f0-cdc3-469f-b10f-bc0bc67c063c 31928 0 2021-05-20 14:14:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-20 14:15:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 14:15:25.772: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-a f5f9d5f0-cdc3-469f-b10f-bc0bc67c063c 31928 0 2021-05-20 14:14:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-20 14:15:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 20 14:15:35.779: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-b 27a8971f-e020-40b0-af5b-6cc12c6ab7d3 31956 0 2021-05-20 14:15:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-20 14:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 14:15:35.780: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-b 27a8971f-e020-40b0-af5b-6cc12c6ab7d3 31956 0 2021-05-20 14:15:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-20 14:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 20 14:15:45.786: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-b 27a8971f-e020-40b0-af5b-6cc12c6ab7d3 31984 0 2021-05-20 14:15:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-20 14:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 20 14:15:45.786: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2395 /api/v1/namespaces/watch-2395/configmaps/e2e-watch-test-configmap-b 27a8971f-e020-40b0-af5b-6cc12c6ab7d3 31984 0 2021-05-20 14:15:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-20 14:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:15:55.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2395" for this suite.

• [SLOW TEST:60.189 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":246,"skipped":4164,"failed":0}
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:15:55.797: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-3a7058dd-cfbf-4dc9-b3c6-8a7d0925b749
STEP: Creating a pod to test consume secrets
May 20 14:15:55.841: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10b8d868-baee-4960-852b-8bc6529e5584" in namespace "projected-1925" to be "Succeeded or Failed"
May 20 14:15:55.845: INFO: Pod "pod-projected-secrets-10b8d868-baee-4960-852b-8bc6529e5584": Phase="Pending", Reason="", readiness=false. Elapsed: 4.120111ms
May 20 14:15:57.848: INFO: Pod "pod-projected-secrets-10b8d868-baee-4960-852b-8bc6529e5584": Phase="Running", Reason="", readiness=true. Elapsed: 2.007196729s
May 20 14:15:59.869: INFO: Pod "pod-projected-secrets-10b8d868-baee-4960-852b-8bc6529e5584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028213085s
STEP: Saw pod success
May 20 14:15:59.869: INFO: Pod "pod-projected-secrets-10b8d868-baee-4960-852b-8bc6529e5584" satisfied condition "Succeeded or Failed"
May 20 14:15:59.871: INFO: Trying to get logs from node k8s-3 pod pod-projected-secrets-10b8d868-baee-4960-852b-8bc6529e5584 container secret-volume-test: <nil>
STEP: delete the pod
May 20 14:15:59.893: INFO: Waiting for pod pod-projected-secrets-10b8d868-baee-4960-852b-8bc6529e5584 to disappear
May 20 14:15:59.896: INFO: Pod pod-projected-secrets-10b8d868-baee-4960-852b-8bc6529e5584 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:15:59.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1925" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":4164,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:15:59.905: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 14:15:59.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb365f35-2d42-4f31-b164-66851b2e7653" in namespace "projected-2048" to be "Succeeded or Failed"
May 20 14:15:59.936: INFO: Pod "downwardapi-volume-eb365f35-2d42-4f31-b164-66851b2e7653": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057955ms
May 20 14:16:01.940: INFO: Pod "downwardapi-volume-eb365f35-2d42-4f31-b164-66851b2e7653": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005772649s
May 20 14:16:03.944: INFO: Pod "downwardapi-volume-eb365f35-2d42-4f31-b164-66851b2e7653": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010262475s
STEP: Saw pod success
May 20 14:16:03.944: INFO: Pod "downwardapi-volume-eb365f35-2d42-4f31-b164-66851b2e7653" satisfied condition "Succeeded or Failed"
May 20 14:16:03.948: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-eb365f35-2d42-4f31-b164-66851b2e7653 container client-container: <nil>
STEP: delete the pod
May 20 14:16:03.972: INFO: Waiting for pod downwardapi-volume-eb365f35-2d42-4f31-b164-66851b2e7653 to disappear
May 20 14:16:03.975: INFO: Pod downwardapi-volume-eb365f35-2d42-4f31-b164-66851b2e7653 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:16:03.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2048" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":248,"skipped":4182,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:16:04.002: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0520 14:16:05.104204      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 20 14:17:07.122: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:17:07.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8183" for this suite.

• [SLOW TEST:63.129 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":249,"skipped":4195,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:17:07.132: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:17:34.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9606" for this suite.

• [SLOW TEST:27.363 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":250,"skipped":4201,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:17:34.495: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 20 14:17:34.535: INFO: Waiting up to 5m0s for pod "downward-api-351f844f-d30d-453d-8f3c-554c50d43639" in namespace "downward-api-2310" to be "Succeeded or Failed"
May 20 14:17:34.537: INFO: Pod "downward-api-351f844f-d30d-453d-8f3c-554c50d43639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144369ms
May 20 14:17:36.541: INFO: Pod "downward-api-351f844f-d30d-453d-8f3c-554c50d43639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006215138s
May 20 14:17:38.545: INFO: Pod "downward-api-351f844f-d30d-453d-8f3c-554c50d43639": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0107051s
STEP: Saw pod success
May 20 14:17:38.545: INFO: Pod "downward-api-351f844f-d30d-453d-8f3c-554c50d43639" satisfied condition "Succeeded or Failed"
May 20 14:17:38.548: INFO: Trying to get logs from node k8s-3 pod downward-api-351f844f-d30d-453d-8f3c-554c50d43639 container dapi-container: <nil>
STEP: delete the pod
May 20 14:17:38.570: INFO: Waiting for pod downward-api-351f844f-d30d-453d-8f3c-554c50d43639 to disappear
May 20 14:17:38.573: INFO: Pod downward-api-351f844f-d30d-453d-8f3c-554c50d43639 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:17:38.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2310" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":251,"skipped":4202,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:17:38.582: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 20 14:17:38.614: INFO: Waiting up to 1m0s for all nodes to be ready
May 20 14:18:38.632: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:18:38.634: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 20 14:18:40.676: INFO: found a healthy node: k8s-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:18:54.752: INFO: pods created so far: [1 1 1]
May 20 14:18:54.752: INFO: length of pods created so far: 3
May 20 14:19:10.768: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:19:17.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3256" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:19:17.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7849" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:99.265 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":252,"skipped":4208,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:19:17.848: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-97c8f034-95af-4f28-a9ee-d663facca0c3
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-97c8f034-95af-4f28-a9ee-d663facca0c3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:19:21.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5276" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":253,"skipped":4211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:19:21.951: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 20 14:19:26.024: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 20 14:19:26.029: INFO: Pod pod-with-prestop-exec-hook still exists
May 20 14:19:28.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 20 14:19:28.041: INFO: Pod pod-with-prestop-exec-hook still exists
May 20 14:19:30.030: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 20 14:19:30.034: INFO: Pod pod-with-prestop-exec-hook still exists
May 20 14:19:32.031: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 20 14:19:32.034: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:19:32.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5650" for this suite.

• [SLOW TEST:10.106 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":254,"skipped":4276,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:19:32.062: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-b2d95fec-cd06-4caa-8ee6-7109112ebcbf
STEP: Creating a pod to test consume secrets
May 20 14:19:32.096: INFO: Waiting up to 5m0s for pod "pod-secrets-e8f7eaaa-f394-4ef5-89c7-daaad908bb5a" in namespace "secrets-5324" to be "Succeeded or Failed"
May 20 14:19:32.098: INFO: Pod "pod-secrets-e8f7eaaa-f394-4ef5-89c7-daaad908bb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.266607ms
May 20 14:19:34.102: INFO: Pod "pod-secrets-e8f7eaaa-f394-4ef5-89c7-daaad908bb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006161531s
May 20 14:19:36.107: INFO: Pod "pod-secrets-e8f7eaaa-f394-4ef5-89c7-daaad908bb5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011153783s
STEP: Saw pod success
May 20 14:19:36.107: INFO: Pod "pod-secrets-e8f7eaaa-f394-4ef5-89c7-daaad908bb5a" satisfied condition "Succeeded or Failed"
May 20 14:19:36.110: INFO: Trying to get logs from node k8s-2 pod pod-secrets-e8f7eaaa-f394-4ef5-89c7-daaad908bb5a container secret-env-test: <nil>
STEP: delete the pod
May 20 14:19:36.129: INFO: Waiting for pod pod-secrets-e8f7eaaa-f394-4ef5-89c7-daaad908bb5a to disappear
May 20 14:19:36.134: INFO: Pod pod-secrets-e8f7eaaa-f394-4ef5-89c7-daaad908bb5a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:19:36.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5324" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":255,"skipped":4294,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:19:36.142: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-7935
STEP: creating replication controller nodeport-test in namespace services-7935
I0520 14:19:36.189142      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-7935, replica count: 2
I0520 14:19:39.240635      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 14:19:39.240: INFO: Creating new exec pod
May 20 14:19:44.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7935 exec execpodgbgpc -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 20 14:19:45.050: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 20 14:19:45.050: INFO: stdout: ""
May 20 14:19:45.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7935 exec execpodgbgpc -- /bin/sh -x -c nc -zv -t -w 2 10.233.33.146 80'
May 20 14:19:45.237: INFO: stderr: "+ nc -zv -t -w 2 10.233.33.146 80\nConnection to 10.233.33.146 80 port [tcp/http] succeeded!\n"
May 20 14:19:45.237: INFO: stdout: ""
May 20 14:19:45.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7935 exec execpodgbgpc -- /bin/sh -x -c nc -zv -t -w 2 172.18.8.101 31042'
May 20 14:19:45.432: INFO: stderr: "+ nc -zv -t -w 2 172.18.8.101 31042\nConnection to 172.18.8.101 31042 port [tcp/31042] succeeded!\n"
May 20 14:19:45.432: INFO: stdout: ""
May 20 14:19:45.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7935 exec execpodgbgpc -- /bin/sh -x -c nc -zv -t -w 2 172.18.8.103 31042'
May 20 14:19:45.622: INFO: stderr: "+ nc -zv -t -w 2 172.18.8.103 31042\nConnection to 172.18.8.103 31042 port [tcp/31042] succeeded!\n"
May 20 14:19:45.622: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:19:45.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7935" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.489 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":256,"skipped":4299,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:19:45.631: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:19:45.682: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b6c26fea-8c01-41d4-924e-26f651326138", Controller:(*bool)(0xc003f20c06), BlockOwnerDeletion:(*bool)(0xc003f20c07)}}
May 20 14:19:45.691: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"12e109f0-7815-4e05-af1c-1f61fde88946", Controller:(*bool)(0xc004a28326), BlockOwnerDeletion:(*bool)(0xc004a28327)}}
May 20 14:19:45.698: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b525d7f8-8a34-48d5-a154-27d0ad366004", Controller:(*bool)(0xc003f20e16), BlockOwnerDeletion:(*bool)(0xc003f20e17)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:19:50.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1913" for this suite.

• [SLOW TEST:5.106 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":257,"skipped":4312,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:19:50.742: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 20 14:19:50.809: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:19:54.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1449" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":258,"skipped":4313,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:19:54.933: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-1438f226-d07f-4c03-a9a7-41ba18a7cca8
STEP: Creating a pod to test consume configMaps
May 20 14:19:54.973: INFO: Waiting up to 5m0s for pod "pod-configmaps-42109413-55d4-4473-9cdd-2e9b55a1d8a7" in namespace "configmap-2734" to be "Succeeded or Failed"
May 20 14:19:54.976: INFO: Pod "pod-configmaps-42109413-55d4-4473-9cdd-2e9b55a1d8a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.211223ms
May 20 14:19:56.980: INFO: Pod "pod-configmaps-42109413-55d4-4473-9cdd-2e9b55a1d8a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007250281s
May 20 14:19:58.983: INFO: Pod "pod-configmaps-42109413-55d4-4473-9cdd-2e9b55a1d8a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009985079s
STEP: Saw pod success
May 20 14:19:58.983: INFO: Pod "pod-configmaps-42109413-55d4-4473-9cdd-2e9b55a1d8a7" satisfied condition "Succeeded or Failed"
May 20 14:19:58.985: INFO: Trying to get logs from node k8s-3 pod pod-configmaps-42109413-55d4-4473-9cdd-2e9b55a1d8a7 container configmap-volume-test: <nil>
STEP: delete the pod
May 20 14:19:59.011: INFO: Waiting for pod pod-configmaps-42109413-55d4-4473-9cdd-2e9b55a1d8a7 to disappear
May 20 14:19:59.014: INFO: Pod pod-configmaps-42109413-55d4-4473-9cdd-2e9b55a1d8a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:19:59.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2734" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":259,"skipped":4326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:19:59.026: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
May 20 14:19:59.060: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-5052 proxy --unix-socket=/tmp/kubectl-proxy-unix200571209/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:19:59.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5052" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":260,"skipped":4355,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:19:59.130: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 20 14:20:03.684: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d619af60-c4c6-432f-99e2-692925583ccf"
May 20 14:20:03.685: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d619af60-c4c6-432f-99e2-692925583ccf" in namespace "pods-6177" to be "terminated due to deadline exceeded"
May 20 14:20:03.687: INFO: Pod "pod-update-activedeadlineseconds-d619af60-c4c6-432f-99e2-692925583ccf": Phase="Running", Reason="", readiness=true. Elapsed: 2.652153ms
May 20 14:20:05.702: INFO: Pod "pod-update-activedeadlineseconds-d619af60-c4c6-432f-99e2-692925583ccf": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.017055997s
May 20 14:20:05.702: INFO: Pod "pod-update-activedeadlineseconds-d619af60-c4c6-432f-99e2-692925583ccf" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:20:05.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6177" for this suite.

• [SLOW TEST:6.579 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":261,"skipped":4371,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:20:05.709: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:20:05.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3505" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":262,"skipped":4382,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:20:05.764: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 20 14:20:09.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 20 14:20:09.844: INFO: Pod pod-with-poststart-exec-hook still exists
May 20 14:20:11.855: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 20 14:20:11.860: INFO: Pod pod-with-poststart-exec-hook still exists
May 20 14:20:13.844: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 20 14:20:13.848: INFO: Pod pod-with-poststart-exec-hook still exists
May 20 14:20:15.845: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 20 14:20:15.849: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:20:15.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8962" for this suite.

• [SLOW TEST:10.095 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":263,"skipped":4399,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:20:15.861: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:20:15.886: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:20:17.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3924" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":264,"skipped":4404,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:20:17.929: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:20:17.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5457" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":265,"skipped":4422,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:20:17.992: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 14:20:18.586: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 14:20:20.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117218, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117218, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117218, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117218, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 14:20:23.619: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:20:33.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1831" for this suite.
STEP: Destroying namespace "webhook-1831-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.826 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":266,"skipped":4422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:20:33.830: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-751a7821-3284-482a-8678-750c2dd66ad6
STEP: Creating configMap with name cm-test-opt-upd-2bb82944-ffe9-4b9c-a4e2-ebd9b1d170fb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-751a7821-3284-482a-8678-750c2dd66ad6
STEP: Updating configmap cm-test-opt-upd-2bb82944-ffe9-4b9c-a4e2-ebd9b1d170fb
STEP: Creating configMap with name cm-test-opt-create-9b814c49-6cd0-4315-9162-b07e3cdf51a4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:20:42.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1022" for this suite.

• [SLOW TEST:8.183 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4445,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:20:42.016: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7299
STEP: creating service affinity-nodeport-transition in namespace services-7299
STEP: creating replication controller affinity-nodeport-transition in namespace services-7299
I0520 14:20:42.056549      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-7299, replica count: 3
I0520 14:20:45.120470      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 20 14:20:45.135: INFO: Creating new exec pod
May 20 14:20:48.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7299 exec execpod-affinityf7tkc -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 20 14:20:48.372: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 20 14:20:48.372: INFO: stdout: ""
May 20 14:20:48.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7299 exec execpod-affinityf7tkc -- /bin/sh -x -c nc -zv -t -w 2 10.233.15.4 80'
May 20 14:20:48.558: INFO: stderr: "+ nc -zv -t -w 2 10.233.15.4 80\nConnection to 10.233.15.4 80 port [tcp/http] succeeded!\n"
May 20 14:20:48.558: INFO: stdout: ""
May 20 14:20:48.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7299 exec execpod-affinityf7tkc -- /bin/sh -x -c nc -zv -t -w 2 172.18.8.101 31561'
May 20 14:20:48.746: INFO: stderr: "+ nc -zv -t -w 2 172.18.8.101 31561\nConnection to 172.18.8.101 31561 port [tcp/31561] succeeded!\n"
May 20 14:20:48.746: INFO: stdout: ""
May 20 14:20:48.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7299 exec execpod-affinityf7tkc -- /bin/sh -x -c nc -zv -t -w 2 172.18.8.103 31561'
May 20 14:20:48.944: INFO: stderr: "+ nc -zv -t -w 2 172.18.8.103 31561\nConnection to 172.18.8.103 31561 port [tcp/31561] succeeded!\n"
May 20 14:20:48.944: INFO: stdout: ""
May 20 14:20:48.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7299 exec execpod-affinityf7tkc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.8.101:31561/ ; done'
May 20 14:20:49.294: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n"
May 20 14:20:49.294: INFO: stdout: "\naffinity-nodeport-transition-56d68\naffinity-nodeport-transition-ks84x\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-56d68\naffinity-nodeport-transition-ks84x\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-56d68\naffinity-nodeport-transition-ks84x\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-56d68\naffinity-nodeport-transition-ks84x\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-56d68\naffinity-nodeport-transition-ks84x\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-56d68"
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-56d68
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-ks84x
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-56d68
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-ks84x
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-56d68
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-ks84x
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-56d68
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-ks84x
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-56d68
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-ks84x
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.294: INFO: Received response from host: affinity-nodeport-transition-56d68
May 20 14:20:49.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=services-7299 exec execpod-affinityf7tkc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.18.8.101:31561/ ; done'
May 20 14:20:49.616: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.18.8.101:31561/\n"
May 20 14:20:49.616: INFO: stdout: "\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv\naffinity-nodeport-transition-gblfv"
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Received response from host: affinity-nodeport-transition-gblfv
May 20 14:20:49.616: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7299, will wait for the garbage collector to delete the pods
May 20 14:20:49.685: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.195734ms
May 20 14:20:49.785: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.217513ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:21:05.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7299" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:23.084 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":268,"skipped":4459,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:21:05.108: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:21:18.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8524" for this suite.

• [SLOW TEST:13.120 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":269,"skipped":4476,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:21:18.228: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-c2622598-a4b9-4196-a402-fa6769f88c84 in namespace container-probe-2664
May 20 14:21:22.261: INFO: Started pod liveness-c2622598-a4b9-4196-a402-fa6769f88c84 in namespace container-probe-2664
STEP: checking the pod's current state and verifying that restartCount is present
May 20 14:21:22.264: INFO: Initial restart count of pod liveness-c2622598-a4b9-4196-a402-fa6769f88c84 is 0
May 20 14:21:40.317: INFO: Restart count of pod container-probe-2664/liveness-c2622598-a4b9-4196-a402-fa6769f88c84 is now 1 (18.053672927s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:21:40.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2664" for this suite.

• [SLOW TEST:22.109 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4495,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:21:40.338: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 14:21:40.367: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b798477-7d9c-4efe-a0f1-dd2c4716b2e4" in namespace "projected-1309" to be "Succeeded or Failed"
May 20 14:21:40.369: INFO: Pod "downwardapi-volume-0b798477-7d9c-4efe-a0f1-dd2c4716b2e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009187ms
May 20 14:21:42.373: INFO: Pod "downwardapi-volume-0b798477-7d9c-4efe-a0f1-dd2c4716b2e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006093112s
STEP: Saw pod success
May 20 14:21:42.373: INFO: Pod "downwardapi-volume-0b798477-7d9c-4efe-a0f1-dd2c4716b2e4" satisfied condition "Succeeded or Failed"
May 20 14:21:42.376: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-0b798477-7d9c-4efe-a0f1-dd2c4716b2e4 container client-container: <nil>
STEP: delete the pod
May 20 14:21:42.394: INFO: Waiting for pod downwardapi-volume-0b798477-7d9c-4efe-a0f1-dd2c4716b2e4 to disappear
May 20 14:21:42.396: INFO: Pod downwardapi-volume-0b798477-7d9c-4efe-a0f1-dd2c4716b2e4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:21:42.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1309" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":271,"skipped":4497,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:21:42.406: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 20 14:21:42.435: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:21:45.153: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:21:55.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7372" for this suite.

• [SLOW TEST:13.352 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":272,"skipped":4508,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:21:55.759: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-7e1e8b51-c69f-49b6-b0fa-a51f0b24ef70
STEP: Creating a pod to test consume secrets
May 20 14:21:55.789: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-19f987ac-37a3-4152-8d76-b34518cf90a3" in namespace "projected-8610" to be "Succeeded or Failed"
May 20 14:21:55.798: INFO: Pod "pod-projected-secrets-19f987ac-37a3-4152-8d76-b34518cf90a3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.964039ms
May 20 14:21:57.803: INFO: Pod "pod-projected-secrets-19f987ac-37a3-4152-8d76-b34518cf90a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013420934s
STEP: Saw pod success
May 20 14:21:57.803: INFO: Pod "pod-projected-secrets-19f987ac-37a3-4152-8d76-b34518cf90a3" satisfied condition "Succeeded or Failed"
May 20 14:21:57.805: INFO: Trying to get logs from node k8s-3 pod pod-projected-secrets-19f987ac-37a3-4152-8d76-b34518cf90a3 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 20 14:21:57.819: INFO: Waiting for pod pod-projected-secrets-19f987ac-37a3-4152-8d76-b34518cf90a3 to disappear
May 20 14:21:57.822: INFO: Pod pod-projected-secrets-19f987ac-37a3-4152-8d76-b34518cf90a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:21:57.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8610" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":273,"skipped":4508,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:21:57.836: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 20 14:21:57.871: INFO: Waiting up to 5m0s for pod "pod-0a6387eb-6547-463c-9586-5a4d10f2d2ba" in namespace "emptydir-9688" to be "Succeeded or Failed"
May 20 14:21:57.874: INFO: Pod "pod-0a6387eb-6547-463c-9586-5a4d10f2d2ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.709726ms
May 20 14:21:59.880: INFO: Pod "pod-0a6387eb-6547-463c-9586-5a4d10f2d2ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009188952s
STEP: Saw pod success
May 20 14:21:59.881: INFO: Pod "pod-0a6387eb-6547-463c-9586-5a4d10f2d2ba" satisfied condition "Succeeded or Failed"
May 20 14:21:59.883: INFO: Trying to get logs from node k8s-3 pod pod-0a6387eb-6547-463c-9586-5a4d10f2d2ba container test-container: <nil>
STEP: delete the pod
May 20 14:21:59.896: INFO: Waiting for pod pod-0a6387eb-6547-463c-9586-5a4d10f2d2ba to disappear
May 20 14:21:59.899: INFO: Pod pod-0a6387eb-6547-463c-9586-5a4d10f2d2ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:21:59.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9688" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4508,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:21:59.907: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:21:59.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-727" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":275,"skipped":4521,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:21:59.943: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 14:21:59.976: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6fd18005-19ca-4d1e-92eb-3ac9d1bbd642" in namespace "projected-69" to be "Succeeded or Failed"
May 20 14:21:59.978: INFO: Pod "downwardapi-volume-6fd18005-19ca-4d1e-92eb-3ac9d1bbd642": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412699ms
May 20 14:22:01.985: INFO: Pod "downwardapi-volume-6fd18005-19ca-4d1e-92eb-3ac9d1bbd642": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008984298s
STEP: Saw pod success
May 20 14:22:01.985: INFO: Pod "downwardapi-volume-6fd18005-19ca-4d1e-92eb-3ac9d1bbd642" satisfied condition "Succeeded or Failed"
May 20 14:22:01.988: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-6fd18005-19ca-4d1e-92eb-3ac9d1bbd642 container client-container: <nil>
STEP: delete the pod
May 20 14:22:02.004: INFO: Waiting for pod downwardapi-volume-6fd18005-19ca-4d1e-92eb-3ac9d1bbd642 to disappear
May 20 14:22:02.007: INFO: Pod downwardapi-volume-6fd18005-19ca-4d1e-92eb-3ac9d1bbd642 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:22:02.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-69" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":276,"skipped":4533,"failed":0}
SSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:22:02.023: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:22:02.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1089" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":277,"skipped":4538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:22:02.093: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 20 14:22:06.659: INFO: Successfully updated pod "labelsupdate505d8613-23bc-4eda-9c8f-afd53a9572bd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:22:08.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9728" for this suite.

• [SLOW TEST:6.604 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":278,"skipped":4564,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:22:08.698: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
May 20 14:22:08.728: INFO: Waiting up to 5m0s for pod "pod-b3ac2442-3d31-46c4-89ee-cc6daa675bd6" in namespace "emptydir-7697" to be "Succeeded or Failed"
May 20 14:22:08.732: INFO: Pod "pod-b3ac2442-3d31-46c4-89ee-cc6daa675bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.835373ms
May 20 14:22:10.736: INFO: Pod "pod-b3ac2442-3d31-46c4-89ee-cc6daa675bd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00739919s
STEP: Saw pod success
May 20 14:22:10.736: INFO: Pod "pod-b3ac2442-3d31-46c4-89ee-cc6daa675bd6" satisfied condition "Succeeded or Failed"
May 20 14:22:10.738: INFO: Trying to get logs from node k8s-3 pod pod-b3ac2442-3d31-46c4-89ee-cc6daa675bd6 container test-container: <nil>
STEP: delete the pod
May 20 14:22:10.751: INFO: Waiting for pod pod-b3ac2442-3d31-46c4-89ee-cc6daa675bd6 to disappear
May 20 14:22:10.755: INFO: Pod pod-b3ac2442-3d31-46c4-89ee-cc6daa675bd6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:22:10.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7697" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":279,"skipped":4585,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:22:10.767: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-61c16720-68c1-4ec6-b7f0-a869c21959dc
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-61c16720-68c1-4ec6-b7f0-a869c21959dc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:22:14.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2794" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":280,"skipped":4591,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:22:14.858: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
May 20 14:22:14.884: INFO: Waiting up to 5m0s for pod "pod-dd39c73a-f642-437f-bb17-e453231c1286" in namespace "emptydir-797" to be "Succeeded or Failed"
May 20 14:22:14.887: INFO: Pod "pod-dd39c73a-f642-437f-bb17-e453231c1286": Phase="Pending", Reason="", readiness=false. Elapsed: 3.326194ms
May 20 14:22:16.890: INFO: Pod "pod-dd39c73a-f642-437f-bb17-e453231c1286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006590081s
STEP: Saw pod success
May 20 14:22:16.890: INFO: Pod "pod-dd39c73a-f642-437f-bb17-e453231c1286" satisfied condition "Succeeded or Failed"
May 20 14:22:16.893: INFO: Trying to get logs from node k8s-3 pod pod-dd39c73a-f642-437f-bb17-e453231c1286 container test-container: <nil>
STEP: delete the pod
May 20 14:22:16.910: INFO: Waiting for pod pod-dd39c73a-f642-437f-bb17-e453231c1286 to disappear
May 20 14:22:16.913: INFO: Pod pod-dd39c73a-f642-437f-bb17-e453231c1286 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:22:16.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-797" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":281,"skipped":4605,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:22:16.921: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-7842
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7842 to expose endpoints map[]
May 20 14:22:16.959: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 20 14:22:17.970: INFO: successfully validated that service multi-endpoint-test in namespace services-7842 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7842
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7842 to expose endpoints map[pod1:[100]]
May 20 14:22:20.006: INFO: successfully validated that service multi-endpoint-test in namespace services-7842 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7842
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7842 to expose endpoints map[pod1:[100] pod2:[101]]
May 20 14:22:22.056: INFO: successfully validated that service multi-endpoint-test in namespace services-7842 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7842
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7842 to expose endpoints map[pod2:[101]]
May 20 14:22:23.117: INFO: successfully validated that service multi-endpoint-test in namespace services-7842 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7842
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7842 to expose endpoints map[]
May 20 14:22:23.142: INFO: successfully validated that service multi-endpoint-test in namespace services-7842 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:22:23.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7842" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.270 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":282,"skipped":4609,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:22:23.194: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3095, will wait for the garbage collector to delete the pods
May 20 14:22:27.307: INFO: Deleting Job.batch foo took: 7.931334ms
May 20 14:22:27.407: INFO: Terminating Job.batch foo pods took: 100.214157ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:23:05.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3095" for this suite.

• [SLOW TEST:42.332 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":283,"skipped":4616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:23:05.536: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:23:16.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7046" for this suite.

• [SLOW TEST:11.116 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":284,"skipped":4703,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:23:16.654: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0520 14:23:56.707175      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 20 14:24:58.731: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 20 14:24:58.731: INFO: Deleting pod "simpletest.rc-2x792" in namespace "gc-8267"
May 20 14:24:58.741: INFO: Deleting pod "simpletest.rc-fld76" in namespace "gc-8267"
May 20 14:24:58.756: INFO: Deleting pod "simpletest.rc-g828n" in namespace "gc-8267"
May 20 14:24:58.795: INFO: Deleting pod "simpletest.rc-jwtc6" in namespace "gc-8267"
May 20 14:24:58.815: INFO: Deleting pod "simpletest.rc-l82tn" in namespace "gc-8267"
May 20 14:24:58.842: INFO: Deleting pod "simpletest.rc-nscnw" in namespace "gc-8267"
May 20 14:24:58.872: INFO: Deleting pod "simpletest.rc-p4pb9" in namespace "gc-8267"
May 20 14:24:58.892: INFO: Deleting pod "simpletest.rc-sbfxg" in namespace "gc-8267"
May 20 14:24:58.918: INFO: Deleting pod "simpletest.rc-stlmc" in namespace "gc-8267"
May 20 14:24:58.942: INFO: Deleting pod "simpletest.rc-z2kcr" in namespace "gc-8267"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:24:58.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8267" for this suite.

• [SLOW TEST:102.359 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":285,"skipped":4709,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:24:59.014: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:25:04.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7902" for this suite.

• [SLOW TEST:5.072 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":286,"skipped":4764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:25:04.090: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 20 14:25:04.127: INFO: Waiting up to 5m0s for pod "downward-api-95e66d9b-4d1c-46c4-98c0-a572266c6bfe" in namespace "downward-api-794" to be "Succeeded or Failed"
May 20 14:25:04.129: INFO: Pod "downward-api-95e66d9b-4d1c-46c4-98c0-a572266c6bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.581855ms
May 20 14:25:06.134: INFO: Pod "downward-api-95e66d9b-4d1c-46c4-98c0-a572266c6bfe": Phase="Running", Reason="", readiness=true. Elapsed: 2.007827157s
May 20 14:25:08.138: INFO: Pod "downward-api-95e66d9b-4d1c-46c4-98c0-a572266c6bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011428183s
STEP: Saw pod success
May 20 14:25:08.138: INFO: Pod "downward-api-95e66d9b-4d1c-46c4-98c0-a572266c6bfe" satisfied condition "Succeeded or Failed"
May 20 14:25:08.141: INFO: Trying to get logs from node k8s-3 pod downward-api-95e66d9b-4d1c-46c4-98c0-a572266c6bfe container dapi-container: <nil>
STEP: delete the pod
May 20 14:25:08.163: INFO: Waiting for pod downward-api-95e66d9b-4d1c-46c4-98c0-a572266c6bfe to disappear
May 20 14:25:08.165: INFO: Pod downward-api-95e66d9b-4d1c-46c4-98c0-a572266c6bfe no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:25:08.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-794" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":287,"skipped":4791,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:25:08.174: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 14:25:08.525: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 14:25:10.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117508, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117508, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117508, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117508, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 14:25:13.549: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:25:13.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8961" for this suite.
STEP: Destroying namespace "webhook-8961-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.472 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":288,"skipped":4807,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:25:13.646: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:25:17.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7529" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":289,"skipped":4829,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:25:17.794: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:25:21.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8642" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":290,"skipped":4842,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:25:21.865: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:25:38.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8271" for this suite.

• [SLOW TEST:17.114 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":291,"skipped":4851,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:25:38.981: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 20 14:25:39.626: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 20 14:25:41.634: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117539, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117539, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117539, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757117539, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 20 14:25:44.676: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:25:44.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5222" for this suite.
STEP: Destroying namespace "webhook-5222-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.847 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":292,"skipped":4870,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:25:44.836: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 20 14:25:46.896: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:25:46.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3750" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":293,"skipped":4881,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:25:46.913: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0520 14:25:52.998184      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 20 14:26:55.016: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:26:55.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3751" for this suite.

• [SLOW TEST:68.110 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":294,"skipped":4881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:26:55.026: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-b3e562b4-558a-4383-b271-d661920cf074
STEP: Creating a pod to test consume secrets
May 20 14:26:55.059: INFO: Waiting up to 5m0s for pod "pod-secrets-5fd32e54-bf0c-4f6c-b9c5-764ad4878a51" in namespace "secrets-1992" to be "Succeeded or Failed"
May 20 14:26:55.064: INFO: Pod "pod-secrets-5fd32e54-bf0c-4f6c-b9c5-764ad4878a51": Phase="Pending", Reason="", readiness=false. Elapsed: 4.959967ms
May 20 14:26:57.071: INFO: Pod "pod-secrets-5fd32e54-bf0c-4f6c-b9c5-764ad4878a51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011949378s
STEP: Saw pod success
May 20 14:26:57.071: INFO: Pod "pod-secrets-5fd32e54-bf0c-4f6c-b9c5-764ad4878a51" satisfied condition "Succeeded or Failed"
May 20 14:26:57.073: INFO: Trying to get logs from node k8s-3 pod pod-secrets-5fd32e54-bf0c-4f6c-b9c5-764ad4878a51 container secret-volume-test: <nil>
STEP: delete the pod
May 20 14:26:57.109: INFO: Waiting for pod pod-secrets-5fd32e54-bf0c-4f6c-b9c5-764ad4878a51 to disappear
May 20 14:26:57.122: INFO: Pod pod-secrets-5fd32e54-bf0c-4f6c-b9c5-764ad4878a51 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:26:57.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1992" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":295,"skipped":4917,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:26:57.133: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
May 20 14:26:59.702: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3428 pod-service-account-5472b1c2-ed4b-4dbc-abe2-49f6b5d5542b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 20 14:26:59.888: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3428 pod-service-account-5472b1c2-ed4b-4dbc-abe2-49f6b5d5542b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 20 14:27:00.076: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3428 pod-service-account-5472b1c2-ed4b-4dbc-abe2-49f6b5d5542b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:27:00.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3428" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":296,"skipped":4926,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:27:00.287: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-de38be83-be78-4f8b-815b-697d39167512
May 20 14:27:00.327: INFO: Pod name my-hostname-basic-de38be83-be78-4f8b-815b-697d39167512: Found 0 pods out of 1
May 20 14:27:05.332: INFO: Pod name my-hostname-basic-de38be83-be78-4f8b-815b-697d39167512: Found 1 pods out of 1
May 20 14:27:05.332: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-de38be83-be78-4f8b-815b-697d39167512" are running
May 20 14:27:05.334: INFO: Pod "my-hostname-basic-de38be83-be78-4f8b-815b-697d39167512-qwdv6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-20 14:27:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-20 14:27:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-20 14:27:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-20 14:27:00 +0000 UTC Reason: Message:}])
May 20 14:27:05.335: INFO: Trying to dial the pod
May 20 14:27:10.346: INFO: Controller my-hostname-basic-de38be83-be78-4f8b-815b-697d39167512: Got expected result from replica 1 [my-hostname-basic-de38be83-be78-4f8b-815b-697d39167512-qwdv6]: "my-hostname-basic-de38be83-be78-4f8b-815b-697d39167512-qwdv6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:27:10.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1090" for this suite.

• [SLOW TEST:10.068 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":297,"skipped":4928,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:27:10.358: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 20 14:27:10.382: INFO: Waiting up to 1m0s for all nodes to be ready
May 20 14:28:10.406: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:28:10.409: INFO: Starting informer...
STEP: Starting pods...
May 20 14:28:10.622: INFO: Pod1 is running on k8s-3. Tainting Node
May 20 14:28:12.846: INFO: Pod2 is running on k8s-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 20 14:28:19.991: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 20 14:28:45.393: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:28:45.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7029" for this suite.

• [SLOW TEST:95.057 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":298,"skipped":4939,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:28:45.416: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 20 14:28:45.460: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb83e7b1-d6b0-4182-b99a-46b6b4c5f960" in namespace "downward-api-3355" to be "Succeeded or Failed"
May 20 14:28:45.462: INFO: Pod "downwardapi-volume-eb83e7b1-d6b0-4182-b99a-46b6b4c5f960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.227544ms
May 20 14:28:47.467: INFO: Pod "downwardapi-volume-eb83e7b1-d6b0-4182-b99a-46b6b4c5f960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006636719s
STEP: Saw pod success
May 20 14:28:47.467: INFO: Pod "downwardapi-volume-eb83e7b1-d6b0-4182-b99a-46b6b4c5f960" satisfied condition "Succeeded or Failed"
May 20 14:28:47.470: INFO: Trying to get logs from node k8s-3 pod downwardapi-volume-eb83e7b1-d6b0-4182-b99a-46b6b4c5f960 container client-container: <nil>
STEP: delete the pod
May 20 14:28:47.494: INFO: Waiting for pod downwardapi-volume-eb83e7b1-d6b0-4182-b99a-46b6b4c5f960 to disappear
May 20 14:28:47.497: INFO: Pod downwardapi-volume-eb83e7b1-d6b0-4182-b99a-46b6b4c5f960 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:28:47.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3355" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":299,"skipped":4973,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:28:47.508: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 20 14:28:47.541: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c7395b07-c5a1-42a4-b823-da88f7155124" in namespace "security-context-test-608" to be "Succeeded or Failed"
May 20 14:28:47.545: INFO: Pod "busybox-user-65534-c7395b07-c5a1-42a4-b823-da88f7155124": Phase="Pending", Reason="", readiness=false. Elapsed: 3.089074ms
May 20 14:28:49.549: INFO: Pod "busybox-user-65534-c7395b07-c5a1-42a4-b823-da88f7155124": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007892021s
May 20 14:28:51.553: INFO: Pod "busybox-user-65534-c7395b07-c5a1-42a4-b823-da88f7155124": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011884027s
May 20 14:28:51.553: INFO: Pod "busybox-user-65534-c7395b07-c5a1-42a4-b823-da88f7155124" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:28:51.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-608" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:28:51.561: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-36
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 20 14:28:51.596: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 20 14:28:51.652: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 14:28:53.687: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 20 14:28:55.658: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:28:57.656: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:28:59.664: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:29:01.657: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:29:03.663: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:29:05.656: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 20 14:29:07.665: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 20 14:29:07.670: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 20 14:29:09.673: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 20 14:29:11.677: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 20 14:29:11.685: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 20 14:29:15.714: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.66:8080/dial?request=hostname&protocol=udp&host=10.233.64.61&port=8081&tries=1'] Namespace:pod-network-test-36 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:29:15.714: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:29:15.900: INFO: Waiting for responses: map[]
May 20 14:29:15.903: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.66:8080/dial?request=hostname&protocol=udp&host=10.233.65.97&port=8081&tries=1'] Namespace:pod-network-test-36 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:29:15.903: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:29:16.026: INFO: Waiting for responses: map[]
May 20 14:29:16.029: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.66:8080/dial?request=hostname&protocol=udp&host=10.233.66.65&port=8081&tries=1'] Namespace:pod-network-test-36 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 20 14:29:16.030: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
May 20 14:29:16.129: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:29:16.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-36" for this suite.

• [SLOW TEST:24.576 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":301,"skipped":5048,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:29:16.139: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 20 14:29:16.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7179 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 20 14:29:16.243: INFO: stderr: ""
May 20 14:29:16.243: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 20 14:29:16.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7179 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
May 20 14:29:16.556: INFO: stderr: ""
May 20 14:29:16.556: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
May 20 14:29:16.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-609854186 --namespace=kubectl-7179 delete pods e2e-test-httpd-pod'
May 20 14:29:17.941: INFO: stderr: ""
May 20 14:29:17.941: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:29:17.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7179" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":302,"skipped":5078,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:29:17.952: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:29:34.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7682" for this suite.

• [SLOW TEST:16.168 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":303,"skipped":5087,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:29:34.120: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:29:34.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4774" for this suite.
STEP: Destroying namespace "nspatchtest-d0aa5663-2c2b-457b-9998-468921669542-2852" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":304,"skipped":5092,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 20 14:29:34.183: INFO: >>> kubeConfig: /tmp/kubeconfig-609854186
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 20 14:29:34.209: INFO: Waiting up to 5m0s for pod "pod-d643ec6e-35c4-4201-bd93-eb1afc30a79d" in namespace "emptydir-9192" to be "Succeeded or Failed"
May 20 14:29:34.212: INFO: Pod "pod-d643ec6e-35c4-4201-bd93-eb1afc30a79d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.403314ms
May 20 14:29:36.215: INFO: Pod "pod-d643ec6e-35c4-4201-bd93-eb1afc30a79d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006435784s
May 20 14:29:38.218: INFO: Pod "pod-d643ec6e-35c4-4201-bd93-eb1afc30a79d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009462523s
STEP: Saw pod success
May 20 14:29:38.219: INFO: Pod "pod-d643ec6e-35c4-4201-bd93-eb1afc30a79d" satisfied condition "Succeeded or Failed"
May 20 14:29:38.221: INFO: Trying to get logs from node k8s-3 pod pod-d643ec6e-35c4-4201-bd93-eb1afc30a79d container test-container: <nil>
STEP: delete the pod
May 20 14:29:38.234: INFO: Waiting for pod pod-d643ec6e-35c4-4201-bd93-eb1afc30a79d to disappear
May 20 14:29:38.236: INFO: Pod pod-d643ec6e-35c4-4201-bd93-eb1afc30a79d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 20 14:29:38.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9192" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":305,"skipped":5112,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSMay 20 14:29:38.253: INFO: Running AfterSuite actions on all nodes
May 20 14:29:38.255: INFO: Running AfterSuite actions on node 1
May 20 14:29:38.255: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":5179,"failed":0}

Ran 305 of 5484 Specs in 6398.444 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 5179 Skipped
PASS

Ginkgo ran 1 suite in 1h46m39.719923525s
Test Suite Passed
