I1129 17:57:31.723032      22 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-120018680
I1129 17:57:31.723061      22 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1129 17:57:31.723157      22 e2e.go:129] Starting e2e run "c8ff2b2c-9d10-46f2-80e9-9abad52ee157" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1606672649 - Will randomize all specs
Will run 305 of 5234 specs

Nov 29 17:57:31.734: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
E1129 17:57:31.735243      22 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Nov 29 17:57:31.736: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 29 17:57:31.747: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 29 17:57:31.769: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 29 17:57:31.769: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Nov 29 17:57:31.769: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 29 17:57:31.778: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)
Nov 29 17:57:31.778: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Nov 29 17:57:31.778: INFO: e2e test version: v1.19.4
Nov 29 17:57:31.779: INFO: kube-apiserver version: v1.19.4
Nov 29 17:57:31.779: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 17:57:31.783: INFO: Cluster IP family: ipv4
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 17:57:31.783: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pods
Nov 29 17:57:31.841: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 17:57:31.843: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 17:57:37.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6898" for this suite.

• [SLOW TEST:6.095 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":1,"skipped":1,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 17:57:37.878: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2379
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2379
STEP: creating replication controller externalsvc in namespace services-2379
I1129 17:57:37.927612      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-2379, replica count: 2
I1129 17:57:40.978000      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 17:57:43.978206      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 17:57:46.978429      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov 29 17:57:47.019: INFO: Creating new exec pod
Nov 29 17:57:51.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2379 execpodkpf8m -- /bin/sh -x -c nslookup nodeport-service.services-2379.svc.cluster.local'
Nov 29 17:57:52.015: INFO: stderr: "+ nslookup nodeport-service.services-2379.svc.cluster.local\n"
Nov 29 17:57:52.015: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2379.svc.cluster.local\tcanonical name = externalsvc.services-2379.svc.cluster.local.\nName:\texternalsvc.services-2379.svc.cluster.local\nAddress: 10.105.98.226\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2379, will wait for the garbage collector to delete the pods
Nov 29 17:57:52.073: INFO: Deleting ReplicationController externalsvc took: 4.973765ms
Nov 29 17:57:52.473: INFO: Terminating ReplicationController externalsvc pods took: 400.215527ms
Nov 29 17:57:56.389: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 17:57:56.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2379" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:18.527 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":2,"skipped":7,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 17:57:56.406: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-4055
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 17:57:56.480: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 29 17:57:56.544: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 17:57:58.547: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 17:58:00.547: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 17:58:02.547: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 17:58:04.547: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 17:58:06.547: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 17:58:08.547: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 17:58:10.547: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 17:58:12.547: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 29 17:58:12.550: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 29 17:58:14.553: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 29 17:58:16.567: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.7:8080/dial?request=hostname&protocol=udp&host=10.244.0.5&port=8081&tries=1'] Namespace:pod-network-test-4055 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 17:58:16.567: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 17:58:16.669: INFO: Waiting for responses: map[]
Nov 29 17:58:16.671: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.7:8080/dial?request=hostname&protocol=udp&host=10.244.1.6&port=8081&tries=1'] Namespace:pod-network-test-4055 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 17:58:16.671: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 17:58:16.752: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 17:58:16.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4055" for this suite.

• [SLOW TEST:20.354 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":3,"skipped":41,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 17:58:16.760: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-0f639423-4c7f-4afa-ba7e-e0207c834f66
STEP: Creating a pod to test consume configMaps
Nov 29 17:58:16.794: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8e0e4cf-1c77-44bc-9fd7-e2860174702c" in namespace "configmap-7764" to be "Succeeded or Failed"
Nov 29 17:58:16.796: INFO: Pod "pod-configmaps-e8e0e4cf-1c77-44bc-9fd7-e2860174702c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780802ms
Nov 29 17:58:18.799: INFO: Pod "pod-configmaps-e8e0e4cf-1c77-44bc-9fd7-e2860174702c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004769465s
STEP: Saw pod success
Nov 29 17:58:18.799: INFO: Pod "pod-configmaps-e8e0e4cf-1c77-44bc-9fd7-e2860174702c" satisfied condition "Succeeded or Failed"
Nov 29 17:58:18.801: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-e8e0e4cf-1c77-44bc-9fd7-e2860174702c container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 17:58:18.814: INFO: Waiting for pod pod-configmaps-e8e0e4cf-1c77-44bc-9fd7-e2860174702c to disappear
Nov 29 17:58:18.815: INFO: Pod pod-configmaps-e8e0e4cf-1c77-44bc-9fd7-e2860174702c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 17:58:18.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7764" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":4,"skipped":47,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 17:58:18.820: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 17:58:26.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5906" for this suite.

• [SLOW TEST:8.032 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":5,"skipped":50,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 17:58:26.853: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-eabca957-9834-4ab8-bc13-23193cd24c38
Nov 29 17:58:26.880: INFO: Pod name my-hostname-basic-eabca957-9834-4ab8-bc13-23193cd24c38: Found 0 pods out of 1
Nov 29 17:58:31.886: INFO: Pod name my-hostname-basic-eabca957-9834-4ab8-bc13-23193cd24c38: Found 1 pods out of 1
Nov 29 17:58:31.887: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-eabca957-9834-4ab8-bc13-23193cd24c38" are running
Nov 29 17:58:31.889: INFO: Pod "my-hostname-basic-eabca957-9834-4ab8-bc13-23193cd24c38-xxjsk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 17:58:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 17:58:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 17:58:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 17:58:26 +0000 UTC Reason: Message:}])
Nov 29 17:58:31.889: INFO: Trying to dial the pod
Nov 29 17:58:36.897: INFO: Controller my-hostname-basic-eabca957-9834-4ab8-bc13-23193cd24c38: Got expected result from replica 1 [my-hostname-basic-eabca957-9834-4ab8-bc13-23193cd24c38-xxjsk]: "my-hostname-basic-eabca957-9834-4ab8-bc13-23193cd24c38-xxjsk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 17:58:36.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2611" for this suite.

• [SLOW TEST:10.050 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":6,"skipped":93,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 17:58:36.903: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 17:58:47.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2255" for this suite.

• [SLOW TEST:11.060 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":7,"skipped":111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 17:58:47.963: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 29 17:58:48.402: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 29 17:58:50.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742269528, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742269528, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742269528, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742269528, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 17:58:53.422: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 17:58:53.425: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 17:58:54.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3030" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.695 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":8,"skipped":143,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 17:58:54.659: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Nov 29 17:58:54.678: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 29 17:59:54.689: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 17:59:54.691: INFO: Starting informer...
STEP: Starting pods...
Nov 29 17:59:54.902: INFO: Pod1 is running on k8sconformance-m02. Tainting Node
Nov 29 17:59:57.115: INFO: Pod2 is running on k8sconformance-m02. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Nov 29 18:00:07.980: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 29 18:00:27.979: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:00:27.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9673" for this suite.

• [SLOW TEST:93.342 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":9,"skipped":149,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:00:28.001: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:00:28.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fa504de-9604-4e8e-ad7e-f4d75463127f" in namespace "downward-api-4491" to be "Succeeded or Failed"
Nov 29 18:00:28.064: INFO: Pod "downwardapi-volume-8fa504de-9604-4e8e-ad7e-f4d75463127f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.110407ms
Nov 29 18:00:30.067: INFO: Pod "downwardapi-volume-8fa504de-9604-4e8e-ad7e-f4d75463127f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007402606s
STEP: Saw pod success
Nov 29 18:00:30.067: INFO: Pod "downwardapi-volume-8fa504de-9604-4e8e-ad7e-f4d75463127f" satisfied condition "Succeeded or Failed"
Nov 29 18:00:30.069: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-8fa504de-9604-4e8e-ad7e-f4d75463127f container client-container: <nil>
STEP: delete the pod
Nov 29 18:00:30.094: INFO: Waiting for pod downwardapi-volume-8fa504de-9604-4e8e-ad7e-f4d75463127f to disappear
Nov 29 18:00:30.096: INFO: Pod downwardapi-volume-8fa504de-9604-4e8e-ad7e-f4d75463127f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:00:30.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4491" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":10,"skipped":152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:00:30.101: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:00:43.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8304" for this suite.

• [SLOW TEST:13.083 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":11,"skipped":195,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:00:43.184: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:00:43.564: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 18:00:45.572: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742269643, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742269643, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742269643, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742269643, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:00:48.588: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:00:48.591: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:00:49.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6567" for this suite.
STEP: Destroying namespace "webhook-6567-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.543 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":12,"skipped":212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:00:49.728: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-90a72959-2cc7-48d8-924d-6e95a4cc8cb2
STEP: Creating a pod to test consume configMaps
Nov 29 18:00:49.782: INFO: Waiting up to 5m0s for pod "pod-configmaps-97767abd-d1c9-4a7e-930a-6d2184151738" in namespace "configmap-1557" to be "Succeeded or Failed"
Nov 29 18:00:49.784: INFO: Pod "pod-configmaps-97767abd-d1c9-4a7e-930a-6d2184151738": Phase="Pending", Reason="", readiness=false. Elapsed: 2.241083ms
Nov 29 18:00:51.787: INFO: Pod "pod-configmaps-97767abd-d1c9-4a7e-930a-6d2184151738": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005427633s
STEP: Saw pod success
Nov 29 18:00:51.787: INFO: Pod "pod-configmaps-97767abd-d1c9-4a7e-930a-6d2184151738" satisfied condition "Succeeded or Failed"
Nov 29 18:00:51.789: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-97767abd-d1c9-4a7e-930a-6d2184151738 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 18:00:51.804: INFO: Waiting for pod pod-configmaps-97767abd-d1c9-4a7e-930a-6d2184151738 to disappear
Nov 29 18:00:51.806: INFO: Pod pod-configmaps-97767abd-d1c9-4a7e-930a-6d2184151738 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:00:51.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1557" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":246,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:00:51.811: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:00:52.204: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:00:55.261: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:01:07.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3205" for this suite.
STEP: Destroying namespace "webhook-3205-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.575 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":14,"skipped":254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:01:07.387: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-85420948-36fe-4d4c-bad9-f5829b5927d7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-85420948-36fe-4d4c-bad9-f5829b5927d7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:01:11.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7520" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":15,"skipped":282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:01:11.461: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 29 18:01:11.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9157'
Nov 29 18:01:11.556: INFO: stderr: ""
Nov 29 18:01:11.556: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Nov 29 18:01:11.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete pods e2e-test-httpd-pod --namespace=kubectl-9157'
Nov 29 18:01:14.535: INFO: stderr: ""
Nov 29 18:01:14.535: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:01:14.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9157" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":16,"skipped":314,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:01:14.542: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6533
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 18:01:14.568: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 29 18:01:14.597: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 18:01:16.697: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 18:01:18.600: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:01:20.600: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:01:22.600: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:01:24.600: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:01:26.600: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:01:28.600: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:01:30.600: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 29 18:01:30.604: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 29 18:01:32.621: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.24:8080/dial?request=hostname&protocol=http&host=10.244.0.6&port=8080&tries=1'] Namespace:pod-network-test-6533 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:01:32.621: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:01:32.721: INFO: Waiting for responses: map[]
Nov 29 18:01:32.724: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.24:8080/dial?request=hostname&protocol=http&host=10.244.1.23&port=8080&tries=1'] Namespace:pod-network-test-6533 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:01:32.724: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:01:32.804: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:01:32.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6533" for this suite.

• [SLOW TEST:18.268 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":327,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:01:32.810: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 29 18:01:32.842: INFO: Waiting up to 5m0s for pod "pod-f3dac511-964d-4b5b-a77e-6c5663a23406" in namespace "emptydir-2382" to be "Succeeded or Failed"
Nov 29 18:01:32.844: INFO: Pod "pod-f3dac511-964d-4b5b-a77e-6c5663a23406": Phase="Pending", Reason="", readiness=false. Elapsed: 1.53346ms
Nov 29 18:01:34.847: INFO: Pod "pod-f3dac511-964d-4b5b-a77e-6c5663a23406": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004450057s
STEP: Saw pod success
Nov 29 18:01:34.847: INFO: Pod "pod-f3dac511-964d-4b5b-a77e-6c5663a23406" satisfied condition "Succeeded or Failed"
Nov 29 18:01:34.848: INFO: Trying to get logs from node k8sconformance-m02 pod pod-f3dac511-964d-4b5b-a77e-6c5663a23406 container test-container: <nil>
STEP: delete the pod
Nov 29 18:01:34.859: INFO: Waiting for pod pod-f3dac511-964d-4b5b-a77e-6c5663a23406 to disappear
Nov 29 18:01:34.864: INFO: Pod pod-f3dac511-964d-4b5b-a77e-6c5663a23406 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:01:34.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2382" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":18,"skipped":332,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:01:34.869: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 18:01:36.902: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:01:36.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6369" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":19,"skipped":341,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:01:36.920: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2633
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2633
I1129 18:01:36.963028      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2633, replica count: 2
Nov 29 18:01:40.013: INFO: Creating new exec pod
I1129 18:01:40.013383      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 18:01:43.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2633 execpodsgwq8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 29 18:01:43.178: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 29 18:01:43.178: INFO: stdout: ""
Nov 29 18:01:43.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2633 execpodsgwq8 -- /bin/sh -x -c nc -zv -t -w 2 10.104.158.198 80'
Nov 29 18:01:43.322: INFO: stderr: "+ nc -zv -t -w 2 10.104.158.198 80\nConnection to 10.104.158.198 80 port [tcp/http] succeeded!\n"
Nov 29 18:01:43.322: INFO: stdout: ""
Nov 29 18:01:43.322: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:01:43.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2633" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.456 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":20,"skipped":343,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:01:43.376: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:01:43.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2421" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":21,"skipped":359,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:01:43.404: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:01:48.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-733" for this suite.

• [SLOW TEST:5.341 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":22,"skipped":364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:01:48.746: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 18:01:50.788: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:01:50.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-496" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":23,"skipped":404,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:01:50.802: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Nov 29 18:03:51.339: INFO: Successfully updated pod "var-expansion-fa2a3334-a9e4-41a9-aeab-cbe843e1314a"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Nov 29 18:03:53.346: INFO: Deleting pod "var-expansion-fa2a3334-a9e4-41a9-aeab-cbe843e1314a" in namespace "var-expansion-7586"
Nov 29 18:03:53.349: INFO: Wait up to 5m0s for pod "var-expansion-fa2a3334-a9e4-41a9-aeab-cbe843e1314a" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:04:29.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7586" for this suite.

• [SLOW TEST:158.560 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":24,"skipped":420,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:04:29.362: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:04:29.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6396" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":25,"skipped":435,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:04:29.389: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:06:29.422: INFO: Deleting pod "var-expansion-0f5011df-9c23-423b-9d80-a10ba8129ea9" in namespace "var-expansion-4282"
Nov 29 18:06:29.426: INFO: Wait up to 5m0s for pod "var-expansion-0f5011df-9c23-423b-9d80-a10ba8129ea9" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:06:31.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4282" for this suite.

• [SLOW TEST:122.050 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":26,"skipped":453,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:06:31.440: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 29 18:06:31.468: INFO: Waiting up to 5m0s for pod "pod-adf4c42f-e12c-4f3b-b963-085638cca9fc" in namespace "emptydir-5005" to be "Succeeded or Failed"
Nov 29 18:06:31.482: INFO: Pod "pod-adf4c42f-e12c-4f3b-b963-085638cca9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.935407ms
Nov 29 18:06:33.485: INFO: Pod "pod-adf4c42f-e12c-4f3b-b963-085638cca9fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016891011s
STEP: Saw pod success
Nov 29 18:06:33.485: INFO: Pod "pod-adf4c42f-e12c-4f3b-b963-085638cca9fc" satisfied condition "Succeeded or Failed"
Nov 29 18:06:33.486: INFO: Trying to get logs from node k8sconformance-m02 pod pod-adf4c42f-e12c-4f3b-b963-085638cca9fc container test-container: <nil>
STEP: delete the pod
Nov 29 18:06:33.508: INFO: Waiting for pod pod-adf4c42f-e12c-4f3b-b963-085638cca9fc to disappear
Nov 29 18:06:33.510: INFO: Pod pod-adf4c42f-e12c-4f3b-b963-085638cca9fc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:06:33.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5005" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":27,"skipped":455,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:06:33.516: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Nov 29 18:06:33.542: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Nov 29 18:06:33.545: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 29 18:06:33.545: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Nov 29 18:06:33.552: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 29 18:06:33.552: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Nov 29 18:06:33.561: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 29 18:06:33.561: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Nov 29 18:06:40.604: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:06:40.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6206" for this suite.

• [SLOW TEST:7.103 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":28,"skipped":464,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:06:40.619: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2942
Nov 29 18:06:42.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2942 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 29 18:06:42.845: INFO: rc: 7
Nov 29 18:06:42.850: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 29 18:06:42.852: INFO: Pod kube-proxy-mode-detector still exists
Nov 29 18:06:44.852: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 29 18:06:44.855: INFO: Pod kube-proxy-mode-detector still exists
Nov 29 18:06:46.852: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 29 18:06:46.855: INFO: Pod kube-proxy-mode-detector no longer exists
Nov 29 18:06:46.855: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2942 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-2942
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2942
I1129 18:06:46.875027      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2942, replica count: 3
I1129 18:06:49.925310      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 18:06:49.929: INFO: Creating new exec pod
Nov 29 18:06:52.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2942 execpod-affinitydfg6f -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Nov 29 18:06:53.098: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Nov 29 18:06:53.098: INFO: stdout: ""
Nov 29 18:06:53.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2942 execpod-affinitydfg6f -- /bin/sh -x -c nc -zv -t -w 2 10.101.111.57 80'
Nov 29 18:06:53.238: INFO: stderr: "+ nc -zv -t -w 2 10.101.111.57 80\nConnection to 10.101.111.57 80 port [tcp/http] succeeded!\n"
Nov 29 18:06:53.238: INFO: stdout: ""
Nov 29 18:06:53.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2942 execpod-affinitydfg6f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.111.57:80/ ; done'
Nov 29 18:06:53.474: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n"
Nov 29 18:06:53.474: INFO: stdout: "\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw\naffinity-clusterip-timeout-p79cw"
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Received response from host: affinity-clusterip-timeout-p79cw
Nov 29 18:06:53.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2942 execpod-affinitydfg6f -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.101.111.57:80/'
Nov 29 18:06:53.623: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n"
Nov 29 18:06:53.623: INFO: stdout: "affinity-clusterip-timeout-p79cw"
Nov 29 18:07:08.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2942 execpod-affinitydfg6f -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.101.111.57:80/'
Nov 29 18:07:08.764: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.101.111.57:80/\n"
Nov 29 18:07:08.764: INFO: stdout: "affinity-clusterip-timeout-98qn6"
Nov 29 18:07:08.765: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2942, will wait for the garbage collector to delete the pods
Nov 29 18:07:08.832: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 4.662501ms
Nov 29 18:07:09.232: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 400.148934ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:07:20.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2942" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:39.532 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":29,"skipped":472,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:07:20.151: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:07:20.179: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c97cb86f-9b7a-46a1-a10c-571e6188eccc" in namespace "downward-api-8993" to be "Succeeded or Failed"
Nov 29 18:07:20.209: INFO: Pod "downwardapi-volume-c97cb86f-9b7a-46a1-a10c-571e6188eccc": Phase="Pending", Reason="", readiness=false. Elapsed: 30.71201ms
Nov 29 18:07:22.212: INFO: Pod "downwardapi-volume-c97cb86f-9b7a-46a1-a10c-571e6188eccc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033480122s
STEP: Saw pod success
Nov 29 18:07:22.212: INFO: Pod "downwardapi-volume-c97cb86f-9b7a-46a1-a10c-571e6188eccc" satisfied condition "Succeeded or Failed"
Nov 29 18:07:22.214: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-c97cb86f-9b7a-46a1-a10c-571e6188eccc container client-container: <nil>
STEP: delete the pod
Nov 29 18:07:22.224: INFO: Waiting for pod downwardapi-volume-c97cb86f-9b7a-46a1-a10c-571e6188eccc to disappear
Nov 29 18:07:22.229: INFO: Pod downwardapi-volume-c97cb86f-9b7a-46a1-a10c-571e6188eccc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:07:22.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8993" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":30,"skipped":476,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:07:22.250: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:07:22.276: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Pending, waiting for it to be Running (with Ready = true)
Nov 29 18:07:24.279: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Running (Ready = false)
Nov 29 18:07:26.279: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Running (Ready = false)
Nov 29 18:07:28.279: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Running (Ready = false)
Nov 29 18:07:30.279: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Running (Ready = false)
Nov 29 18:07:32.279: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Running (Ready = false)
Nov 29 18:07:34.312: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Running (Ready = false)
Nov 29 18:07:36.279: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Running (Ready = false)
Nov 29 18:07:38.279: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Running (Ready = false)
Nov 29 18:07:40.279: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Running (Ready = false)
Nov 29 18:07:42.282: INFO: The status of Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f is Running (Ready = true)
Nov 29 18:07:42.284: INFO: Container started at 2020-11-29 18:07:23 +0000 UTC, pod became ready at 2020-11-29 18:07:40 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:07:42.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7043" for this suite.

• [SLOW TEST:20.040 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":31,"skipped":477,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:07:42.290: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 29 18:07:42.327: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 18:07:42.331: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 18:07:42.332: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Nov 29 18:07:42.336: INFO: coredns-f9fd979d6-brw29 from kube-system started at 2020-11-29 17:56:10 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.336: INFO: 	Container coredns ready: true, restart count 0
Nov 29 18:07:42.336: INFO: etcd-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.336: INFO: 	Container etcd ready: true, restart count 0
Nov 29 18:07:42.336: INFO: kindnet-w9hf5 from kube-system started at 2020-11-29 17:56:09 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.336: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 29 18:07:42.336: INFO: kube-apiserver-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.336: INFO: 	Container kube-apiserver ready: true, restart count 0
Nov 29 18:07:42.336: INFO: kube-controller-manager-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.336: INFO: 	Container kube-controller-manager ready: true, restart count 0
Nov 29 18:07:42.336: INFO: kube-proxy-rk57s from kube-system started at 2020-11-29 17:56:09 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.336: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 18:07:42.336: INFO: kube-scheduler-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.336: INFO: 	Container kube-scheduler ready: true, restart count 0
Nov 29 18:07:42.336: INFO: storage-provisioner from kube-system started at 2020-11-29 17:56:11 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.336: INFO: 	Container storage-provisioner ready: true, restart count 0
Nov 29 18:07:42.336: INFO: sonobuoy from sonobuoy started at 2020-11-29 17:57:02 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.336: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 29 18:07:42.336: INFO: sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-bfkxd from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:07:42.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:07:42.336: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 18:07:42.336: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Nov 29 18:07:42.339: INFO: test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f from container-probe-7043 started at 2020-11-29 18:07:22 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.339: INFO: 	Container test-webserver ready: true, restart count 0
Nov 29 18:07:42.339: INFO: kindnet-hq29j from kube-system started at 2020-11-29 18:00:28 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.339: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 29 18:07:42.339: INFO: kube-proxy-p6zkw from kube-system started at 2020-11-29 17:56:58 +0000 UTC (1 container statuses recorded)
Nov 29 18:07:42.339: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 18:07:42.339: INFO: sonobuoy-e2e-job-9d13ec04340c4e3d from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:07:42.339: INFO: 	Container e2e ready: true, restart count 0
Nov 29 18:07:42.339: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:07:42.339: INFO: sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-nkv9s from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:07:42.339: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:07:42.339: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node k8sconformance
STEP: verifying the node has the label node k8sconformance-m02
Nov 29 18:07:42.364: INFO: Pod test-webserver-2f0c3c32-5ccf-428c-a695-f5dc785d321f requesting resource cpu=0m on Node k8sconformance-m02
Nov 29 18:07:42.364: INFO: Pod coredns-f9fd979d6-brw29 requesting resource cpu=100m on Node k8sconformance
Nov 29 18:07:42.364: INFO: Pod etcd-k8sconformance requesting resource cpu=0m on Node k8sconformance
Nov 29 18:07:42.364: INFO: Pod kindnet-hq29j requesting resource cpu=100m on Node k8sconformance-m02
Nov 29 18:07:42.364: INFO: Pod kindnet-w9hf5 requesting resource cpu=100m on Node k8sconformance
Nov 29 18:07:42.364: INFO: Pod kube-apiserver-k8sconformance requesting resource cpu=250m on Node k8sconformance
Nov 29 18:07:42.364: INFO: Pod kube-controller-manager-k8sconformance requesting resource cpu=200m on Node k8sconformance
Nov 29 18:07:42.364: INFO: Pod kube-proxy-p6zkw requesting resource cpu=0m on Node k8sconformance-m02
Nov 29 18:07:42.364: INFO: Pod kube-proxy-rk57s requesting resource cpu=0m on Node k8sconformance
Nov 29 18:07:42.364: INFO: Pod kube-scheduler-k8sconformance requesting resource cpu=100m on Node k8sconformance
Nov 29 18:07:42.364: INFO: Pod storage-provisioner requesting resource cpu=0m on Node k8sconformance
Nov 29 18:07:42.364: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8sconformance
Nov 29 18:07:42.364: INFO: Pod sonobuoy-e2e-job-9d13ec04340c4e3d requesting resource cpu=0m on Node k8sconformance-m02
Nov 29 18:07:42.364: INFO: Pod sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-bfkxd requesting resource cpu=0m on Node k8sconformance
Nov 29 18:07:42.364: INFO: Pod sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-nkv9s requesting resource cpu=0m on Node k8sconformance-m02
STEP: Starting Pods to consume most of the cluster CPU.
Nov 29 18:07:42.364: INFO: Creating a pod which consumes cpu=5075m on Node k8sconformance
Nov 29 18:07:42.368: INFO: Creating a pod which consumes cpu=5530m on Node k8sconformance-m02
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-053bc7d7-6171-430b-bfb6-e8345d682d5f.164c0cd1d237dd5f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3116/filler-pod-053bc7d7-6171-430b-bfb6-e8345d682d5f to k8sconformance-m02]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-053bc7d7-6171-430b-bfb6-e8345d682d5f.164c0cd1e66e007f], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-3116.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-053bc7d7-6171-430b-bfb6-e8345d682d5f.164c0cd1f628f628], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-053bc7d7-6171-430b-bfb6-e8345d682d5f.164c0cd207bda7ff], Reason = [Created], Message = [Created container filler-pod-053bc7d7-6171-430b-bfb6-e8345d682d5f]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-053bc7d7-6171-430b-bfb6-e8345d682d5f.164c0cd212679a1b], Reason = [Failed], Message = [Error: failed to create containerd task: OCI runtime create failed: container_linux.go:349: starting container process caused "process_linux.go:449: container init caused \"process_linux.go:415: setting cgroup config for procHooks process caused \\\"failed to write \\\\\\\"553000\\\\\\\" to \\\\\\\"/sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pode0d559c8-b2cd-4a6a-a4ef-52fb8756f7e7/filler-pod-053bc7d7-6171-430b-bfb6-e8345d682d5f/cpu.cfs_quota_us\\\\\\\": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pode0d559c8-b2cd-4a6a-a4ef-52fb8756f7e7/filler-pod-053bc7d7-6171-430b-bfb6-e8345d682d5f/cpu.cfs_quota_us: invalid argument\\\"\"": unknown]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26411952-eb4d-4997-a8f5-0f45cf8eb42a.164c0cd1d1d8d391], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3116/filler-pod-26411952-eb4d-4997-a8f5-0f45cf8eb42a to k8sconformance]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-26411952-eb4d-4997-a8f5-0f45cf8eb42a.164c0cd1e6854b25], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-3116.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26411952-eb4d-4997-a8f5-0f45cf8eb42a.164c0cd1faf6364e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26411952-eb4d-4997-a8f5-0f45cf8eb42a.164c0cd20799f140], Reason = [Created], Message = [Created container filler-pod-26411952-eb4d-4997-a8f5-0f45cf8eb42a]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-26411952-eb4d-4997-a8f5-0f45cf8eb42a.164c0cd20f8c0469], Reason = [Failed], Message = [Error: failed to create containerd task: OCI runtime create failed: container_linux.go:349: starting container process caused "process_linux.go:449: container init caused \"process_linux.go:415: setting cgroup config for procHooks process caused \\\"failed to write \\\\\\\"507500\\\\\\\" to \\\\\\\"/sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod8ed376aa-cc7c-43a0-bc96-908b28e6d676/filler-pod-26411952-eb4d-4997-a8f5-0f45cf8eb42a/cpu.cfs_quota_us\\\\\\\": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod8ed376aa-cc7c-43a0-bc96-908b28e6d676/filler-pod-26411952-eb4d-4997-a8f5-0f45cf8eb42a/cpu.cfs_quota_us: invalid argument\\\"\"": unknown]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.164c0cd24a77e9a0], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-053bc7d7-6171-430b-bfb6-e8345d682d5f.164c0cd2763507ca], Reason = [BackOff], Message = [Back-off restarting failed container]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-26411952-eb4d-4997-a8f5-0f45cf8eb42a.164c0cd27be85795], Reason = [BackOff], Message = [Back-off restarting failed container]
STEP: removing the label node off the node k8sconformance
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8sconformance-m02
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:07:45.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3116" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":32,"skipped":487,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:07:45.422: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-8566/configmap-test-0404d9cc-c2e6-49f1-9400-17b0aa788af2
STEP: Creating a pod to test consume configMaps
Nov 29 18:07:45.449: INFO: Waiting up to 5m0s for pod "pod-configmaps-97e24468-6688-4f18-bfb4-e860974305d8" in namespace "configmap-8566" to be "Succeeded or Failed"
Nov 29 18:07:45.450: INFO: Pod "pod-configmaps-97e24468-6688-4f18-bfb4-e860974305d8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.654195ms
Nov 29 18:07:47.453: INFO: Pod "pod-configmaps-97e24468-6688-4f18-bfb4-e860974305d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003828438s
STEP: Saw pod success
Nov 29 18:07:47.453: INFO: Pod "pod-configmaps-97e24468-6688-4f18-bfb4-e860974305d8" satisfied condition "Succeeded or Failed"
Nov 29 18:07:47.454: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-97e24468-6688-4f18-bfb4-e860974305d8 container env-test: <nil>
STEP: delete the pod
Nov 29 18:07:47.467: INFO: Waiting for pod pod-configmaps-97e24468-6688-4f18-bfb4-e860974305d8 to disappear
Nov 29 18:07:47.471: INFO: Pod pod-configmaps-97e24468-6688-4f18-bfb4-e860974305d8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:07:47.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8566" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":33,"skipped":491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:07:47.476: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2914 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2914;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2914 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2914;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2914.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2914.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2914.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2914.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2914.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2914.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2914.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2914.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2914.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2914.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2914.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 22.43.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.43.22_udp@PTR;check="$$(dig +tcp +noall +answer +search 22.43.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.43.22_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2914 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2914;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2914 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2914;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2914.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2914.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2914.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2914.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2914.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2914.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2914.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2914.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2914.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2914.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2914.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2914.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 22.43.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.43.22_udp@PTR;check="$$(dig +tcp +noall +answer +search 22.43.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.43.22_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 18:07:57.566: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.568: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.570: INFO: Unable to read wheezy_udp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.572: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.574: INFO: Unable to read wheezy_udp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.576: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.578: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.580: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.594: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.596: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.598: INFO: Unable to read jessie_udp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.600: INFO: Unable to read jessie_tcp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.602: INFO: Unable to read jessie_udp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.604: INFO: Unable to read jessie_tcp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.606: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.608: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:07:57.622: INFO: Lookups using dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2914 wheezy_tcp@dns-test-service.dns-2914 wheezy_udp@dns-test-service.dns-2914.svc wheezy_tcp@dns-test-service.dns-2914.svc wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2914 jessie_tcp@dns-test-service.dns-2914 jessie_udp@dns-test-service.dns-2914.svc jessie_tcp@dns-test-service.dns-2914.svc jessie_udp@_http._tcp.dns-test-service.dns-2914.svc jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc]

Nov 29 18:08:02.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.628: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.630: INFO: Unable to read wheezy_udp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.633: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.635: INFO: Unable to read wheezy_udp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.637: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.639: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.642: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.659: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.661: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.663: INFO: Unable to read jessie_udp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.665: INFO: Unable to read jessie_tcp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.667: INFO: Unable to read jessie_udp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.669: INFO: Unable to read jessie_tcp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.671: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.673: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:02.685: INFO: Lookups using dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2914 wheezy_tcp@dns-test-service.dns-2914 wheezy_udp@dns-test-service.dns-2914.svc wheezy_tcp@dns-test-service.dns-2914.svc wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2914 jessie_tcp@dns-test-service.dns-2914 jessie_udp@dns-test-service.dns-2914.svc jessie_tcp@dns-test-service.dns-2914.svc jessie_udp@_http._tcp.dns-test-service.dns-2914.svc jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc]

Nov 29 18:08:07.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.628: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.630: INFO: Unable to read wheezy_udp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.632: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.636: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.638: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.640: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.654: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.656: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.658: INFO: Unable to read jessie_udp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.660: INFO: Unable to read jessie_tcp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.662: INFO: Unable to read jessie_udp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.664: INFO: Unable to read jessie_tcp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.666: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.668: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:07.681: INFO: Lookups using dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2914 wheezy_tcp@dns-test-service.dns-2914 wheezy_udp@dns-test-service.dns-2914.svc wheezy_tcp@dns-test-service.dns-2914.svc wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2914 jessie_tcp@dns-test-service.dns-2914 jessie_udp@dns-test-service.dns-2914.svc jessie_tcp@dns-test-service.dns-2914.svc jessie_udp@_http._tcp.dns-test-service.dns-2914.svc jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc]

Nov 29 18:08:12.626: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.628: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.630: INFO: Unable to read wheezy_udp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.632: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.636: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.638: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.640: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.655: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.657: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.658: INFO: Unable to read jessie_udp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.660: INFO: Unable to read jessie_tcp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.662: INFO: Unable to read jessie_udp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.664: INFO: Unable to read jessie_tcp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.667: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.669: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:12.680: INFO: Lookups using dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2914 wheezy_tcp@dns-test-service.dns-2914 wheezy_udp@dns-test-service.dns-2914.svc wheezy_tcp@dns-test-service.dns-2914.svc wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2914 jessie_tcp@dns-test-service.dns-2914 jessie_udp@dns-test-service.dns-2914.svc jessie_tcp@dns-test-service.dns-2914.svc jessie_udp@_http._tcp.dns-test-service.dns-2914.svc jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc]

Nov 29 18:08:17.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.628: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.630: INFO: Unable to read wheezy_udp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.632: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.636: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.638: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.640: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.655: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.657: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.659: INFO: Unable to read jessie_udp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.661: INFO: Unable to read jessie_tcp@dns-test-service.dns-2914 from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.662: INFO: Unable to read jessie_udp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.664: INFO: Unable to read jessie_tcp@dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.666: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.668: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc from pod dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2: the server could not find the requested resource (get pods dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2)
Nov 29 18:08:17.680: INFO: Lookups using dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2914 wheezy_tcp@dns-test-service.dns-2914 wheezy_udp@dns-test-service.dns-2914.svc wheezy_tcp@dns-test-service.dns-2914.svc wheezy_udp@_http._tcp.dns-test-service.dns-2914.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2914.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2914 jessie_tcp@dns-test-service.dns-2914 jessie_udp@dns-test-service.dns-2914.svc jessie_tcp@dns-test-service.dns-2914.svc jessie_udp@_http._tcp.dns-test-service.dns-2914.svc jessie_tcp@_http._tcp.dns-test-service.dns-2914.svc]

Nov 29 18:08:22.681: INFO: DNS probes using dns-2914/dns-test-f3a0e088-fb6d-440a-ba84-a3d2ac6243b2 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:08:22.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2914" for this suite.

• [SLOW TEST:35.267 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":34,"skipped":513,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:08:22.743: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Nov 29 18:08:23.520: INFO: created pod pod-service-account-defaultsa
Nov 29 18:08:23.520: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 29 18:08:23.525: INFO: created pod pod-service-account-mountsa
Nov 29 18:08:23.525: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 29 18:08:23.528: INFO: created pod pod-service-account-nomountsa
Nov 29 18:08:23.528: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 29 18:08:23.534: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 29 18:08:23.534: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 29 18:08:23.540: INFO: created pod pod-service-account-mountsa-mountspec
Nov 29 18:08:23.540: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 29 18:08:23.544: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 29 18:08:23.544: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 29 18:08:23.550: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 29 18:08:23.550: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 29 18:08:23.557: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 29 18:08:23.557: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 29 18:08:23.561: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 29 18:08:23.561: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:08:23.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3618" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":35,"skipped":520,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:08:23.656: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:08:24.560: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 18:08:26.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742270104, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742270104, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742270104, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742270104, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 18:08:28.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742270104, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742270104, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742270104, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742270104, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:08:31.581: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:08:31.584: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6317-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:08:32.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9251" for this suite.
STEP: Destroying namespace "webhook-9251-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.136 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":36,"skipped":529,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:08:32.792: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:08:32.828: INFO: Waiting up to 5m0s for pod "busybox-user-65534-674e7c25-1ed0-45b9-b05e-dc50dd9fd313" in namespace "security-context-test-265" to be "Succeeded or Failed"
Nov 29 18:08:32.832: INFO: Pod "busybox-user-65534-674e7c25-1ed0-45b9-b05e-dc50dd9fd313": Phase="Pending", Reason="", readiness=false. Elapsed: 3.56309ms
Nov 29 18:08:34.834: INFO: Pod "busybox-user-65534-674e7c25-1ed0-45b9-b05e-dc50dd9fd313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006181542s
Nov 29 18:08:36.837: INFO: Pod "busybox-user-65534-674e7c25-1ed0-45b9-b05e-dc50dd9fd313": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008849113s
Nov 29 18:08:38.840: INFO: Pod "busybox-user-65534-674e7c25-1ed0-45b9-b05e-dc50dd9fd313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011608668s
Nov 29 18:08:38.840: INFO: Pod "busybox-user-65534-674e7c25-1ed0-45b9-b05e-dc50dd9fd313" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:08:38.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-265" for this suite.

• [SLOW TEST:6.053 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a container with runAsUser
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":538,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:08:38.845: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-6938d738-cc64-4bcf-8802-b6b8e9f3efe3
STEP: Creating a pod to test consume configMaps
Nov 29 18:08:38.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4596c40-c4fc-4263-8333-30d26a9452e2" in namespace "configmap-6405" to be "Succeeded or Failed"
Nov 29 18:08:38.890: INFO: Pod "pod-configmaps-d4596c40-c4fc-4263-8333-30d26a9452e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.001329ms
Nov 29 18:08:40.892: INFO: Pod "pod-configmaps-d4596c40-c4fc-4263-8333-30d26a9452e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004576626s
STEP: Saw pod success
Nov 29 18:08:40.892: INFO: Pod "pod-configmaps-d4596c40-c4fc-4263-8333-30d26a9452e2" satisfied condition "Succeeded or Failed"
Nov 29 18:08:40.894: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-d4596c40-c4fc-4263-8333-30d26a9452e2 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 18:08:40.909: INFO: Waiting for pod pod-configmaps-d4596c40-c4fc-4263-8333-30d26a9452e2 to disappear
Nov 29 18:08:40.911: INFO: Pod pod-configmaps-d4596c40-c4fc-4263-8333-30d26a9452e2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:08:40.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6405" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":38,"skipped":539,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:08:40.916: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Nov 29 18:08:40.941: INFO: Waiting up to 5m0s for pod "client-containers-c7e28f9d-2a98-4468-b7b0-8a1252b4e27a" in namespace "containers-8291" to be "Succeeded or Failed"
Nov 29 18:08:40.942: INFO: Pod "client-containers-c7e28f9d-2a98-4468-b7b0-8a1252b4e27a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.526337ms
Nov 29 18:08:42.947: INFO: Pod "client-containers-c7e28f9d-2a98-4468-b7b0-8a1252b4e27a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005643293s
STEP: Saw pod success
Nov 29 18:08:42.947: INFO: Pod "client-containers-c7e28f9d-2a98-4468-b7b0-8a1252b4e27a" satisfied condition "Succeeded or Failed"
Nov 29 18:08:42.949: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-c7e28f9d-2a98-4468-b7b0-8a1252b4e27a container test-container: <nil>
STEP: delete the pod
Nov 29 18:08:42.958: INFO: Waiting for pod client-containers-c7e28f9d-2a98-4468-b7b0-8a1252b4e27a to disappear
Nov 29 18:08:42.960: INFO: Pod client-containers-c7e28f9d-2a98-4468-b7b0-8a1252b4e27a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:08:42.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8291" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":541,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:08:42.965: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 29 18:08:42.991: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-a d1dd0629-d28d-43ee-97b7-44efb46491b2 3334 0 2020-11-29 18:08:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 18:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:08:42.991: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-a d1dd0629-d28d-43ee-97b7-44efb46491b2 3334 0 2020-11-29 18:08:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 18:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 29 18:08:52.997: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-a d1dd0629-d28d-43ee-97b7-44efb46491b2 3363 0 2020-11-29 18:08:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 18:08:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:08:52.998: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-a d1dd0629-d28d-43ee-97b7-44efb46491b2 3363 0 2020-11-29 18:08:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 18:08:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 29 18:09:03.003: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-a d1dd0629-d28d-43ee-97b7-44efb46491b2 3373 0 2020-11-29 18:08:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 18:09:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:09:03.003: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-a d1dd0629-d28d-43ee-97b7-44efb46491b2 3373 0 2020-11-29 18:08:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 18:09:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 29 18:09:13.008: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-a d1dd0629-d28d-43ee-97b7-44efb46491b2 3383 0 2020-11-29 18:08:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 18:09:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:09:13.008: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-a d1dd0629-d28d-43ee-97b7-44efb46491b2 3383 0 2020-11-29 18:08:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 18:09:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 29 18:09:23.014: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-b 1bcbca7f-cb08-4fa2-8df9-860b6e81b0ed 3393 0 2020-11-29 18:09:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-29 18:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:09:23.014: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-b 1bcbca7f-cb08-4fa2-8df9-860b6e81b0ed 3393 0 2020-11-29 18:09:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-29 18:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 29 18:09:33.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-b 1bcbca7f-cb08-4fa2-8df9-860b6e81b0ed 3402 0 2020-11-29 18:09:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-29 18:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:09:33.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7312 /api/v1/namespaces/watch-7312/configmaps/e2e-watch-test-configmap-b 1bcbca7f-cb08-4fa2-8df9-860b6e81b0ed 3402 0 2020-11-29 18:09:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-29 18:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:09:43.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7312" for this suite.

• [SLOW TEST:60.061 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":40,"skipped":542,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:09:43.026: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:09:47.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1453" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":41,"skipped":553,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:09:47.061: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-6f4609d5-3be1-462f-a792-fe93e16b94e3
STEP: Creating a pod to test consume secrets
Nov 29 18:09:47.092: INFO: Waiting up to 5m0s for pod "pod-secrets-ba7d8bd6-5820-4ac0-acf6-7656a9bfbd4e" in namespace "secrets-5450" to be "Succeeded or Failed"
Nov 29 18:09:47.094: INFO: Pod "pod-secrets-ba7d8bd6-5820-4ac0-acf6-7656a9bfbd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.797748ms
Nov 29 18:09:49.097: INFO: Pod "pod-secrets-ba7d8bd6-5820-4ac0-acf6-7656a9bfbd4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004558458s
STEP: Saw pod success
Nov 29 18:09:49.097: INFO: Pod "pod-secrets-ba7d8bd6-5820-4ac0-acf6-7656a9bfbd4e" satisfied condition "Succeeded or Failed"
Nov 29 18:09:49.098: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-ba7d8bd6-5820-4ac0-acf6-7656a9bfbd4e container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 18:09:49.112: INFO: Waiting for pod pod-secrets-ba7d8bd6-5820-4ac0-acf6-7656a9bfbd4e to disappear
Nov 29 18:09:49.114: INFO: Pod pod-secrets-ba7d8bd6-5820-4ac0-acf6-7656a9bfbd4e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:09:49.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5450" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":42,"skipped":567,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:09:49.119: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:09:49.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3026" for this suite.
STEP: Destroying namespace "nspatchtest-44ca9760-077b-4e5d-99da-ea9e1d8e964a-6349" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":43,"skipped":577,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:09:49.169: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:10:18.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5641" for this suite.
STEP: Destroying namespace "nsdeletetest-9016" for this suite.
Nov 29 18:10:18.276: INFO: Namespace nsdeletetest-9016 was already deleted
STEP: Destroying namespace "nsdeletetest-2521" for this suite.

• [SLOW TEST:29.109 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":44,"skipped":596,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:10:18.279: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-83
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 18:10:18.298: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 29 18:10:18.314: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 18:10:20.317: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:10:22.317: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:10:24.317: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:10:26.317: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:10:28.317: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:10:30.317: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:10:32.317: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:10:34.317: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:10:36.317: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:10:38.317: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 29 18:10:38.320: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 29 18:10:40.346: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.11 8081 | grep -v '^\s*$'] Namespace:pod-network-test-83 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:10:40.346: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:10:41.432: INFO: Found all expected endpoints: [netserver-0]
Nov 29 18:10:41.435: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-83 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:10:41.435: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:10:42.501: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:10:42.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-83" for this suite.

• [SLOW TEST:24.229 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":45,"skipped":598,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:10:42.508: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Nov 29 18:10:42.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-4943'
Nov 29 18:10:43.383: INFO: stderr: ""
Nov 29 18:10:43.383: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 29 18:10:44.386: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:10:44.386: INFO: Found 0 / 1
Nov 29 18:10:45.386: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:10:45.386: INFO: Found 1 / 1
Nov 29 18:10:45.386: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 29 18:10:45.388: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:10:45.388: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 29 18:10:45.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 patch pod agnhost-primary-5bgtd --namespace=kubectl-4943 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 29 18:10:45.459: INFO: stderr: ""
Nov 29 18:10:45.459: INFO: stdout: "pod/agnhost-primary-5bgtd patched\n"
STEP: checking annotations
Nov 29 18:10:45.462: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:10:45.462: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:10:45.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4943" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":46,"skipped":610,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:10:45.468: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 29 18:10:49.511: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 18:10:49.513: INFO: Pod pod-with-prestop-http-hook still exists
Nov 29 18:10:51.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 18:10:51.516: INFO: Pod pod-with-prestop-http-hook still exists
Nov 29 18:10:53.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 18:10:53.516: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:10:53.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2552" for this suite.

• [SLOW TEST:8.058 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":47,"skipped":623,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:10:53.526: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Nov 29 18:10:53.551: INFO: created test-pod-1
Nov 29 18:10:53.553: INFO: created test-pod-2
Nov 29 18:10:53.559: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:10:53.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9682" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":48,"skipped":627,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:10:53.591: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1129 18:11:03.776285      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 29 18:12:05.789: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:12:05.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-29" for this suite.

• [SLOW TEST:72.204 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":49,"skipped":632,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:12:05.795: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:12:05.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17353518-118f-42bb-a723-3c36512f3c78" in namespace "downward-api-6995" to be "Succeeded or Failed"
Nov 29 18:12:05.825: INFO: Pod "downwardapi-volume-17353518-118f-42bb-a723-3c36512f3c78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430861ms
Nov 29 18:12:07.828: INFO: Pod "downwardapi-volume-17353518-118f-42bb-a723-3c36512f3c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007107879s
STEP: Saw pod success
Nov 29 18:12:07.828: INFO: Pod "downwardapi-volume-17353518-118f-42bb-a723-3c36512f3c78" satisfied condition "Succeeded or Failed"
Nov 29 18:12:07.830: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-17353518-118f-42bb-a723-3c36512f3c78 container client-container: <nil>
STEP: delete the pod
Nov 29 18:12:07.847: INFO: Waiting for pod downwardapi-volume-17353518-118f-42bb-a723-3c36512f3c78 to disappear
Nov 29 18:12:07.849: INFO: Pod downwardapi-volume-17353518-118f-42bb-a723-3c36512f3c78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:12:07.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6995" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":50,"skipped":647,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:12:07.854: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2981
Nov 29 18:12:09.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2981 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 29 18:12:10.046: INFO: rc: 7
Nov 29 18:12:10.049: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 29 18:12:10.068: INFO: Pod kube-proxy-mode-detector still exists
Nov 29 18:12:12.068: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 29 18:12:12.072: INFO: Pod kube-proxy-mode-detector still exists
Nov 29 18:12:14.068: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 29 18:12:14.072: INFO: Pod kube-proxy-mode-detector no longer exists
Nov 29 18:12:14.072: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2981 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-2981
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2981
I1129 18:12:14.092289      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2981, replica count: 3
I1129 18:12:17.142717      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 18:12:17.149: INFO: Creating new exec pod
Nov 29 18:12:20.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2981 execpod-affinitymmtnr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Nov 29 18:12:20.314: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Nov 29 18:12:20.314: INFO: stdout: ""
Nov 29 18:12:20.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2981 execpod-affinitymmtnr -- /bin/sh -x -c nc -zv -t -w 2 10.110.118.36 80'
Nov 29 18:12:20.459: INFO: stderr: "+ nc -zv -t -w 2 10.110.118.36 80\nConnection to 10.110.118.36 80 port [tcp/http] succeeded!\n"
Nov 29 18:12:20.459: INFO: stdout: ""
Nov 29 18:12:20.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2981 execpod-affinitymmtnr -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.2 30946'
Nov 29 18:12:20.611: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.2 30946\nConnection to 192.168.49.2 30946 port [tcp/30946] succeeded!\n"
Nov 29 18:12:20.611: INFO: stdout: ""
Nov 29 18:12:20.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2981 execpod-affinitymmtnr -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.3 30946'
Nov 29 18:12:20.754: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.3 30946\nConnection to 192.168.49.3 30946 port [tcp/30946] succeeded!\n"
Nov 29 18:12:20.754: INFO: stdout: ""
Nov 29 18:12:20.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2981 execpod-affinitymmtnr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:30946/ ; done'
Nov 29 18:12:20.995: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n"
Nov 29 18:12:20.995: INFO: stdout: "\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq\naffinity-nodeport-timeout-xbwbq"
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.995: INFO: Received response from host: affinity-nodeport-timeout-xbwbq
Nov 29 18:12:20.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2981 execpod-affinitymmtnr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.49.2:30946/'
Nov 29 18:12:21.154: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n"
Nov 29 18:12:21.154: INFO: stdout: "affinity-nodeport-timeout-xbwbq"
Nov 29 18:12:36.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-2981 execpod-affinitymmtnr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.49.2:30946/'
Nov 29 18:12:36.302: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.49.2:30946/\n"
Nov 29 18:12:36.302: INFO: stdout: "affinity-nodeport-timeout-b48hv"
Nov 29 18:12:36.302: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2981, will wait for the garbage collector to delete the pods
Nov 29 18:12:36.385: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.677783ms
Nov 29 18:12:36.785: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 400.163843ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:12:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2981" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:40.154 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":51,"skipped":651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:12:48.009: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:12:48.048: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad3868db-6c16-4c18-8a23-0b86be141204" in namespace "downward-api-7572" to be "Succeeded or Failed"
Nov 29 18:12:48.055: INFO: Pod "downwardapi-volume-ad3868db-6c16-4c18-8a23-0b86be141204": Phase="Pending", Reason="", readiness=false. Elapsed: 6.592204ms
Nov 29 18:12:50.058: INFO: Pod "downwardapi-volume-ad3868db-6c16-4c18-8a23-0b86be141204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009299751s
STEP: Saw pod success
Nov 29 18:12:50.058: INFO: Pod "downwardapi-volume-ad3868db-6c16-4c18-8a23-0b86be141204" satisfied condition "Succeeded or Failed"
Nov 29 18:12:50.060: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-ad3868db-6c16-4c18-8a23-0b86be141204 container client-container: <nil>
STEP: delete the pod
Nov 29 18:12:50.074: INFO: Waiting for pod downwardapi-volume-ad3868db-6c16-4c18-8a23-0b86be141204 to disappear
Nov 29 18:12:50.076: INFO: Pod downwardapi-volume-ad3868db-6c16-4c18-8a23-0b86be141204 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:12:50.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7572" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":52,"skipped":681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:12:50.084: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-6cg4
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 18:12:50.112: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6cg4" in namespace "subpath-2846" to be "Succeeded or Failed"
Nov 29 18:12:50.116: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.264126ms
Nov 29 18:12:52.119: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006272758s
Nov 29 18:12:54.121: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Running", Reason="", readiness=true. Elapsed: 4.009093202s
Nov 29 18:12:56.124: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Running", Reason="", readiness=true. Elapsed: 6.011923502s
Nov 29 18:12:58.127: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Running", Reason="", readiness=true. Elapsed: 8.014738259s
Nov 29 18:13:00.130: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Running", Reason="", readiness=true. Elapsed: 10.017649254s
Nov 29 18:13:02.133: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Running", Reason="", readiness=true. Elapsed: 12.020461647s
Nov 29 18:13:04.136: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Running", Reason="", readiness=true. Elapsed: 14.023315683s
Nov 29 18:13:06.138: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Running", Reason="", readiness=true. Elapsed: 16.026096265s
Nov 29 18:13:08.141: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Running", Reason="", readiness=true. Elapsed: 18.028863675s
Nov 29 18:13:10.144: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Running", Reason="", readiness=true. Elapsed: 20.031631756s
Nov 29 18:13:12.147: INFO: Pod "pod-subpath-test-projected-6cg4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034427692s
STEP: Saw pod success
Nov 29 18:13:12.147: INFO: Pod "pod-subpath-test-projected-6cg4" satisfied condition "Succeeded or Failed"
Nov 29 18:13:12.149: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-projected-6cg4 container test-container-subpath-projected-6cg4: <nil>
STEP: delete the pod
Nov 29 18:13:12.207: INFO: Waiting for pod pod-subpath-test-projected-6cg4 to disappear
Nov 29 18:13:12.209: INFO: Pod pod-subpath-test-projected-6cg4 no longer exists
STEP: Deleting pod pod-subpath-test-projected-6cg4
Nov 29 18:13:12.209: INFO: Deleting pod "pod-subpath-test-projected-6cg4" in namespace "subpath-2846"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:13:12.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2846" for this suite.

• [SLOW TEST:22.133 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":53,"skipped":728,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:13:12.217: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-4m7c
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 18:13:12.246: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-4m7c" in namespace "subpath-5170" to be "Succeeded or Failed"
Nov 29 18:13:12.251: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.632731ms
Nov 29 18:13:14.254: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Running", Reason="", readiness=true. Elapsed: 2.0076503s
Nov 29 18:13:16.256: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Running", Reason="", readiness=true. Elapsed: 4.010496382s
Nov 29 18:13:18.260: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Running", Reason="", readiness=true. Elapsed: 6.013864803s
Nov 29 18:13:20.263: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Running", Reason="", readiness=true. Elapsed: 8.016607424s
Nov 29 18:13:22.265: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Running", Reason="", readiness=true. Elapsed: 10.019585501s
Nov 29 18:13:24.268: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Running", Reason="", readiness=true. Elapsed: 12.022440717s
Nov 29 18:13:26.271: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Running", Reason="", readiness=true. Elapsed: 14.025386952s
Nov 29 18:13:28.274: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Running", Reason="", readiness=true. Elapsed: 16.028301159s
Nov 29 18:13:30.281: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Running", Reason="", readiness=true. Elapsed: 18.034634719s
Nov 29 18:13:32.283: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Running", Reason="", readiness=true. Elapsed: 20.037417487s
Nov 29 18:13:34.286: INFO: Pod "pod-subpath-test-downwardapi-4m7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.040281114s
STEP: Saw pod success
Nov 29 18:13:34.286: INFO: Pod "pod-subpath-test-downwardapi-4m7c" satisfied condition "Succeeded or Failed"
Nov 29 18:13:34.288: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-downwardapi-4m7c container test-container-subpath-downwardapi-4m7c: <nil>
STEP: delete the pod
Nov 29 18:13:34.303: INFO: Waiting for pod pod-subpath-test-downwardapi-4m7c to disappear
Nov 29 18:13:34.305: INFO: Pod pod-subpath-test-downwardapi-4m7c no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-4m7c
Nov 29 18:13:34.305: INFO: Deleting pod "pod-subpath-test-downwardapi-4m7c" in namespace "subpath-5170"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:13:34.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5170" for this suite.

• [SLOW TEST:22.096 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":54,"skipped":732,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:13:34.313: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-dc1597d8-5618-4b94-b16e-38cff5ff7d6e
STEP: Creating a pod to test consume secrets
Nov 29 18:13:34.343: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-48714332-e3ac-4349-a44d-4a9015add9da" in namespace "projected-1156" to be "Succeeded or Failed"
Nov 29 18:13:34.346: INFO: Pod "pod-projected-secrets-48714332-e3ac-4349-a44d-4a9015add9da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.491402ms
Nov 29 18:13:36.349: INFO: Pod "pod-projected-secrets-48714332-e3ac-4349-a44d-4a9015add9da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005439165s
STEP: Saw pod success
Nov 29 18:13:36.349: INFO: Pod "pod-projected-secrets-48714332-e3ac-4349-a44d-4a9015add9da" satisfied condition "Succeeded or Failed"
Nov 29 18:13:36.350: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-48714332-e3ac-4349-a44d-4a9015add9da container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 18:13:36.361: INFO: Waiting for pod pod-projected-secrets-48714332-e3ac-4349-a44d-4a9015add9da to disappear
Nov 29 18:13:36.362: INFO: Pod pod-projected-secrets-48714332-e3ac-4349-a44d-4a9015add9da no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:13:36.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1156" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":55,"skipped":790,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:13:36.367: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:13:36.393: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53e0aada-d3a2-4162-a27b-0569204e4334" in namespace "projected-7877" to be "Succeeded or Failed"
Nov 29 18:13:36.394: INFO: Pod "downwardapi-volume-53e0aada-d3a2-4162-a27b-0569204e4334": Phase="Pending", Reason="", readiness=false. Elapsed: 1.6207ms
Nov 29 18:13:38.397: INFO: Pod "downwardapi-volume-53e0aada-d3a2-4162-a27b-0569204e4334": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004577551s
STEP: Saw pod success
Nov 29 18:13:38.397: INFO: Pod "downwardapi-volume-53e0aada-d3a2-4162-a27b-0569204e4334" satisfied condition "Succeeded or Failed"
Nov 29 18:13:38.399: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-53e0aada-d3a2-4162-a27b-0569204e4334 container client-container: <nil>
STEP: delete the pod
Nov 29 18:13:38.415: INFO: Waiting for pod downwardapi-volume-53e0aada-d3a2-4162-a27b-0569204e4334 to disappear
Nov 29 18:13:38.417: INFO: Pod downwardapi-volume-53e0aada-d3a2-4162-a27b-0569204e4334 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:13:38.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7877" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":56,"skipped":798,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:13:38.422: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:13:39.076: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:13:42.100: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:13:42.102: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7769-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:13:43.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7171" for this suite.
STEP: Destroying namespace "webhook-7171-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":57,"skipped":808,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:13:43.234: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:13:43.712: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:13:46.730: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:13:46.734: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9703-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:13:47.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5987" for this suite.
STEP: Destroying namespace "webhook-5987-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":58,"skipped":809,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:13:47.876: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:14:47.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5411" for this suite.

• [SLOW TEST:60.127 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":59,"skipped":821,"failed":0}
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:14:48.004: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Nov 29 18:14:48.031: INFO: Waiting up to 5m0s for pod "var-expansion-38915875-a06c-4d61-b2ed-0bf99fe03174" in namespace "var-expansion-7077" to be "Succeeded or Failed"
Nov 29 18:14:48.033: INFO: Pod "var-expansion-38915875-a06c-4d61-b2ed-0bf99fe03174": Phase="Pending", Reason="", readiness=false. Elapsed: 1.623956ms
Nov 29 18:14:50.036: INFO: Pod "var-expansion-38915875-a06c-4d61-b2ed-0bf99fe03174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004815425s
Nov 29 18:14:52.039: INFO: Pod "var-expansion-38915875-a06c-4d61-b2ed-0bf99fe03174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007835748s
STEP: Saw pod success
Nov 29 18:14:52.039: INFO: Pod "var-expansion-38915875-a06c-4d61-b2ed-0bf99fe03174" satisfied condition "Succeeded or Failed"
Nov 29 18:14:52.041: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-38915875-a06c-4d61-b2ed-0bf99fe03174 container dapi-container: <nil>
STEP: delete the pod
Nov 29 18:14:52.053: INFO: Waiting for pod var-expansion-38915875-a06c-4d61-b2ed-0bf99fe03174 to disappear
Nov 29 18:14:52.055: INFO: Pod var-expansion-38915875-a06c-4d61-b2ed-0bf99fe03174 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:14:52.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7077" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":60,"skipped":821,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:14:52.060: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 29 18:14:52.083: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 29 18:14:57.085: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:14:58.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4085" for this suite.

• [SLOW TEST:6.040 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":61,"skipped":830,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:14:58.100: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 29 18:14:58.129: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 29 18:15:58.143: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Nov 29 18:15:58.164: INFO: Created pod: pod0-sched-preemption-low-priority
Nov 29 18:15:58.173: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:16:22.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5783" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:84.125 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":62,"skipped":831,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:16:22.225: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Nov 29 18:16:22.250: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:16:34.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7630" for this suite.

• [SLOW TEST:12.675 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":63,"skipped":848,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:16:34.900: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:16:34.922: INFO: Creating ReplicaSet my-hostname-basic-d3a79858-d2e7-4a05-ab4e-0d9071ca769c
Nov 29 18:16:34.928: INFO: Pod name my-hostname-basic-d3a79858-d2e7-4a05-ab4e-0d9071ca769c: Found 0 pods out of 1
Nov 29 18:16:39.930: INFO: Pod name my-hostname-basic-d3a79858-d2e7-4a05-ab4e-0d9071ca769c: Found 1 pods out of 1
Nov 29 18:16:39.930: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d3a79858-d2e7-4a05-ab4e-0d9071ca769c" is running
Nov 29 18:16:39.932: INFO: Pod "my-hostname-basic-d3a79858-d2e7-4a05-ab4e-0d9071ca769c-fp97c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 18:16:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 18:16:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 18:16:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 18:16:34 +0000 UTC Reason: Message:}])
Nov 29 18:16:39.932: INFO: Trying to dial the pod
Nov 29 18:16:44.940: INFO: Controller my-hostname-basic-d3a79858-d2e7-4a05-ab4e-0d9071ca769c: Got expected result from replica 1 [my-hostname-basic-d3a79858-d2e7-4a05-ab4e-0d9071ca769c-fp97c]: "my-hostname-basic-d3a79858-d2e7-4a05-ab4e-0d9071ca769c-fp97c", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:16:44.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9672" for this suite.

• [SLOW TEST:10.046 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":64,"skipped":852,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:16:44.946: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-c2ba5d4a-1819-44ee-9e7e-d6f83af123f8
STEP: Creating a pod to test consume secrets
Nov 29 18:16:44.974: INFO: Waiting up to 5m0s for pod "pod-secrets-d3ee2746-20b3-4f88-84b5-eaebc6d62728" in namespace "secrets-3743" to be "Succeeded or Failed"
Nov 29 18:16:44.976: INFO: Pod "pod-secrets-d3ee2746-20b3-4f88-84b5-eaebc6d62728": Phase="Pending", Reason="", readiness=false. Elapsed: 1.725253ms
Nov 29 18:16:46.979: INFO: Pod "pod-secrets-d3ee2746-20b3-4f88-84b5-eaebc6d62728": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004955159s
STEP: Saw pod success
Nov 29 18:16:46.979: INFO: Pod "pod-secrets-d3ee2746-20b3-4f88-84b5-eaebc6d62728" satisfied condition "Succeeded or Failed"
Nov 29 18:16:46.981: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-d3ee2746-20b3-4f88-84b5-eaebc6d62728 container secret-env-test: <nil>
STEP: delete the pod
Nov 29 18:16:47.018: INFO: Waiting for pod pod-secrets-d3ee2746-20b3-4f88-84b5-eaebc6d62728 to disappear
Nov 29 18:16:47.020: INFO: Pod pod-secrets-d3ee2746-20b3-4f88-84b5-eaebc6d62728 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:16:47.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3743" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":65,"skipped":860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:16:47.025: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9267.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9267.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9267.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9267.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 157.29.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.29.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.29.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.29.157_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9267.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9267.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9267.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9267.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9267.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9267.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9267.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 157.29.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.29.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.29.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.29.157_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 18:16:51.120: INFO: Unable to read wheezy_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:51.122: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:51.124: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:51.126: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:51.141: INFO: Unable to read jessie_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:51.143: INFO: Unable to read jessie_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:51.145: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:51.148: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:51.160: INFO: Lookups using dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032 failed for: [wheezy_udp@dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_udp@dns-test-service.dns-9267.svc.cluster.local jessie_tcp@dns-test-service.dns-9267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local]

Nov 29 18:16:56.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:56.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:56.168: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:56.170: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:56.185: INFO: Unable to read jessie_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:56.187: INFO: Unable to read jessie_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:56.189: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:56.191: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:16:56.203: INFO: Lookups using dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032 failed for: [wheezy_udp@dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_udp@dns-test-service.dns-9267.svc.cluster.local jessie_tcp@dns-test-service.dns-9267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local]

Nov 29 18:17:01.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:01.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:01.168: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:01.171: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:01.186: INFO: Unable to read jessie_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:01.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:01.191: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:01.193: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:01.204: INFO: Lookups using dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032 failed for: [wheezy_udp@dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_udp@dns-test-service.dns-9267.svc.cluster.local jessie_tcp@dns-test-service.dns-9267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local]

Nov 29 18:17:06.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:06.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:06.168: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:06.170: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:06.185: INFO: Unable to read jessie_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:06.187: INFO: Unable to read jessie_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:06.189: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:06.191: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:06.203: INFO: Lookups using dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032 failed for: [wheezy_udp@dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_udp@dns-test-service.dns-9267.svc.cluster.local jessie_tcp@dns-test-service.dns-9267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local]

Nov 29 18:17:11.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:11.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:11.168: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:11.170: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:11.185: INFO: Unable to read jessie_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:11.187: INFO: Unable to read jessie_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:11.189: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:11.191: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:11.202: INFO: Lookups using dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032 failed for: [wheezy_udp@dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_udp@dns-test-service.dns-9267.svc.cluster.local jessie_tcp@dns-test-service.dns-9267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local]

Nov 29 18:17:16.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:16.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:16.168: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:16.170: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:16.185: INFO: Unable to read jessie_udp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:16.187: INFO: Unable to read jessie_tcp@dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:16.189: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:16.191: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local from pod dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032: the server could not find the requested resource (get pods dns-test-8895e1b4-1283-41b6-a391-c1506d367032)
Nov 29 18:17:16.203: INFO: Lookups using dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032 failed for: [wheezy_udp@dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@dns-test-service.dns-9267.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_udp@dns-test-service.dns-9267.svc.cluster.local jessie_tcp@dns-test-service.dns-9267.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9267.svc.cluster.local]

Nov 29 18:17:21.204: INFO: DNS probes using dns-9267/dns-test-8895e1b4-1283-41b6-a391-c1506d367032 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:17:21.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9267" for this suite.

• [SLOW TEST:34.390 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":66,"skipped":898,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:17:21.415: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 29 18:17:23.964: INFO: Successfully updated pod "annotationupdate14e5f5e0-69a3-4e71-bfef-5789eb0ef591"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:17:27.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5975" for this suite.

• [SLOW TEST:6.569 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":67,"skipped":923,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:17:27.985: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 29 18:17:30.523: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4583 pod-service-account-4f48753d-61d0-4196-a940-fa1590473086 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 29 18:17:30.692: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4583 pod-service-account-4f48753d-61d0-4196-a940-fa1590473086 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 29 18:17:30.869: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4583 pod-service-account-4f48753d-61d0-4196-a940-fa1590473086 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:17:31.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4583" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":68,"skipped":931,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:17:31.018: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 29 18:17:31.043: INFO: Waiting up to 5m0s for pod "downward-api-5ce4ac45-b40a-40d9-a2a5-f6a023c5eb0f" in namespace "downward-api-3812" to be "Succeeded or Failed"
Nov 29 18:17:31.047: INFO: Pod "downward-api-5ce4ac45-b40a-40d9-a2a5-f6a023c5eb0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.662562ms
Nov 29 18:17:33.050: INFO: Pod "downward-api-5ce4ac45-b40a-40d9-a2a5-f6a023c5eb0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006817497s
STEP: Saw pod success
Nov 29 18:17:33.050: INFO: Pod "downward-api-5ce4ac45-b40a-40d9-a2a5-f6a023c5eb0f" satisfied condition "Succeeded or Failed"
Nov 29 18:17:33.051: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-5ce4ac45-b40a-40d9-a2a5-f6a023c5eb0f container dapi-container: <nil>
STEP: delete the pod
Nov 29 18:17:33.063: INFO: Waiting for pod downward-api-5ce4ac45-b40a-40d9-a2a5-f6a023c5eb0f to disappear
Nov 29 18:17:33.065: INFO: Pod downward-api-5ce4ac45-b40a-40d9-a2a5-f6a023c5eb0f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:17:33.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3812" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":69,"skipped":945,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:17:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 29 18:17:33.112: INFO: Waiting up to 5m0s for pod "pod-f0c16828-dbe6-4ab7-941e-f94aa3f46572" in namespace "emptydir-5043" to be "Succeeded or Failed"
Nov 29 18:17:33.119: INFO: Pod "pod-f0c16828-dbe6-4ab7-941e-f94aa3f46572": Phase="Pending", Reason="", readiness=false. Elapsed: 6.739269ms
Nov 29 18:17:35.122: INFO: Pod "pod-f0c16828-dbe6-4ab7-941e-f94aa3f46572": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009530337s
STEP: Saw pod success
Nov 29 18:17:35.122: INFO: Pod "pod-f0c16828-dbe6-4ab7-941e-f94aa3f46572" satisfied condition "Succeeded or Failed"
Nov 29 18:17:35.124: INFO: Trying to get logs from node k8sconformance-m02 pod pod-f0c16828-dbe6-4ab7-941e-f94aa3f46572 container test-container: <nil>
STEP: delete the pod
Nov 29 18:17:35.138: INFO: Waiting for pod pod-f0c16828-dbe6-4ab7-941e-f94aa3f46572 to disappear
Nov 29 18:17:35.140: INFO: Pod pod-f0c16828-dbe6-4ab7-941e-f94aa3f46572 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:17:35.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5043" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":70,"skipped":949,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:17:35.145: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:17:35.178: INFO: Create a RollingUpdate DaemonSet
Nov 29 18:17:35.181: INFO: Check that daemon pods launch on every node of the cluster
Nov 29 18:17:35.186: INFO: Number of nodes with available pods: 0
Nov 29 18:17:35.187: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:17:36.191: INFO: Number of nodes with available pods: 0
Nov 29 18:17:36.191: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:17:37.192: INFO: Number of nodes with available pods: 1
Nov 29 18:17:37.192: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:17:38.383: INFO: Number of nodes with available pods: 1
Nov 29 18:17:38.383: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:17:39.191: INFO: Number of nodes with available pods: 1
Nov 29 18:17:39.191: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:17:40.212: INFO: Number of nodes with available pods: 1
Nov 29 18:17:40.212: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:17:41.192: INFO: Number of nodes with available pods: 1
Nov 29 18:17:41.192: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:17:42.192: INFO: Number of nodes with available pods: 1
Nov 29 18:17:42.192: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:17:43.192: INFO: Number of nodes with available pods: 1
Nov 29 18:17:43.192: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:17:44.192: INFO: Number of nodes with available pods: 2
Nov 29 18:17:44.192: INFO: Number of running nodes: 2, number of available pods: 2
Nov 29 18:17:44.192: INFO: Update the DaemonSet to trigger a rollout
Nov 29 18:17:44.197: INFO: Updating DaemonSet daemon-set
Nov 29 18:17:50.207: INFO: Roll back the DaemonSet before rollout is complete
Nov 29 18:17:50.212: INFO: Updating DaemonSet daemon-set
Nov 29 18:17:50.212: INFO: Make sure DaemonSet rollback is complete
Nov 29 18:17:50.214: INFO: Wrong image for pod: daemon-set-2l62x. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 18:17:50.214: INFO: Pod daemon-set-2l62x is not available
Nov 29 18:17:51.224: INFO: Wrong image for pod: daemon-set-2l62x. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 18:17:51.224: INFO: Pod daemon-set-2l62x is not available
Nov 29 18:17:52.222: INFO: Wrong image for pod: daemon-set-2l62x. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 18:17:52.222: INFO: Pod daemon-set-2l62x is not available
Nov 29 18:17:53.223: INFO: Wrong image for pod: daemon-set-2l62x. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 18:17:53.223: INFO: Pod daemon-set-2l62x is not available
Nov 29 18:17:54.223: INFO: Wrong image for pod: daemon-set-2l62x. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 18:17:54.223: INFO: Pod daemon-set-2l62x is not available
Nov 29 18:17:55.223: INFO: Wrong image for pod: daemon-set-2l62x. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 18:17:55.223: INFO: Pod daemon-set-2l62x is not available
Nov 29 18:17:56.223: INFO: Pod daemon-set-rqzh4 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7420, will wait for the garbage collector to delete the pods
Nov 29 18:17:56.286: INFO: Deleting DaemonSet.extensions daemon-set took: 4.606884ms
Nov 29 18:17:56.687: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.207441ms
Nov 29 18:18:00.389: INFO: Number of nodes with available pods: 0
Nov 29 18:18:00.389: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 18:18:00.393: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7420/daemonsets","resourceVersion":"5357"},"items":null}

Nov 29 18:18:00.395: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7420/pods","resourceVersion":"5357"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:00.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7420" for this suite.

• [SLOW TEST:25.261 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":71,"skipped":965,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:00.406: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-65b64605-926c-4e21-8fbf-a71e584f6948
STEP: Creating a pod to test consume secrets
Nov 29 18:18:00.434: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eddb0dcc-4db6-455f-b72b-4fb464dc616a" in namespace "projected-6070" to be "Succeeded or Failed"
Nov 29 18:18:00.436: INFO: Pod "pod-projected-secrets-eddb0dcc-4db6-455f-b72b-4fb464dc616a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.980313ms
Nov 29 18:18:02.453: INFO: Pod "pod-projected-secrets-eddb0dcc-4db6-455f-b72b-4fb464dc616a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019835865s
STEP: Saw pod success
Nov 29 18:18:02.453: INFO: Pod "pod-projected-secrets-eddb0dcc-4db6-455f-b72b-4fb464dc616a" satisfied condition "Succeeded or Failed"
Nov 29 18:18:02.455: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-eddb0dcc-4db6-455f-b72b-4fb464dc616a container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 18:18:02.468: INFO: Waiting for pod pod-projected-secrets-eddb0dcc-4db6-455f-b72b-4fb464dc616a to disappear
Nov 29 18:18:02.471: INFO: Pod pod-projected-secrets-eddb0dcc-4db6-455f-b72b-4fb464dc616a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:02.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6070" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":72,"skipped":969,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:02.477: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:18:02.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 version'
Nov 29 18:18:02.571: INFO: stderr: ""
Nov 29 18:18:02.571: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:17:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:09:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:02.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4885" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":73,"skipped":969,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:02.578: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:18:04.639: INFO: Waiting up to 5m0s for pod "client-envvars-28972320-481a-477d-a19e-33754eb39817" in namespace "pods-964" to be "Succeeded or Failed"
Nov 29 18:18:04.643: INFO: Pod "client-envvars-28972320-481a-477d-a19e-33754eb39817": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031984ms
Nov 29 18:18:06.645: INFO: Pod "client-envvars-28972320-481a-477d-a19e-33754eb39817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006255438s
STEP: Saw pod success
Nov 29 18:18:06.645: INFO: Pod "client-envvars-28972320-481a-477d-a19e-33754eb39817" satisfied condition "Succeeded or Failed"
Nov 29 18:18:06.647: INFO: Trying to get logs from node k8sconformance-m02 pod client-envvars-28972320-481a-477d-a19e-33754eb39817 container env3cont: <nil>
STEP: delete the pod
Nov 29 18:18:06.659: INFO: Waiting for pod client-envvars-28972320-481a-477d-a19e-33754eb39817 to disappear
Nov 29 18:18:06.661: INFO: Pod client-envvars-28972320-481a-477d-a19e-33754eb39817 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:06.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-964" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":74,"skipped":985,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:06.666: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:18:07.112: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:18:10.145: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Nov 29 18:18:10.158: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:10.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3615" for this suite.
STEP: Destroying namespace "webhook-3615-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":75,"skipped":990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:10.212: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-214
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-214
I1129 18:18:10.371987      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-214, replica count: 2
Nov 29 18:18:13.422: INFO: Creating new exec pod
I1129 18:18:13.422393      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 18:18:16.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-214 execpodttpkk -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 29 18:18:16.632: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 29 18:18:16.632: INFO: stdout: ""
Nov 29 18:18:16.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-214 execpodttpkk -- /bin/sh -x -c nc -zv -t -w 2 10.106.12.168 80'
Nov 29 18:18:16.786: INFO: stderr: "+ nc -zv -t -w 2 10.106.12.168 80\nConnection to 10.106.12.168 80 port [tcp/http] succeeded!\n"
Nov 29 18:18:16.786: INFO: stdout: ""
Nov 29 18:18:16.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-214 execpodttpkk -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.2 30133'
Nov 29 18:18:16.951: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.2 30133\nConnection to 192.168.49.2 30133 port [tcp/30133] succeeded!\n"
Nov 29 18:18:16.951: INFO: stdout: ""
Nov 29 18:18:16.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-214 execpodttpkk -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.3 30133'
Nov 29 18:18:17.090: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.3 30133\nConnection to 192.168.49.3 30133 port [tcp/30133] succeeded!\n"
Nov 29 18:18:17.090: INFO: stdout: ""
Nov 29 18:18:17.090: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:17.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-214" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.903 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":76,"skipped":1015,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:17.115: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1662
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 18:18:17.161: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 29 18:18:17.184: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 18:18:19.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:18:21.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:18:23.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:18:25.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:18:27.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:18:29.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 18:18:31.187: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 29 18:18:31.191: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 29 18:18:33.194: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 29 18:18:37.219: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.21:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1662 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:18:37.219: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:18:37.314: INFO: Found all expected endpoints: [netserver-0]
Nov 29 18:18:37.316: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.95:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1662 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:18:37.316: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:18:37.392: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:37.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1662" for this suite.

• [SLOW TEST:20.284 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":77,"skipped":1029,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:37.399: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-85fc5e14-3e85-42e4-888f-ea5a6a7624aa
STEP: Creating a pod to test consume configMaps
Nov 29 18:18:37.427: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5297a0f-c995-497c-9e0e-5a7804149bef" in namespace "configmap-4253" to be "Succeeded or Failed"
Nov 29 18:18:37.429: INFO: Pod "pod-configmaps-c5297a0f-c995-497c-9e0e-5a7804149bef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.727524ms
Nov 29 18:18:39.432: INFO: Pod "pod-configmaps-c5297a0f-c995-497c-9e0e-5a7804149bef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004710351s
Nov 29 18:18:41.435: INFO: Pod "pod-configmaps-c5297a0f-c995-497c-9e0e-5a7804149bef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007865588s
STEP: Saw pod success
Nov 29 18:18:41.435: INFO: Pod "pod-configmaps-c5297a0f-c995-497c-9e0e-5a7804149bef" satisfied condition "Succeeded or Failed"
Nov 29 18:18:41.439: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-c5297a0f-c995-497c-9e0e-5a7804149bef container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 18:18:41.452: INFO: Waiting for pod pod-configmaps-c5297a0f-c995-497c-9e0e-5a7804149bef to disappear
Nov 29 18:18:41.454: INFO: Pod pod-configmaps-c5297a0f-c995-497c-9e0e-5a7804149bef no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:41.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4253" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":78,"skipped":1036,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:41.459: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-8937
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8937
STEP: Deleting pre-stop pod
Nov 29 18:18:50.506: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:50.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8937" for this suite.

• [SLOW TEST:9.062 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":79,"skipped":1038,"failed":0}
SSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:50.521: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Nov 29 18:18:50.547: INFO: created test-podtemplate-1
Nov 29 18:18:50.550: INFO: created test-podtemplate-2
Nov 29 18:18:50.687: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Nov 29 18:18:50.690: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Nov 29 18:18:50.701: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:50.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4069" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":80,"skipped":1046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:50.708: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-c37923f9-d341-4fa3-97a1-1958d6a756d1
STEP: Creating a pod to test consume secrets
Nov 29 18:18:50.754: INFO: Waiting up to 5m0s for pod "pod-secrets-7bcc6ea8-0510-4c55-aab8-ea055d149627" in namespace "secrets-1288" to be "Succeeded or Failed"
Nov 29 18:18:50.756: INFO: Pod "pod-secrets-7bcc6ea8-0510-4c55-aab8-ea055d149627": Phase="Pending", Reason="", readiness=false. Elapsed: 1.62263ms
Nov 29 18:18:52.759: INFO: Pod "pod-secrets-7bcc6ea8-0510-4c55-aab8-ea055d149627": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004544649s
STEP: Saw pod success
Nov 29 18:18:52.759: INFO: Pod "pod-secrets-7bcc6ea8-0510-4c55-aab8-ea055d149627" satisfied condition "Succeeded or Failed"
Nov 29 18:18:52.761: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-7bcc6ea8-0510-4c55-aab8-ea055d149627 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 18:18:52.780: INFO: Waiting for pod pod-secrets-7bcc6ea8-0510-4c55-aab8-ea055d149627 to disappear
Nov 29 18:18:52.782: INFO: Pod pod-secrets-7bcc6ea8-0510-4c55-aab8-ea055d149627 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:18:52.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1288" for this suite.
STEP: Destroying namespace "secret-namespace-5547" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":81,"skipped":1089,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:18:52.790: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 29 18:18:52.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9767'
Nov 29 18:18:52.887: INFO: stderr: ""
Nov 29 18:18:52.887: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Nov 29 18:18:57.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pod e2e-test-httpd-pod --namespace=kubectl-9767 -o json'
Nov 29 18:18:58.059: INFO: stderr: ""
Nov 29 18:18:58.059: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-11-29T18:18:52Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-29T18:18:52Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.1.101\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-29T18:18:54Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9767\",\n        \"resourceVersion\": \"5898\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9767/pods/e2e-test-httpd-pod\",\n        \"uid\": \"96e0eea2-4c63-4785-832e-5db9c36acafe\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8m8qw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8sconformance-m02\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8m8qw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8m8qw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T18:18:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T18:18:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T18:18:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T18:18:52Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://f32b3fb79b8806b59916e59c1848bc728daf898603ff4c09141e2c969e47e248\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-11-29T18:18:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.49.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.101\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.101\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-29T18:18:52Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 29 18:18:58.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 replace -f - --namespace=kubectl-9767'
Nov 29 18:18:58.293: INFO: stderr: ""
Nov 29 18:18:58.293: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Nov 29 18:18:58.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete pods e2e-test-httpd-pod --namespace=kubectl-9767'
Nov 29 18:19:00.595: INFO: stderr: ""
Nov 29 18:19:00.595: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:19:00.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9767" for this suite.

• [SLOW TEST:7.810 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":82,"skipped":1117,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:19:00.600: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:19:00.621: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 29 18:19:00.625: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 29 18:19:05.628: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 29 18:19:05.629: INFO: Creating deployment "test-rolling-update-deployment"
Nov 29 18:19:05.631: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 29 18:19:05.634: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 29 18:19:07.642: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 29 18:19:07.647: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 29 18:19:07.652: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2495 /apis/apps/v1/namespaces/deployment-2495/deployments/test-rolling-update-deployment d4fddc40-44ca-4a47-a84f-bb7f446800a8 6024 1 2020-11-29 18:19:05 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-11-29 18:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-29 18:19:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002672278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-29 18:19:05 +0000 UTC,LastTransitionTime:2020-11-29 18:19:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-11-29 18:19:07 +0000 UTC,LastTransitionTime:2020-11-29 18:19:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 29 18:19:07.654: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-2495 /apis/apps/v1/namespaces/deployment-2495/replicasets/test-rolling-update-deployment-c4cb8d6d9 c2f58599-dcb3-4584-8219-730a3e76b712 6013 1 2020-11-29 18:19:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d4fddc40-44ca-4a47-a84f-bb7f446800a8 0xc003ef8c00 0xc003ef8c01}] []  [{kube-controller-manager Update apps/v1 2020-11-29 18:19:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4fddc40-44ca-4a47-a84f-bb7f446800a8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ef8c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 29 18:19:07.654: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 29 18:19:07.654: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2495 /apis/apps/v1/namespaces/deployment-2495/replicasets/test-rolling-update-controller 6288606e-cbd2-4aa5-adb0-2bcab7efed62 6023 2 2020-11-29 18:19:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d4fddc40-44ca-4a47-a84f-bb7f446800a8 0xc003ef8aff 0xc003ef8b10}] []  [{e2e.test Update apps/v1 2020-11-29 18:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-29 18:19:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4fddc40-44ca-4a47-a84f-bb7f446800a8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ef8ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 18:19:07.657: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-xtlbj" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-xtlbj test-rolling-update-deployment-c4cb8d6d9- deployment-2495 /api/v1/namespaces/deployment-2495/pods/test-rolling-update-deployment-c4cb8d6d9-xtlbj 9df2fb44-5295-4cde-9919-89328fa5812e 6012 0 2020-11-29 18:19:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 c2f58599-dcb3-4584-8219-730a3e76b712 0xc0026726d0 0xc0026726d1}] []  [{kube-controller-manager Update v1 2020-11-29 18:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2f58599-dcb3-4584-8219-730a3e76b712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-llskz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-llskz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-llskz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.103,StartTime:2020-11-29 18:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 18:19:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://4b32c4477d9c18bbb383b4e05c8d0c90715553734cccd1bfd81602cfc702781a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:19:07.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2495" for this suite.

• [SLOW TEST:7.062 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":83,"skipped":1122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:19:07.662: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:21:07.821: INFO: Deleting pod "var-expansion-089971f7-c703-4415-9e76-c43f940a0c02" in namespace "var-expansion-6290"
Nov 29 18:21:07.825: INFO: Wait up to 5m0s for pod "var-expansion-089971f7-c703-4415-9e76-c43f940a0c02" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:21:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6290" for this suite.

• [SLOW TEST:122.177 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":84,"skipped":1183,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:21:09.839: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:21:09.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3145" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":85,"skipped":1192,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:21:09.880: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Nov 29 18:21:09.916: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-120018680 proxy --unix-socket=/tmp/kubectl-proxy-unix816858931/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:21:09.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5839" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":86,"skipped":1194,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:21:09.978: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:21:10.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2350" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:21:10.058: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-df1515c3-41b2-4dd6-a648-f10093336d14
STEP: Creating a pod to test consume secrets
Nov 29 18:21:10.089: INFO: Waiting up to 5m0s for pod "pod-secrets-66342159-0043-4408-88c8-3e4d9a1288e7" in namespace "secrets-6675" to be "Succeeded or Failed"
Nov 29 18:21:10.090: INFO: Pod "pod-secrets-66342159-0043-4408-88c8-3e4d9a1288e7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.327271ms
Nov 29 18:21:12.093: INFO: Pod "pod-secrets-66342159-0043-4408-88c8-3e4d9a1288e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003649193s
STEP: Saw pod success
Nov 29 18:21:12.093: INFO: Pod "pod-secrets-66342159-0043-4408-88c8-3e4d9a1288e7" satisfied condition "Succeeded or Failed"
Nov 29 18:21:12.095: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-66342159-0043-4408-88c8-3e4d9a1288e7 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 18:21:12.130: INFO: Waiting for pod pod-secrets-66342159-0043-4408-88c8-3e4d9a1288e7 to disappear
Nov 29 18:21:12.132: INFO: Pod pod-secrets-66342159-0043-4408-88c8-3e4d9a1288e7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:21:12.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6675" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":88,"skipped":1249,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:21:12.137: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:21:12.613: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:21:15.630: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:21:15.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9282" for this suite.
STEP: Destroying namespace "webhook-9282-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":89,"skipped":1269,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:21:15.690: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9039.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9039.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9039.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9039.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9039.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9039.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 18:21:19.773: INFO: DNS probes using dns-9039/dns-test-3b30b965-c86d-46ce-94c9-089bc2b23cd1 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:21:19.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9039" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":90,"skipped":1281,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:21:19.841: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1950, will wait for the garbage collector to delete the pods
Nov 29 18:21:22.304: INFO: Deleting Job.batch foo took: 4.812104ms
Nov 29 18:21:22.405: INFO: Terminating Job.batch foo pods took: 100.222294ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:21:58.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1950" for this suite.

• [SLOW TEST:38.171 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":91,"skipped":1287,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:21:58.012: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:21:58.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4927" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":92,"skipped":1288,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:21:58.040: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 29 18:22:02.091: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 18:22:02.096: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 18:22:04.096: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 18:22:04.099: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 18:22:06.096: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 18:22:06.099: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 18:22:08.096: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 18:22:08.099: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:22:08.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5293" for this suite.

• [SLOW TEST:10.064 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1303,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:22:08.104: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:22:08.140: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 29 18:22:10.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-6156 create -f -'
Nov 29 18:22:11.870: INFO: stderr: ""
Nov 29 18:22:11.870: INFO: stdout: "e2e-test-crd-publish-openapi-8346-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 29 18:22:11.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-6156 delete e2e-test-crd-publish-openapi-8346-crds test-cr'
Nov 29 18:22:11.943: INFO: stderr: ""
Nov 29 18:22:11.943: INFO: stdout: "e2e-test-crd-publish-openapi-8346-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 29 18:22:11.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-6156 apply -f -'
Nov 29 18:22:12.178: INFO: stderr: ""
Nov 29 18:22:12.178: INFO: stdout: "e2e-test-crd-publish-openapi-8346-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 29 18:22:12.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-6156 delete e2e-test-crd-publish-openapi-8346-crds test-cr'
Nov 29 18:22:12.252: INFO: stderr: ""
Nov 29 18:22:12.252: INFO: stdout: "e2e-test-crd-publish-openapi-8346-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 29 18:22:12.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 explain e2e-test-crd-publish-openapi-8346-crds'
Nov 29 18:22:12.422: INFO: stderr: ""
Nov 29 18:22:12.422: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8346-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:22:15.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6156" for this suite.

• [SLOW TEST:7.141 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":94,"skipped":1306,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:22:15.245: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 29 18:22:15.297: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 29 18:23:15.311: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:23:15.312: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Nov 29 18:23:17.371: INFO: found a healthy node: k8sconformance-m02
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:23:25.420: INFO: pods created so far: [1 1 1]
Nov 29 18:23:25.420: INFO: length of pods created so far: 3
Nov 29 18:23:43.426: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:23:50.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9620" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:23:50.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-902" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:95.229 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":95,"skipped":1310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:23:50.475: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 29 18:23:50.507: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 29 18:23:50.509: INFO: starting watch
STEP: patching
STEP: updating
Nov 29 18:23:50.533: INFO: waiting for watch events with expected annotations
Nov 29 18:23:50.533: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:23:50.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8954" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":96,"skipped":1359,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:23:50.591: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 29 18:23:53.143: INFO: Successfully updated pod "labelsupdate0ed5c2e2-cad5-4a0f-8b13-94c41c6a2d79"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:23:57.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6390" for this suite.

• [SLOW TEST:6.576 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":97,"skipped":1377,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:23:57.167: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov 29 18:23:57.187: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:24:00.014: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:24:11.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7637" for this suite.

• [SLOW TEST:14.040 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":98,"skipped":1388,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:24:11.207: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov 29 18:24:11.243: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:24:14.076: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:24:25.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-592" for this suite.

• [SLOW TEST:14.018 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":99,"skipped":1389,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:24:25.225: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:24:25.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4673" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":100,"skipped":1402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:24:25.269: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:24:25.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1892" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":101,"skipped":1440,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:24:25.316: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Nov 29 18:24:25.337: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-120018680 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:24:25.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-903" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":102,"skipped":1461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:24:25.412: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8123
STEP: creating service affinity-clusterip-transition in namespace services-8123
STEP: creating replication controller affinity-clusterip-transition in namespace services-8123
I1129 18:24:25.472091      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-8123, replica count: 3
I1129 18:24:28.522525      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 18:24:28.527: INFO: Creating new exec pod
Nov 29 18:24:31.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-8123 execpod-affinitylspnl -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Nov 29 18:24:31.739: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 29 18:24:31.739: INFO: stdout: ""
Nov 29 18:24:31.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-8123 execpod-affinitylspnl -- /bin/sh -x -c nc -zv -t -w 2 10.96.1.196 80'
Nov 29 18:24:31.886: INFO: stderr: "+ nc -zv -t -w 2 10.96.1.196 80\nConnection to 10.96.1.196 80 port [tcp/http] succeeded!\n"
Nov 29 18:24:31.886: INFO: stdout: ""
Nov 29 18:24:31.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-8123 execpod-affinitylspnl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.1.196:80/ ; done'
Nov 29 18:24:32.131: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n"
Nov 29 18:24:32.131: INFO: stdout: "\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-ffnrz\naffinity-clusterip-transition-87bb4\naffinity-clusterip-transition-87bb4\naffinity-clusterip-transition-ffnrz\naffinity-clusterip-transition-87bb4\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-ffnrz\naffinity-clusterip-transition-ffnrz\naffinity-clusterip-transition-87bb4\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-87bb4\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-ffnrz"
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-ffnrz
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-87bb4
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-87bb4
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-ffnrz
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-87bb4
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-ffnrz
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-ffnrz
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-87bb4
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-87bb4
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.131: INFO: Received response from host: affinity-clusterip-transition-ffnrz
Nov 29 18:24:32.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-8123 execpod-affinitylspnl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.1.196:80/ ; done'
Nov 29 18:24:32.372: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.1.196:80/\n"
Nov 29 18:24:32.372: INFO: stdout: "\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz\naffinity-clusterip-transition-52fpz"
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Received response from host: affinity-clusterip-transition-52fpz
Nov 29 18:24:32.372: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8123, will wait for the garbage collector to delete the pods
Nov 29 18:24:32.440: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.493705ms
Nov 29 18:24:32.840: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 400.211508ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:24:48.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8123" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:22.718 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":103,"skipped":1489,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:24:48.131: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:24:48.157: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4776603-9513-4f8b-b62c-a962b6059b41" in namespace "downward-api-6116" to be "Succeeded or Failed"
Nov 29 18:24:48.159: INFO: Pod "downwardapi-volume-a4776603-9513-4f8b-b62c-a962b6059b41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1822ms
Nov 29 18:24:50.164: INFO: Pod "downwardapi-volume-a4776603-9513-4f8b-b62c-a962b6059b41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006889486s
STEP: Saw pod success
Nov 29 18:24:50.164: INFO: Pod "downwardapi-volume-a4776603-9513-4f8b-b62c-a962b6059b41" satisfied condition "Succeeded or Failed"
Nov 29 18:24:50.166: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-a4776603-9513-4f8b-b62c-a962b6059b41 container client-container: <nil>
STEP: delete the pod
Nov 29 18:24:50.178: INFO: Waiting for pod downwardapi-volume-a4776603-9513-4f8b-b62c-a962b6059b41 to disappear
Nov 29 18:24:50.180: INFO: Pod downwardapi-volume-a4776603-9513-4f8b-b62c-a962b6059b41 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:24:50.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6116" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":104,"skipped":1570,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:24:50.186: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 29 18:24:50.225: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9666 /api/v1/namespaces/watch-9666/configmaps/e2e-watch-test-resource-version 87f32b14-392f-47e9-b452-62bb4462df39 7231 0 2020-11-29 18:24:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-29 18:24:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:24:50.225: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9666 /api/v1/namespaces/watch-9666/configmaps/e2e-watch-test-resource-version 87f32b14-392f-47e9-b452-62bb4462df39 7232 0 2020-11-29 18:24:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-29 18:24:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:24:50.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9666" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":105,"skipped":1581,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:24:50.230: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1129 18:25:00.340276      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 29 18:26:02.352: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Nov 29 18:26:02.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-25mk9" in namespace "gc-7550"
Nov 29 18:26:02.364: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pq2d" in namespace "gc-7550"
Nov 29 18:26:02.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-g784h" in namespace "gc-7550"
Nov 29 18:26:02.386: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmwd4" in namespace "gc-7550"
Nov 29 18:26:02.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-m6ckc" in namespace "gc-7550"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:26:02.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7550" for this suite.

• [SLOW TEST:72.194 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":106,"skipped":1630,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:26:02.425: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6748
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6748
STEP: Creating statefulset with conflicting port in namespace statefulset-6748
STEP: Waiting until pod test-pod will start running in namespace statefulset-6748
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6748
Nov 29 18:26:06.532: INFO: Observed stateful pod in namespace: statefulset-6748, name: ss-0, uid: 745df15a-7848-4626-b3af-13c9a6925be3, status phase: Pending. Waiting for statefulset controller to delete.
Nov 29 18:26:06.562: INFO: Observed stateful pod in namespace: statefulset-6748, name: ss-0, uid: 745df15a-7848-4626-b3af-13c9a6925be3, status phase: Failed. Waiting for statefulset controller to delete.
Nov 29 18:26:06.566: INFO: Observed stateful pod in namespace: statefulset-6748, name: ss-0, uid: 745df15a-7848-4626-b3af-13c9a6925be3, status phase: Failed. Waiting for statefulset controller to delete.
Nov 29 18:26:06.575: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6748
STEP: Removing pod with conflicting port in namespace statefulset-6748
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6748 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 29 18:26:10.600: INFO: Deleting all statefulset in ns statefulset-6748
Nov 29 18:26:10.602: INFO: Scaling statefulset ss to 0
Nov 29 18:26:20.612: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 18:26:20.614: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:26:20.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6748" for this suite.

• [SLOW TEST:18.203 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":107,"skipped":1631,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:26:20.628: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-ad24fadd-0e83-43f4-abc2-050124728419
STEP: Creating secret with name s-test-opt-upd-196b02aa-ffb3-406e-87d1-88cb380a3f4c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ad24fadd-0e83-43f4-abc2-050124728419
STEP: Updating secret s-test-opt-upd-196b02aa-ffb3-406e-87d1-88cb380a3f4c
STEP: Creating secret with name s-test-opt-create-81f666f8-bbb2-4f1e-917a-939e8a8d6017
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:27:32.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5589" for this suite.

• [SLOW TEST:72.288 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1660,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:27:32.916: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 29 18:27:32.942: INFO: Waiting up to 5m0s for pod "pod-bf3c3b3e-cbe9-49bb-9660-3539c7aeee8a" in namespace "emptydir-9629" to be "Succeeded or Failed"
Nov 29 18:27:32.944: INFO: Pod "pod-bf3c3b3e-cbe9-49bb-9660-3539c7aeee8a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.622987ms
Nov 29 18:27:34.947: INFO: Pod "pod-bf3c3b3e-cbe9-49bb-9660-3539c7aeee8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004699937s
STEP: Saw pod success
Nov 29 18:27:34.947: INFO: Pod "pod-bf3c3b3e-cbe9-49bb-9660-3539c7aeee8a" satisfied condition "Succeeded or Failed"
Nov 29 18:27:34.949: INFO: Trying to get logs from node k8sconformance-m02 pod pod-bf3c3b3e-cbe9-49bb-9660-3539c7aeee8a container test-container: <nil>
STEP: delete the pod
Nov 29 18:27:34.963: INFO: Waiting for pod pod-bf3c3b3e-cbe9-49bb-9660-3539c7aeee8a to disappear
Nov 29 18:27:34.965: INFO: Pod pod-bf3c3b3e-cbe9-49bb-9660-3539c7aeee8a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:27:34.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9629" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":109,"skipped":1671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:27:34.970: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:27:41.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1981" for this suite.
STEP: Destroying namespace "nsdeletetest-9915" for this suite.
Nov 29 18:27:41.090: INFO: Namespace nsdeletetest-9915 was already deleted
STEP: Destroying namespace "nsdeletetest-3320" for this suite.

• [SLOW TEST:6.123 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":110,"skipped":1701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:27:41.093: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 29 18:27:41.112: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:27:44.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3203" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":111,"skipped":1726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:27:44.443: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:27:44.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7778" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":112,"skipped":1751,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:27:44.483: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:27:44.509: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 29 18:27:49.511: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 29 18:27:49.511: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 29 18:27:49.527: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-509 /apis/apps/v1/namespaces/deployment-509/deployments/test-cleanup-deployment 1e0dedf4-18aa-4f48-a4ef-150e4922328b 8050 1 2020-11-29 18:27:49 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-11-29 18:27:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a36dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 29 18:27:49.530: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Nov 29 18:27:49.530: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 29 18:27:49.530: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-509 /apis/apps/v1/namespaces/deployment-509/replicasets/test-cleanup-controller 7f25e936-01a0-45c3-b43c-3b19ebaabc6a 8053 1 2020-11-29 18:27:44 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 1e0dedf4-18aa-4f48-a4ef-150e4922328b 0xc003fb0097 0xc003fb0098}] []  [{e2e.test Update apps/v1 2020-11-29 18:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-29 18:27:49 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"1e0dedf4-18aa-4f48-a4ef-150e4922328b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fb0138 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 29 18:27:49.532: INFO: Pod "test-cleanup-controller-t7clv" is available:
&Pod{ObjectMeta:{test-cleanup-controller-t7clv test-cleanup-controller- deployment-509 /api/v1/namespaces/deployment-509/pods/test-cleanup-controller-t7clv 3920de68-d18b-4c8b-b503-5894b2d57e23 8024 0 2020-11-29 18:27:44 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 7f25e936-01a0-45c3-b43c-3b19ebaabc6a 0xc003fb049f 0xc003fb04b0}] []  [{kube-controller-manager Update v1 2020-11-29 18:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f25e936-01a0-45c3-b43c-3b19ebaabc6a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:27:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wcgr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wcgr8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wcgr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:27:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:27:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:27:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:27:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.132,StartTime:2020-11-29 18:27:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 18:27:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://761ead56c28e9e3ab8ff5cf3a0f1a3feb553efc436a14f5109b9c7bc054688ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:27:49.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-509" for this suite.

• [SLOW TEST:5.068 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":113,"skipped":1777,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:27:49.551: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 29 18:27:49.645: INFO: Waiting up to 5m0s for pod "pod-f604764c-52ea-40cb-997b-9504148909f4" in namespace "emptydir-1651" to be "Succeeded or Failed"
Nov 29 18:27:49.647: INFO: Pod "pod-f604764c-52ea-40cb-997b-9504148909f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047447ms
Nov 29 18:27:51.650: INFO: Pod "pod-f604764c-52ea-40cb-997b-9504148909f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004891556s
STEP: Saw pod success
Nov 29 18:27:51.650: INFO: Pod "pod-f604764c-52ea-40cb-997b-9504148909f4" satisfied condition "Succeeded or Failed"
Nov 29 18:27:51.652: INFO: Trying to get logs from node k8sconformance-m02 pod pod-f604764c-52ea-40cb-997b-9504148909f4 container test-container: <nil>
STEP: delete the pod
Nov 29 18:27:51.664: INFO: Waiting for pod pod-f604764c-52ea-40cb-997b-9504148909f4 to disappear
Nov 29 18:27:51.665: INFO: Pod pod-f604764c-52ea-40cb-997b-9504148909f4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:27:51.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1651" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":1777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:27:51.670: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 29 18:27:51.731: INFO: Number of nodes with available pods: 0
Nov 29 18:27:51.731: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:27:52.737: INFO: Number of nodes with available pods: 0
Nov 29 18:27:52.737: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:27:53.737: INFO: Number of nodes with available pods: 2
Nov 29 18:27:53.737: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 29 18:27:53.755: INFO: Number of nodes with available pods: 1
Nov 29 18:27:53.755: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 29 18:27:54.760: INFO: Number of nodes with available pods: 1
Nov 29 18:27:54.760: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 29 18:27:55.761: INFO: Number of nodes with available pods: 2
Nov 29 18:27:55.761: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2884, will wait for the garbage collector to delete the pods
Nov 29 18:27:55.824: INFO: Deleting DaemonSet.extensions daemon-set took: 4.430781ms
Nov 29 18:27:55.924: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.3187ms
Nov 29 18:28:08.027: INFO: Number of nodes with available pods: 0
Nov 29 18:28:08.027: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 18:28:08.029: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2884/daemonsets","resourceVersion":"8228"},"items":null}

Nov 29 18:28:08.030: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2884/pods","resourceVersion":"8228"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:28:08.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2884" for this suite.

• [SLOW TEST:16.371 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":115,"skipped":1804,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:28:08.041: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:28:08.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1727" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":116,"skipped":1810,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:28:08.124: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:28:08.147: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-f1c041a2-6894-4ff3-a1c3-c1a9b3acaa08" in namespace "security-context-test-8796" to be "Succeeded or Failed"
Nov 29 18:28:08.173: INFO: Pod "busybox-privileged-false-f1c041a2-6894-4ff3-a1c3-c1a9b3acaa08": Phase="Pending", Reason="", readiness=false. Elapsed: 26.03084ms
Nov 29 18:28:10.176: INFO: Pod "busybox-privileged-false-f1c041a2-6894-4ff3-a1c3-c1a9b3acaa08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029033273s
Nov 29 18:28:10.176: INFO: Pod "busybox-privileged-false-f1c041a2-6894-4ff3-a1c3-c1a9b3acaa08" satisfied condition "Succeeded or Failed"
Nov 29 18:28:10.181: INFO: Got logs for pod "busybox-privileged-false-f1c041a2-6894-4ff3-a1c3-c1a9b3acaa08": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:28:10.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8796" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":117,"skipped":1840,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:28:10.187: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:28:10.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:28:13.847: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:28:13.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2888" for this suite.
STEP: Destroying namespace "webhook-2888-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":118,"skipped":1847,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:28:13.893: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:28:13.920: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:28:14.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1329" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":119,"skipped":1895,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:28:14.455: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 29 18:28:14.480: INFO: Waiting up to 5m0s for pod "downward-api-849d6a50-ef1b-4188-8939-ffdfe61a945d" in namespace "downward-api-3626" to be "Succeeded or Failed"
Nov 29 18:28:14.482: INFO: Pod "downward-api-849d6a50-ef1b-4188-8939-ffdfe61a945d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.654262ms
Nov 29 18:28:16.485: INFO: Pod "downward-api-849d6a50-ef1b-4188-8939-ffdfe61a945d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004667836s
Nov 29 18:28:18.488: INFO: Pod "downward-api-849d6a50-ef1b-4188-8939-ffdfe61a945d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007796992s
STEP: Saw pod success
Nov 29 18:28:18.488: INFO: Pod "downward-api-849d6a50-ef1b-4188-8939-ffdfe61a945d" satisfied condition "Succeeded or Failed"
Nov 29 18:28:18.490: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-849d6a50-ef1b-4188-8939-ffdfe61a945d container dapi-container: <nil>
STEP: delete the pod
Nov 29 18:28:18.505: INFO: Waiting for pod downward-api-849d6a50-ef1b-4188-8939-ffdfe61a945d to disappear
Nov 29 18:28:18.507: INFO: Pod downward-api-849d6a50-ef1b-4188-8939-ffdfe61a945d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:28:18.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3626" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":120,"skipped":1900,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:28:18.512: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 29 18:28:18.543: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9681 /api/v1/namespaces/watch-9681/configmaps/e2e-watch-test-label-changed 72496dc4-7fce-4094-b90b-954d15ad19ea 8406 0 2020-11-29 18:28:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 18:28:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:28:18.543: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9681 /api/v1/namespaces/watch-9681/configmaps/e2e-watch-test-label-changed 72496dc4-7fce-4094-b90b-954d15ad19ea 8407 0 2020-11-29 18:28:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 18:28:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:28:18.543: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9681 /api/v1/namespaces/watch-9681/configmaps/e2e-watch-test-label-changed 72496dc4-7fce-4094-b90b-954d15ad19ea 8408 0 2020-11-29 18:28:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 18:28:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 29 18:28:28.561: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9681 /api/v1/namespaces/watch-9681/configmaps/e2e-watch-test-label-changed 72496dc4-7fce-4094-b90b-954d15ad19ea 8447 0 2020-11-29 18:28:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 18:28:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:28:28.561: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9681 /api/v1/namespaces/watch-9681/configmaps/e2e-watch-test-label-changed 72496dc4-7fce-4094-b90b-954d15ad19ea 8448 0 2020-11-29 18:28:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 18:28:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:28:28.561: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9681 /api/v1/namespaces/watch-9681/configmaps/e2e-watch-test-label-changed 72496dc4-7fce-4094-b90b-954d15ad19ea 8449 0 2020-11-29 18:28:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 18:28:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:28:28.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9681" for this suite.

• [SLOW TEST:10.054 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":121,"skipped":1942,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:28:28.567: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov 29 18:28:28.587: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov 29 18:28:38.542: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:28:41.378: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:28:52.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3470" for this suite.

• [SLOW TEST:23.959 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":122,"skipped":1942,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:28:52.526: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-7360f6ce-5b1e-43f5-902f-a2d01dae35d4
STEP: Creating a pod to test consume secrets
Nov 29 18:28:52.555: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a461c65-4c2f-491c-bf2e-141604ab55c5" in namespace "projected-9321" to be "Succeeded or Failed"
Nov 29 18:28:52.559: INFO: Pod "pod-projected-secrets-5a461c65-4c2f-491c-bf2e-141604ab55c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.157621ms
Nov 29 18:28:54.563: INFO: Pod "pod-projected-secrets-5a461c65-4c2f-491c-bf2e-141604ab55c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007411061s
Nov 29 18:28:56.566: INFO: Pod "pod-projected-secrets-5a461c65-4c2f-491c-bf2e-141604ab55c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010567185s
STEP: Saw pod success
Nov 29 18:28:56.566: INFO: Pod "pod-projected-secrets-5a461c65-4c2f-491c-bf2e-141604ab55c5" satisfied condition "Succeeded or Failed"
Nov 29 18:28:56.568: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-5a461c65-4c2f-491c-bf2e-141604ab55c5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 18:28:56.647: INFO: Waiting for pod pod-projected-secrets-5a461c65-4c2f-491c-bf2e-141604ab55c5 to disappear
Nov 29 18:28:56.649: INFO: Pod pod-projected-secrets-5a461c65-4c2f-491c-bf2e-141604ab55c5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:28:56.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9321" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":1976,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:28:56.654: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:28:56.676: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 29 18:28:58.696: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:28:59.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5976" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":124,"skipped":1980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:28:59.711: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 29 18:28:59.738: INFO: Waiting up to 5m0s for pod "pod-8a4a52c9-eb60-470a-b78e-590588fccc17" in namespace "emptydir-8112" to be "Succeeded or Failed"
Nov 29 18:28:59.740: INFO: Pod "pod-8a4a52c9-eb60-470a-b78e-590588fccc17": Phase="Pending", Reason="", readiness=false. Elapsed: 1.736538ms
Nov 29 18:29:01.742: INFO: Pod "pod-8a4a52c9-eb60-470a-b78e-590588fccc17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003864162s
STEP: Saw pod success
Nov 29 18:29:01.742: INFO: Pod "pod-8a4a52c9-eb60-470a-b78e-590588fccc17" satisfied condition "Succeeded or Failed"
Nov 29 18:29:01.744: INFO: Trying to get logs from node k8sconformance-m02 pod pod-8a4a52c9-eb60-470a-b78e-590588fccc17 container test-container: <nil>
STEP: delete the pod
Nov 29 18:29:01.756: INFO: Waiting for pod pod-8a4a52c9-eb60-470a-b78e-590588fccc17 to disappear
Nov 29 18:29:01.758: INFO: Pod pod-8a4a52c9-eb60-470a-b78e-590588fccc17 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:29:01.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8112" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":2014,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:29:01.763: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-cgjm
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 18:29:01.819: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cgjm" in namespace "subpath-7337" to be "Succeeded or Failed"
Nov 29 18:29:01.825: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.254898ms
Nov 29 18:29:03.828: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008314676s
Nov 29 18:29:05.831: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Running", Reason="", readiness=true. Elapsed: 4.011505217s
Nov 29 18:29:07.834: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Running", Reason="", readiness=true. Elapsed: 6.014603172s
Nov 29 18:29:09.837: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Running", Reason="", readiness=true. Elapsed: 8.017382958s
Nov 29 18:29:11.839: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Running", Reason="", readiness=true. Elapsed: 10.020020564s
Nov 29 18:29:13.842: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Running", Reason="", readiness=true. Elapsed: 12.023095377s
Nov 29 18:29:15.845: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Running", Reason="", readiness=true. Elapsed: 14.026155431s
Nov 29 18:29:17.849: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Running", Reason="", readiness=true. Elapsed: 16.029379493s
Nov 29 18:29:19.852: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Running", Reason="", readiness=true. Elapsed: 18.032533674s
Nov 29 18:29:21.854: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Running", Reason="", readiness=true. Elapsed: 20.035196037s
Nov 29 18:29:23.858: INFO: Pod "pod-subpath-test-configmap-cgjm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.038252937s
STEP: Saw pod success
Nov 29 18:29:23.858: INFO: Pod "pod-subpath-test-configmap-cgjm" satisfied condition "Succeeded or Failed"
Nov 29 18:29:23.860: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-configmap-cgjm container test-container-subpath-configmap-cgjm: <nil>
STEP: delete the pod
Nov 29 18:29:23.873: INFO: Waiting for pod pod-subpath-test-configmap-cgjm to disappear
Nov 29 18:29:23.875: INFO: Pod pod-subpath-test-configmap-cgjm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cgjm
Nov 29 18:29:23.875: INFO: Deleting pod "pod-subpath-test-configmap-cgjm" in namespace "subpath-7337"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:29:23.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7337" for this suite.

• [SLOW TEST:22.119 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":126,"skipped":2024,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:29:23.882: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:29:23.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-3700'
Nov 29 18:29:24.208: INFO: stderr: ""
Nov 29 18:29:24.208: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 29 18:29:24.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-3700'
Nov 29 18:29:24.447: INFO: stderr: ""
Nov 29 18:29:24.447: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 29 18:29:25.450: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:29:25.450: INFO: Found 0 / 1
Nov 29 18:29:26.450: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:29:26.450: INFO: Found 1 / 1
Nov 29 18:29:26.450: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 29 18:29:26.453: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:29:26.453: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 29 18:29:26.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 describe pod agnhost-primary-qdsq8 --namespace=kubectl-3700'
Nov 29 18:29:26.534: INFO: stderr: ""
Nov 29 18:29:26.534: INFO: stdout: "Name:         agnhost-primary-qdsq8\nNamespace:    kubectl-3700\nPriority:     0\nNode:         k8sconformance-m02/192.168.49.3\nStart Time:   Sun, 29 Nov 2020 18:29:24 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.1.144\nIPs:\n  IP:           10.244.1.144\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://f4d708888803154a6d7222e12dc90699c06d8048aa394a369d61a01e2e5112f1\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 29 Nov 2020 18:29:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-kdc9b (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-kdc9b:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-kdc9b\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason            Age              From               Message\n  ----     ------            ----             ----               -------\n  Normal   Scheduled         2s               default-scheduler  Successfully assigned kubectl-3700/agnhost-primary-qdsq8 to k8sconformance-m02\n  Normal   Pulled            2s               kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Warning  DNSConfigForming  1s (x3 over 2s)  kubelet            Search Line limits were exceeded, some search paths have been omitted, the applied search line is: kubectl-3700.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com\n  Normal   Created           1s               kubelet            Created container agnhost-primary\n  Normal   Started           1s               kubelet            Started container agnhost-primary\n"
Nov 29 18:29:26.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 describe rc agnhost-primary --namespace=kubectl-3700'
Nov 29 18:29:26.618: INFO: stderr: ""
Nov 29 18:29:26.618: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3700\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-qdsq8\n"
Nov 29 18:29:26.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 describe service agnhost-primary --namespace=kubectl-3700'
Nov 29 18:29:26.695: INFO: stderr: ""
Nov 29 18:29:26.695: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3700\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.104.226.82\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.144:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 29 18:29:26.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 describe node k8sconformance'
Nov 29 18:29:26.791: INFO: stderr: ""
Nov 29 18:29:26.791: INFO: stdout: "Name:               k8sconformance\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8sconformance\n                    kubernetes.io/os=linux\n                    minikube.k8s.io/commit=640e1cb255210abb49eb7aab8001b9b425f1a161-dirty\n                    minikube.k8s.io/name=k8sconformance\n                    minikube.k8s.io/updated_at=2020_11_29T17_55_55_0700\n                    minikube.k8s.io/version=v1.15.1\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 29 Nov 2020 17:55:51 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8sconformance\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 29 Nov 2020 18:29:21 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sun, 29 Nov 2020 18:28:50 +0000   Sun, 29 Nov 2020 17:55:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sun, 29 Nov 2020 18:28:50 +0000   Sun, 29 Nov 2020 17:55:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sun, 29 Nov 2020 18:28:50 +0000   Sun, 29 Nov 2020 17:55:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sun, 29 Nov 2020 18:28:50 +0000   Sun, 29 Nov 2020 17:56:09 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.49.2\n  Hostname:    k8sconformance\nCapacity:\n  cpu:                8\n  ephemeral-storage:  510022312Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             107153064Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  510022312Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             107153064Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 80824279616f463b9866b2f8be3e4a17\n  System UUID:                92c747ce-fae4-487f-b94e-127c92a9cc21\n  Boot ID:                    46278987-f672-4a3d-8bc8-9b2102acd9b7\n  Kernel Version:             5.7.17-1rodete4-amd64\n  OS Image:                   Ubuntu 20.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.7\n  Kubelet Version:            v1.19.4\n  Kube-Proxy Version:         v1.19.4\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-f9fd979d6-brw29                                    100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     33m\n  kube-system                 etcd-k8sconformance                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 kindnet-w9hf5                                              100m (1%)     100m (1%)   50Mi (0%)        50Mi (0%)      33m\n  kube-system                 kube-apiserver-k8sconformance                              250m (3%)     0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 kube-controller-manager-k8sconformance                     200m (2%)     0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 kube-proxy-rk57s                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 kube-scheduler-k8sconformance                              100m (1%)     0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 storage-provisioner                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         32m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-bfkxd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         32m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (9%)   100m (1%)\n  memory             120Mi (0%)  220Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                    Age                From        Message\n  ----     ------                    ----               ----        -------\n  Normal   NodeHasSufficientMemory   33m (x4 over 33m)  kubelet     Node k8sconformance status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure     33m (x4 over 33m)  kubelet     Node k8sconformance status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID      33m (x3 over 33m)  kubelet     Node k8sconformance status is now: NodeHasSufficientPID\n  Normal   Starting                  33m                kubelet     Starting kubelet.\n  Warning  CheckLimitsForResolvConf  33m                kubelet     Resolv.conf file '/etc/resolv.conf' contains search line consisting of more than 3 domains!\n  Normal   NodeHasSufficientMemory   33m                kubelet     Node k8sconformance status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure     33m                kubelet     Node k8sconformance status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID      33m                kubelet     Node k8sconformance status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced   33m                kubelet     Updated Node Allocatable limit across pods\n  Normal   NodeReady                 33m                kubelet     Node k8sconformance status is now: NodeReady\n  Normal   Starting                  33m                kube-proxy  Starting kube-proxy.\n"
Nov 29 18:29:26.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 describe namespace kubectl-3700'
Nov 29 18:29:26.868: INFO: stderr: ""
Nov 29 18:29:26.868: INFO: stdout: "Name:         kubectl-3700\nLabels:       e2e-framework=kubectl\n              e2e-run=c8ff2b2c-9d10-46f2-80e9-9abad52ee157\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:29:26.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3700" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":127,"skipped":2038,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:29:26.873: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1129 18:29:27.943625      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 29 18:30:29.956: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:30:29.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4556" for this suite.

• [SLOW TEST:63.088 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":128,"skipped":2047,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:30:29.962: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:30:29.982: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov 29 18:30:32.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-2073 create -f -'
Nov 29 18:30:33.739: INFO: stderr: ""
Nov 29 18:30:33.739: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 29 18:30:33.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-2073 delete e2e-test-crd-publish-openapi-2361-crds test-foo'
Nov 29 18:30:33.813: INFO: stderr: ""
Nov 29 18:30:33.813: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 29 18:30:33.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-2073 apply -f -'
Nov 29 18:30:34.038: INFO: stderr: ""
Nov 29 18:30:34.038: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 29 18:30:34.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-2073 delete e2e-test-crd-publish-openapi-2361-crds test-foo'
Nov 29 18:30:34.111: INFO: stderr: ""
Nov 29 18:30:34.111: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov 29 18:30:34.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-2073 create -f -'
Nov 29 18:30:34.288: INFO: rc: 1
Nov 29 18:30:34.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-2073 apply -f -'
Nov 29 18:30:34.470: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov 29 18:30:34.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-2073 create -f -'
Nov 29 18:30:34.662: INFO: rc: 1
Nov 29 18:30:34.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-2073 apply -f -'
Nov 29 18:30:34.829: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov 29 18:30:34.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 explain e2e-test-crd-publish-openapi-2361-crds'
Nov 29 18:30:34.999: INFO: stderr: ""
Nov 29 18:30:34.999: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov 29 18:30:34.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 explain e2e-test-crd-publish-openapi-2361-crds.metadata'
Nov 29 18:30:35.260: INFO: stderr: ""
Nov 29 18:30:35.260: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 29 18:30:35.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 explain e2e-test-crd-publish-openapi-2361-crds.spec'
Nov 29 18:30:35.459: INFO: stderr: ""
Nov 29 18:30:35.459: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 29 18:30:35.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 explain e2e-test-crd-publish-openapi-2361-crds.spec.bars'
Nov 29 18:30:35.633: INFO: stderr: ""
Nov 29 18:30:35.633: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov 29 18:30:35.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 explain e2e-test-crd-publish-openapi-2361-crds.spec.bars2'
Nov 29 18:30:35.806: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:30:38.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2073" for this suite.

• [SLOW TEST:8.675 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":129,"skipped":2047,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:30:38.637: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 29 18:30:38.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-8868'
Nov 29 18:30:38.818: INFO: stderr: ""
Nov 29 18:30:38.818: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Nov 29 18:30:38.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pod e2e-test-httpd-pod -o json --namespace=kubectl-8868'
Nov 29 18:30:38.887: INFO: stderr: ""
Nov 29 18:30:38.887: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-11-29T18:30:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-29T18:30:38Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-29T18:30:38Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8868\",\n        \"resourceVersion\": \"8926\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8868/pods/e2e-test-httpd-pod\",\n        \"uid\": \"b4673f9d-9baf-410e-a367-69013214d534\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-glptq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8sconformance-m02\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-glptq\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-glptq\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T18:30:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T18:30:38Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T18:30:38Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T18:30:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.49.3\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-29T18:30:38Z\"\n    }\n}\n"
Nov 29 18:30:38.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 replace -f - --dry-run server --namespace=kubectl-8868'
Nov 29 18:30:39.184: INFO: stderr: "W1129 18:30:38.939743     727 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Nov 29 18:30:39.184: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Nov 29 18:30:39.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete pods e2e-test-httpd-pod --namespace=kubectl-8868'
Nov 29 18:30:40.744: INFO: stderr: ""
Nov 29 18:30:40.744: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:30:40.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8868" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":130,"skipped":2051,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:30:40.750: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:30:56.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3411" for this suite.

• [SLOW TEST:16.129 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":131,"skipped":2061,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:30:56.878: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 29 18:31:00.965: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 18:31:00.967: INFO: Pod pod-with-poststart-http-hook still exists
Nov 29 18:31:02.967: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 18:31:02.970: INFO: Pod pod-with-poststart-http-hook still exists
Nov 29 18:31:04.967: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 18:31:04.970: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:31:04.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9450" for this suite.

• [SLOW TEST:8.097 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":132,"skipped":2062,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:31:04.975: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-086cdc0d-ed11-4302-acbb-aa6ad88e1cb2
STEP: Creating a pod to test consume secrets
Nov 29 18:31:05.048: INFO: Waiting up to 5m0s for pod "pod-secrets-e89e5f01-38a1-4dbe-b7b5-c909f91b9157" in namespace "secrets-1515" to be "Succeeded or Failed"
Nov 29 18:31:05.050: INFO: Pod "pod-secrets-e89e5f01-38a1-4dbe-b7b5-c909f91b9157": Phase="Pending", Reason="", readiness=false. Elapsed: 1.699375ms
Nov 29 18:31:07.053: INFO: Pod "pod-secrets-e89e5f01-38a1-4dbe-b7b5-c909f91b9157": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004806577s
STEP: Saw pod success
Nov 29 18:31:07.053: INFO: Pod "pod-secrets-e89e5f01-38a1-4dbe-b7b5-c909f91b9157" satisfied condition "Succeeded or Failed"
Nov 29 18:31:07.055: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-e89e5f01-38a1-4dbe-b7b5-c909f91b9157 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 18:31:07.068: INFO: Waiting for pod pod-secrets-e89e5f01-38a1-4dbe-b7b5-c909f91b9157 to disappear
Nov 29 18:31:07.070: INFO: Pod pod-secrets-e89e5f01-38a1-4dbe-b7b5-c909f91b9157 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:31:07.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1515" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2083,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:31:07.075: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 29 18:31:07.107: INFO: Waiting up to 5m0s for pod "downward-api-bd1fb81b-af81-47c0-a7ba-45b71dc7a260" in namespace "downward-api-1633" to be "Succeeded or Failed"
Nov 29 18:31:07.109: INFO: Pod "downward-api-bd1fb81b-af81-47c0-a7ba-45b71dc7a260": Phase="Pending", Reason="", readiness=false. Elapsed: 1.960934ms
Nov 29 18:31:09.112: INFO: Pod "downward-api-bd1fb81b-af81-47c0-a7ba-45b71dc7a260": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004773441s
STEP: Saw pod success
Nov 29 18:31:09.112: INFO: Pod "downward-api-bd1fb81b-af81-47c0-a7ba-45b71dc7a260" satisfied condition "Succeeded or Failed"
Nov 29 18:31:09.114: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-bd1fb81b-af81-47c0-a7ba-45b71dc7a260 container dapi-container: <nil>
STEP: delete the pod
Nov 29 18:31:09.129: INFO: Waiting for pod downward-api-bd1fb81b-af81-47c0-a7ba-45b71dc7a260 to disappear
Nov 29 18:31:09.130: INFO: Pod downward-api-bd1fb81b-af81-47c0-a7ba-45b71dc7a260 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:31:09.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1633" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":134,"skipped":2087,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:31:09.135: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:31:09.163: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb6eb0a5-d8e2-4595-a8a5-5550625e01a9" in namespace "downward-api-4541" to be "Succeeded or Failed"
Nov 29 18:31:09.167: INFO: Pod "downwardapi-volume-cb6eb0a5-d8e2-4595-a8a5-5550625e01a9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.830405ms
Nov 29 18:31:11.170: INFO: Pod "downwardapi-volume-cb6eb0a5-d8e2-4595-a8a5-5550625e01a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006947595s
STEP: Saw pod success
Nov 29 18:31:11.170: INFO: Pod "downwardapi-volume-cb6eb0a5-d8e2-4595-a8a5-5550625e01a9" satisfied condition "Succeeded or Failed"
Nov 29 18:31:11.172: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-cb6eb0a5-d8e2-4595-a8a5-5550625e01a9 container client-container: <nil>
STEP: delete the pod
Nov 29 18:31:11.187: INFO: Waiting for pod downwardapi-volume-cb6eb0a5-d8e2-4595-a8a5-5550625e01a9 to disappear
Nov 29 18:31:11.188: INFO: Pod downwardapi-volume-cb6eb0a5-d8e2-4595-a8a5-5550625e01a9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:31:11.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4541" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2088,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:31:11.193: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:31:13.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7117" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":136,"skipped":2095,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:31:13.254: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:31:13.824: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 18:31:15.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271473, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271473, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271473, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271473, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:31:18.846: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov 29 18:31:20.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 attach --namespace=webhook-2498 to-be-attached-pod -i -c=container1'
Nov 29 18:31:20.954: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:31:20.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2498" for this suite.
STEP: Destroying namespace "webhook-2498-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.743 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":137,"skipped":2112,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:31:20.998: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:31:21.025: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba32af83-3bab-4a1c-8074-dbcdebd3b98d" in namespace "projected-4375" to be "Succeeded or Failed"
Nov 29 18:31:21.027: INFO: Pod "downwardapi-volume-ba32af83-3bab-4a1c-8074-dbcdebd3b98d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.728195ms
Nov 29 18:31:23.030: INFO: Pod "downwardapi-volume-ba32af83-3bab-4a1c-8074-dbcdebd3b98d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004557123s
STEP: Saw pod success
Nov 29 18:31:23.030: INFO: Pod "downwardapi-volume-ba32af83-3bab-4a1c-8074-dbcdebd3b98d" satisfied condition "Succeeded or Failed"
Nov 29 18:31:23.032: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-ba32af83-3bab-4a1c-8074-dbcdebd3b98d container client-container: <nil>
STEP: delete the pod
Nov 29 18:31:23.046: INFO: Waiting for pod downwardapi-volume-ba32af83-3bab-4a1c-8074-dbcdebd3b98d to disappear
Nov 29 18:31:23.047: INFO: Pod downwardapi-volume-ba32af83-3bab-4a1c-8074-dbcdebd3b98d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:31:23.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4375" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":138,"skipped":2123,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:31:23.054: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 29 18:31:23.084: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9471 /api/v1/namespaces/watch-9471/configmaps/e2e-watch-test-watch-closed 3ce642c2-7455-4056-bf2f-9757549ab06d 9275 0 2020-11-29 18:31:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-29 18:31:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:31:23.085: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9471 /api/v1/namespaces/watch-9471/configmaps/e2e-watch-test-watch-closed 3ce642c2-7455-4056-bf2f-9757549ab06d 9276 0 2020-11-29 18:31:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-29 18:31:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 29 18:31:23.093: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9471 /api/v1/namespaces/watch-9471/configmaps/e2e-watch-test-watch-closed 3ce642c2-7455-4056-bf2f-9757549ab06d 9277 0 2020-11-29 18:31:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-29 18:31:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 18:31:23.093: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9471 /api/v1/namespaces/watch-9471/configmaps/e2e-watch-test-watch-closed 3ce642c2-7455-4056-bf2f-9757549ab06d 9278 0 2020-11-29 18:31:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-29 18:31:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:31:23.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9471" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":139,"skipped":2134,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:31:23.098: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:31:23.127: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1220
I1129 18:31:23.135809      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1220, replica count: 1
I1129 18:31:24.186339      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 18:31:25.186528      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 18:31:25.298: INFO: Created: latency-svc-p28vb
Nov 29 18:31:25.318: INFO: Got endpoints: latency-svc-p28vb [31.59661ms]
Nov 29 18:31:25.342: INFO: Created: latency-svc-rb6nt
Nov 29 18:31:25.346: INFO: Got endpoints: latency-svc-rb6nt [28.405442ms]
Nov 29 18:31:25.377: INFO: Created: latency-svc-k2fzh
Nov 29 18:31:25.473: INFO: Created: latency-svc-mcdbw
Nov 29 18:31:25.473: INFO: Got endpoints: latency-svc-k2fzh [154.922149ms]
Nov 29 18:31:25.492: INFO: Got endpoints: latency-svc-mcdbw [174.040069ms]
Nov 29 18:31:25.493: INFO: Created: latency-svc-mjncb
Nov 29 18:31:25.499: INFO: Got endpoints: latency-svc-mjncb [181.278543ms]
Nov 29 18:31:25.528: INFO: Created: latency-svc-hmrvp
Nov 29 18:31:25.532: INFO: Got endpoints: latency-svc-hmrvp [213.741119ms]
Nov 29 18:31:25.552: INFO: Created: latency-svc-vpxpz
Nov 29 18:31:25.554: INFO: Got endpoints: latency-svc-vpxpz [235.750111ms]
Nov 29 18:31:25.564: INFO: Created: latency-svc-698jh
Nov 29 18:31:25.568: INFO: Got endpoints: latency-svc-698jh [249.829402ms]
Nov 29 18:31:25.590: INFO: Created: latency-svc-lpfv9
Nov 29 18:31:25.592: INFO: Got endpoints: latency-svc-lpfv9 [273.933547ms]
Nov 29 18:31:25.599: INFO: Created: latency-svc-rbkrs
Nov 29 18:31:25.603: INFO: Got endpoints: latency-svc-rbkrs [284.996685ms]
Nov 29 18:31:25.626: INFO: Created: latency-svc-s8vfj
Nov 29 18:31:25.655: INFO: Got endpoints: latency-svc-s8vfj [336.487552ms]
Nov 29 18:31:25.673: INFO: Created: latency-svc-6z2m9
Nov 29 18:31:25.727: INFO: Created: latency-svc-dsc7n
Nov 29 18:31:25.727: INFO: Got endpoints: latency-svc-6z2m9 [408.442537ms]
Nov 29 18:31:25.735: INFO: Got endpoints: latency-svc-dsc7n [416.825781ms]
Nov 29 18:31:25.741: INFO: Created: latency-svc-5s6kg
Nov 29 18:31:25.741: INFO: Got endpoints: latency-svc-5s6kg [422.737272ms]
Nov 29 18:31:25.769: INFO: Created: latency-svc-w8cbq
Nov 29 18:31:25.777: INFO: Got endpoints: latency-svc-w8cbq [459.084586ms]
Nov 29 18:31:25.777: INFO: Created: latency-svc-v5vxz
Nov 29 18:31:25.781: INFO: Got endpoints: latency-svc-v5vxz [462.236316ms]
Nov 29 18:31:25.787: INFO: Created: latency-svc-jxkhl
Nov 29 18:31:25.812: INFO: Got endpoints: latency-svc-jxkhl [465.939647ms]
Nov 29 18:31:25.814: INFO: Created: latency-svc-lv98d
Nov 29 18:31:25.824: INFO: Got endpoints: latency-svc-lv98d [43.920651ms]
Nov 29 18:31:25.853: INFO: Created: latency-svc-mw6fv
Nov 29 18:31:25.889: INFO: Got endpoints: latency-svc-mw6fv [415.936186ms]
Nov 29 18:31:25.889: INFO: Created: latency-svc-cld5r
Nov 29 18:31:25.893: INFO: Got endpoints: latency-svc-cld5r [400.716603ms]
Nov 29 18:31:25.926: INFO: Created: latency-svc-g95mf
Nov 29 18:31:25.943: INFO: Got endpoints: latency-svc-g95mf [443.722295ms]
Nov 29 18:31:25.944: INFO: Created: latency-svc-pnclf
Nov 29 18:31:25.948: INFO: Got endpoints: latency-svc-pnclf [416.336168ms]
Nov 29 18:31:25.984: INFO: Created: latency-svc-fpfpl
Nov 29 18:31:25.986: INFO: Got endpoints: latency-svc-fpfpl [431.919611ms]
Nov 29 18:31:26.025: INFO: Created: latency-svc-n6nmf
Nov 29 18:31:26.034: INFO: Got endpoints: latency-svc-n6nmf [465.649046ms]
Nov 29 18:31:26.035: INFO: Created: latency-svc-4n6kr
Nov 29 18:31:26.037: INFO: Got endpoints: latency-svc-4n6kr [445.06983ms]
Nov 29 18:31:26.060: INFO: Created: latency-svc-9x7dz
Nov 29 18:31:26.062: INFO: Got endpoints: latency-svc-9x7dz [458.357278ms]
Nov 29 18:31:26.071: INFO: Created: latency-svc-5w9zh
Nov 29 18:31:26.075: INFO: Got endpoints: latency-svc-5w9zh [420.259611ms]
Nov 29 18:31:26.080: INFO: Created: latency-svc-rwb9x
Nov 29 18:31:26.116: INFO: Got endpoints: latency-svc-rwb9x [389.274388ms]
Nov 29 18:31:26.129: INFO: Created: latency-svc-8fzc7
Nov 29 18:31:26.132: INFO: Got endpoints: latency-svc-8fzc7 [396.585166ms]
Nov 29 18:31:26.147: INFO: Created: latency-svc-jmcrt
Nov 29 18:31:26.153: INFO: Got endpoints: latency-svc-jmcrt [411.89001ms]
Nov 29 18:31:26.162: INFO: Created: latency-svc-fnvrx
Nov 29 18:31:26.165: INFO: Got endpoints: latency-svc-fnvrx [388.372971ms]
Nov 29 18:31:26.171: INFO: Created: latency-svc-mskzs
Nov 29 18:31:26.174: INFO: Got endpoints: latency-svc-mskzs [361.619732ms]
Nov 29 18:31:26.182: INFO: Created: latency-svc-zlhf2
Nov 29 18:31:26.209: INFO: Got endpoints: latency-svc-zlhf2 [384.83124ms]
Nov 29 18:31:26.211: INFO: Created: latency-svc-dgzlv
Nov 29 18:31:26.215: INFO: Got endpoints: latency-svc-dgzlv [325.64591ms]
Nov 29 18:31:26.227: INFO: Created: latency-svc-vjbwb
Nov 29 18:31:26.231: INFO: Got endpoints: latency-svc-vjbwb [337.633548ms]
Nov 29 18:31:26.236: INFO: Created: latency-svc-svhq7
Nov 29 18:31:26.239: INFO: Got endpoints: latency-svc-svhq7 [295.95215ms]
Nov 29 18:31:26.248: INFO: Created: latency-svc-5f2sf
Nov 29 18:31:26.257: INFO: Got endpoints: latency-svc-5f2sf [308.971761ms]
Nov 29 18:31:26.268: INFO: Created: latency-svc-ztwbd
Nov 29 18:31:26.275: INFO: Got endpoints: latency-svc-ztwbd [289.124169ms]
Nov 29 18:31:26.275: INFO: Created: latency-svc-xlzjk
Nov 29 18:31:26.293: INFO: Created: latency-svc-xqtg5
Nov 29 18:31:26.317: INFO: Created: latency-svc-thz5c
Nov 29 18:31:26.317: INFO: Got endpoints: latency-svc-xlzjk [283.725131ms]
Nov 29 18:31:26.318: INFO: Got endpoints: latency-svc-xqtg5 [280.192428ms]
Nov 29 18:31:26.321: INFO: Got endpoints: latency-svc-thz5c [259.300536ms]
Nov 29 18:31:26.340: INFO: Created: latency-svc-v9v4s
Nov 29 18:31:26.342: INFO: Got endpoints: latency-svc-v9v4s [267.037736ms]
Nov 29 18:31:26.350: INFO: Created: latency-svc-wfdns
Nov 29 18:31:26.354: INFO: Got endpoints: latency-svc-wfdns [238.439593ms]
Nov 29 18:31:26.413: INFO: Created: latency-svc-j94bq
Nov 29 18:31:26.417: INFO: Got endpoints: latency-svc-j94bq [285.217582ms]
Nov 29 18:31:26.426: INFO: Created: latency-svc-qpq48
Nov 29 18:31:26.428: INFO: Got endpoints: latency-svc-qpq48 [275.069985ms]
Nov 29 18:31:26.437: INFO: Created: latency-svc-hbj7k
Nov 29 18:31:26.501: INFO: Got endpoints: latency-svc-hbj7k [335.062885ms]
Nov 29 18:31:26.509: INFO: Created: latency-svc-lpc5c
Nov 29 18:31:26.512: INFO: Got endpoints: latency-svc-lpc5c [337.720546ms]
Nov 29 18:31:26.519: INFO: Created: latency-svc-b2d2x
Nov 29 18:31:26.522: INFO: Got endpoints: latency-svc-b2d2x [312.192669ms]
Nov 29 18:31:26.528: INFO: Created: latency-svc-dc4lx
Nov 29 18:31:26.532: INFO: Got endpoints: latency-svc-dc4lx [316.994178ms]
Nov 29 18:31:26.539: INFO: Created: latency-svc-rvz9f
Nov 29 18:31:26.622: INFO: Got endpoints: latency-svc-rvz9f [391.223168ms]
Nov 29 18:31:26.623: INFO: Created: latency-svc-dzr78
Nov 29 18:31:26.626: INFO: Got endpoints: latency-svc-dzr78 [386.380938ms]
Nov 29 18:31:26.650: INFO: Created: latency-svc-t94s5
Nov 29 18:31:26.653: INFO: Got endpoints: latency-svc-t94s5 [395.522383ms]
Nov 29 18:31:26.710: INFO: Created: latency-svc-ktvx4
Nov 29 18:31:26.713: INFO: Got endpoints: latency-svc-ktvx4 [437.940416ms]
Nov 29 18:31:26.733: INFO: Created: latency-svc-zbpxl
Nov 29 18:31:26.736: INFO: Got endpoints: latency-svc-zbpxl [418.689835ms]
Nov 29 18:31:26.749: INFO: Created: latency-svc-f7lwd
Nov 29 18:31:26.759: INFO: Got endpoints: latency-svc-f7lwd [441.557685ms]
Nov 29 18:31:26.761: INFO: Created: latency-svc-gbj7q
Nov 29 18:31:26.764: INFO: Got endpoints: latency-svc-gbj7q [443.297899ms]
Nov 29 18:31:26.976: INFO: Created: latency-svc-f8vs9
Nov 29 18:31:26.984: INFO: Got endpoints: latency-svc-f8vs9 [642.255766ms]
Nov 29 18:31:27.014: INFO: Created: latency-svc-xxxpz
Nov 29 18:31:27.016: INFO: Got endpoints: latency-svc-xxxpz [661.366265ms]
Nov 29 18:31:27.043: INFO: Created: latency-svc-rffd2
Nov 29 18:31:27.047: INFO: Got endpoints: latency-svc-rffd2 [630.014242ms]
Nov 29 18:31:27.055: INFO: Created: latency-svc-wz97d
Nov 29 18:31:27.058: INFO: Got endpoints: latency-svc-wz97d [630.011466ms]
Nov 29 18:31:27.065: INFO: Created: latency-svc-gc64k
Nov 29 18:31:27.072: INFO: Got endpoints: latency-svc-gc64k [571.30954ms]
Nov 29 18:31:27.106: INFO: Created: latency-svc-kvshc
Nov 29 18:31:27.116: INFO: Got endpoints: latency-svc-kvshc [603.444134ms]
Nov 29 18:31:27.116: INFO: Created: latency-svc-4s8x5
Nov 29 18:31:27.121: INFO: Got endpoints: latency-svc-4s8x5 [599.353629ms]
Nov 29 18:31:27.130: INFO: Created: latency-svc-pxrxr
Nov 29 18:31:27.136: INFO: Got endpoints: latency-svc-pxrxr [603.862307ms]
Nov 29 18:31:27.141: INFO: Created: latency-svc-tk8cx
Nov 29 18:31:27.157: INFO: Got endpoints: latency-svc-tk8cx [535.131894ms]
Nov 29 18:31:27.159: INFO: Created: latency-svc-225ks
Nov 29 18:31:27.162: INFO: Got endpoints: latency-svc-225ks [535.907976ms]
Nov 29 18:31:27.186: INFO: Created: latency-svc-sff26
Nov 29 18:31:27.190: INFO: Got endpoints: latency-svc-sff26 [536.657448ms]
Nov 29 18:31:27.196: INFO: Created: latency-svc-9hjjt
Nov 29 18:31:27.209: INFO: Created: latency-svc-9mdfh
Nov 29 18:31:27.222: INFO: Created: latency-svc-42dm2
Nov 29 18:31:27.227: INFO: Got endpoints: latency-svc-9hjjt [514.236811ms]
Nov 29 18:31:27.232: INFO: Created: latency-svc-6xwx4
Nov 29 18:31:27.239: INFO: Created: latency-svc-w5nmb
Nov 29 18:31:27.260: INFO: Created: latency-svc-bxkcq
Nov 29 18:31:27.281: INFO: Got endpoints: latency-svc-9mdfh [544.494294ms]
Nov 29 18:31:27.281: INFO: Created: latency-svc-7v9tj
Nov 29 18:31:27.292: INFO: Created: latency-svc-sh7qn
Nov 29 18:31:27.299: INFO: Created: latency-svc-c6frw
Nov 29 18:31:27.308: INFO: Created: latency-svc-hlxkr
Nov 29 18:31:27.315: INFO: Created: latency-svc-lcr2t
Nov 29 18:31:27.347: INFO: Created: latency-svc-w68j5
Nov 29 18:31:27.349: INFO: Got endpoints: latency-svc-42dm2 [589.427189ms]
Nov 29 18:31:27.355: INFO: Created: latency-svc-s6jg7
Nov 29 18:31:27.365: INFO: Created: latency-svc-jvq4g
Nov 29 18:31:27.371: INFO: Created: latency-svc-rmzhb
Nov 29 18:31:27.379: INFO: Created: latency-svc-8hwcc
Nov 29 18:31:27.379: INFO: Got endpoints: latency-svc-6xwx4 [614.949032ms]
Nov 29 18:31:27.389: INFO: Created: latency-svc-4g5p4
Nov 29 18:31:27.397: INFO: Created: latency-svc-xhmlg
Nov 29 18:31:27.418: INFO: Created: latency-svc-xrq48
Nov 29 18:31:27.426: INFO: Got endpoints: latency-svc-w5nmb [441.486575ms]
Nov 29 18:31:27.436: INFO: Created: latency-svc-7w8d8
Nov 29 18:31:27.476: INFO: Got endpoints: latency-svc-bxkcq [460.090864ms]
Nov 29 18:31:27.487: INFO: Created: latency-svc-cwv9v
Nov 29 18:31:27.526: INFO: Got endpoints: latency-svc-7v9tj [478.835179ms]
Nov 29 18:31:27.548: INFO: Created: latency-svc-r7j2p
Nov 29 18:31:27.650: INFO: Got endpoints: latency-svc-c6frw [577.641498ms]
Nov 29 18:31:27.650: INFO: Got endpoints: latency-svc-sh7qn [591.616166ms]
Nov 29 18:31:27.685: INFO: Got endpoints: latency-svc-hlxkr [568.970027ms]
Nov 29 18:31:27.685: INFO: Created: latency-svc-qwjhc
Nov 29 18:31:27.737: INFO: Got endpoints: latency-svc-lcr2t [616.286036ms]
Nov 29 18:31:27.739: INFO: Created: latency-svc-q2phg
Nov 29 18:31:27.745: INFO: Created: latency-svc-s42sc
Nov 29 18:31:27.758: INFO: Created: latency-svc-pb4z5
Nov 29 18:31:27.776: INFO: Got endpoints: latency-svc-w68j5 [639.864266ms]
Nov 29 18:31:27.787: INFO: Created: latency-svc-v28bh
Nov 29 18:31:27.826: INFO: Got endpoints: latency-svc-jvq4g [664.078686ms]
Nov 29 18:31:27.837: INFO: Created: latency-svc-8fvqt
Nov 29 18:31:27.894: INFO: Got endpoints: latency-svc-s6jg7 [736.251654ms]
Nov 29 18:31:27.933: INFO: Created: latency-svc-7tj9l
Nov 29 18:31:27.933: INFO: Got endpoints: latency-svc-rmzhb [742.909991ms]
Nov 29 18:31:27.967: INFO: Created: latency-svc-m4v98
Nov 29 18:31:27.977: INFO: Got endpoints: latency-svc-8hwcc [749.244657ms]
Nov 29 18:31:27.988: INFO: Created: latency-svc-8cm48
Nov 29 18:31:28.031: INFO: Got endpoints: latency-svc-4g5p4 [749.889591ms]
Nov 29 18:31:28.052: INFO: Created: latency-svc-vkjmm
Nov 29 18:31:28.075: INFO: Got endpoints: latency-svc-xhmlg [726.623376ms]
Nov 29 18:31:28.098: INFO: Created: latency-svc-kxtn5
Nov 29 18:31:28.138: INFO: Got endpoints: latency-svc-xrq48 [758.205698ms]
Nov 29 18:31:28.148: INFO: Created: latency-svc-hkwtw
Nov 29 18:31:28.175: INFO: Got endpoints: latency-svc-7w8d8 [749.219545ms]
Nov 29 18:31:28.211: INFO: Created: latency-svc-xvdx7
Nov 29 18:31:28.229: INFO: Got endpoints: latency-svc-cwv9v [753.356363ms]
Nov 29 18:31:28.249: INFO: Created: latency-svc-588sz
Nov 29 18:31:28.309: INFO: Got endpoints: latency-svc-r7j2p [783.32288ms]
Nov 29 18:31:28.322: INFO: Created: latency-svc-c2cfk
Nov 29 18:31:28.325: INFO: Got endpoints: latency-svc-qwjhc [675.32757ms]
Nov 29 18:31:28.372: INFO: Created: latency-svc-sv57j
Nov 29 18:31:28.376: INFO: Got endpoints: latency-svc-q2phg [725.88467ms]
Nov 29 18:31:28.421: INFO: Created: latency-svc-bqpvs
Nov 29 18:31:28.430: INFO: Got endpoints: latency-svc-s42sc [745.099954ms]
Nov 29 18:31:28.441: INFO: Created: latency-svc-4dr8g
Nov 29 18:31:28.484: INFO: Got endpoints: latency-svc-pb4z5 [746.681216ms]
Nov 29 18:31:28.495: INFO: Created: latency-svc-lmr8f
Nov 29 18:31:28.526: INFO: Got endpoints: latency-svc-v28bh [750.291805ms]
Nov 29 18:31:28.536: INFO: Created: latency-svc-qn6wj
Nov 29 18:31:28.576: INFO: Got endpoints: latency-svc-8fvqt [750.053865ms]
Nov 29 18:31:28.601: INFO: Created: latency-svc-fqtsq
Nov 29 18:31:28.626: INFO: Got endpoints: latency-svc-7tj9l [732.21233ms]
Nov 29 18:31:28.652: INFO: Created: latency-svc-zn9tw
Nov 29 18:31:28.678: INFO: Got endpoints: latency-svc-m4v98 [744.926537ms]
Nov 29 18:31:28.701: INFO: Created: latency-svc-jwhvr
Nov 29 18:31:28.726: INFO: Got endpoints: latency-svc-8cm48 [748.905907ms]
Nov 29 18:31:28.742: INFO: Created: latency-svc-djqtf
Nov 29 18:31:28.776: INFO: Got endpoints: latency-svc-vkjmm [745.478753ms]
Nov 29 18:31:28.805: INFO: Created: latency-svc-w9qnv
Nov 29 18:31:28.830: INFO: Got endpoints: latency-svc-kxtn5 [754.498666ms]
Nov 29 18:31:28.874: INFO: Created: latency-svc-45gmx
Nov 29 18:31:28.876: INFO: Got endpoints: latency-svc-hkwtw [737.858617ms]
Nov 29 18:31:28.888: INFO: Created: latency-svc-82f2n
Nov 29 18:31:28.927: INFO: Got endpoints: latency-svc-xvdx7 [751.735114ms]
Nov 29 18:31:28.964: INFO: Created: latency-svc-nt8p5
Nov 29 18:31:28.980: INFO: Got endpoints: latency-svc-588sz [750.354574ms]
Nov 29 18:31:29.016: INFO: Created: latency-svc-vn9bd
Nov 29 18:31:29.032: INFO: Got endpoints: latency-svc-c2cfk [723.095776ms]
Nov 29 18:31:29.045: INFO: Created: latency-svc-7r8j9
Nov 29 18:31:29.076: INFO: Got endpoints: latency-svc-sv57j [750.846104ms]
Nov 29 18:31:29.089: INFO: Created: latency-svc-fgq9c
Nov 29 18:31:29.126: INFO: Got endpoints: latency-svc-bqpvs [749.97362ms]
Nov 29 18:31:29.176: INFO: Got endpoints: latency-svc-4dr8g [745.737698ms]
Nov 29 18:31:29.178: INFO: Created: latency-svc-scmpc
Nov 29 18:31:29.210: INFO: Created: latency-svc-wl4xh
Nov 29 18:31:29.226: INFO: Got endpoints: latency-svc-lmr8f [741.404092ms]
Nov 29 18:31:29.251: INFO: Created: latency-svc-fvw7j
Nov 29 18:31:29.282: INFO: Got endpoints: latency-svc-qn6wj [756.477887ms]
Nov 29 18:31:29.294: INFO: Created: latency-svc-pxv7c
Nov 29 18:31:29.326: INFO: Got endpoints: latency-svc-fqtsq [749.852886ms]
Nov 29 18:31:29.429: INFO: Got endpoints: latency-svc-jwhvr [751.244693ms]
Nov 29 18:31:29.429: INFO: Got endpoints: latency-svc-zn9tw [803.39546ms]
Nov 29 18:31:29.433: INFO: Created: latency-svc-b6wtm
Nov 29 18:31:29.464: INFO: Created: latency-svc-2f9zz
Nov 29 18:31:29.471: INFO: Created: latency-svc-hns6j
Nov 29 18:31:29.476: INFO: Got endpoints: latency-svc-djqtf [749.996152ms]
Nov 29 18:31:29.511: INFO: Created: latency-svc-9jr9x
Nov 29 18:31:29.526: INFO: Got endpoints: latency-svc-w9qnv [749.495262ms]
Nov 29 18:31:29.560: INFO: Created: latency-svc-757r5
Nov 29 18:31:29.576: INFO: Got endpoints: latency-svc-45gmx [745.942322ms]
Nov 29 18:31:29.589: INFO: Created: latency-svc-q82n8
Nov 29 18:31:29.627: INFO: Got endpoints: latency-svc-82f2n [751.704316ms]
Nov 29 18:31:29.638: INFO: Created: latency-svc-4524h
Nov 29 18:31:29.677: INFO: Got endpoints: latency-svc-nt8p5 [749.842648ms]
Nov 29 18:31:29.689: INFO: Created: latency-svc-zw5dk
Nov 29 18:31:29.726: INFO: Got endpoints: latency-svc-vn9bd [746.198549ms]
Nov 29 18:31:29.739: INFO: Created: latency-svc-5j44p
Nov 29 18:31:29.788: INFO: Got endpoints: latency-svc-7r8j9 [755.105008ms]
Nov 29 18:31:29.798: INFO: Created: latency-svc-59n5l
Nov 29 18:31:29.828: INFO: Got endpoints: latency-svc-fgq9c [751.651723ms]
Nov 29 18:31:29.839: INFO: Created: latency-svc-fplf8
Nov 29 18:31:29.876: INFO: Got endpoints: latency-svc-scmpc [750.279973ms]
Nov 29 18:31:29.897: INFO: Created: latency-svc-xkxkr
Nov 29 18:31:29.926: INFO: Got endpoints: latency-svc-wl4xh [749.934552ms]
Nov 29 18:31:29.974: INFO: Created: latency-svc-9mh2q
Nov 29 18:31:29.980: INFO: Got endpoints: latency-svc-fvw7j [754.099821ms]
Nov 29 18:31:29.990: INFO: Created: latency-svc-9wckh
Nov 29 18:31:30.033: INFO: Got endpoints: latency-svc-pxv7c [750.551451ms]
Nov 29 18:31:30.044: INFO: Created: latency-svc-6wbs2
Nov 29 18:31:30.076: INFO: Got endpoints: latency-svc-b6wtm [749.755084ms]
Nov 29 18:31:30.086: INFO: Created: latency-svc-ts2kf
Nov 29 18:31:30.126: INFO: Got endpoints: latency-svc-2f9zz [696.66581ms]
Nov 29 18:31:30.141: INFO: Created: latency-svc-8bthg
Nov 29 18:31:30.176: INFO: Got endpoints: latency-svc-hns6j [746.538961ms]
Nov 29 18:31:30.188: INFO: Created: latency-svc-jdgw7
Nov 29 18:31:30.227: INFO: Got endpoints: latency-svc-9jr9x [751.652101ms]
Nov 29 18:31:30.239: INFO: Created: latency-svc-22r4r
Nov 29 18:31:30.276: INFO: Got endpoints: latency-svc-757r5 [750.314673ms]
Nov 29 18:31:30.286: INFO: Created: latency-svc-8l88d
Nov 29 18:31:30.327: INFO: Got endpoints: latency-svc-q82n8 [751.598498ms]
Nov 29 18:31:30.337: INFO: Created: latency-svc-4wk46
Nov 29 18:31:30.376: INFO: Got endpoints: latency-svc-4524h [748.888065ms]
Nov 29 18:31:30.395: INFO: Created: latency-svc-dk2nz
Nov 29 18:31:30.428: INFO: Got endpoints: latency-svc-zw5dk [750.66972ms]
Nov 29 18:31:30.439: INFO: Created: latency-svc-jqlkd
Nov 29 18:31:30.486: INFO: Got endpoints: latency-svc-5j44p [759.962305ms]
Nov 29 18:31:30.497: INFO: Created: latency-svc-p2ldq
Nov 29 18:31:30.526: INFO: Got endpoints: latency-svc-59n5l [738.757139ms]
Nov 29 18:31:30.537: INFO: Created: latency-svc-6wvpm
Nov 29 18:31:30.580: INFO: Got endpoints: latency-svc-fplf8 [752.821746ms]
Nov 29 18:31:30.614: INFO: Created: latency-svc-ktb7z
Nov 29 18:31:30.626: INFO: Got endpoints: latency-svc-xkxkr [749.529217ms]
Nov 29 18:31:30.638: INFO: Created: latency-svc-7ffpn
Nov 29 18:31:30.677: INFO: Got endpoints: latency-svc-9mh2q [751.702912ms]
Nov 29 18:31:30.688: INFO: Created: latency-svc-bjxk2
Nov 29 18:31:30.730: INFO: Got endpoints: latency-svc-9wckh [750.109032ms]
Nov 29 18:31:30.740: INFO: Created: latency-svc-sshs2
Nov 29 18:31:30.776: INFO: Got endpoints: latency-svc-6wbs2 [742.604894ms]
Nov 29 18:31:30.812: INFO: Created: latency-svc-cwsrt
Nov 29 18:31:30.831: INFO: Got endpoints: latency-svc-ts2kf [755.420572ms]
Nov 29 18:31:30.842: INFO: Created: latency-svc-2528p
Nov 29 18:31:30.878: INFO: Got endpoints: latency-svc-8bthg [752.13316ms]
Nov 29 18:31:30.889: INFO: Created: latency-svc-9rk2n
Nov 29 18:31:30.926: INFO: Got endpoints: latency-svc-jdgw7 [749.920147ms]
Nov 29 18:31:30.957: INFO: Created: latency-svc-nl8rw
Nov 29 18:31:30.976: INFO: Got endpoints: latency-svc-22r4r [748.459555ms]
Nov 29 18:31:30.987: INFO: Created: latency-svc-pxcx7
Nov 29 18:31:31.027: INFO: Got endpoints: latency-svc-8l88d [751.226836ms]
Nov 29 18:31:31.038: INFO: Created: latency-svc-c6qp5
Nov 29 18:31:31.076: INFO: Got endpoints: latency-svc-4wk46 [748.625522ms]
Nov 29 18:31:31.089: INFO: Created: latency-svc-jm9bp
Nov 29 18:31:31.130: INFO: Got endpoints: latency-svc-dk2nz [753.985862ms]
Nov 29 18:31:31.142: INFO: Created: latency-svc-nj2dh
Nov 29 18:31:31.176: INFO: Got endpoints: latency-svc-jqlkd [748.381263ms]
Nov 29 18:31:31.189: INFO: Created: latency-svc-qnkrl
Nov 29 18:31:31.226: INFO: Got endpoints: latency-svc-p2ldq [739.966748ms]
Nov 29 18:31:31.236: INFO: Created: latency-svc-wjsdj
Nov 29 18:31:31.284: INFO: Got endpoints: latency-svc-6wvpm [757.299699ms]
Nov 29 18:31:31.294: INFO: Created: latency-svc-k2zr6
Nov 29 18:31:31.330: INFO: Got endpoints: latency-svc-ktb7z [749.821987ms]
Nov 29 18:31:31.343: INFO: Created: latency-svc-kpxpf
Nov 29 18:31:31.377: INFO: Got endpoints: latency-svc-7ffpn [751.685944ms]
Nov 29 18:31:31.420: INFO: Created: latency-svc-w4mwp
Nov 29 18:31:31.426: INFO: Got endpoints: latency-svc-bjxk2 [748.0957ms]
Nov 29 18:31:31.437: INFO: Created: latency-svc-tzpzn
Nov 29 18:31:31.511: INFO: Got endpoints: latency-svc-sshs2 [781.394025ms]
Nov 29 18:31:31.527: INFO: Got endpoints: latency-svc-cwsrt [750.905771ms]
Nov 29 18:31:31.528: INFO: Created: latency-svc-hjwhf
Nov 29 18:31:31.542: INFO: Created: latency-svc-9n58q
Nov 29 18:31:31.610: INFO: Got endpoints: latency-svc-2528p [778.666599ms]
Nov 29 18:31:31.623: INFO: Created: latency-svc-7wklq
Nov 29 18:31:31.633: INFO: Got endpoints: latency-svc-9rk2n [755.296424ms]
Nov 29 18:31:31.644: INFO: Created: latency-svc-rwcqp
Nov 29 18:31:31.676: INFO: Got endpoints: latency-svc-nl8rw [749.840072ms]
Nov 29 18:31:31.686: INFO: Created: latency-svc-8zqq5
Nov 29 18:31:31.726: INFO: Got endpoints: latency-svc-pxcx7 [749.688781ms]
Nov 29 18:31:31.746: INFO: Created: latency-svc-gm7d4
Nov 29 18:31:31.777: INFO: Got endpoints: latency-svc-c6qp5 [749.572253ms]
Nov 29 18:31:31.787: INFO: Created: latency-svc-wf7cl
Nov 29 18:31:31.826: INFO: Got endpoints: latency-svc-jm9bp [749.739165ms]
Nov 29 18:31:31.841: INFO: Created: latency-svc-7xr77
Nov 29 18:31:31.876: INFO: Got endpoints: latency-svc-nj2dh [745.571806ms]
Nov 29 18:31:31.888: INFO: Created: latency-svc-tl7ck
Nov 29 18:31:31.926: INFO: Got endpoints: latency-svc-qnkrl [749.70861ms]
Nov 29 18:31:31.939: INFO: Created: latency-svc-jvs4x
Nov 29 18:31:31.980: INFO: Got endpoints: latency-svc-wjsdj [754.025474ms]
Nov 29 18:31:31.992: INFO: Created: latency-svc-bm2b6
Nov 29 18:31:32.028: INFO: Got endpoints: latency-svc-k2zr6 [743.719252ms]
Nov 29 18:31:32.038: INFO: Created: latency-svc-5wsln
Nov 29 18:31:32.076: INFO: Got endpoints: latency-svc-kpxpf [745.543589ms]
Nov 29 18:31:32.090: INFO: Created: latency-svc-hf5qk
Nov 29 18:31:32.126: INFO: Got endpoints: latency-svc-w4mwp [748.351738ms]
Nov 29 18:31:32.140: INFO: Created: latency-svc-gbng4
Nov 29 18:31:32.177: INFO: Got endpoints: latency-svc-tzpzn [751.719401ms]
Nov 29 18:31:32.188: INFO: Created: latency-svc-4sspr
Nov 29 18:31:32.225: INFO: Got endpoints: latency-svc-hjwhf [713.718617ms]
Nov 29 18:31:32.239: INFO: Created: latency-svc-rk2bs
Nov 29 18:31:32.292: INFO: Got endpoints: latency-svc-9n58q [764.907902ms]
Nov 29 18:31:32.302: INFO: Created: latency-svc-fmqvg
Nov 29 18:31:32.326: INFO: Got endpoints: latency-svc-7wklq [715.890045ms]
Nov 29 18:31:32.336: INFO: Created: latency-svc-nvvvn
Nov 29 18:31:32.376: INFO: Got endpoints: latency-svc-rwcqp [742.8741ms]
Nov 29 18:31:32.387: INFO: Created: latency-svc-tzx5s
Nov 29 18:31:32.430: INFO: Got endpoints: latency-svc-8zqq5 [754.50765ms]
Nov 29 18:31:32.441: INFO: Created: latency-svc-tbnt2
Nov 29 18:31:32.476: INFO: Got endpoints: latency-svc-gm7d4 [750.613738ms]
Nov 29 18:31:32.488: INFO: Created: latency-svc-77b28
Nov 29 18:31:32.526: INFO: Got endpoints: latency-svc-wf7cl [748.745998ms]
Nov 29 18:31:32.536: INFO: Created: latency-svc-knnf7
Nov 29 18:31:32.578: INFO: Got endpoints: latency-svc-7xr77 [752.042621ms]
Nov 29 18:31:32.597: INFO: Created: latency-svc-26zpp
Nov 29 18:31:32.630: INFO: Got endpoints: latency-svc-tl7ck [753.752702ms]
Nov 29 18:31:32.641: INFO: Created: latency-svc-tdctn
Nov 29 18:31:32.676: INFO: Got endpoints: latency-svc-jvs4x [749.923387ms]
Nov 29 18:31:32.689: INFO: Created: latency-svc-r6rcc
Nov 29 18:31:32.726: INFO: Got endpoints: latency-svc-bm2b6 [746.240206ms]
Nov 29 18:31:32.736: INFO: Created: latency-svc-t2c5g
Nov 29 18:31:32.780: INFO: Got endpoints: latency-svc-5wsln [752.713675ms]
Nov 29 18:31:32.791: INFO: Created: latency-svc-6zpkn
Nov 29 18:31:32.826: INFO: Got endpoints: latency-svc-hf5qk [749.915354ms]
Nov 29 18:31:32.846: INFO: Created: latency-svc-tbgcz
Nov 29 18:31:32.876: INFO: Got endpoints: latency-svc-gbng4 [750.358489ms]
Nov 29 18:31:32.886: INFO: Created: latency-svc-6bgt5
Nov 29 18:31:32.928: INFO: Got endpoints: latency-svc-4sspr [750.173159ms]
Nov 29 18:31:33.012: INFO: Created: latency-svc-pwwfl
Nov 29 18:31:33.013: INFO: Got endpoints: latency-svc-rk2bs [788.069346ms]
Nov 29 18:31:33.025: INFO: Created: latency-svc-c2nlq
Nov 29 18:31:33.026: INFO: Got endpoints: latency-svc-fmqvg [734.280796ms]
Nov 29 18:31:33.039: INFO: Created: latency-svc-kn6fh
Nov 29 18:31:33.076: INFO: Got endpoints: latency-svc-nvvvn [750.13386ms]
Nov 29 18:31:33.089: INFO: Created: latency-svc-k2ft9
Nov 29 18:31:33.127: INFO: Got endpoints: latency-svc-tzx5s [751.15081ms]
Nov 29 18:31:33.154: INFO: Created: latency-svc-2c7hg
Nov 29 18:31:33.180: INFO: Got endpoints: latency-svc-tbnt2 [749.514155ms]
Nov 29 18:31:33.226: INFO: Got endpoints: latency-svc-77b28 [749.423087ms]
Nov 29 18:31:33.281: INFO: Got endpoints: latency-svc-knnf7 [755.417008ms]
Nov 29 18:31:33.325: INFO: Got endpoints: latency-svc-26zpp [747.49777ms]
Nov 29 18:31:33.377: INFO: Got endpoints: latency-svc-tdctn [747.53649ms]
Nov 29 18:31:33.428: INFO: Got endpoints: latency-svc-r6rcc [751.774021ms]
Nov 29 18:31:33.476: INFO: Got endpoints: latency-svc-t2c5g [749.194332ms]
Nov 29 18:31:33.526: INFO: Got endpoints: latency-svc-6zpkn [745.873664ms]
Nov 29 18:31:33.576: INFO: Got endpoints: latency-svc-tbgcz [749.802278ms]
Nov 29 18:31:33.630: INFO: Got endpoints: latency-svc-6bgt5 [754.186914ms]
Nov 29 18:31:33.676: INFO: Got endpoints: latency-svc-pwwfl [748.240601ms]
Nov 29 18:31:33.730: INFO: Got endpoints: latency-svc-c2nlq [716.992672ms]
Nov 29 18:31:33.778: INFO: Got endpoints: latency-svc-kn6fh [751.790213ms]
Nov 29 18:31:33.830: INFO: Got endpoints: latency-svc-k2ft9 [754.343092ms]
Nov 29 18:31:33.876: INFO: Got endpoints: latency-svc-2c7hg [748.496942ms]
Nov 29 18:31:33.876: INFO: Latencies: [28.405442ms 43.920651ms 154.922149ms 174.040069ms 181.278543ms 213.741119ms 235.750111ms 238.439593ms 249.829402ms 259.300536ms 267.037736ms 273.933547ms 275.069985ms 280.192428ms 283.725131ms 284.996685ms 285.217582ms 289.124169ms 295.95215ms 308.971761ms 312.192669ms 316.994178ms 325.64591ms 335.062885ms 336.487552ms 337.633548ms 337.720546ms 361.619732ms 384.83124ms 386.380938ms 388.372971ms 389.274388ms 391.223168ms 395.522383ms 396.585166ms 400.716603ms 408.442537ms 411.89001ms 415.936186ms 416.336168ms 416.825781ms 418.689835ms 420.259611ms 422.737272ms 431.919611ms 437.940416ms 441.486575ms 441.557685ms 443.297899ms 443.722295ms 445.06983ms 458.357278ms 459.084586ms 460.090864ms 462.236316ms 465.649046ms 465.939647ms 478.835179ms 514.236811ms 535.131894ms 535.907976ms 536.657448ms 544.494294ms 568.970027ms 571.30954ms 577.641498ms 589.427189ms 591.616166ms 599.353629ms 603.444134ms 603.862307ms 614.949032ms 616.286036ms 630.011466ms 630.014242ms 639.864266ms 642.255766ms 661.366265ms 664.078686ms 675.32757ms 696.66581ms 713.718617ms 715.890045ms 716.992672ms 723.095776ms 725.88467ms 726.623376ms 732.21233ms 734.280796ms 736.251654ms 737.858617ms 738.757139ms 739.966748ms 741.404092ms 742.604894ms 742.8741ms 742.909991ms 743.719252ms 744.926537ms 745.099954ms 745.478753ms 745.543589ms 745.571806ms 745.737698ms 745.873664ms 745.942322ms 746.198549ms 746.240206ms 746.538961ms 746.681216ms 747.49777ms 747.53649ms 748.0957ms 748.240601ms 748.351738ms 748.381263ms 748.459555ms 748.496942ms 748.625522ms 748.745998ms 748.888065ms 748.905907ms 749.194332ms 749.219545ms 749.244657ms 749.423087ms 749.495262ms 749.514155ms 749.529217ms 749.572253ms 749.688781ms 749.70861ms 749.739165ms 749.755084ms 749.802278ms 749.821987ms 749.840072ms 749.842648ms 749.852886ms 749.889591ms 749.915354ms 749.920147ms 749.923387ms 749.934552ms 749.97362ms 749.996152ms 750.053865ms 750.109032ms 750.13386ms 750.173159ms 750.279973ms 750.291805ms 750.314673ms 750.354574ms 750.358489ms 750.551451ms 750.613738ms 750.66972ms 750.846104ms 750.905771ms 751.15081ms 751.226836ms 751.244693ms 751.598498ms 751.651723ms 751.652101ms 751.685944ms 751.702912ms 751.704316ms 751.719401ms 751.735114ms 751.774021ms 751.790213ms 752.042621ms 752.13316ms 752.713675ms 752.821746ms 753.356363ms 753.752702ms 753.985862ms 754.025474ms 754.099821ms 754.186914ms 754.343092ms 754.498666ms 754.50765ms 755.105008ms 755.296424ms 755.417008ms 755.420572ms 756.477887ms 757.299699ms 758.205698ms 759.962305ms 764.907902ms 778.666599ms 781.394025ms 783.32288ms 788.069346ms 803.39546ms]
Nov 29 18:31:33.876: INFO: 50 %ile: 745.478753ms
Nov 29 18:31:33.876: INFO: 90 %ile: 754.025474ms
Nov 29 18:31:33.876: INFO: 99 %ile: 788.069346ms
Nov 29 18:31:33.876: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:31:33.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1220" for this suite.

• [SLOW TEST:10.787 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":140,"skipped":2142,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:31:33.886: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:31:33.915: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 29 18:31:33.925: INFO: Number of nodes with available pods: 0
Nov 29 18:31:33.925: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:31:34.931: INFO: Number of nodes with available pods: 0
Nov 29 18:31:34.931: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:31:35.930: INFO: Number of nodes with available pods: 2
Nov 29 18:31:35.930: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 29 18:31:35.949: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:35.949: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:36.959: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:36.959: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:38.050: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:38.050: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:38.959: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:38.959: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:38.959: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:39.963: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:39.963: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:39.963: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:40.959: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:40.959: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:40.959: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:41.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:41.958: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:41.958: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:42.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:42.958: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:42.958: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:43.959: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:43.959: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:43.959: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:44.962: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:44.962: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:44.962: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:45.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:45.958: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:45.958: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:46.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:46.958: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:46.958: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:47.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:47.958: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:47.958: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:48.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:48.958: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:48.958: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:49.959: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:49.959: INFO: Wrong image for pod: daemon-set-wz6jj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:49.959: INFO: Pod daemon-set-wz6jj is not available
Nov 29 18:31:50.958: INFO: Pod daemon-set-kdjmv is not available
Nov 29 18:31:50.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:51.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:52.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:52.958: INFO: Pod daemon-set-rkcwl is not available
Nov 29 18:31:53.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:53.958: INFO: Pod daemon-set-rkcwl is not available
Nov 29 18:31:54.959: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:54.959: INFO: Pod daemon-set-rkcwl is not available
Nov 29 18:31:55.959: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:55.959: INFO: Pod daemon-set-rkcwl is not available
Nov 29 18:31:56.959: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:56.959: INFO: Pod daemon-set-rkcwl is not available
Nov 29 18:31:57.958: INFO: Wrong image for pod: daemon-set-rkcwl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 18:31:57.958: INFO: Pod daemon-set-rkcwl is not available
Nov 29 18:31:58.958: INFO: Pod daemon-set-qmcf6 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 29 18:31:58.965: INFO: Number of nodes with available pods: 1
Nov 29 18:31:58.965: INFO: Node k8sconformance-m02 is running more than one daemon pod
Nov 29 18:31:59.971: INFO: Number of nodes with available pods: 2
Nov 29 18:31:59.971: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7447, will wait for the garbage collector to delete the pods
Nov 29 18:32:00.039: INFO: Deleting DaemonSet.extensions daemon-set took: 5.23606ms
Nov 29 18:32:00.439: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.24989ms
Nov 29 18:32:10.242: INFO: Number of nodes with available pods: 0
Nov 29 18:32:10.242: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 18:32:10.244: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7447/daemonsets","resourceVersion":"11101"},"items":null}

Nov 29 18:32:10.245: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7447/pods","resourceVersion":"11101"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:32:10.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7447" for this suite.

• [SLOW TEST:36.370 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":141,"skipped":2158,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:32:10.256: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:32:26.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8578" for this suite.

• [SLOW TEST:16.086 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":142,"skipped":2164,"failed":0}
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:32:26.342: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-b2edfcf3-f9ab-4c3f-922f-33a45b8fea01
STEP: Creating configMap with name cm-test-opt-upd-f6124777-1c09-4030-ae3b-2f73003df66b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b2edfcf3-f9ab-4c3f-922f-33a45b8fea01
STEP: Updating configmap cm-test-opt-upd-f6124777-1c09-4030-ae3b-2f73003df66b
STEP: Creating configMap with name cm-test-opt-create-056bee84-eaca-42cc-b82a-4fd4073a5d00
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:33:52.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7857" for this suite.

• [SLOW TEST:86.368 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":143,"skipped":2164,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:33:52.710: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 29 18:33:52.781: INFO: Waiting up to 5m0s for pod "pod-79c6f4bf-efb4-4010-b9df-4cbeff2f01c1" in namespace "emptydir-1724" to be "Succeeded or Failed"
Nov 29 18:33:52.782: INFO: Pod "pod-79c6f4bf-efb4-4010-b9df-4cbeff2f01c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.583327ms
Nov 29 18:33:54.785: INFO: Pod "pod-79c6f4bf-efb4-4010-b9df-4cbeff2f01c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004536999s
Nov 29 18:33:56.788: INFO: Pod "pod-79c6f4bf-efb4-4010-b9df-4cbeff2f01c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007678304s
STEP: Saw pod success
Nov 29 18:33:56.788: INFO: Pod "pod-79c6f4bf-efb4-4010-b9df-4cbeff2f01c1" satisfied condition "Succeeded or Failed"
Nov 29 18:33:56.790: INFO: Trying to get logs from node k8sconformance-m02 pod pod-79c6f4bf-efb4-4010-b9df-4cbeff2f01c1 container test-container: <nil>
STEP: delete the pod
Nov 29 18:33:56.808: INFO: Waiting for pod pod-79c6f4bf-efb4-4010-b9df-4cbeff2f01c1 to disappear
Nov 29 18:33:56.810: INFO: Pod pod-79c6f4bf-efb4-4010-b9df-4cbeff2f01c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:33:56.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1724" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":144,"skipped":2170,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:33:56.815: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:33:58.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6191" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2176,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:33:58.860: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:34:00.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7426" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":146,"skipped":2179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:34:00.899: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Nov 29 18:34:00.921: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Nov 29 18:34:01.283: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 29 18:34:03.320: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271641, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271641, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271641, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271641, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 18:34:06.139: INFO: Waited 811.464198ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:34:06.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3873" for this suite.

• [SLOW TEST:5.917 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":147,"skipped":2203,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:34:06.817: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 29 18:34:06.852: INFO: Waiting up to 5m0s for pod "pod-73cb00ed-6a10-4425-8b62-2ca5fa1551dc" in namespace "emptydir-7691" to be "Succeeded or Failed"
Nov 29 18:34:06.911: INFO: Pod "pod-73cb00ed-6a10-4425-8b62-2ca5fa1551dc": Phase="Pending", Reason="", readiness=false. Elapsed: 58.879458ms
Nov 29 18:34:08.914: INFO: Pod "pod-73cb00ed-6a10-4425-8b62-2ca5fa1551dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.061983944s
STEP: Saw pod success
Nov 29 18:34:08.914: INFO: Pod "pod-73cb00ed-6a10-4425-8b62-2ca5fa1551dc" satisfied condition "Succeeded or Failed"
Nov 29 18:34:08.916: INFO: Trying to get logs from node k8sconformance-m02 pod pod-73cb00ed-6a10-4425-8b62-2ca5fa1551dc container test-container: <nil>
STEP: delete the pod
Nov 29 18:34:08.941: INFO: Waiting for pod pod-73cb00ed-6a10-4425-8b62-2ca5fa1551dc to disappear
Nov 29 18:34:08.943: INFO: Pod pod-73cb00ed-6a10-4425-8b62-2ca5fa1551dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:34:08.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7691" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2268,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:34:08.949: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Nov 29 18:34:08.973: INFO: Created pod &Pod{ObjectMeta:{dns-8647  dns-8647 /api/v1/namespaces/dns-8647/pods/dns-8647 ed66b6e2-8c31-401c-8431-d657928526ae 11507 0 2020-11-29 18:34:08 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-11-29 18:34:08 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6nxjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6nxjb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6nxjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:34:08.975: INFO: The status of Pod dns-8647 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 18:34:10.978: INFO: The status of Pod dns-8647 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Nov 29 18:34:10.978: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8647 PodName:dns-8647 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:34:10.978: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Verifying customized DNS server is configured on pod...
Nov 29 18:34:11.073: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8647 PodName:dns-8647 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:34:11.073: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:34:11.152: INFO: Deleting pod dns-8647...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:34:11.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8647" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":149,"skipped":2285,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:34:11.169: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-7367
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7367 to expose endpoints map[]
Nov 29 18:34:11.320: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Nov 29 18:34:12.326: INFO: successfully validated that service endpoint-test2 in namespace services-7367 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7367
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7367 to expose endpoints map[pod1:[80]]
Nov 29 18:34:14.342: INFO: successfully validated that service endpoint-test2 in namespace services-7367 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-7367
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7367 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 29 18:34:16.360: INFO: successfully validated that service endpoint-test2 in namespace services-7367 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-7367
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7367 to expose endpoints map[pod2:[80]]
Nov 29 18:34:16.375: INFO: successfully validated that service endpoint-test2 in namespace services-7367 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-7367
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7367 to expose endpoints map[]
Nov 29 18:34:17.389: INFO: successfully validated that service endpoint-test2 in namespace services-7367 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:34:17.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7367" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.244 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":150,"skipped":2299,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:34:17.414: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 29 18:34:17.481: INFO: Number of nodes with available pods: 0
Nov 29 18:34:17.481: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:18.488: INFO: Number of nodes with available pods: 0
Nov 29 18:34:18.488: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:19.486: INFO: Number of nodes with available pods: 2
Nov 29 18:34:19.486: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 29 18:34:19.497: INFO: Number of nodes with available pods: 1
Nov 29 18:34:19.498: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:20.503: INFO: Number of nodes with available pods: 1
Nov 29 18:34:20.503: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:21.503: INFO: Number of nodes with available pods: 1
Nov 29 18:34:21.503: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:22.502: INFO: Number of nodes with available pods: 1
Nov 29 18:34:22.502: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:23.504: INFO: Number of nodes with available pods: 1
Nov 29 18:34:23.504: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:24.504: INFO: Number of nodes with available pods: 2
Nov 29 18:34:24.504: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2620, will wait for the garbage collector to delete the pods
Nov 29 18:34:24.562: INFO: Deleting DaemonSet.extensions daemon-set took: 4.982807ms
Nov 29 18:34:24.963: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.224809ms
Nov 29 18:34:38.065: INFO: Number of nodes with available pods: 0
Nov 29 18:34:38.065: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 18:34:38.068: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2620/daemonsets","resourceVersion":"11729"},"items":null}

Nov 29 18:34:38.069: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2620/pods","resourceVersion":"11729"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:34:38.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2620" for this suite.

• [SLOW TEST:20.666 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":151,"skipped":2314,"failed":0}
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:34:38.080: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-8691/configmap-test-eb240ec7-2726-4e60-9828-a4ac17a25ba3
STEP: Creating a pod to test consume configMaps
Nov 29 18:34:38.108: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c0dd11b-d51a-4c11-89f8-cc38a4f971f2" in namespace "configmap-8691" to be "Succeeded or Failed"
Nov 29 18:34:38.112: INFO: Pod "pod-configmaps-7c0dd11b-d51a-4c11-89f8-cc38a4f971f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052555ms
Nov 29 18:34:40.115: INFO: Pod "pod-configmaps-7c0dd11b-d51a-4c11-89f8-cc38a4f971f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006706219s
Nov 29 18:34:42.118: INFO: Pod "pod-configmaps-7c0dd11b-d51a-4c11-89f8-cc38a4f971f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010027909s
STEP: Saw pod success
Nov 29 18:34:42.118: INFO: Pod "pod-configmaps-7c0dd11b-d51a-4c11-89f8-cc38a4f971f2" satisfied condition "Succeeded or Failed"
Nov 29 18:34:42.121: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-7c0dd11b-d51a-4c11-89f8-cc38a4f971f2 container env-test: <nil>
STEP: delete the pod
Nov 29 18:34:42.135: INFO: Waiting for pod pod-configmaps-7c0dd11b-d51a-4c11-89f8-cc38a4f971f2 to disappear
Nov 29 18:34:42.137: INFO: Pod pod-configmaps-7c0dd11b-d51a-4c11-89f8-cc38a4f971f2 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:34:42.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8691" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":152,"skipped":2314,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:34:42.145: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:34:42.178: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 29 18:34:42.183: INFO: Number of nodes with available pods: 0
Nov 29 18:34:42.183: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 29 18:34:42.198: INFO: Number of nodes with available pods: 0
Nov 29 18:34:42.199: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:43.202: INFO: Number of nodes with available pods: 0
Nov 29 18:34:43.202: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:44.201: INFO: Number of nodes with available pods: 1
Nov 29 18:34:44.202: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 29 18:34:44.215: INFO: Number of nodes with available pods: 1
Nov 29 18:34:44.215: INFO: Number of running nodes: 0, number of available pods: 1
Nov 29 18:34:45.218: INFO: Number of nodes with available pods: 0
Nov 29 18:34:45.218: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 29 18:34:45.224: INFO: Number of nodes with available pods: 0
Nov 29 18:34:45.224: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:46.304: INFO: Number of nodes with available pods: 0
Nov 29 18:34:46.304: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:47.226: INFO: Number of nodes with available pods: 0
Nov 29 18:34:47.226: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:48.227: INFO: Number of nodes with available pods: 0
Nov 29 18:34:48.227: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:49.227: INFO: Number of nodes with available pods: 0
Nov 29 18:34:49.227: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:50.227: INFO: Number of nodes with available pods: 0
Nov 29 18:34:50.227: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:51.227: INFO: Number of nodes with available pods: 0
Nov 29 18:34:51.227: INFO: Node k8sconformance is running more than one daemon pod
Nov 29 18:34:52.227: INFO: Number of nodes with available pods: 1
Nov 29 18:34:52.227: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2735, will wait for the garbage collector to delete the pods
Nov 29 18:34:52.287: INFO: Deleting DaemonSet.extensions daemon-set took: 4.48366ms
Nov 29 18:34:52.387: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.314379ms
Nov 29 18:35:00.190: INFO: Number of nodes with available pods: 0
Nov 29 18:35:00.190: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 18:35:00.192: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2735/daemonsets","resourceVersion":"11872"},"items":null}

Nov 29 18:35:00.194: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2735/pods","resourceVersion":"11872"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:35:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2735" for this suite.

• [SLOW TEST:18.067 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":153,"skipped":2316,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:35:00.212: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 29 18:35:00.242: INFO: Waiting up to 5m0s for pod "pod-e2492879-ff74-4cbd-aa44-0d7c1a66da83" in namespace "emptydir-8746" to be "Succeeded or Failed"
Nov 29 18:35:00.244: INFO: Pod "pod-e2492879-ff74-4cbd-aa44-0d7c1a66da83": Phase="Pending", Reason="", readiness=false. Elapsed: 1.752555ms
Nov 29 18:35:02.247: INFO: Pod "pod-e2492879-ff74-4cbd-aa44-0d7c1a66da83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005341403s
STEP: Saw pod success
Nov 29 18:35:02.248: INFO: Pod "pod-e2492879-ff74-4cbd-aa44-0d7c1a66da83" satisfied condition "Succeeded or Failed"
Nov 29 18:35:02.249: INFO: Trying to get logs from node k8sconformance-m02 pod pod-e2492879-ff74-4cbd-aa44-0d7c1a66da83 container test-container: <nil>
STEP: delete the pod
Nov 29 18:35:02.266: INFO: Waiting for pod pod-e2492879-ff74-4cbd-aa44-0d7c1a66da83 to disappear
Nov 29 18:35:02.268: INFO: Pod pod-e2492879-ff74-4cbd-aa44-0d7c1a66da83 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:35:02.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8746" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":154,"skipped":2337,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:35:02.273: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 29 18:35:05.315: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:35:05.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2624" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":155,"skipped":2342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:35:05.352: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:35:05.518: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9057b89-485c-4ae4-b8b1-c5fe4995137e" in namespace "projected-4016" to be "Succeeded or Failed"
Nov 29 18:35:05.529: INFO: Pod "downwardapi-volume-f9057b89-485c-4ae4-b8b1-c5fe4995137e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.82769ms
Nov 29 18:35:07.531: INFO: Pod "downwardapi-volume-f9057b89-485c-4ae4-b8b1-c5fe4995137e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013605968s
Nov 29 18:35:09.534: INFO: Pod "downwardapi-volume-f9057b89-485c-4ae4-b8b1-c5fe4995137e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016620442s
STEP: Saw pod success
Nov 29 18:35:09.534: INFO: Pod "downwardapi-volume-f9057b89-485c-4ae4-b8b1-c5fe4995137e" satisfied condition "Succeeded or Failed"
Nov 29 18:35:09.536: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-f9057b89-485c-4ae4-b8b1-c5fe4995137e container client-container: <nil>
STEP: delete the pod
Nov 29 18:35:09.552: INFO: Waiting for pod downwardapi-volume-f9057b89-485c-4ae4-b8b1-c5fe4995137e to disappear
Nov 29 18:35:09.554: INFO: Pod downwardapi-volume-f9057b89-485c-4ae4-b8b1-c5fe4995137e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:35:09.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4016" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":156,"skipped":2369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:35:09.560: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-f44a0778-66c5-4e23-baa0-ad459b978b74
STEP: Creating a pod to test consume configMaps
Nov 29 18:35:09.613: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-72a6ffca-7774-486b-9392-c6fe69e64210" in namespace "projected-4620" to be "Succeeded or Failed"
Nov 29 18:35:09.616: INFO: Pod "pod-projected-configmaps-72a6ffca-7774-486b-9392-c6fe69e64210": Phase="Pending", Reason="", readiness=false. Elapsed: 2.143755ms
Nov 29 18:35:11.618: INFO: Pod "pod-projected-configmaps-72a6ffca-7774-486b-9392-c6fe69e64210": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005004104s
STEP: Saw pod success
Nov 29 18:35:11.618: INFO: Pod "pod-projected-configmaps-72a6ffca-7774-486b-9392-c6fe69e64210" satisfied condition "Succeeded or Failed"
Nov 29 18:35:11.620: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-72a6ffca-7774-486b-9392-c6fe69e64210 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 18:35:11.634: INFO: Waiting for pod pod-projected-configmaps-72a6ffca-7774-486b-9392-c6fe69e64210 to disappear
Nov 29 18:35:11.636: INFO: Pod pod-projected-configmaps-72a6ffca-7774-486b-9392-c6fe69e64210 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:35:11.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4620" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2405,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:35:11.641: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-7837309c-293b-4bac-a61e-24f5babf89ad
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:35:13.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2200" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":158,"skipped":2418,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:35:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 29 18:35:13.714: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 18:35:13.730: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 18:35:13.732: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Nov 29 18:35:13.736: INFO: coredns-f9fd979d6-brw29 from kube-system started at 2020-11-29 17:56:10 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.736: INFO: 	Container coredns ready: true, restart count 0
Nov 29 18:35:13.736: INFO: etcd-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.736: INFO: 	Container etcd ready: true, restart count 0
Nov 29 18:35:13.736: INFO: kindnet-w9hf5 from kube-system started at 2020-11-29 17:56:09 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.736: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 29 18:35:13.736: INFO: kube-apiserver-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.736: INFO: 	Container kube-apiserver ready: true, restart count 0
Nov 29 18:35:13.736: INFO: kube-controller-manager-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.736: INFO: 	Container kube-controller-manager ready: true, restart count 0
Nov 29 18:35:13.736: INFO: kube-proxy-rk57s from kube-system started at 2020-11-29 17:56:09 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.736: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 18:35:13.736: INFO: kube-scheduler-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.736: INFO: 	Container kube-scheduler ready: true, restart count 0
Nov 29 18:35:13.736: INFO: storage-provisioner from kube-system started at 2020-11-29 17:56:11 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.736: INFO: 	Container storage-provisioner ready: true, restart count 0
Nov 29 18:35:13.736: INFO: sonobuoy from sonobuoy started at 2020-11-29 17:57:02 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.736: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 29 18:35:13.736: INFO: sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-bfkxd from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:35:13.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:35:13.736: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 18:35:13.736: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Nov 29 18:35:13.740: INFO: pod-configmaps-c66896dc-e3d5-4e01-bf39-7ef2f7e58906 from configmap-2200 started at 2020-11-29 18:35:11 +0000 UTC (2 container statuses recorded)
Nov 29 18:35:13.740: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Nov 29 18:35:13.740: INFO: 	Container configmap-volume-data-test ready: true, restart count 0
Nov 29 18:35:13.740: INFO: kindnet-hq29j from kube-system started at 2020-11-29 18:00:28 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.740: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 29 18:35:13.740: INFO: kube-proxy-p6zkw from kube-system started at 2020-11-29 17:56:58 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.740: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 18:35:13.740: INFO: pod-adoption-release from replicaset-2624 started at 2020-11-29 18:35:02 +0000 UTC (1 container statuses recorded)
Nov 29 18:35:13.740: INFO: 	Container pod-adoption-release ready: false, restart count 0
Nov 29 18:35:13.740: INFO: sonobuoy-e2e-job-9d13ec04340c4e3d from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:35:13.740: INFO: 	Container e2e ready: true, restart count 0
Nov 29 18:35:13.740: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:35:13.740: INFO: sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-nkv9s from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:35:13.740: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:35:13.740: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.164c0e524fd6adb0], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:35:14.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5324" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":159,"skipped":2426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:35:14.761: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-a470a4b4-24ad-43ff-ad9d-fa887f1c6d4a in namespace container-probe-2071
Nov 29 18:35:16.807: INFO: Started pod liveness-a470a4b4-24ad-43ff-ad9d-fa887f1c6d4a in namespace container-probe-2071
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 18:35:16.809: INFO: Initial restart count of pod liveness-a470a4b4-24ad-43ff-ad9d-fa887f1c6d4a is 0
Nov 29 18:35:32.840: INFO: Restart count of pod container-probe-2071/liveness-a470a4b4-24ad-43ff-ad9d-fa887f1c6d4a is now 1 (16.031099534s elapsed)
Nov 29 18:35:52.872: INFO: Restart count of pod container-probe-2071/liveness-a470a4b4-24ad-43ff-ad9d-fa887f1c6d4a is now 2 (36.062467106s elapsed)
Nov 29 18:36:12.903: INFO: Restart count of pod container-probe-2071/liveness-a470a4b4-24ad-43ff-ad9d-fa887f1c6d4a is now 3 (56.093574655s elapsed)
Nov 29 18:36:32.935: INFO: Restart count of pod container-probe-2071/liveness-a470a4b4-24ad-43ff-ad9d-fa887f1c6d4a is now 4 (1m16.12588528s elapsed)
Nov 29 18:37:41.049: INFO: Restart count of pod container-probe-2071/liveness-a470a4b4-24ad-43ff-ad9d-fa887f1c6d4a is now 5 (2m24.239917762s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:37:41.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2071" for this suite.

• [SLOW TEST:146.304 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":160,"skipped":2486,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:37:41.065: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Nov 29 18:37:41.089: INFO: Major version: 1
STEP: Confirm minor version
Nov 29 18:37:41.089: INFO: cleanMinorVersion: 19
Nov 29 18:37:41.089: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:37:41.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1528" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":161,"skipped":2500,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:37:41.094: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-0aa3a638-fd78-44db-9221-1a6c1bd66260
STEP: Creating a pod to test consume configMaps
Nov 29 18:37:41.180: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e341907-c613-4358-894b-8c11f51fa51d" in namespace "configmap-3852" to be "Succeeded or Failed"
Nov 29 18:37:41.188: INFO: Pod "pod-configmaps-8e341907-c613-4358-894b-8c11f51fa51d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.943571ms
Nov 29 18:37:43.191: INFO: Pod "pod-configmaps-8e341907-c613-4358-894b-8c11f51fa51d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010938474s
STEP: Saw pod success
Nov 29 18:37:43.191: INFO: Pod "pod-configmaps-8e341907-c613-4358-894b-8c11f51fa51d" satisfied condition "Succeeded or Failed"
Nov 29 18:37:43.193: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-8e341907-c613-4358-894b-8c11f51fa51d container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 18:37:43.233: INFO: Waiting for pod pod-configmaps-8e341907-c613-4358-894b-8c11f51fa51d to disappear
Nov 29 18:37:43.235: INFO: Pod pod-configmaps-8e341907-c613-4358-894b-8c11f51fa51d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:37:43.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3852" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":162,"skipped":2514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:37:43.242: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 29 18:37:43.263: INFO: PodSpec: initContainers in spec.initContainers
Nov 29 18:38:30.587: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ffec71e1-3c3d-4884-b385-a0862426039c", GenerateName:"", Namespace:"init-container-2397", SelfLink:"/api/v1/namespaces/init-container-2397/pods/pod-init-ffec71e1-3c3d-4884-b385-a0862426039c", UID:"e0ac8673-7a20-4a33-be1f-9365692cb13d", ResourceVersion:"12405", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63742271863, loc:(*time.Location)(0x77108c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"263760080"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0043b0be0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0043b0c00)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0043b0c20), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0043b0c40)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-2v625", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0068ea6c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2v625", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2v625", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2v625", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006c69538), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8sconformance-m02", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002592ee0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006c695b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006c695d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006c695d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc006c695dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002691410), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271863, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271863, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271863, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742271863, loc:(*time.Location)(0x77108c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.49.3", PodIP:"10.244.1.179", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.179"}}, StartTime:(*v1.Time)(0xc0043b0c60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0043b0ca0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002592fc0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://a34323c5b78c6b093ee28b5b41820f355cd71b055b366d9200e4a12455e4f8a6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0043b0cc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0043b0c80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc006c6965f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:38:30.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2397" for this suite.

• [SLOW TEST:47.359 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":163,"skipped":2543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:38:30.601: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 29 18:38:30.627: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:38:37.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1778" for this suite.

• [SLOW TEST:7.373 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2575,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:38:37.975: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 29 18:38:38.021: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 18:38:38.025: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 18:38:38.027: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Nov 29 18:38:38.031: INFO: coredns-f9fd979d6-brw29 from kube-system started at 2020-11-29 17:56:10 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.031: INFO: 	Container coredns ready: true, restart count 0
Nov 29 18:38:38.031: INFO: etcd-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.031: INFO: 	Container etcd ready: true, restart count 0
Nov 29 18:38:38.031: INFO: kindnet-w9hf5 from kube-system started at 2020-11-29 17:56:09 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.031: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 29 18:38:38.031: INFO: kube-apiserver-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.031: INFO: 	Container kube-apiserver ready: true, restart count 0
Nov 29 18:38:38.031: INFO: kube-controller-manager-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.031: INFO: 	Container kube-controller-manager ready: true, restart count 0
Nov 29 18:38:38.031: INFO: kube-proxy-rk57s from kube-system started at 2020-11-29 17:56:09 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.031: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 18:38:38.031: INFO: kube-scheduler-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.031: INFO: 	Container kube-scheduler ready: true, restart count 0
Nov 29 18:38:38.031: INFO: storage-provisioner from kube-system started at 2020-11-29 17:56:11 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.031: INFO: 	Container storage-provisioner ready: true, restart count 0
Nov 29 18:38:38.031: INFO: sonobuoy from sonobuoy started at 2020-11-29 17:57:02 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.031: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 29 18:38:38.031: INFO: sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-bfkxd from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:38:38.031: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:38:38.031: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 18:38:38.031: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Nov 29 18:38:38.034: INFO: pod-init-ffec71e1-3c3d-4884-b385-a0862426039c from init-container-2397 started at 2020-11-29 18:37:43 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.034: INFO: 	Container run1 ready: false, restart count 0
Nov 29 18:38:38.034: INFO: kindnet-hq29j from kube-system started at 2020-11-29 18:00:28 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.034: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 29 18:38:38.034: INFO: kube-proxy-p6zkw from kube-system started at 2020-11-29 17:56:58 +0000 UTC (1 container statuses recorded)
Nov 29 18:38:38.034: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 18:38:38.034: INFO: sonobuoy-e2e-job-9d13ec04340c4e3d from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:38:38.034: INFO: 	Container e2e ready: true, restart count 0
Nov 29 18:38:38.034: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:38:38.034: INFO: sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-nkv9s from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:38:38.034: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:38:38.034: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-43c9ea15-0728-4bf7-b8e9-b331719b0e0c 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-43c9ea15-0728-4bf7-b8e9-b331719b0e0c off the node k8sconformance-m02
STEP: verifying the node doesn't have the label kubernetes.io/e2e-43c9ea15-0728-4bf7-b8e9-b331719b0e0c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:43:42.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8232" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.157 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":165,"skipped":2575,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:43:42.132: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1383
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1383
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1383
Nov 29 18:43:42.184: INFO: Found 0 stateful pods, waiting for 1
Nov 29 18:43:52.213: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 29 18:43:52.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-1383 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 18:43:52.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 18:43:52.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 18:43:52.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 18:43:52.895: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 29 18:44:02.900: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 18:44:02.900: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 18:44:02.909: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999643s
Nov 29 18:44:03.912: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997981903s
Nov 29 18:44:04.915: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994735973s
Nov 29 18:44:05.918: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991497656s
Nov 29 18:44:06.922: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988190568s
Nov 29 18:44:07.925: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985012458s
Nov 29 18:44:08.928: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.981367217s
Nov 29 18:44:09.932: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.978336794s
Nov 29 18:44:10.935: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.974911228s
Nov 29 18:44:11.938: INFO: Verifying statefulset ss doesn't scale past 1 for another 971.534151ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1383
Nov 29 18:44:12.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-1383 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 18:44:13.095: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 18:44:13.095: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 18:44:13.095: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 18:44:13.097: INFO: Found 1 stateful pods, waiting for 3
Nov 29 18:44:23.100: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 18:44:23.101: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 18:44:23.101: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 29 18:44:23.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-1383 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 18:44:23.272: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 18:44:23.272: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 18:44:23.272: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 18:44:23.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-1383 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 18:44:23.432: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 18:44:23.432: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 18:44:23.432: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 18:44:23.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-1383 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 18:44:23.591: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 18:44:23.591: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 18:44:23.591: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 18:44:23.591: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 18:44:23.593: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 29 18:44:33.599: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 18:44:33.599: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 18:44:33.599: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 18:44:33.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999774s
Nov 29 18:44:34.615: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993293936s
Nov 29 18:44:35.619: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98925046s
Nov 29 18:44:36.622: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985923504s
Nov 29 18:44:37.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982254765s
Nov 29 18:44:38.629: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978805518s
Nov 29 18:44:39.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975273292s
Nov 29 18:44:40.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971949619s
Nov 29 18:44:41.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968408186s
Nov 29 18:44:42.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.969576ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1383
Nov 29 18:44:43.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-1383 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 18:44:43.799: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 18:44:43.799: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 18:44:43.799: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 18:44:43.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-1383 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 18:44:43.950: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 18:44:43.950: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 18:44:43.950: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 18:44:43.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-1383 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 18:44:44.119: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 18:44:44.119: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 18:44:44.119: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 18:44:44.119: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 29 18:45:14.130: INFO: Deleting all statefulset in ns statefulset-1383
Nov 29 18:45:14.132: INFO: Scaling statefulset ss to 0
Nov 29 18:45:14.139: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 18:45:14.141: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:45:14.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1383" for this suite.

• [SLOW TEST:92.022 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":166,"skipped":2581,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:45:14.154: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-775.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-775.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-775.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 18:45:16.195: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:16.198: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:16.200: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:16.202: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:16.209: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:16.211: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:16.213: INFO: Unable to read jessie_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:16.220: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:16.223: INFO: Unable to read jessie_udp@PodARecord from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:16.225: INFO: Unable to read jessie_tcp@PodARecord from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:16.225: INFO: Lookups using dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Nov 29 18:45:21.229: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:21.231: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:21.233: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:21.236: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:21.242: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:21.244: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:21.247: INFO: Unable to read jessie_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:21.249: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:21.253: INFO: Lookups using dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local]

Nov 29 18:45:26.229: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:26.232: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:26.234: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:26.236: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:26.244: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:26.246: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:26.248: INFO: Unable to read jessie_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:26.251: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:26.257: INFO: Lookups using dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local]

Nov 29 18:45:31.228: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:31.231: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:31.233: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:31.236: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:31.242: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:31.244: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:31.246: INFO: Unable to read jessie_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:31.248: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:31.252: INFO: Lookups using dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local]

Nov 29 18:45:36.229: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:36.231: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:36.233: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:36.235: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:36.241: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:36.243: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:36.245: INFO: Unable to read jessie_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:36.247: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:36.251: INFO: Lookups using dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local]

Nov 29 18:45:41.228: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:41.231: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:41.233: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:41.235: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:41.241: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:41.244: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:41.246: INFO: Unable to read jessie_udp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:41.248: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local from pod dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e: the server could not find the requested resource (get pods dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e)
Nov 29 18:45:41.252: INFO: Lookups using dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-775.svc.cluster.local jessie_udp@dns-test-service-2.dns-775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-775.svc.cluster.local]

Nov 29 18:45:46.253: INFO: DNS probes using dns-775/dns-test-0db141fb-9d65-4ce0-9239-cdc17924ca9e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:45:46.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-775" for this suite.

• [SLOW TEST:32.177 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":167,"skipped":2587,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:45:46.332: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:45:46.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e727f33c-0178-460b-bef8-8617510ede5b" in namespace "projected-7339" to be "Succeeded or Failed"
Nov 29 18:45:46.373: INFO: Pod "downwardapi-volume-e727f33c-0178-460b-bef8-8617510ede5b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.314622ms
Nov 29 18:45:48.378: INFO: Pod "downwardapi-volume-e727f33c-0178-460b-bef8-8617510ede5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012754384s
STEP: Saw pod success
Nov 29 18:45:48.378: INFO: Pod "downwardapi-volume-e727f33c-0178-460b-bef8-8617510ede5b" satisfied condition "Succeeded or Failed"
Nov 29 18:45:48.380: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-e727f33c-0178-460b-bef8-8617510ede5b container client-container: <nil>
STEP: delete the pod
Nov 29 18:45:48.400: INFO: Waiting for pod downwardapi-volume-e727f33c-0178-460b-bef8-8617510ede5b to disappear
Nov 29 18:45:48.402: INFO: Pod downwardapi-volume-e727f33c-0178-460b-bef8-8617510ede5b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:45:48.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7339" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":168,"skipped":2623,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:45:48.408: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 29 18:45:48.489: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 18:45:48.493: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 18:45:48.495: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Nov 29 18:45:48.498: INFO: coredns-f9fd979d6-brw29 from kube-system started at 2020-11-29 17:56:10 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.498: INFO: 	Container coredns ready: true, restart count 0
Nov 29 18:45:48.498: INFO: etcd-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.498: INFO: 	Container etcd ready: true, restart count 0
Nov 29 18:45:48.498: INFO: kindnet-w9hf5 from kube-system started at 2020-11-29 17:56:09 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.498: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 29 18:45:48.498: INFO: kube-apiserver-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.498: INFO: 	Container kube-apiserver ready: true, restart count 0
Nov 29 18:45:48.498: INFO: kube-controller-manager-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.498: INFO: 	Container kube-controller-manager ready: true, restart count 0
Nov 29 18:45:48.498: INFO: kube-proxy-rk57s from kube-system started at 2020-11-29 17:56:09 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.498: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 18:45:48.498: INFO: kube-scheduler-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.498: INFO: 	Container kube-scheduler ready: true, restart count 0
Nov 29 18:45:48.498: INFO: storage-provisioner from kube-system started at 2020-11-29 17:56:11 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.498: INFO: 	Container storage-provisioner ready: true, restart count 0
Nov 29 18:45:48.498: INFO: sonobuoy from sonobuoy started at 2020-11-29 17:57:02 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.498: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 29 18:45:48.498: INFO: sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-bfkxd from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:45:48.498: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:45:48.499: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 18:45:48.499: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Nov 29 18:45:48.502: INFO: kindnet-hq29j from kube-system started at 2020-11-29 18:00:28 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.502: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 29 18:45:48.502: INFO: kube-proxy-p6zkw from kube-system started at 2020-11-29 17:56:58 +0000 UTC (1 container statuses recorded)
Nov 29 18:45:48.502: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 18:45:48.502: INFO: sonobuoy-e2e-job-9d13ec04340c4e3d from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:45:48.502: INFO: 	Container e2e ready: true, restart count 0
Nov 29 18:45:48.502: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:45:48.502: INFO: sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-nkv9s from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 18:45:48.502: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 18:45:48.502: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4a33bdfd-568c-4618-9ed0-b601a178f2e7 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-4a33bdfd-568c-4618-9ed0-b601a178f2e7 off the node k8sconformance-m02
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4a33bdfd-568c-4618-9ed0-b601a178f2e7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:45:56.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6321" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.189 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":169,"skipped":2628,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:45:56.597: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Nov 29 18:45:56.621: INFO: Waiting up to 5m0s for pod "var-expansion-7bc565c3-dbc2-4a6f-a0c8-8da54eb89b8c" in namespace "var-expansion-6593" to be "Succeeded or Failed"
Nov 29 18:45:56.622: INFO: Pod "var-expansion-7bc565c3-dbc2-4a6f-a0c8-8da54eb89b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.491494ms
Nov 29 18:45:58.625: INFO: Pod "var-expansion-7bc565c3-dbc2-4a6f-a0c8-8da54eb89b8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00455447s
STEP: Saw pod success
Nov 29 18:45:58.625: INFO: Pod "var-expansion-7bc565c3-dbc2-4a6f-a0c8-8da54eb89b8c" satisfied condition "Succeeded or Failed"
Nov 29 18:45:58.627: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-7bc565c3-dbc2-4a6f-a0c8-8da54eb89b8c container dapi-container: <nil>
STEP: delete the pod
Nov 29 18:45:58.637: INFO: Waiting for pod var-expansion-7bc565c3-dbc2-4a6f-a0c8-8da54eb89b8c to disappear
Nov 29 18:45:58.639: INFO: Pod var-expansion-7bc565c3-dbc2-4a6f-a0c8-8da54eb89b8c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:45:58.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6593" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":170,"skipped":2639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:45:58.645: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 29 18:45:58.668: INFO: Waiting up to 5m0s for pod "pod-19c0cbfa-269d-4554-8096-379fbe91b50f" in namespace "emptydir-9213" to be "Succeeded or Failed"
Nov 29 18:45:58.670: INFO: Pod "pod-19c0cbfa-269d-4554-8096-379fbe91b50f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.550912ms
Nov 29 18:46:00.673: INFO: Pod "pod-19c0cbfa-269d-4554-8096-379fbe91b50f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004824013s
STEP: Saw pod success
Nov 29 18:46:00.673: INFO: Pod "pod-19c0cbfa-269d-4554-8096-379fbe91b50f" satisfied condition "Succeeded or Failed"
Nov 29 18:46:00.675: INFO: Trying to get logs from node k8sconformance-m02 pod pod-19c0cbfa-269d-4554-8096-379fbe91b50f container test-container: <nil>
STEP: delete the pod
Nov 29 18:46:00.686: INFO: Waiting for pod pod-19c0cbfa-269d-4554-8096-379fbe91b50f to disappear
Nov 29 18:46:00.716: INFO: Pod pod-19c0cbfa-269d-4554-8096-379fbe91b50f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:46:00.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9213" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":171,"skipped":2663,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:46:00.722: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-a7f4cd18-05fd-4e25-9eb3-7ee3ad0a4257
STEP: Creating a pod to test consume configMaps
Nov 29 18:46:00.751: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a9a9b7b-3c9c-4a7f-bbdb-964e7be44023" in namespace "projected-2665" to be "Succeeded or Failed"
Nov 29 18:46:00.753: INFO: Pod "pod-projected-configmaps-2a9a9b7b-3c9c-4a7f-bbdb-964e7be44023": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489487ms
Nov 29 18:46:02.755: INFO: Pod "pod-projected-configmaps-2a9a9b7b-3c9c-4a7f-bbdb-964e7be44023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003415309s
STEP: Saw pod success
Nov 29 18:46:02.755: INFO: Pod "pod-projected-configmaps-2a9a9b7b-3c9c-4a7f-bbdb-964e7be44023" satisfied condition "Succeeded or Failed"
Nov 29 18:46:02.756: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-2a9a9b7b-3c9c-4a7f-bbdb-964e7be44023 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 18:46:02.770: INFO: Waiting for pod pod-projected-configmaps-2a9a9b7b-3c9c-4a7f-bbdb-964e7be44023 to disappear
Nov 29 18:46:02.772: INFO: Pod pod-projected-configmaps-2a9a9b7b-3c9c-4a7f-bbdb-964e7be44023 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:46:02.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2665" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:46:02.776: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 29 18:46:05.396: INFO: Successfully updated pod "pod-update-e14797c8-93f5-49a4-a888-138a1d49279d"
STEP: verifying the updated pod is in kubernetes
Nov 29 18:46:05.402: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:46:05.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2160" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2734,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:46:05.408: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-d6b2b421-4bba-4bba-a81d-1cad98c19f66
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:46:05.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8650" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":174,"skipped":2747,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:46:05.433: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Nov 29 18:46:05.454: INFO: Waiting up to 5m0s for pod "var-expansion-018f2b0c-f0ac-4631-8d42-f61d094fe5c8" in namespace "var-expansion-8834" to be "Succeeded or Failed"
Nov 29 18:46:05.460: INFO: Pod "var-expansion-018f2b0c-f0ac-4631-8d42-f61d094fe5c8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.715049ms
Nov 29 18:46:07.463: INFO: Pod "var-expansion-018f2b0c-f0ac-4631-8d42-f61d094fe5c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00863939s
STEP: Saw pod success
Nov 29 18:46:07.463: INFO: Pod "var-expansion-018f2b0c-f0ac-4631-8d42-f61d094fe5c8" satisfied condition "Succeeded or Failed"
Nov 29 18:46:07.465: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-018f2b0c-f0ac-4631-8d42-f61d094fe5c8 container dapi-container: <nil>
STEP: delete the pod
Nov 29 18:46:07.484: INFO: Waiting for pod var-expansion-018f2b0c-f0ac-4631-8d42-f61d094fe5c8 to disappear
Nov 29 18:46:07.486: INFO: Pod var-expansion-018f2b0c-f0ac-4631-8d42-f61d094fe5c8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:46:07.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8834" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":175,"skipped":2750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:46:07.491: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6894
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Nov 29 18:46:07.520: INFO: Found 0 stateful pods, waiting for 3
Nov 29 18:46:17.524: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 18:46:17.524: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 18:46:17.524: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 18:46:17.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-6894 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 18:46:17.685: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 18:46:17.685: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 18:46:17.685: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 29 18:46:27.712: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 29 18:46:37.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-6894 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 18:46:37.892: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 18:46:37.893: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 18:46:37.893: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 18:46:47.907: INFO: Waiting for StatefulSet statefulset-6894/ss2 to complete update
Nov 29 18:46:47.907: INFO: Waiting for Pod statefulset-6894/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 18:46:47.907: INFO: Waiting for Pod statefulset-6894/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 18:46:47.907: INFO: Waiting for Pod statefulset-6894/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 18:46:57.913: INFO: Waiting for StatefulSet statefulset-6894/ss2 to complete update
Nov 29 18:46:57.913: INFO: Waiting for Pod statefulset-6894/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 18:46:57.913: INFO: Waiting for Pod statefulset-6894/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 18:47:07.913: INFO: Waiting for StatefulSet statefulset-6894/ss2 to complete update
Nov 29 18:47:07.913: INFO: Waiting for Pod statefulset-6894/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 18:47:17.913: INFO: Waiting for StatefulSet statefulset-6894/ss2 to complete update
Nov 29 18:47:17.913: INFO: Waiting for Pod statefulset-6894/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Nov 29 18:47:27.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-6894 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 18:47:28.098: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 18:47:28.098: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 18:47:28.098: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 18:47:38.124: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 29 18:47:48.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-6894 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 18:47:48.277: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 18:47:48.277: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 18:47:48.277: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 18:47:58.291: INFO: Waiting for StatefulSet statefulset-6894/ss2 to complete update
Nov 29 18:47:58.291: INFO: Waiting for Pod statefulset-6894/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov 29 18:48:08.297: INFO: Waiting for StatefulSet statefulset-6894/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 29 18:48:18.297: INFO: Deleting all statefulset in ns statefulset-6894
Nov 29 18:48:18.299: INFO: Scaling statefulset ss2 to 0
Nov 29 18:48:28.335: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 18:48:28.337: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:48:28.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6894" for this suite.

• [SLOW TEST:140.860 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":176,"skipped":2780,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:48:28.351: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7748.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7748.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 18:48:32.401: INFO: DNS probes using dns-7748/dns-test-e99f70c9-65a9-44d7-9a21-40ae02cc472f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:48:32.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7748" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":177,"skipped":2784,"failed":0}
S
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:48:32.425: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:48:32.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9510" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":178,"skipped":2785,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:48:32.862: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:48:35.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5393" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":179,"skipped":2857,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:48:35.921: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:48:35.944: INFO: Creating deployment "webserver-deployment"
Nov 29 18:48:35.946: INFO: Waiting for observed generation 1
Nov 29 18:48:38.144: INFO: Waiting for all required pods to come up
Nov 29 18:48:38.573: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 29 18:48:40.647: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 29 18:48:40.651: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 29 18:48:40.655: INFO: Updating deployment webserver-deployment
Nov 29 18:48:40.655: INFO: Waiting for observed generation 2
Nov 29 18:48:42.663: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 29 18:48:42.665: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 29 18:48:42.667: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 29 18:48:42.672: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 29 18:48:42.672: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 29 18:48:42.674: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 29 18:48:42.677: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 29 18:48:42.677: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 29 18:48:42.681: INFO: Updating deployment webserver-deployment
Nov 29 18:48:42.681: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 29 18:48:42.690: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 29 18:48:42.696: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 29 18:48:42.705: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-778 /apis/apps/v1/namespaces/deployment-778/deployments/webserver-deployment 4918d613-7c7d-486e-a2fc-970506a9ebed 14364 3 2020-11-29 18:48:35 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031fab98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-11-29 18:48:41 +0000 UTC,LastTransitionTime:2020-11-29 18:48:35 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-29 18:48:42 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 29 18:48:42.712: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-778 /apis/apps/v1/namespaces/deployment-778/replicasets/webserver-deployment-795d758f88 f63e4aa0-e600-465f-842a-48e885b6948b 14362 3 2020-11-29 18:48:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4918d613-7c7d-486e-a2fc-970506a9ebed 0xc0031fb047 0xc0031fb048}] []  [{kube-controller-manager Update apps/v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4918d613-7c7d-486e-a2fc-970506a9ebed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031fb0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 18:48:42.712: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 29 18:48:42.712: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-778 /apis/apps/v1/namespaces/deployment-778/replicasets/webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 14360 3 2020-11-29 18:48:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4918d613-7c7d-486e-a2fc-970506a9ebed 0xc0031fb127 0xc0031fb128}] []  [{kube-controller-manager Update apps/v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4918d613-7c7d-486e-a2fc-970506a9ebed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031fb198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 29 18:48:42.724: INFO: Pod "webserver-deployment-795d758f88-5gv99" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5gv99 webserver-deployment-795d758f88- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-795d758f88-5gv99 7319a103-af9a-4849-8251-d9629ca41b7a 14299 0 2020-11-29 18:48:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f63e4aa0-e600-465f-842a-48e885b6948b 0xc0031fb707 0xc0031fb708}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63e4aa0-e600-465f-842a-48e885b6948b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2020-11-29 18:48:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.724: INFO: Pod "webserver-deployment-795d758f88-9n88q" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9n88q webserver-deployment-795d758f88- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-795d758f88-9n88q 69f9a12e-3e6d-4d95-be43-eca2e492c3d9 14305 0 2020-11-29 18:48:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f63e4aa0-e600-465f-842a-48e885b6948b 0xc0031fb8b7 0xc0031fb8b8}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63e4aa0-e600-465f-842a-48e885b6948b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2020-11-29 18:48:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.724: INFO: Pod "webserver-deployment-795d758f88-c7sxm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c7sxm webserver-deployment-795d758f88- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-795d758f88-c7sxm 17c2fe6a-789a-4de5-96de-6bbbbdd5140e 14313 0 2020-11-29 18:48:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f63e4aa0-e600-465f-842a-48e885b6948b 0xc0031fba67 0xc0031fba68}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63e4aa0-e600-465f-842a-48e885b6948b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2020-11-29 18:48:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.724: INFO: Pod "webserver-deployment-795d758f88-jr9pb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jr9pb webserver-deployment-795d758f88- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-795d758f88-jr9pb 1ec16600-c980-400f-bd5e-1dc158dc9908 14330 0 2020-11-29 18:48:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f63e4aa0-e600-465f-842a-48e885b6948b 0xc0031fbc07 0xc0031fbc08}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63e4aa0-e600-465f-842a-48e885b6948b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2020-11-29 18:48:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.724: INFO: Pod "webserver-deployment-795d758f88-m7ppr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m7ppr webserver-deployment-795d758f88- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-795d758f88-m7ppr fbdfa95d-2a0a-40e7-b33e-8055749b223c 14385 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f63e4aa0-e600-465f-842a-48e885b6948b 0xc0031fbda7 0xc0031fbda8}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63e4aa0-e600-465f-842a-48e885b6948b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.724: INFO: Pod "webserver-deployment-795d758f88-mvn8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mvn8n webserver-deployment-795d758f88- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-795d758f88-mvn8n de3f6965-4acf-4bd9-bc0b-f1ca46d367f1 14378 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f63e4aa0-e600-465f-842a-48e885b6948b 0xc0031fbed0 0xc0031fbed1}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63e4aa0-e600-465f-842a-48e885b6948b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.725: INFO: Pod "webserver-deployment-795d758f88-r4nhg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-r4nhg webserver-deployment-795d758f88- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-795d758f88-r4nhg 508b32e6-6f4b-4fec-8f4c-7012af4d58cf 14390 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f63e4aa0-e600-465f-842a-48e885b6948b 0xc0041a0010 0xc0041a0011}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63e4aa0-e600-465f-842a-48e885b6948b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.725: INFO: Pod "webserver-deployment-795d758f88-xt8pz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xt8pz webserver-deployment-795d758f88- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-795d758f88-xt8pz 015061f9-ca69-4ea3-a8a0-ffb64389e003 14326 0 2020-11-29 18:48:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f63e4aa0-e600-465f-842a-48e885b6948b 0xc0041a0160 0xc0041a0161}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63e4aa0-e600-465f-842a-48e885b6948b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2020-11-29 18:48:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.725: INFO: Pod "webserver-deployment-795d758f88-xtmzk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xtmzk webserver-deployment-795d758f88- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-795d758f88-xtmzk d4c5811a-645a-49f4-b5c1-a06cdc8930a3 14387 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f63e4aa0-e600-465f-842a-48e885b6948b 0xc0041a0307 0xc0041a0308}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63e4aa0-e600-465f-842a-48e885b6948b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.725: INFO: Pod "webserver-deployment-dd94f59b7-6kr7w" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6kr7w webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-6kr7w ef0fe6a7-c2b9-4cc6-addc-95fdf9648261 14248 0 2020-11-29 18:48:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a0417 0xc0041a0418}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.48,StartTime:2020-11-29 18:48:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 18:48:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f9ca896640106f09dd4477c683066fab659b12b2fe04797514c4b04d6c12884e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.725: INFO: Pod "webserver-deployment-dd94f59b7-d6c4j" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d6c4j webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-d6c4j c156c194-bd47-475c-bb6c-f4e3028ec7d7 14384 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a05c0 0xc0041a05c1}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.725: INFO: Pod "webserver-deployment-dd94f59b7-d6c7s" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d6c7s webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-d6c7s 1da27391-7dc9-4383-b7d5-21e5d1ad887d 14251 0 2020-11-29 18:48:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a06c7 0xc0041a06c8}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.45,StartTime:2020-11-29 18:48:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 18:48:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://9c20a12ff5b23bbdbdacf5ff8864bb87c0eee2b339257052cc2164ef4b1a5838,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.725: INFO: Pod "webserver-deployment-dd94f59b7-f2ntz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-f2ntz webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-f2ntz dda44bd6-4033-4e32-97e3-59a6ed0c6637 14194 0 2020-11-29 18:48:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a0870 0xc0041a0871}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.210,StartTime:2020-11-29 18:48:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 18:48:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://50bda1029912661997ec3db54c845252ea4e4cd7695ce9e5006621fb1fc4f3bd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.726: INFO: Pod "webserver-deployment-dd94f59b7-fsz66" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fsz66 webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-fsz66 46019035-be7f-426b-98e9-5c9f36da23b4 14382 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a0a07 0xc0041a0a08}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.726: INFO: Pod "webserver-deployment-dd94f59b7-g92jv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-g92jv webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-g92jv 4a9271fc-ebfd-426f-881a-c25901627867 14381 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a0b50 0xc0041a0b51}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.726: INFO: Pod "webserver-deployment-dd94f59b7-hzqqh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hzqqh webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-hzqqh 61f93563-1800-4698-8c31-2257543e4e39 14259 0 2020-11-29 18:48:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a0c70 0xc0041a0c71}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.47,StartTime:2020-11-29 18:48:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 18:48:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2d03ab52fe9713a048f694294dff051e537e4a669d0a9a5e42c839b7325c26f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.726: INFO: Pod "webserver-deployment-dd94f59b7-jxmbk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jxmbk webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-jxmbk a1774792-0f4f-4c2e-8fa1-2886a36ae47c 14392 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a0e20 0xc0041a0e21}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.726: INFO: Pod "webserver-deployment-dd94f59b7-kk9x6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kk9x6 webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-kk9x6 987adcff-d4ce-4471-b83d-12f61a95eec1 14391 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a0f50 0xc0041a0f51}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.726: INFO: Pod "webserver-deployment-dd94f59b7-lb56n" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lb56n webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-lb56n 34e5c6ad-2c95-4e02-bf57-07f20cec74bf 14203 0 2020-11-29 18:48:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a1057 0xc0041a1058}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.206,StartTime:2020-11-29 18:48:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 18:48:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://1c0f7424203619159ddc207c9a7d23b6d9c451ab8766fe3e5c8b5b0786ed5af6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.726: INFO: Pod "webserver-deployment-dd94f59b7-pcdsq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pcdsq webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-pcdsq 7b7e1d69-bf38-4e82-82d3-b2ab2cf80971 14217 0 2020-11-29 18:48:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a1407 0xc0041a1408}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.44,StartTime:2020-11-29 18:48:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 18:48:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://bdc7480db19b2fe85bcf1826db202310644a5e92b2e351351cec240a4cf90c76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.726: INFO: Pod "webserver-deployment-dd94f59b7-rhtk9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rhtk9 webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-rhtk9 aa8d0539-8148-4069-a3b4-9ad66615d178 14380 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a1770 0xc0041a1771}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.726: INFO: Pod "webserver-deployment-dd94f59b7-rslvg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rslvg webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-rslvg a2d40a15-2d27-47fb-b94b-9e93f7330cad 14389 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a1890 0xc0041a1891}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.727: INFO: Pod "webserver-deployment-dd94f59b7-sj5nz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sj5nz webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-sj5nz ba12d64d-d925-4c1d-8403-2a1f0d19e158 14367 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a19b0 0xc0041a19b1}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.727: INFO: Pod "webserver-deployment-dd94f59b7-sk98s" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sk98s webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-sk98s 48cf77d6-fead-4cb5-91b5-fa519eee2359 14254 0 2020-11-29 18:48:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a1ad0 0xc0041a1ad1}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.46,StartTime:2020-11-29 18:48:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 18:48:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://424aa55d72bf0ca87886525ed06f9cccc9b09c809770aba3286ded68934f9c54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.727: INFO: Pod "webserver-deployment-dd94f59b7-t9lfd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-t9lfd webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-t9lfd 335fbe08-ebf2-4f2d-af40-8c344f6cbb1c 14210 0 2020-11-29 18:48:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a1c70 0xc0041a1c71}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 18:48:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.207,StartTime:2020-11-29 18:48:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 18:48:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://9c64f2bdca759c5b5e6f04cc74b473ba5992482d48abecc7555726e23bb98e7c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.727: INFO: Pod "webserver-deployment-dd94f59b7-w48rx" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w48rx webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-w48rx 18b12221-fdc8-4f1a-bb8b-5aaf4512fc2d 14393 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a1e07 0xc0041a1e08}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.727: INFO: Pod "webserver-deployment-dd94f59b7-wb4tk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wb4tk webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-wb4tk e526c031-abe6-405b-a7c2-277b23e9ff86 14386 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc0041a1f07 0xc0041a1f08}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 18:48:42.727: INFO: Pod "webserver-deployment-dd94f59b7-wpw69" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wpw69 webserver-deployment-dd94f59b7- deployment-778 /api/v1/namespaces/deployment-778/pods/webserver-deployment-dd94f59b7-wpw69 2d5f5135-a6a8-4d77-a543-97482cb8ba22 14388 0 2020-11-29 18:48:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 a041cd52-8721-433e-bed9-aeeec856f643 0xc004370007 0xc004370008}] []  [{kube-controller-manager Update v1 2020-11-29 18:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a041cd52-8721-433e-bed9-aeeec856f643\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bpz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bpz6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bpz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 18:48:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:48:42.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-778" for this suite.

• [SLOW TEST:6.933 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":180,"skipped":2871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:48:42.854: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-b5d94c5a-8174-457b-a7ae-e9a767a22fe9
STEP: Creating a pod to test consume secrets
Nov 29 18:48:43.015: INFO: Waiting up to 5m0s for pod "pod-secrets-5a69c455-45c2-4180-99f5-98d2a14ad8e0" in namespace "secrets-3450" to be "Succeeded or Failed"
Nov 29 18:48:43.027: INFO: Pod "pod-secrets-5a69c455-45c2-4180-99f5-98d2a14ad8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.207551ms
Nov 29 18:48:45.048: INFO: Pod "pod-secrets-5a69c455-45c2-4180-99f5-98d2a14ad8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033031728s
Nov 29 18:48:47.245: INFO: Pod "pod-secrets-5a69c455-45c2-4180-99f5-98d2a14ad8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.229879548s
Nov 29 18:48:49.318: INFO: Pod "pod-secrets-5a69c455-45c2-4180-99f5-98d2a14ad8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.302613057s
Nov 29 18:48:51.321: INFO: Pod "pod-secrets-5a69c455-45c2-4180-99f5-98d2a14ad8e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.305698362s
STEP: Saw pod success
Nov 29 18:48:51.321: INFO: Pod "pod-secrets-5a69c455-45c2-4180-99f5-98d2a14ad8e0" satisfied condition "Succeeded or Failed"
Nov 29 18:48:51.323: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-5a69c455-45c2-4180-99f5-98d2a14ad8e0 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 18:48:51.339: INFO: Waiting for pod pod-secrets-5a69c455-45c2-4180-99f5-98d2a14ad8e0 to disappear
Nov 29 18:48:51.341: INFO: Pod pod-secrets-5a69c455-45c2-4180-99f5-98d2a14ad8e0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:48:51.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3450" for this suite.

• [SLOW TEST:8.491 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":181,"skipped":2909,"failed":0}
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:48:51.346: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:48:51.385: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-22434743-6448-4ce1-b0e7-06efc5a9a2e9" in namespace "security-context-test-6073" to be "Succeeded or Failed"
Nov 29 18:48:51.387: INFO: Pod "busybox-readonly-false-22434743-6448-4ce1-b0e7-06efc5a9a2e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471591ms
Nov 29 18:48:53.410: INFO: Pod "busybox-readonly-false-22434743-6448-4ce1-b0e7-06efc5a9a2e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025212516s
Nov 29 18:48:55.413: INFO: Pod "busybox-readonly-false-22434743-6448-4ce1-b0e7-06efc5a9a2e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028120673s
Nov 29 18:48:55.413: INFO: Pod "busybox-readonly-false-22434743-6448-4ce1-b0e7-06efc5a9a2e9" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:48:55.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6073" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":182,"skipped":2909,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:48:55.419: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:48:55.441: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Creating first CR 
Nov 29 18:48:56.047: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T18:48:56Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T18:48:56Z]] name:name1 resourceVersion:14692 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b77c7a77-77e0-4ea9-9559-060e7cdf8341] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov 29 18:49:06.051: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T18:49:06Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T18:49:06Z]] name:name2 resourceVersion:14719 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:df2b340f-a803-498d-94fa-82baa2315514] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov 29 18:49:16.055: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T18:48:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T18:49:16Z]] name:name1 resourceVersion:14728 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b77c7a77-77e0-4ea9-9559-060e7cdf8341] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov 29 18:49:26.061: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T18:49:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T18:49:26Z]] name:name2 resourceVersion:14737 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:df2b340f-a803-498d-94fa-82baa2315514] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov 29 18:49:36.067: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T18:48:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T18:49:16Z]] name:name1 resourceVersion:14746 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b77c7a77-77e0-4ea9-9559-060e7cdf8341] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov 29 18:49:46.074: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T18:49:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T18:49:26Z]] name:name2 resourceVersion:14755 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:df2b340f-a803-498d-94fa-82baa2315514] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:49:56.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8373" for this suite.

• [SLOW TEST:61.169 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":183,"skipped":2911,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:49:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 29 18:49:56.636: INFO: Waiting up to 5m0s for pod "downward-api-e0564ced-eb03-4ead-b21a-3a84b02bb035" in namespace "downward-api-521" to be "Succeeded or Failed"
Nov 29 18:49:56.639: INFO: Pod "downward-api-e0564ced-eb03-4ead-b21a-3a84b02bb035": Phase="Pending", Reason="", readiness=false. Elapsed: 3.714915ms
Nov 29 18:49:58.642: INFO: Pod "downward-api-e0564ced-eb03-4ead-b21a-3a84b02bb035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006815675s
STEP: Saw pod success
Nov 29 18:49:58.643: INFO: Pod "downward-api-e0564ced-eb03-4ead-b21a-3a84b02bb035" satisfied condition "Succeeded or Failed"
Nov 29 18:49:58.645: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-e0564ced-eb03-4ead-b21a-3a84b02bb035 container dapi-container: <nil>
STEP: delete the pod
Nov 29 18:49:58.666: INFO: Waiting for pod downward-api-e0564ced-eb03-4ead-b21a-3a84b02bb035 to disappear
Nov 29 18:49:58.667: INFO: Pod downward-api-e0564ced-eb03-4ead-b21a-3a84b02bb035 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:49:58.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-521" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":184,"skipped":2916,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:49:58.673: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Nov 29 18:49:58.694: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 29 18:49:58.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-1868'
Nov 29 18:49:58.948: INFO: stderr: ""
Nov 29 18:49:58.949: INFO: stdout: "service/agnhost-replica created\n"
Nov 29 18:49:58.949: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 29 18:49:58.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-1868'
Nov 29 18:49:59.238: INFO: stderr: ""
Nov 29 18:49:59.238: INFO: stdout: "service/agnhost-primary created\n"
Nov 29 18:49:59.238: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 29 18:49:59.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-1868'
Nov 29 18:49:59.429: INFO: stderr: ""
Nov 29 18:49:59.429: INFO: stdout: "service/frontend created\n"
Nov 29 18:49:59.429: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 29 18:49:59.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-1868'
Nov 29 18:49:59.613: INFO: stderr: ""
Nov 29 18:49:59.614: INFO: stdout: "deployment.apps/frontend created\n"
Nov 29 18:49:59.614: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 29 18:49:59.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-1868'
Nov 29 18:49:59.810: INFO: stderr: ""
Nov 29 18:49:59.810: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 29 18:49:59.811: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 29 18:49:59.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-1868'
Nov 29 18:49:59.984: INFO: stderr: ""
Nov 29 18:49:59.984: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Nov 29 18:49:59.984: INFO: Waiting for all frontend pods to be Running.
Nov 29 18:50:05.034: INFO: Waiting for frontend to serve content.
Nov 29 18:50:05.041: INFO: Trying to add a new entry to the guestbook.
Nov 29 18:50:05.047: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Nov 29 18:50:05.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete --grace-period=0 --force -f - --namespace=kubectl-1868'
Nov 29 18:50:05.139: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 18:50:05.139: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 18:50:05.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete --grace-period=0 --force -f - --namespace=kubectl-1868'
Nov 29 18:50:05.224: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 18:50:05.224: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 18:50:05.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete --grace-period=0 --force -f - --namespace=kubectl-1868'
Nov 29 18:50:05.303: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 18:50:05.303: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 18:50:05.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete --grace-period=0 --force -f - --namespace=kubectl-1868'
Nov 29 18:50:05.376: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 18:50:05.376: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 18:50:05.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete --grace-period=0 --force -f - --namespace=kubectl-1868'
Nov 29 18:50:05.451: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 18:50:05.451: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 18:50:05.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete --grace-period=0 --force -f - --namespace=kubectl-1868'
Nov 29 18:50:05.531: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 18:50:05.532: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:50:05.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1868" for this suite.

• [SLOW TEST:6.865 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":185,"skipped":2939,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:50:05.539: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 29 18:50:11.826: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2891 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:50:11.826: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:50:11.893: INFO: Exec stderr: ""
Nov 29 18:50:11.893: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2891 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:50:11.893: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:50:11.942: INFO: Exec stderr: ""
Nov 29 18:50:11.942: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2891 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:50:11.942: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:50:12.021: INFO: Exec stderr: ""
Nov 29 18:50:12.021: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2891 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:50:12.021: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:50:12.073: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 29 18:50:12.073: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2891 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:50:12.073: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:50:12.153: INFO: Exec stderr: ""
Nov 29 18:50:12.153: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2891 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:50:12.153: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:50:12.236: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 29 18:50:12.236: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2891 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:50:12.236: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:50:12.308: INFO: Exec stderr: ""
Nov 29 18:50:12.308: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2891 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:50:12.309: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:50:12.384: INFO: Exec stderr: ""
Nov 29 18:50:12.384: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2891 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:50:12.384: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:50:12.452: INFO: Exec stderr: ""
Nov 29 18:50:12.452: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2891 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 18:50:12.452: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 18:50:12.516: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:50:12.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2891" for this suite.

• [SLOW TEST:6.983 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":186,"skipped":2973,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:50:12.523: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-jl2c
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 18:50:12.570: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jl2c" in namespace "subpath-474" to be "Succeeded or Failed"
Nov 29 18:50:12.572: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.71049ms
Nov 29 18:50:14.575: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004956181s
Nov 29 18:50:16.578: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Running", Reason="", readiness=true. Elapsed: 4.008198766s
Nov 29 18:50:18.581: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Running", Reason="", readiness=true. Elapsed: 6.011212255s
Nov 29 18:50:20.584: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Running", Reason="", readiness=true. Elapsed: 8.014399599s
Nov 29 18:50:22.587: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Running", Reason="", readiness=true. Elapsed: 10.017338018s
Nov 29 18:50:24.590: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Running", Reason="", readiness=true. Elapsed: 12.020596814s
Nov 29 18:50:26.594: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Running", Reason="", readiness=true. Elapsed: 14.023837312s
Nov 29 18:50:28.597: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Running", Reason="", readiness=true. Elapsed: 16.027052392s
Nov 29 18:50:30.600: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Running", Reason="", readiness=true. Elapsed: 18.030241498s
Nov 29 18:50:32.602: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Running", Reason="", readiness=true. Elapsed: 20.032637348s
Nov 29 18:50:34.606: INFO: Pod "pod-subpath-test-configmap-jl2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.03606079s
STEP: Saw pod success
Nov 29 18:50:34.606: INFO: Pod "pod-subpath-test-configmap-jl2c" satisfied condition "Succeeded or Failed"
Nov 29 18:50:34.608: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-configmap-jl2c container test-container-subpath-configmap-jl2c: <nil>
STEP: delete the pod
Nov 29 18:50:34.654: INFO: Waiting for pod pod-subpath-test-configmap-jl2c to disappear
Nov 29 18:50:34.658: INFO: Pod pod-subpath-test-configmap-jl2c no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jl2c
Nov 29 18:50:34.658: INFO: Deleting pod "pod-subpath-test-configmap-jl2c" in namespace "subpath-474"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:50:34.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-474" for this suite.

• [SLOW TEST:22.180 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":187,"skipped":2978,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:50:34.703: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:50:35.369: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 18:50:37.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742272635, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742272635, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742272635, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742272635, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:50:40.391: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:50:50.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1139" for this suite.
STEP: Destroying namespace "webhook-1139-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.840 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":188,"skipped":3015,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:50:50.544: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 18:50:52.580: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:50:52.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4985" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":3023,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:50:52.597: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1129 18:50:58.639676      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 29 18:52:00.652: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:00.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4475" for this suite.

• [SLOW TEST:68.061 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":190,"skipped":3027,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:00.658: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-8e63b0d3-54f4-4119-8a5c-279935c0ada2
STEP: Creating a pod to test consume configMaps
Nov 29 18:52:00.702: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-136ffbb9-b48e-40f3-9d88-48cb03c1724e" in namespace "projected-2031" to be "Succeeded or Failed"
Nov 29 18:52:00.704: INFO: Pod "pod-projected-configmaps-136ffbb9-b48e-40f3-9d88-48cb03c1724e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.847024ms
Nov 29 18:52:02.706: INFO: Pod "pod-projected-configmaps-136ffbb9-b48e-40f3-9d88-48cb03c1724e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004754476s
STEP: Saw pod success
Nov 29 18:52:02.707: INFO: Pod "pod-projected-configmaps-136ffbb9-b48e-40f3-9d88-48cb03c1724e" satisfied condition "Succeeded or Failed"
Nov 29 18:52:02.709: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-136ffbb9-b48e-40f3-9d88-48cb03c1724e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 18:52:02.724: INFO: Waiting for pod pod-projected-configmaps-136ffbb9-b48e-40f3-9d88-48cb03c1724e to disappear
Nov 29 18:52:02.729: INFO: Pod pod-projected-configmaps-136ffbb9-b48e-40f3-9d88-48cb03c1724e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:02.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2031" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":191,"skipped":3037,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:02.735: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-4f23ce52-043e-439f-92c2-62f8a4e9d41f
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-4f23ce52-043e-439f-92c2-62f8a4e9d41f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:06.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1633" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":192,"skipped":3055,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:06.793: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:52:06.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed93f6b1-6f97-40d9-aa53-7bb656973ecc" in namespace "projected-7917" to be "Succeeded or Failed"
Nov 29 18:52:06.819: INFO: Pod "downwardapi-volume-ed93f6b1-6f97-40d9-aa53-7bb656973ecc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.524805ms
Nov 29 18:52:08.822: INFO: Pod "downwardapi-volume-ed93f6b1-6f97-40d9-aa53-7bb656973ecc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004530753s
STEP: Saw pod success
Nov 29 18:52:08.822: INFO: Pod "downwardapi-volume-ed93f6b1-6f97-40d9-aa53-7bb656973ecc" satisfied condition "Succeeded or Failed"
Nov 29 18:52:08.824: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-ed93f6b1-6f97-40d9-aa53-7bb656973ecc container client-container: <nil>
STEP: delete the pod
Nov 29 18:52:08.838: INFO: Waiting for pod downwardapi-volume-ed93f6b1-6f97-40d9-aa53-7bb656973ecc to disappear
Nov 29 18:52:08.840: INFO: Pod downwardapi-volume-ed93f6b1-6f97-40d9-aa53-7bb656973ecc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:08.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7917" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3057,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:08.844: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 18:52:09.356: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 18:52:12.374: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:12.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5110" for this suite.
STEP: Destroying namespace "webhook-5110-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":194,"skipped":3058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:12.456: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 29 18:52:12.539: INFO: Waiting up to 5m0s for pod "pod-f4207ece-e684-4d04-8614-a0014746e213" in namespace "emptydir-6163" to be "Succeeded or Failed"
Nov 29 18:52:12.544: INFO: Pod "pod-f4207ece-e684-4d04-8614-a0014746e213": Phase="Pending", Reason="", readiness=false. Elapsed: 4.980308ms
Nov 29 18:52:14.547: INFO: Pod "pod-f4207ece-e684-4d04-8614-a0014746e213": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008029006s
STEP: Saw pod success
Nov 29 18:52:14.547: INFO: Pod "pod-f4207ece-e684-4d04-8614-a0014746e213" satisfied condition "Succeeded or Failed"
Nov 29 18:52:14.549: INFO: Trying to get logs from node k8sconformance-m02 pod pod-f4207ece-e684-4d04-8614-a0014746e213 container test-container: <nil>
STEP: delete the pod
Nov 29 18:52:14.568: INFO: Waiting for pod pod-f4207ece-e684-4d04-8614-a0014746e213 to disappear
Nov 29 18:52:14.570: INFO: Pod pod-f4207ece-e684-4d04-8614-a0014746e213 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:14.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6163" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":195,"skipped":3086,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:14.576: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-8v8v
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 18:52:14.604: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8v8v" in namespace "subpath-3563" to be "Succeeded or Failed"
Nov 29 18:52:14.606: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Pending", Reason="", readiness=false. Elapsed: 1.627953ms
Nov 29 18:52:16.621: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.016527417s
Nov 29 18:52:18.624: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Running", Reason="", readiness=true. Elapsed: 4.019839344s
Nov 29 18:52:20.627: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Running", Reason="", readiness=true. Elapsed: 6.023015847s
Nov 29 18:52:22.630: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Running", Reason="", readiness=true. Elapsed: 8.026012407s
Nov 29 18:52:24.634: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Running", Reason="", readiness=true. Elapsed: 10.029192805s
Nov 29 18:52:26.637: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Running", Reason="", readiness=true. Elapsed: 12.032516138s
Nov 29 18:52:28.640: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Running", Reason="", readiness=true. Elapsed: 14.035888804s
Nov 29 18:52:30.644: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Running", Reason="", readiness=true. Elapsed: 16.039181471s
Nov 29 18:52:32.647: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Running", Reason="", readiness=true. Elapsed: 18.042265523s
Nov 29 18:52:34.650: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Running", Reason="", readiness=true. Elapsed: 20.045781484s
Nov 29 18:52:36.653: INFO: Pod "pod-subpath-test-secret-8v8v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.048918202s
STEP: Saw pod success
Nov 29 18:52:36.653: INFO: Pod "pod-subpath-test-secret-8v8v" satisfied condition "Succeeded or Failed"
Nov 29 18:52:36.655: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-secret-8v8v container test-container-subpath-secret-8v8v: <nil>
STEP: delete the pod
Nov 29 18:52:36.676: INFO: Waiting for pod pod-subpath-test-secret-8v8v to disappear
Nov 29 18:52:36.679: INFO: Pod pod-subpath-test-secret-8v8v no longer exists
STEP: Deleting pod pod-subpath-test-secret-8v8v
Nov 29 18:52:36.679: INFO: Deleting pod "pod-subpath-test-secret-8v8v" in namespace "subpath-3563"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:36.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3563" for this suite.

• [SLOW TEST:22.110 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":196,"skipped":3159,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:36.686: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-294c4131-1fbe-43eb-934f-eea430114024
STEP: Creating configMap with name cm-test-opt-upd-a53e2ddd-67f8-4232-9f00-e2e5e4a753cb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-294c4131-1fbe-43eb-934f-eea430114024
STEP: Updating configmap cm-test-opt-upd-a53e2ddd-67f8-4232-9f00-e2e5e4a753cb
STEP: Creating configMap with name cm-test-opt-create-32300c3a-552b-4bb7-be7a-6e2f9ed4a523
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:40.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4527" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":197,"skipped":3166,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:40.782: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2903.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2903.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2903.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2903.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2903.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2903.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 18:52:44.854: INFO: DNS probes using dns-2903/dns-test-b9b16086-7e9c-4936-aab9-303f8ec82feb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:44.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2903" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":198,"skipped":3171,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:44.873: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:45.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3156" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":199,"skipped":3191,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:45.417: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 18:52:47.455: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:47.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3083" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":200,"skipped":3208,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:47.472: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Nov 29 18:52:47.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 cluster-info'
Nov 29 18:52:47.573: INFO: stderr: ""
Nov 29 18:52:47.573: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:47.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6950" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":201,"skipped":3219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:47.580: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 29 18:52:50.132: INFO: Successfully updated pod "annotationupdate809a2869-17aa-4b21-a758-a72af544eb57"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:54.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1500" for this suite.

• [SLOW TEST:6.575 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":202,"skipped":3250,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:54.155: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:56.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6764" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":203,"skipped":3252,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:56.233: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 29 18:52:56.280: INFO: Waiting up to 5m0s for pod "downward-api-0f816eaa-af3f-4380-9af1-1ec9c0024bba" in namespace "downward-api-1007" to be "Succeeded or Failed"
Nov 29 18:52:56.284: INFO: Pod "downward-api-0f816eaa-af3f-4380-9af1-1ec9c0024bba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.167426ms
Nov 29 18:52:58.287: INFO: Pod "downward-api-0f816eaa-af3f-4380-9af1-1ec9c0024bba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00722221s
STEP: Saw pod success
Nov 29 18:52:58.287: INFO: Pod "downward-api-0f816eaa-af3f-4380-9af1-1ec9c0024bba" satisfied condition "Succeeded or Failed"
Nov 29 18:52:58.289: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-0f816eaa-af3f-4380-9af1-1ec9c0024bba container dapi-container: <nil>
STEP: delete the pod
Nov 29 18:52:58.303: INFO: Waiting for pod downward-api-0f816eaa-af3f-4380-9af1-1ec9c0024bba to disappear
Nov 29 18:52:58.305: INFO: Pod downward-api-0f816eaa-af3f-4380-9af1-1ec9c0024bba no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:52:58.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1007" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":204,"skipped":3252,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:52:58.309: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:52:58.334: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1e88857-c5d7-41fb-a09a-51d1c677a378" in namespace "projected-3447" to be "Succeeded or Failed"
Nov 29 18:52:58.338: INFO: Pod "downwardapi-volume-e1e88857-c5d7-41fb-a09a-51d1c677a378": Phase="Pending", Reason="", readiness=false. Elapsed: 3.811944ms
Nov 29 18:53:00.341: INFO: Pod "downwardapi-volume-e1e88857-c5d7-41fb-a09a-51d1c677a378": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006703963s
STEP: Saw pod success
Nov 29 18:53:00.341: INFO: Pod "downwardapi-volume-e1e88857-c5d7-41fb-a09a-51d1c677a378" satisfied condition "Succeeded or Failed"
Nov 29 18:53:00.343: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-e1e88857-c5d7-41fb-a09a-51d1c677a378 container client-container: <nil>
STEP: delete the pod
Nov 29 18:53:00.357: INFO: Waiting for pod downwardapi-volume-e1e88857-c5d7-41fb-a09a-51d1c677a378 to disappear
Nov 29 18:53:00.358: INFO: Pod downwardapi-volume-e1e88857-c5d7-41fb-a09a-51d1c677a378 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:53:00.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3447" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":205,"skipped":3256,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:53:00.363: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 29 18:53:02.909: INFO: Successfully updated pod "labelsupdate2db2dd27-8df5-41cc-b28f-ba482ca992a6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:53:06.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7674" for this suite.

• [SLOW TEST:6.565 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3263,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:53:06.929: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 18:53:06.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7265940-bdd2-4a37-9825-31d602f6f2d9" in namespace "downward-api-757" to be "Succeeded or Failed"
Nov 29 18:53:06.957: INFO: Pod "downwardapi-volume-c7265940-bdd2-4a37-9825-31d602f6f2d9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.899179ms
Nov 29 18:53:08.960: INFO: Pod "downwardapi-volume-c7265940-bdd2-4a37-9825-31d602f6f2d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007132977s
STEP: Saw pod success
Nov 29 18:53:08.960: INFO: Pod "downwardapi-volume-c7265940-bdd2-4a37-9825-31d602f6f2d9" satisfied condition "Succeeded or Failed"
Nov 29 18:53:08.962: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-c7265940-bdd2-4a37-9825-31d602f6f2d9 container client-container: <nil>
STEP: delete the pod
Nov 29 18:53:08.976: INFO: Waiting for pod downwardapi-volume-c7265940-bdd2-4a37-9825-31d602f6f2d9 to disappear
Nov 29 18:53:08.978: INFO: Pod downwardapi-volume-c7265940-bdd2-4a37-9825-31d602f6f2d9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:53:08.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-757" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":207,"skipped":3264,"failed":0}

------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:53:08.984: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 29 18:53:09.005: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:53:11.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8298" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":208,"skipped":3264,"failed":0}

------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:53:11.744: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-933ba08e-58db-4957-9efe-b50613d1ad04 in namespace container-probe-6322
Nov 29 18:53:13.774: INFO: Started pod liveness-933ba08e-58db-4957-9efe-b50613d1ad04 in namespace container-probe-6322
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 18:53:13.776: INFO: Initial restart count of pod liveness-933ba08e-58db-4957-9efe-b50613d1ad04 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:57:14.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6322" for this suite.

• [SLOW TEST:242.409 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":209,"skipped":3264,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:57:14.154: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Nov 29 18:57:14.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-9311'
Nov 29 18:57:15.075: INFO: stderr: ""
Nov 29 18:57:15.075: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 18:57:15.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9311'
Nov 29 18:57:15.157: INFO: stderr: ""
Nov 29 18:57:15.157: INFO: stdout: "update-demo-nautilus-jvrrl update-demo-nautilus-klqv6 "
Nov 29 18:57:15.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-jvrrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:15.224: INFO: stderr: ""
Nov 29 18:57:15.224: INFO: stdout: ""
Nov 29 18:57:15.224: INFO: update-demo-nautilus-jvrrl is created but not running
Nov 29 18:57:20.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9311'
Nov 29 18:57:20.299: INFO: stderr: ""
Nov 29 18:57:20.299: INFO: stdout: "update-demo-nautilus-jvrrl update-demo-nautilus-klqv6 "
Nov 29 18:57:20.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-jvrrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:20.369: INFO: stderr: ""
Nov 29 18:57:20.369: INFO: stdout: "true"
Nov 29 18:57:20.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-jvrrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:20.441: INFO: stderr: ""
Nov 29 18:57:20.441: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 18:57:20.441: INFO: validating pod update-demo-nautilus-jvrrl
Nov 29 18:57:20.445: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 18:57:20.445: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 18:57:20.445: INFO: update-demo-nautilus-jvrrl is verified up and running
Nov 29 18:57:20.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-klqv6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:20.512: INFO: stderr: ""
Nov 29 18:57:20.512: INFO: stdout: "true"
Nov 29 18:57:20.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-klqv6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:20.582: INFO: stderr: ""
Nov 29 18:57:20.582: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 18:57:20.582: INFO: validating pod update-demo-nautilus-klqv6
Nov 29 18:57:20.586: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 18:57:20.586: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 18:57:20.586: INFO: update-demo-nautilus-klqv6 is verified up and running
STEP: scaling down the replication controller
Nov 29 18:57:20.588: INFO: scanned /root for discovery docs: <nil>
Nov 29 18:57:20.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9311'
Nov 29 18:57:21.673: INFO: stderr: ""
Nov 29 18:57:21.673: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 18:57:21.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9311'
Nov 29 18:57:21.745: INFO: stderr: ""
Nov 29 18:57:21.745: INFO: stdout: "update-demo-nautilus-jvrrl update-demo-nautilus-klqv6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 29 18:57:26.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9311'
Nov 29 18:57:26.818: INFO: stderr: ""
Nov 29 18:57:26.818: INFO: stdout: "update-demo-nautilus-jvrrl "
Nov 29 18:57:26.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-jvrrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:26.886: INFO: stderr: ""
Nov 29 18:57:26.886: INFO: stdout: "true"
Nov 29 18:57:26.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-jvrrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:26.954: INFO: stderr: ""
Nov 29 18:57:26.954: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 18:57:26.954: INFO: validating pod update-demo-nautilus-jvrrl
Nov 29 18:57:26.957: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 18:57:26.957: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 18:57:26.957: INFO: update-demo-nautilus-jvrrl is verified up and running
STEP: scaling up the replication controller
Nov 29 18:57:26.959: INFO: scanned /root for discovery docs: <nil>
Nov 29 18:57:26.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9311'
Nov 29 18:57:28.042: INFO: stderr: ""
Nov 29 18:57:28.042: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 18:57:28.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9311'
Nov 29 18:57:28.120: INFO: stderr: ""
Nov 29 18:57:28.120: INFO: stdout: "update-demo-nautilus-jvrrl update-demo-nautilus-n6lfn "
Nov 29 18:57:28.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-jvrrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:28.188: INFO: stderr: ""
Nov 29 18:57:28.188: INFO: stdout: "true"
Nov 29 18:57:28.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-jvrrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:28.258: INFO: stderr: ""
Nov 29 18:57:28.258: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 18:57:28.258: INFO: validating pod update-demo-nautilus-jvrrl
Nov 29 18:57:28.260: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 18:57:28.260: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 18:57:28.260: INFO: update-demo-nautilus-jvrrl is verified up and running
Nov 29 18:57:28.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-n6lfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:28.330: INFO: stderr: ""
Nov 29 18:57:28.330: INFO: stdout: "true"
Nov 29 18:57:28.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-n6lfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9311'
Nov 29 18:57:28.407: INFO: stderr: ""
Nov 29 18:57:28.407: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 18:57:28.407: INFO: validating pod update-demo-nautilus-n6lfn
Nov 29 18:57:28.411: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 18:57:28.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 18:57:28.411: INFO: update-demo-nautilus-n6lfn is verified up and running
STEP: using delete to clean up resources
Nov 29 18:57:28.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete --grace-period=0 --force -f - --namespace=kubectl-9311'
Nov 29 18:57:28.485: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 18:57:28.485: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 29 18:57:28.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9311'
Nov 29 18:57:28.557: INFO: stderr: "No resources found in kubectl-9311 namespace.\n"
Nov 29 18:57:28.557: INFO: stdout: ""
Nov 29 18:57:28.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -l name=update-demo --namespace=kubectl-9311 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 18:57:28.629: INFO: stderr: ""
Nov 29 18:57:28.629: INFO: stdout: "update-demo-nautilus-jvrrl\nupdate-demo-nautilus-n6lfn\n"
Nov 29 18:57:29.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9311'
Nov 29 18:57:29.203: INFO: stderr: "No resources found in kubectl-9311 namespace.\n"
Nov 29 18:57:29.203: INFO: stdout: ""
Nov 29 18:57:29.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -l name=update-demo --namespace=kubectl-9311 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 18:57:29.273: INFO: stderr: ""
Nov 29 18:57:29.273: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:57:29.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9311" for this suite.

• [SLOW TEST:15.126 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":210,"skipped":3274,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:57:29.280: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 29 18:57:29.304: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:57:33.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2699" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":211,"skipped":3277,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:57:33.117: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Nov 29 18:57:33.137: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:57:48.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7460" for this suite.

• [SLOW TEST:15.589 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":212,"skipped":3277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:57:48.706: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Nov 29 18:57:48.730: INFO: namespace kubectl-7823
Nov 29 18:57:48.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-7823'
Nov 29 18:57:49.032: INFO: stderr: ""
Nov 29 18:57:49.032: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 29 18:57:50.035: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:57:50.035: INFO: Found 0 / 1
Nov 29 18:57:51.035: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:57:51.035: INFO: Found 0 / 1
Nov 29 18:57:52.035: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:57:52.035: INFO: Found 1 / 1
Nov 29 18:57:52.035: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 29 18:57:52.037: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 18:57:52.037: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 29 18:57:52.037: INFO: wait on agnhost-primary startup in kubectl-7823 
Nov 29 18:57:52.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 logs agnhost-primary-6tl5g agnhost-primary --namespace=kubectl-7823'
Nov 29 18:57:52.131: INFO: stderr: ""
Nov 29 18:57:52.131: INFO: stdout: "Paused\n"
STEP: exposing RC
Nov 29 18:57:52.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7823'
Nov 29 18:57:52.241: INFO: stderr: ""
Nov 29 18:57:52.241: INFO: stdout: "service/rm2 exposed\n"
Nov 29 18:57:52.247: INFO: Service rm2 in namespace kubectl-7823 found.
STEP: exposing service
Nov 29 18:57:54.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7823'
Nov 29 18:57:54.345: INFO: stderr: ""
Nov 29 18:57:54.345: INFO: stdout: "service/rm3 exposed\n"
Nov 29 18:57:54.348: INFO: Service rm3 in namespace kubectl-7823 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:57:56.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7823" for this suite.

• [SLOW TEST:7.653 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":213,"skipped":3317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:57:56.360: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-127d4ca9-1fa6-417f-8fdd-8c0e4daaafb7
STEP: Creating a pod to test consume configMaps
Nov 29 18:57:56.426: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07e6a6ed-22b1-40b0-8757-6a2596fa2f88" in namespace "projected-3254" to be "Succeeded or Failed"
Nov 29 18:57:56.428: INFO: Pod "pod-projected-configmaps-07e6a6ed-22b1-40b0-8757-6a2596fa2f88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.644263ms
Nov 29 18:57:58.431: INFO: Pod "pod-projected-configmaps-07e6a6ed-22b1-40b0-8757-6a2596fa2f88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004931574s
STEP: Saw pod success
Nov 29 18:57:58.432: INFO: Pod "pod-projected-configmaps-07e6a6ed-22b1-40b0-8757-6a2596fa2f88" satisfied condition "Succeeded or Failed"
Nov 29 18:57:58.434: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-07e6a6ed-22b1-40b0-8757-6a2596fa2f88 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 18:57:58.449: INFO: Waiting for pod pod-projected-configmaps-07e6a6ed-22b1-40b0-8757-6a2596fa2f88 to disappear
Nov 29 18:57:58.453: INFO: Pod pod-projected-configmaps-07e6a6ed-22b1-40b0-8757-6a2596fa2f88 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:57:58.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3254" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":214,"skipped":3363,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:57:58.459: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 29 18:57:58.484: INFO: Waiting up to 5m0s for pod "pod-1e4dfe0b-3df1-4119-92ea-b287d6ef35ff" in namespace "emptydir-7456" to be "Succeeded or Failed"
Nov 29 18:57:58.486: INFO: Pod "pod-1e4dfe0b-3df1-4119-92ea-b287d6ef35ff": Phase="Pending", Reason="", readiness=false. Elapsed: 1.441106ms
Nov 29 18:58:00.489: INFO: Pod "pod-1e4dfe0b-3df1-4119-92ea-b287d6ef35ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004640595s
STEP: Saw pod success
Nov 29 18:58:00.489: INFO: Pod "pod-1e4dfe0b-3df1-4119-92ea-b287d6ef35ff" satisfied condition "Succeeded or Failed"
Nov 29 18:58:00.491: INFO: Trying to get logs from node k8sconformance-m02 pod pod-1e4dfe0b-3df1-4119-92ea-b287d6ef35ff container test-container: <nil>
STEP: delete the pod
Nov 29 18:58:00.501: INFO: Waiting for pod pod-1e4dfe0b-3df1-4119-92ea-b287d6ef35ff to disappear
Nov 29 18:58:00.505: INFO: Pod pod-1e4dfe0b-3df1-4119-92ea-b287d6ef35ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:58:00.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7456" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":215,"skipped":3382,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:58:00.511: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-a438980e-0f98-4245-b79d-94ccf67c4938
STEP: Creating a pod to test consume secrets
Nov 29 18:58:00.580: INFO: Waiting up to 5m0s for pod "pod-secrets-9d209b67-8831-402e-a773-85d6ea46e452" in namespace "secrets-4808" to be "Succeeded or Failed"
Nov 29 18:58:00.584: INFO: Pod "pod-secrets-9d209b67-8831-402e-a773-85d6ea46e452": Phase="Pending", Reason="", readiness=false. Elapsed: 3.543946ms
Nov 29 18:58:02.587: INFO: Pod "pod-secrets-9d209b67-8831-402e-a773-85d6ea46e452": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006559959s
STEP: Saw pod success
Nov 29 18:58:02.587: INFO: Pod "pod-secrets-9d209b67-8831-402e-a773-85d6ea46e452" satisfied condition "Succeeded or Failed"
Nov 29 18:58:02.589: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-9d209b67-8831-402e-a773-85d6ea46e452 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 18:58:02.631: INFO: Waiting for pod pod-secrets-9d209b67-8831-402e-a773-85d6ea46e452 to disappear
Nov 29 18:58:02.633: INFO: Pod pod-secrets-9d209b67-8831-402e-a773-85d6ea46e452 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 18:58:02.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4808" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":216,"skipped":3434,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 18:58:02.638: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Nov 29 18:58:02.660: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 29 18:59:02.670: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 18:59:02.672: INFO: Starting informer...
STEP: Starting pod...
Nov 29 18:59:02.881: INFO: Pod is running on k8sconformance-m02. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Nov 29 18:59:02.913: INFO: Pod wasn't evicted. Proceeding
Nov 29 18:59:02.913: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Nov 29 19:00:17.944: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:00:17.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4136" for this suite.

• [SLOW TEST:135.313 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":217,"skipped":3455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:00:17.952: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-a5c849a1-fb89-4c5f-ba49-af7cde1bfa6d
STEP: Creating a pod to test consume secrets
Nov 29 19:00:18.035: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-273cbf41-fc8e-4083-8155-99d47e0e995f" in namespace "projected-1471" to be "Succeeded or Failed"
Nov 29 19:00:18.037: INFO: Pod "pod-projected-secrets-273cbf41-fc8e-4083-8155-99d47e0e995f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.86842ms
Nov 29 19:00:20.040: INFO: Pod "pod-projected-secrets-273cbf41-fc8e-4083-8155-99d47e0e995f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00486908s
STEP: Saw pod success
Nov 29 19:00:20.040: INFO: Pod "pod-projected-secrets-273cbf41-fc8e-4083-8155-99d47e0e995f" satisfied condition "Succeeded or Failed"
Nov 29 19:00:20.042: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-273cbf41-fc8e-4083-8155-99d47e0e995f container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 19:00:20.060: INFO: Waiting for pod pod-projected-secrets-273cbf41-fc8e-4083-8155-99d47e0e995f to disappear
Nov 29 19:00:20.065: INFO: Pod pod-projected-secrets-273cbf41-fc8e-4083-8155-99d47e0e995f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:00:20.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1471" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":218,"skipped":3539,"failed":0}

------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:00:20.071: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:00:20.097: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0de5ee88-cbb2-4d63-a0c4-d40ab99c4906" in namespace "security-context-test-2293" to be "Succeeded or Failed"
Nov 29 19:00:20.099: INFO: Pod "alpine-nnp-false-0de5ee88-cbb2-4d63-a0c4-d40ab99c4906": Phase="Pending", Reason="", readiness=false. Elapsed: 1.589666ms
Nov 29 19:00:22.102: INFO: Pod "alpine-nnp-false-0de5ee88-cbb2-4d63-a0c4-d40ab99c4906": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004480945s
Nov 29 19:00:24.104: INFO: Pod "alpine-nnp-false-0de5ee88-cbb2-4d63-a0c4-d40ab99c4906": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007287296s
Nov 29 19:00:24.104: INFO: Pod "alpine-nnp-false-0de5ee88-cbb2-4d63-a0c4-d40ab99c4906" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:00:24.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2293" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":219,"skipped":3539,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:00:24.115: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:00:26.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2569" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":220,"skipped":3558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:00:26.160: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Nov 29 19:00:26.185: INFO: Waiting up to 5m0s for pod "var-expansion-222ec662-8de1-4eda-b032-21850e9ed411" in namespace "var-expansion-5833" to be "Succeeded or Failed"
Nov 29 19:00:26.189: INFO: Pod "var-expansion-222ec662-8de1-4eda-b032-21850e9ed411": Phase="Pending", Reason="", readiness=false. Elapsed: 4.000618ms
Nov 29 19:00:28.192: INFO: Pod "var-expansion-222ec662-8de1-4eda-b032-21850e9ed411": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007283216s
STEP: Saw pod success
Nov 29 19:00:28.193: INFO: Pod "var-expansion-222ec662-8de1-4eda-b032-21850e9ed411" satisfied condition "Succeeded or Failed"
Nov 29 19:00:28.194: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-222ec662-8de1-4eda-b032-21850e9ed411 container dapi-container: <nil>
STEP: delete the pod
Nov 29 19:00:28.217: INFO: Waiting for pod var-expansion-222ec662-8de1-4eda-b032-21850e9ed411 to disappear
Nov 29 19:00:28.221: INFO: Pod var-expansion-222ec662-8de1-4eda-b032-21850e9ed411 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:00:28.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5833" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":221,"skipped":3593,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:00:28.227: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:00:28.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4653" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":222,"skipped":3612,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:00:28.256: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Nov 29 19:00:30.312: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7951 PodName:pod-sharedvolume-32966c10-8f48-4c75-8346-e5ee8df3adfb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 19:00:30.312: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
Nov 29 19:00:30.383: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:00:30.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7951" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":223,"skipped":3628,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:00:30.390: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6164
STEP: creating service affinity-nodeport in namespace services-6164
STEP: creating replication controller affinity-nodeport in namespace services-6164
I1129 19:00:30.440388      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-6164, replica count: 3
I1129 19:00:33.491264      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 19:00:33.497: INFO: Creating new exec pod
Nov 29 19:00:36.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-6164 execpod-affinityprz8j -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Nov 29 19:00:36.680: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 29 19:00:36.680: INFO: stdout: ""
Nov 29 19:00:36.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-6164 execpod-affinityprz8j -- /bin/sh -x -c nc -zv -t -w 2 10.96.118.206 80'
Nov 29 19:00:36.838: INFO: stderr: "+ nc -zv -t -w 2 10.96.118.206 80\nConnection to 10.96.118.206 80 port [tcp/http] succeeded!\n"
Nov 29 19:00:36.838: INFO: stdout: ""
Nov 29 19:00:36.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-6164 execpod-affinityprz8j -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.2 31551'
Nov 29 19:00:36.982: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.2 31551\nConnection to 192.168.49.2 31551 port [tcp/31551] succeeded!\n"
Nov 29 19:00:36.982: INFO: stdout: ""
Nov 29 19:00:36.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-6164 execpod-affinityprz8j -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.3 31551'
Nov 29 19:00:37.139: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.3 31551\nConnection to 192.168.49.3 31551 port [tcp/31551] succeeded!\n"
Nov 29 19:00:37.139: INFO: stdout: ""
Nov 29 19:00:37.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-6164 execpod-affinityprz8j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:31551/ ; done'
Nov 29 19:00:37.365: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31551/\n"
Nov 29 19:00:37.365: INFO: stdout: "\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z\naffinity-nodeport-mw87z"
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Received response from host: affinity-nodeport-mw87z
Nov 29 19:00:37.365: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6164, will wait for the garbage collector to delete the pods
Nov 29 19:00:37.433: INFO: Deleting ReplicationController affinity-nodeport took: 5.303976ms
Nov 29 19:00:37.833: INFO: Terminating ReplicationController affinity-nodeport pods took: 400.202078ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:00:50.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6164" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:19.766 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":224,"skipped":3635,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:00:50.156: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 29 19:00:50.183: INFO: Waiting up to 5m0s for pod "pod-27c318f7-fd8d-4f50-bae7-5011abc88fc2" in namespace "emptydir-2666" to be "Succeeded or Failed"
Nov 29 19:00:50.239: INFO: Pod "pod-27c318f7-fd8d-4f50-bae7-5011abc88fc2": Phase="Pending", Reason="", readiness=false. Elapsed: 55.586492ms
Nov 29 19:00:52.242: INFO: Pod "pod-27c318f7-fd8d-4f50-bae7-5011abc88fc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.058571139s
STEP: Saw pod success
Nov 29 19:00:52.242: INFO: Pod "pod-27c318f7-fd8d-4f50-bae7-5011abc88fc2" satisfied condition "Succeeded or Failed"
Nov 29 19:00:52.244: INFO: Trying to get logs from node k8sconformance-m02 pod pod-27c318f7-fd8d-4f50-bae7-5011abc88fc2 container test-container: <nil>
STEP: delete the pod
Nov 29 19:00:52.259: INFO: Waiting for pod pod-27c318f7-fd8d-4f50-bae7-5011abc88fc2 to disappear
Nov 29 19:00:52.261: INFO: Pod pod-27c318f7-fd8d-4f50-bae7-5011abc88fc2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:00:52.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2666" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":225,"skipped":3640,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:00:52.266: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 19:00:52.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bacff5a-1e6f-4163-be7e-efa5dcb598d8" in namespace "projected-7782" to be "Succeeded or Failed"
Nov 29 19:00:52.296: INFO: Pod "downwardapi-volume-6bacff5a-1e6f-4163-be7e-efa5dcb598d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660735ms
Nov 29 19:00:54.299: INFO: Pod "downwardapi-volume-6bacff5a-1e6f-4163-be7e-efa5dcb598d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007749433s
STEP: Saw pod success
Nov 29 19:00:54.299: INFO: Pod "downwardapi-volume-6bacff5a-1e6f-4163-be7e-efa5dcb598d8" satisfied condition "Succeeded or Failed"
Nov 29 19:00:54.301: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-6bacff5a-1e6f-4163-be7e-efa5dcb598d8 container client-container: <nil>
STEP: delete the pod
Nov 29 19:00:54.318: INFO: Waiting for pod downwardapi-volume-6bacff5a-1e6f-4163-be7e-efa5dcb598d8 to disappear
Nov 29 19:00:54.320: INFO: Pod downwardapi-volume-6bacff5a-1e6f-4163-be7e-efa5dcb598d8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:00:54.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7782" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":226,"skipped":3658,"failed":0}

------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:00:54.325: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8551
STEP: creating service affinity-nodeport-transition in namespace services-8551
STEP: creating replication controller affinity-nodeport-transition in namespace services-8551
I1129 19:00:54.405336      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-8551, replica count: 3
I1129 19:00:57.455759      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 19:00:57.462: INFO: Creating new exec pod
Nov 29 19:01:00.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-8551 execpod-affinityphdkg -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Nov 29 19:01:00.632: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 29 19:01:00.632: INFO: stdout: ""
Nov 29 19:01:00.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-8551 execpod-affinityphdkg -- /bin/sh -x -c nc -zv -t -w 2 10.105.66.71 80'
Nov 29 19:01:00.779: INFO: stderr: "+ nc -zv -t -w 2 10.105.66.71 80\nConnection to 10.105.66.71 80 port [tcp/http] succeeded!\n"
Nov 29 19:01:00.779: INFO: stdout: ""
Nov 29 19:01:00.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-8551 execpod-affinityphdkg -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.2 32614'
Nov 29 19:01:00.920: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.2 32614\nConnection to 192.168.49.2 32614 port [tcp/32614] succeeded!\n"
Nov 29 19:01:00.920: INFO: stdout: ""
Nov 29 19:01:00.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-8551 execpod-affinityphdkg -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.3 32614'
Nov 29 19:01:01.058: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.3 32614\nConnection to 192.168.49.3 32614 port [tcp/32614] succeeded!\n"
Nov 29 19:01:01.058: INFO: stdout: ""
Nov 29 19:01:01.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-8551 execpod-affinityphdkg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:32614/ ; done'
Nov 29 19:01:01.307: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n"
Nov 29 19:01:01.307: INFO: stdout: "\naffinity-nodeport-transition-bd8vv\naffinity-nodeport-transition-nxb7t\naffinity-nodeport-transition-nxb7t\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-nxb7t\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-nxb7t\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-nxb7t\naffinity-nodeport-transition-nxb7t\naffinity-nodeport-transition-bd8vv\naffinity-nodeport-transition-nxb7t\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md"
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-bd8vv
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-nxb7t
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-nxb7t
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-nxb7t
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-nxb7t
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-nxb7t
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-nxb7t
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-bd8vv
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-nxb7t
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.307: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-8551 execpod-affinityphdkg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:32614/ ; done'
Nov 29 19:01:01.567: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:32614/\n"
Nov 29 19:01:01.567: INFO: stdout: "\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md\naffinity-nodeport-transition-gq6md"
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Received response from host: affinity-nodeport-transition-gq6md
Nov 29 19:01:01.567: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8551, will wait for the garbage collector to delete the pods
Nov 29 19:01:01.636: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.647921ms
Nov 29 19:01:02.037: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 400.209295ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:01:10.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8551" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:15.842 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":227,"skipped":3658,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:01:10.168: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Nov 29 19:01:12.218: INFO: Pod pod-hostip-f8e27651-aae8-4b62-bff0-e1f048346651 has hostIP: 192.168.49.3
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:01:12.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5088" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":228,"skipped":3662,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:01:12.225: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 29 19:01:16.277: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 19:01:16.290: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 19:01:18.290: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 19:01:18.293: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 19:01:20.290: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 19:01:20.293: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 19:01:22.290: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 19:01:22.294: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 19:01:24.290: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 19:01:24.293: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 19:01:26.290: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 19:01:26.293: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 19:01:28.290: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 19:01:28.293: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:01:28.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4091" for this suite.

• [SLOW TEST:16.078 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":229,"skipped":3695,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:01:28.303: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:01:28.327: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 29 19:01:33.329: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 29 19:01:33.329: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 29 19:01:35.332: INFO: Creating deployment "test-rollover-deployment"
Nov 29 19:01:35.337: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 29 19:01:37.341: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 29 19:01:37.345: INFO: Ensure that both replica sets have 1 created replica
Nov 29 19:01:37.349: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 29 19:01:37.353: INFO: Updating deployment test-rollover-deployment
Nov 29 19:01:37.353: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 29 19:01:39.358: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 29 19:01:39.362: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 29 19:01:39.366: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 19:01:39.366: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273298, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 19:01:41.372: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 19:01:41.372: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273298, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 19:01:43.374: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 19:01:43.374: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273298, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 19:01:45.371: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 19:01:45.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273298, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 19:01:47.371: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 19:01:47.372: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273298, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273295, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 19:01:49.371: INFO: 
Nov 29 19:01:49.371: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 29 19:01:49.432: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7471 /apis/apps/v1/namespaces/deployment-7471/deployments/test-rollover-deployment fa6c7814-15b9-443b-9efe-a2d5c9a2dd1c 17797 2 2020-11-29 19:01:35 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-29 19:01:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-29 19:01:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0061971a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-29 19:01:35 +0000 UTC,LastTransitionTime:2020-11-29 19:01:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-11-29 19:01:48 +0000 UTC,LastTransitionTime:2020-11-29 19:01:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 29 19:01:49.435: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-7471 /apis/apps/v1/namespaces/deployment-7471/replicasets/test-rollover-deployment-5797c7764 cdb6b7e3-6a78-43f8-bd35-6eece75aba32 17786 2 2020-11-29 19:01:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fa6c7814-15b9-443b-9efe-a2d5c9a2dd1c 0xc0061978b0 0xc0061978b1}] []  [{kube-controller-manager Update apps/v1 2020-11-29 19:01:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa6c7814-15b9-443b-9efe-a2d5c9a2dd1c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006197928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 29 19:01:49.435: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 29 19:01:49.435: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7471 /apis/apps/v1/namespaces/deployment-7471/replicasets/test-rollover-controller 70788f3c-566d-4619-9c20-caea6360c37c 17796 2 2020-11-29 19:01:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fa6c7814-15b9-443b-9efe-a2d5c9a2dd1c 0xc0061977a7 0xc0061977a8}] []  [{e2e.test Update apps/v1 2020-11-29 19:01:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-29 19:01:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa6c7814-15b9-443b-9efe-a2d5c9a2dd1c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006197848 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 19:01:49.435: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-7471 /apis/apps/v1/namespaces/deployment-7471/replicasets/test-rollover-deployment-78bc8b888c 9188fbac-132c-4e5c-84f1-7f802457bd1e 17756 2 2020-11-29 19:01:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fa6c7814-15b9-443b-9efe-a2d5c9a2dd1c 0xc006197997 0xc006197998}] []  [{kube-controller-manager Update apps/v1 2020-11-29 19:01:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa6c7814-15b9-443b-9efe-a2d5c9a2dd1c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006197a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 19:01:49.437: INFO: Pod "test-rollover-deployment-5797c7764-n2t9x" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-n2t9x test-rollover-deployment-5797c7764- deployment-7471 /api/v1/namespaces/deployment-7471/pods/test-rollover-deployment-5797c7764-n2t9x bf6abd71-9dd1-4430-9507-e4ebba17ae9a 17770 0 2020-11-29 19:01:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 cdb6b7e3-6a78-43f8-bd35-6eece75aba32 0xc006197fd0 0xc006197fd1}] []  [{kube-controller-manager Update v1 2020-11-29 19:01:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cdb6b7e3-6a78-43f8-bd35-6eece75aba32\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 19:01:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tflgv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tflgv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tflgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:01:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:01:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:01:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:01:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.32,StartTime:2020-11-29 19:01:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 19:01:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://bff9237c6d7085a90099c78c3eb4f877de73d784df484e68e78f0f1f3d6b4b1b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:01:49.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7471" for this suite.

• [SLOW TEST:21.139 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":230,"skipped":3725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:01:49.443: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-1762/secret-test-eab9db09-081f-4980-908c-1a987c11d014
STEP: Creating a pod to test consume secrets
Nov 29 19:01:49.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-50b8b167-a4f3-4f75-bbf4-3babfd7c093b" in namespace "secrets-1762" to be "Succeeded or Failed"
Nov 29 19:01:49.473: INFO: Pod "pod-configmaps-50b8b167-a4f3-4f75-bbf4-3babfd7c093b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.320658ms
Nov 29 19:01:51.476: INFO: Pod "pod-configmaps-50b8b167-a4f3-4f75-bbf4-3babfd7c093b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005229339s
STEP: Saw pod success
Nov 29 19:01:51.476: INFO: Pod "pod-configmaps-50b8b167-a4f3-4f75-bbf4-3babfd7c093b" satisfied condition "Succeeded or Failed"
Nov 29 19:01:51.478: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-50b8b167-a4f3-4f75-bbf4-3babfd7c093b container env-test: <nil>
STEP: delete the pod
Nov 29 19:01:51.489: INFO: Waiting for pod pod-configmaps-50b8b167-a4f3-4f75-bbf4-3babfd7c093b to disappear
Nov 29 19:01:51.496: INFO: Pod pod-configmaps-50b8b167-a4f3-4f75-bbf4-3babfd7c093b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:01:51.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1762" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":231,"skipped":3769,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:01:51.502: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-7398
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7398 to expose endpoints map[]
Nov 29 19:01:51.569: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Nov 29 19:01:52.575: INFO: successfully validated that service multi-endpoint-test in namespace services-7398 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7398
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7398 to expose endpoints map[pod1:[100]]
Nov 29 19:01:54.592: INFO: successfully validated that service multi-endpoint-test in namespace services-7398 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7398
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7398 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 29 19:01:56.630: INFO: successfully validated that service multi-endpoint-test in namespace services-7398 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7398
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7398 to expose endpoints map[pod2:[101]]
Nov 29 19:01:56.643: INFO: successfully validated that service multi-endpoint-test in namespace services-7398 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7398
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7398 to expose endpoints map[]
Nov 29 19:01:57.657: INFO: successfully validated that service multi-endpoint-test in namespace services-7398 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:01:57.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7398" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.191 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":232,"skipped":3772,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:01:57.692: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Nov 29 19:01:57.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f -'
Nov 29 19:01:57.995: INFO: stderr: ""
Nov 29 19:01:57.995: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Nov 29 19:01:57.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 diff -f -'
Nov 29 19:01:58.361: INFO: rc: 1
Nov 29 19:01:58.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete -f -'
Nov 29 19:01:58.433: INFO: stderr: ""
Nov 29 19:01:58.433: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:01:58.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8753" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":233,"skipped":3775,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:01:58.462: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 29 19:02:01.007: INFO: Successfully updated pod "pod-update-activedeadlineseconds-844556f5-8017-462d-bdba-f992f9a6881d"
Nov 29 19:02:01.007: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-844556f5-8017-462d-bdba-f992f9a6881d" in namespace "pods-2255" to be "terminated due to deadline exceeded"
Nov 29 19:02:01.013: INFO: Pod "pod-update-activedeadlineseconds-844556f5-8017-462d-bdba-f992f9a6881d": Phase="Running", Reason="", readiness=true. Elapsed: 6.191738ms
Nov 29 19:02:03.016: INFO: Pod "pod-update-activedeadlineseconds-844556f5-8017-462d-bdba-f992f9a6881d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009008041s
Nov 29 19:02:05.019: INFO: Pod "pod-update-activedeadlineseconds-844556f5-8017-462d-bdba-f992f9a6881d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012344545s
Nov 29 19:02:05.019: INFO: Pod "pod-update-activedeadlineseconds-844556f5-8017-462d-bdba-f992f9a6881d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:02:05.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2255" for this suite.

• [SLOW TEST:6.563 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":234,"skipped":3794,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:02:05.025: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-4d352d9a-fb86-470c-ad41-7941617d370d
STEP: Creating a pod to test consume configMaps
Nov 29 19:02:05.081: INFO: Waiting up to 5m0s for pod "pod-configmaps-bd0aa721-5bf7-43f6-8d55-555e81273ae4" in namespace "configmap-1692" to be "Succeeded or Failed"
Nov 29 19:02:05.086: INFO: Pod "pod-configmaps-bd0aa721-5bf7-43f6-8d55-555e81273ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476372ms
Nov 29 19:02:07.090: INFO: Pod "pod-configmaps-bd0aa721-5bf7-43f6-8d55-555e81273ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009052048s
STEP: Saw pod success
Nov 29 19:02:07.091: INFO: Pod "pod-configmaps-bd0aa721-5bf7-43f6-8d55-555e81273ae4" satisfied condition "Succeeded or Failed"
Nov 29 19:02:07.093: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-bd0aa721-5bf7-43f6-8d55-555e81273ae4 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 19:02:07.107: INFO: Waiting for pod pod-configmaps-bd0aa721-5bf7-43f6-8d55-555e81273ae4 to disappear
Nov 29 19:02:07.109: INFO: Pod pod-configmaps-bd0aa721-5bf7-43f6-8d55-555e81273ae4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:02:07.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1692" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":235,"skipped":3798,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:02:07.115: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:02:07.136: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 29 19:02:09.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-1525 create -f -'
Nov 29 19:02:10.694: INFO: stderr: ""
Nov 29 19:02:10.694: INFO: stdout: "e2e-test-crd-publish-openapi-2140-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 29 19:02:10.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-1525 delete e2e-test-crd-publish-openapi-2140-crds test-cr'
Nov 29 19:02:10.766: INFO: stderr: ""
Nov 29 19:02:10.766: INFO: stdout: "e2e-test-crd-publish-openapi-2140-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 29 19:02:10.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-1525 apply -f -'
Nov 29 19:02:10.952: INFO: stderr: ""
Nov 29 19:02:10.952: INFO: stdout: "e2e-test-crd-publish-openapi-2140-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 29 19:02:10.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-1525 delete e2e-test-crd-publish-openapi-2140-crds test-cr'
Nov 29 19:02:11.025: INFO: stderr: ""
Nov 29 19:02:11.025: INFO: stdout: "e2e-test-crd-publish-openapi-2140-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov 29 19:02:11.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 explain e2e-test-crd-publish-openapi-2140-crds'
Nov 29 19:02:11.192: INFO: stderr: ""
Nov 29 19:02:11.192: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2140-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:02:14.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1525" for this suite.

• [SLOW TEST:6.904 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":236,"skipped":3805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:02:14.019: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-7797
STEP: creating replication controller nodeport-test in namespace services-7797
I1129 19:02:14.073667      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-7797, replica count: 2
Nov 29 19:02:17.124: INFO: Creating new exec pod
I1129 19:02:17.124115      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 19:02:20.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-7797 execpod24xjk -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Nov 29 19:02:20.285: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 29 19:02:20.285: INFO: stdout: ""
Nov 29 19:02:20.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-7797 execpod24xjk -- /bin/sh -x -c nc -zv -t -w 2 10.96.239.52 80'
Nov 29 19:02:20.430: INFO: stderr: "+ nc -zv -t -w 2 10.96.239.52 80\nConnection to 10.96.239.52 80 port [tcp/http] succeeded!\n"
Nov 29 19:02:20.430: INFO: stdout: ""
Nov 29 19:02:20.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-7797 execpod24xjk -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.2 31499'
Nov 29 19:02:20.582: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.2 31499\nConnection to 192.168.49.2 31499 port [tcp/31499] succeeded!\n"
Nov 29 19:02:20.582: INFO: stdout: ""
Nov 29 19:02:20.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-7797 execpod24xjk -- /bin/sh -x -c nc -zv -t -w 2 192.168.49.3 31499'
Nov 29 19:02:20.726: INFO: stderr: "+ nc -zv -t -w 2 192.168.49.3 31499\nConnection to 192.168.49.3 31499 port [tcp/31499] succeeded!\n"
Nov 29 19:02:20.726: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:02:20.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7797" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.713 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":237,"skipped":3842,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:02:20.733: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-306
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-306
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-306
Nov 29 19:02:20.767: INFO: Found 0 stateful pods, waiting for 1
Nov 29 19:02:30.770: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 29 19:02:30.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-306 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 19:02:30.929: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 19:02:30.930: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 19:02:30.930: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 19:02:30.932: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 29 19:02:40.936: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 19:02:40.936: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 19:02:40.945: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 29 19:02:40.945: INFO: ss-0  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  }]
Nov 29 19:02:40.945: INFO: 
Nov 29 19:02:40.945: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 29 19:02:41.948: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997564111s
Nov 29 19:02:42.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994369595s
Nov 29 19:02:43.976: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.969341639s
Nov 29 19:02:44.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966253178s
Nov 29 19:02:45.984: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962187683s
Nov 29 19:02:46.988: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.958532723s
Nov 29 19:02:47.991: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.954617511s
Nov 29 19:02:48.994: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951111943s
Nov 29 19:02:49.998: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.83459ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-306
Nov 29 19:02:51.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-306 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 19:02:51.143: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 19:02:51.143: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 19:02:51.143: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 19:02:51.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-306 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 19:02:51.294: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 29 19:02:51.294: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 19:02:51.294: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 19:02:51.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-306 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 19:02:51.451: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 29 19:02:51.451: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 19:02:51.451: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 19:02:51.454: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 19:02:51.454: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 19:02:51.454: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 29 19:02:51.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-306 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 19:02:51.610: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 19:02:51.610: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 19:02:51.610: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 19:02:51.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-306 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 19:02:51.779: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 19:02:51.779: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 19:02:51.779: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 19:02:51.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=statefulset-306 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 19:02:51.919: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 19:02:51.919: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 19:02:51.919: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 19:02:51.919: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 19:02:51.922: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 29 19:03:01.928: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 19:03:01.928: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 19:03:01.928: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 19:03:01.940: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 29 19:03:01.940: INFO: ss-0  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  }]
Nov 29 19:03:01.940: INFO: ss-1  k8sconformance      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:01.940: INFO: ss-2  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:01.940: INFO: 
Nov 29 19:03:01.940: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 19:03:02.943: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 29 19:03:02.944: INFO: ss-0  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  }]
Nov 29 19:03:02.944: INFO: ss-1  k8sconformance      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:02.944: INFO: ss-2  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:02.944: INFO: 
Nov 29 19:03:02.944: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 19:03:03.947: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 29 19:03:03.947: INFO: ss-0  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  }]
Nov 29 19:03:03.947: INFO: ss-1  k8sconformance      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:03.947: INFO: ss-2  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:03.947: INFO: 
Nov 29 19:03:03.947: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 19:03:04.951: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 29 19:03:04.951: INFO: ss-0  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  }]
Nov 29 19:03:04.951: INFO: ss-1  k8sconformance      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:04.951: INFO: ss-2  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:04.951: INFO: 
Nov 29 19:03:04.951: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 19:03:05.955: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 29 19:03:05.955: INFO: ss-0  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  }]
Nov 29 19:03:05.955: INFO: ss-1  k8sconformance      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:05.955: INFO: ss-2  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:05.955: INFO: 
Nov 29 19:03:05.955: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 19:03:06.959: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 29 19:03:06.959: INFO: ss-0  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  }]
Nov 29 19:03:06.959: INFO: ss-1  k8sconformance      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:06.959: INFO: ss-2  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:06.959: INFO: 
Nov 29 19:03:06.959: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 19:03:07.984: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Nov 29 19:03:07.984: INFO: ss-0  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:20 +0000 UTC  }]
Nov 29 19:03:07.984: INFO: ss-1  k8sconformance      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:07.984: INFO: ss-2  k8sconformance-m02  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:07.984: INFO: 
Nov 29 19:03:07.984: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 19:03:08.987: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 29 19:03:08.987: INFO: ss-1  k8sconformance  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:08.987: INFO: 
Nov 29 19:03:08.987: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 29 19:03:09.990: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 29 19:03:09.990: INFO: ss-1  k8sconformance  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 19:02:40 +0000 UTC  }]
Nov 29 19:03:09.990: INFO: 
Nov 29 19:03:09.990: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 29 19:03:10.993: INFO: Verifying statefulset ss doesn't scale past 0 for another 942.985268ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-306
Nov 29 19:03:11.996: INFO: Scaling statefulset ss to 0
Nov 29 19:03:12.003: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 29 19:03:12.005: INFO: Deleting all statefulset in ns statefulset-306
Nov 29 19:03:12.006: INFO: Scaling statefulset ss to 0
Nov 29 19:03:12.012: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 19:03:12.014: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:03:12.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-306" for this suite.

• [SLOW TEST:51.295 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":238,"skipped":3851,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:03:12.028: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1129 19:03:52.069820      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 29 19:04:54.096: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Nov 29 19:04:54.096: INFO: Deleting pod "simpletest.rc-28njh" in namespace "gc-6023"
Nov 29 19:04:54.107: INFO: Deleting pod "simpletest.rc-7p68z" in namespace "gc-6023"
Nov 29 19:04:54.116: INFO: Deleting pod "simpletest.rc-84wps" in namespace "gc-6023"
Nov 29 19:04:54.131: INFO: Deleting pod "simpletest.rc-bqmhl" in namespace "gc-6023"
Nov 29 19:04:54.142: INFO: Deleting pod "simpletest.rc-j4thb" in namespace "gc-6023"
Nov 29 19:04:54.153: INFO: Deleting pod "simpletest.rc-lnfpz" in namespace "gc-6023"
Nov 29 19:04:54.167: INFO: Deleting pod "simpletest.rc-lnp5q" in namespace "gc-6023"
Nov 29 19:04:54.210: INFO: Deleting pod "simpletest.rc-qwhvf" in namespace "gc-6023"
Nov 29 19:04:54.230: INFO: Deleting pod "simpletest.rc-t8hrd" in namespace "gc-6023"
Nov 29 19:04:54.266: INFO: Deleting pod "simpletest.rc-vlkpl" in namespace "gc-6023"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:04:54.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6023" for this suite.

• [SLOW TEST:102.384 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":239,"skipped":3891,"failed":0}
SSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:04:54.413: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Nov 29 19:04:54.455: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:04:54.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3756" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":240,"skipped":3899,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:04:54.526: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 19:04:54.560: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f2328154-509a-4cda-bdf7-87bc032df50a" in namespace "downward-api-1158" to be "Succeeded or Failed"
Nov 29 19:04:54.562: INFO: Pod "downwardapi-volume-f2328154-509a-4cda-bdf7-87bc032df50a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039924ms
Nov 29 19:04:56.565: INFO: Pod "downwardapi-volume-f2328154-509a-4cda-bdf7-87bc032df50a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005042666s
STEP: Saw pod success
Nov 29 19:04:56.566: INFO: Pod "downwardapi-volume-f2328154-509a-4cda-bdf7-87bc032df50a" satisfied condition "Succeeded or Failed"
Nov 29 19:04:56.568: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-f2328154-509a-4cda-bdf7-87bc032df50a container client-container: <nil>
STEP: delete the pod
Nov 29 19:04:56.590: INFO: Waiting for pod downwardapi-volume-f2328154-509a-4cda-bdf7-87bc032df50a to disappear
Nov 29 19:04:56.592: INFO: Pod downwardapi-volume-f2328154-509a-4cda-bdf7-87bc032df50a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:04:56.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1158" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":3912,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:04:56.597: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-88480d48-c363-44a6-ac09-d96ddefc1ee8
STEP: Creating a pod to test consume secrets
Nov 29 19:04:56.623: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d74c2dfb-a1bc-48e1-97d4-8c4a22ae76dc" in namespace "projected-6786" to be "Succeeded or Failed"
Nov 29 19:04:56.625: INFO: Pod "pod-projected-secrets-d74c2dfb-a1bc-48e1-97d4-8c4a22ae76dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5819ms
Nov 29 19:04:58.628: INFO: Pod "pod-projected-secrets-d74c2dfb-a1bc-48e1-97d4-8c4a22ae76dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004756691s
STEP: Saw pod success
Nov 29 19:04:58.628: INFO: Pod "pod-projected-secrets-d74c2dfb-a1bc-48e1-97d4-8c4a22ae76dc" satisfied condition "Succeeded or Failed"
Nov 29 19:04:58.630: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-d74c2dfb-a1bc-48e1-97d4-8c4a22ae76dc container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 19:04:58.646: INFO: Waiting for pod pod-projected-secrets-d74c2dfb-a1bc-48e1-97d4-8c4a22ae76dc to disappear
Nov 29 19:04:58.648: INFO: Pod pod-projected-secrets-d74c2dfb-a1bc-48e1-97d4-8c4a22ae76dc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:04:58.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6786" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":242,"skipped":3913,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:04:58.653: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:04:58.677: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 29 19:05:01.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-8522 create -f -'
Nov 29 19:05:02.260: INFO: stderr: ""
Nov 29 19:05:02.260: INFO: stdout: "e2e-test-crd-publish-openapi-5614-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 29 19:05:02.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-8522 delete e2e-test-crd-publish-openapi-5614-crds test-cr'
Nov 29 19:05:02.332: INFO: stderr: ""
Nov 29 19:05:02.332: INFO: stdout: "e2e-test-crd-publish-openapi-5614-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 29 19:05:02.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-8522 apply -f -'
Nov 29 19:05:02.606: INFO: stderr: ""
Nov 29 19:05:02.606: INFO: stdout: "e2e-test-crd-publish-openapi-5614-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 29 19:05:02.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 --namespace=crd-publish-openapi-8522 delete e2e-test-crd-publish-openapi-5614-crds test-cr'
Nov 29 19:05:02.679: INFO: stderr: ""
Nov 29 19:05:02.679: INFO: stdout: "e2e-test-crd-publish-openapi-5614-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 29 19:05:02.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 explain e2e-test-crd-publish-openapi-5614-crds'
Nov 29 19:05:02.845: INFO: stderr: ""
Nov 29 19:05:02.845: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5614-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:05:04.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8522" for this suite.

• [SLOW TEST:6.021 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":243,"skipped":3931,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:05:04.675: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Nov 29 19:05:04.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-4104 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 29 19:05:04.770: INFO: stderr: ""
Nov 29 19:05:04.770: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Nov 29 19:05:04.770: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 29 19:05:04.770: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4104" to be "running and ready, or succeeded"
Nov 29 19:05:04.774: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.724662ms
Nov 29 19:05:06.777: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006729764s
Nov 29 19:05:06.777: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 29 19:05:06.777: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov 29 19:05:06.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 logs logs-generator logs-generator --namespace=kubectl-4104'
Nov 29 19:05:06.855: INFO: stderr: ""
Nov 29 19:05:06.855: INFO: stdout: "I1129 19:05:05.684185       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/467 437\nI1129 19:05:05.884332       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/wplg 452\nI1129 19:05:06.084259       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/88pb 514\nI1129 19:05:06.284374       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/c4vz 382\nI1129 19:05:06.484349       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/7l6 252\nI1129 19:05:06.684357       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/wkqq 542\n"
STEP: limiting log lines
Nov 29 19:05:06.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 logs logs-generator logs-generator --namespace=kubectl-4104 --tail=1'
Nov 29 19:05:06.930: INFO: stderr: ""
Nov 29 19:05:06.930: INFO: stdout: "I1129 19:05:06.884373       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/b4zs 304\n"
Nov 29 19:05:06.930: INFO: got output "I1129 19:05:06.884373       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/b4zs 304\n"
STEP: limiting log bytes
Nov 29 19:05:06.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 logs logs-generator logs-generator --namespace=kubectl-4104 --limit-bytes=1'
Nov 29 19:05:07.006: INFO: stderr: ""
Nov 29 19:05:07.006: INFO: stdout: "I"
Nov 29 19:05:07.006: INFO: got output "I"
STEP: exposing timestamps
Nov 29 19:05:07.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 logs logs-generator logs-generator --namespace=kubectl-4104 --tail=1 --timestamps'
Nov 29 19:05:07.082: INFO: stderr: ""
Nov 29 19:05:07.082: INFO: stdout: "2020-11-29T19:05:06.884548263Z I1129 19:05:06.884373       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/b4zs 304\n"
Nov 29 19:05:07.082: INFO: got output "2020-11-29T19:05:06.884548263Z I1129 19:05:06.884373       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/b4zs 304\n"
STEP: restricting to a time range
Nov 29 19:05:09.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 logs logs-generator logs-generator --namespace=kubectl-4104 --since=1s'
Nov 29 19:05:09.659: INFO: stderr: ""
Nov 29 19:05:09.659: INFO: stdout: "I1129 19:05:08.684379       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/l4r4 588\nI1129 19:05:08.884250       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/kw6 233\nI1129 19:05:09.084382       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/mcn 338\nI1129 19:05:09.284407       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/b7v 312\nI1129 19:05:09.484367       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/q6gl 575\n"
Nov 29 19:05:09.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 logs logs-generator logs-generator --namespace=kubectl-4104 --since=24h'
Nov 29 19:05:09.737: INFO: stderr: ""
Nov 29 19:05:09.737: INFO: stdout: "I1129 19:05:05.684185       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/467 437\nI1129 19:05:05.884332       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/wplg 452\nI1129 19:05:06.084259       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/88pb 514\nI1129 19:05:06.284374       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/c4vz 382\nI1129 19:05:06.484349       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/7l6 252\nI1129 19:05:06.684357       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/wkqq 542\nI1129 19:05:06.884373       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/b4zs 304\nI1129 19:05:07.084355       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/dqq 508\nI1129 19:05:07.284358       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/tfw4 290\nI1129 19:05:07.484376       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/pc7 331\nI1129 19:05:07.684384       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/9stl 554\nI1129 19:05:07.884405       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/tx9p 346\nI1129 19:05:08.084402       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/lxg 353\nI1129 19:05:08.284430       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/p94 231\nI1129 19:05:08.484354       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/mhn 274\nI1129 19:05:08.684379       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/l4r4 588\nI1129 19:05:08.884250       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/kw6 233\nI1129 19:05:09.084382       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/mcn 338\nI1129 19:05:09.284407       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/b7v 312\nI1129 19:05:09.484367       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/q6gl 575\nI1129 19:05:09.684338       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/57h 357\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Nov 29 19:05:09.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete pod logs-generator --namespace=kubectl-4104'
Nov 29 19:05:11.972: INFO: stderr: ""
Nov 29 19:05:11.972: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:05:11.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4104" for this suite.

• [SLOW TEST:7.303 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":244,"skipped":3948,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:05:11.978: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 29 19:05:12.001: INFO: Waiting up to 5m0s for pod "pod-695a2e0b-8c24-4f04-a766-d594bd946ddd" in namespace "emptydir-4587" to be "Succeeded or Failed"
Nov 29 19:05:12.005: INFO: Pod "pod-695a2e0b-8c24-4f04-a766-d594bd946ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.922306ms
Nov 29 19:05:14.008: INFO: Pod "pod-695a2e0b-8c24-4f04-a766-d594bd946ddd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006998577s
STEP: Saw pod success
Nov 29 19:05:14.008: INFO: Pod "pod-695a2e0b-8c24-4f04-a766-d594bd946ddd" satisfied condition "Succeeded or Failed"
Nov 29 19:05:14.010: INFO: Trying to get logs from node k8sconformance-m02 pod pod-695a2e0b-8c24-4f04-a766-d594bd946ddd container test-container: <nil>
STEP: delete the pod
Nov 29 19:05:14.024: INFO: Waiting for pod pod-695a2e0b-8c24-4f04-a766-d594bd946ddd to disappear
Nov 29 19:05:14.058: INFO: Pod pod-695a2e0b-8c24-4f04-a766-d594bd946ddd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:05:14.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4587" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":245,"skipped":3960,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:05:14.064: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:05:14.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-935" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":246,"skipped":3971,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:05:14.122: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-ab69696e-1bc9-4598-9648-bc74cf908cab
STEP: Creating a pod to test consume configMaps
Nov 29 19:05:14.150: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98652733-2208-4a31-b217-ef4beebc7ec8" in namespace "projected-1681" to be "Succeeded or Failed"
Nov 29 19:05:14.153: INFO: Pod "pod-projected-configmaps-98652733-2208-4a31-b217-ef4beebc7ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.683527ms
Nov 29 19:05:16.156: INFO: Pod "pod-projected-configmaps-98652733-2208-4a31-b217-ef4beebc7ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006731936s
STEP: Saw pod success
Nov 29 19:05:16.156: INFO: Pod "pod-projected-configmaps-98652733-2208-4a31-b217-ef4beebc7ec8" satisfied condition "Succeeded or Failed"
Nov 29 19:05:16.158: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-98652733-2208-4a31-b217-ef4beebc7ec8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 19:05:16.175: INFO: Waiting for pod pod-projected-configmaps-98652733-2208-4a31-b217-ef4beebc7ec8 to disappear
Nov 29 19:05:16.177: INFO: Pod pod-projected-configmaps-98652733-2208-4a31-b217-ef4beebc7ec8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:05:16.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1681" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":3978,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:05:16.182: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 29 19:05:16.225: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 29 19:06:16.238: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Nov 29 19:06:16.256: INFO: Created pod: pod0-sched-preemption-low-priority
Nov 29 19:06:16.266: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:06:42.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5728" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:86.151 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":248,"skipped":4012,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:06:42.333: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-61205494-54ab-4108-8a45-a935bf62bcba
STEP: Creating secret with name s-test-opt-upd-8b7bc23d-bd5c-481a-9b2d-5109bb61656f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-61205494-54ab-4108-8a45-a935bf62bcba
STEP: Updating secret s-test-opt-upd-8b7bc23d-bd5c-481a-9b2d-5109bb61656f
STEP: Creating secret with name s-test-opt-create-12e996d5-f999-4f8a-8d11-3b4a173794ea
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:07:50.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4561" for this suite.

• [SLOW TEST:68.320 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":249,"skipped":4024,"failed":0}
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:07:50.653: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 19:07:50.681: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc00e809-b902-4987-847c-5d8311394f24" in namespace "downward-api-5250" to be "Succeeded or Failed"
Nov 29 19:07:50.691: INFO: Pod "downwardapi-volume-cc00e809-b902-4987-847c-5d8311394f24": Phase="Pending", Reason="", readiness=false. Elapsed: 10.231008ms
Nov 29 19:07:52.694: INFO: Pod "downwardapi-volume-cc00e809-b902-4987-847c-5d8311394f24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013296425s
STEP: Saw pod success
Nov 29 19:07:52.694: INFO: Pod "downwardapi-volume-cc00e809-b902-4987-847c-5d8311394f24" satisfied condition "Succeeded or Failed"
Nov 29 19:07:52.696: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-cc00e809-b902-4987-847c-5d8311394f24 container client-container: <nil>
STEP: delete the pod
Nov 29 19:07:52.727: INFO: Waiting for pod downwardapi-volume-cc00e809-b902-4987-847c-5d8311394f24 to disappear
Nov 29 19:07:52.729: INFO: Pod downwardapi-volume-cc00e809-b902-4987-847c-5d8311394f24 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:07:52.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5250" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":250,"skipped":4024,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:07:52.734: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 29 19:07:53.171: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 19:07:56.189: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:07:56.193: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:07:57.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-773" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":251,"skipped":4028,"failed":0}

------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:07:57.457: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Nov 29 19:07:57.538: INFO: Waiting up to 5m0s for pod "client-containers-1d9fee05-c8ec-4e2d-97ea-8af5ef801dbf" in namespace "containers-3924" to be "Succeeded or Failed"
Nov 29 19:07:57.540: INFO: Pod "client-containers-1d9fee05-c8ec-4e2d-97ea-8af5ef801dbf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.912456ms
Nov 29 19:07:59.543: INFO: Pod "client-containers-1d9fee05-c8ec-4e2d-97ea-8af5ef801dbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004939537s
STEP: Saw pod success
Nov 29 19:07:59.543: INFO: Pod "client-containers-1d9fee05-c8ec-4e2d-97ea-8af5ef801dbf" satisfied condition "Succeeded or Failed"
Nov 29 19:07:59.545: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-1d9fee05-c8ec-4e2d-97ea-8af5ef801dbf container test-container: <nil>
STEP: delete the pod
Nov 29 19:07:59.557: INFO: Waiting for pod client-containers-1d9fee05-c8ec-4e2d-97ea-8af5ef801dbf to disappear
Nov 29 19:07:59.559: INFO: Pod client-containers-1d9fee05-c8ec-4e2d-97ea-8af5ef801dbf no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:07:59.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3924" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":252,"skipped":4028,"failed":0}
SS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:07:59.564: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:07:59.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-998" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":253,"skipped":4030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:07:59.635: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-7c0d64f1-d398-4a5b-b325-5b34c66ed32e
STEP: Creating a pod to test consume configMaps
Nov 29 19:07:59.664: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7395a210-0fd7-478d-af99-327f210e3921" in namespace "projected-653" to be "Succeeded or Failed"
Nov 29 19:07:59.665: INFO: Pod "pod-projected-configmaps-7395a210-0fd7-478d-af99-327f210e3921": Phase="Pending", Reason="", readiness=false. Elapsed: 1.585337ms
Nov 29 19:08:01.667: INFO: Pod "pod-projected-configmaps-7395a210-0fd7-478d-af99-327f210e3921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003796976s
STEP: Saw pod success
Nov 29 19:08:01.667: INFO: Pod "pod-projected-configmaps-7395a210-0fd7-478d-af99-327f210e3921" satisfied condition "Succeeded or Failed"
Nov 29 19:08:01.669: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-7395a210-0fd7-478d-af99-327f210e3921 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 19:08:01.683: INFO: Waiting for pod pod-projected-configmaps-7395a210-0fd7-478d-af99-327f210e3921 to disappear
Nov 29 19:08:01.685: INFO: Pod pod-projected-configmaps-7395a210-0fd7-478d-af99-327f210e3921 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:08:01.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-653" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":254,"skipped":4055,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:08:01.690: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 19:08:02.654: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 19:08:04.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273682, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273682, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273682, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742273682, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 19:08:07.675: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:08:07.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4920" for this suite.
STEP: Destroying namespace "webhook-4920-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.153 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":255,"skipped":4055,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:08:07.843: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:08:07.873: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:08:14.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7251" for this suite.

• [SLOW TEST:6.185 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":256,"skipped":4063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:08:14.028: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4886
STEP: creating service affinity-clusterip in namespace services-4886
STEP: creating replication controller affinity-clusterip in namespace services-4886
I1129 19:08:14.209907      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4886, replica count: 3
I1129 19:08:17.260743      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 19:08:17.265: INFO: Creating new exec pod
Nov 29 19:08:20.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-4886 execpod-affinityc2kgx -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Nov 29 19:08:20.442: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 29 19:08:20.442: INFO: stdout: ""
Nov 29 19:08:20.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-4886 execpod-affinityc2kgx -- /bin/sh -x -c nc -zv -t -w 2 10.104.156.173 80'
Nov 29 19:08:20.595: INFO: stderr: "+ nc -zv -t -w 2 10.104.156.173 80\nConnection to 10.104.156.173 80 port [tcp/http] succeeded!\n"
Nov 29 19:08:20.595: INFO: stdout: ""
Nov 29 19:08:20.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-4886 execpod-affinityc2kgx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.156.173:80/ ; done'
Nov 29 19:08:20.822: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.156.173:80/\n"
Nov 29 19:08:20.822: INFO: stdout: "\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h\naffinity-clusterip-7778h"
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Received response from host: affinity-clusterip-7778h
Nov 29 19:08:20.822: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4886, will wait for the garbage collector to delete the pods
Nov 29 19:08:20.906: INFO: Deleting ReplicationController affinity-clusterip took: 5.004904ms
Nov 29 19:08:21.006: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.232254ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:08:28.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4886" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.029 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":257,"skipped":4096,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:08:28.058: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 19:08:28.513: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 19:08:31.544: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:08:31.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5629" for this suite.
STEP: Destroying namespace "webhook-5629-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":258,"skipped":4114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:08:31.634: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:08:32.304: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 29 19:08:32.305: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 29 19:08:32.305: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.305: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 29 19:08:32.305: INFO: Checking APIGroup: extensions
Nov 29 19:08:32.306: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Nov 29 19:08:32.306: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Nov 29 19:08:32.306: INFO: extensions/v1beta1 matches extensions/v1beta1
Nov 29 19:08:32.306: INFO: Checking APIGroup: apps
Nov 29 19:08:32.307: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 29 19:08:32.307: INFO: Versions found [{apps/v1 v1}]
Nov 29 19:08:32.307: INFO: apps/v1 matches apps/v1
Nov 29 19:08:32.307: INFO: Checking APIGroup: events.k8s.io
Nov 29 19:08:32.308: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 29 19:08:32.308: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.308: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 29 19:08:32.308: INFO: Checking APIGroup: authentication.k8s.io
Nov 29 19:08:32.308: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 29 19:08:32.308: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.308: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 29 19:08:32.308: INFO: Checking APIGroup: authorization.k8s.io
Nov 29 19:08:32.309: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 29 19:08:32.309: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.309: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 29 19:08:32.309: INFO: Checking APIGroup: autoscaling
Nov 29 19:08:32.310: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Nov 29 19:08:32.310: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Nov 29 19:08:32.310: INFO: autoscaling/v1 matches autoscaling/v1
Nov 29 19:08:32.310: INFO: Checking APIGroup: batch
Nov 29 19:08:32.310: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 29 19:08:32.310: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Nov 29 19:08:32.310: INFO: batch/v1 matches batch/v1
Nov 29 19:08:32.310: INFO: Checking APIGroup: certificates.k8s.io
Nov 29 19:08:32.311: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 29 19:08:32.311: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.311: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 29 19:08:32.311: INFO: Checking APIGroup: networking.k8s.io
Nov 29 19:08:32.312: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 29 19:08:32.312: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.312: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 29 19:08:32.312: INFO: Checking APIGroup: policy
Nov 29 19:08:32.312: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Nov 29 19:08:32.312: INFO: Versions found [{policy/v1beta1 v1beta1}]
Nov 29 19:08:32.312: INFO: policy/v1beta1 matches policy/v1beta1
Nov 29 19:08:32.312: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 29 19:08:32.313: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 29 19:08:32.313: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.313: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 29 19:08:32.313: INFO: Checking APIGroup: storage.k8s.io
Nov 29 19:08:32.314: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 29 19:08:32.314: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.314: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 29 19:08:32.314: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 29 19:08:32.315: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 29 19:08:32.315: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.315: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 29 19:08:32.315: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 29 19:08:32.315: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 29 19:08:32.315: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.315: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 29 19:08:32.315: INFO: Checking APIGroup: scheduling.k8s.io
Nov 29 19:08:32.316: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 29 19:08:32.316: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.316: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 29 19:08:32.316: INFO: Checking APIGroup: coordination.k8s.io
Nov 29 19:08:32.317: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 29 19:08:32.317: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.317: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 29 19:08:32.317: INFO: Checking APIGroup: node.k8s.io
Nov 29 19:08:32.317: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Nov 29 19:08:32.317: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.317: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Nov 29 19:08:32.317: INFO: Checking APIGroup: discovery.k8s.io
Nov 29 19:08:32.318: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Nov 29 19:08:32.318: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Nov 29 19:08:32.318: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:08:32.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8739" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":259,"skipped":4166,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:08:32.324: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov 29 19:08:34.863: INFO: Successfully updated pod "adopt-release-f8fbl"
STEP: Checking that the Job readopts the Pod
Nov 29 19:08:34.863: INFO: Waiting up to 15m0s for pod "adopt-release-f8fbl" in namespace "job-242" to be "adopted"
Nov 29 19:08:34.867: INFO: Pod "adopt-release-f8fbl": Phase="Running", Reason="", readiness=true. Elapsed: 3.70963ms
Nov 29 19:08:36.869: INFO: Pod "adopt-release-f8fbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.006285141s
Nov 29 19:08:36.869: INFO: Pod "adopt-release-f8fbl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov 29 19:08:37.375: INFO: Successfully updated pod "adopt-release-f8fbl"
STEP: Checking that the Job releases the Pod
Nov 29 19:08:37.375: INFO: Waiting up to 15m0s for pod "adopt-release-f8fbl" in namespace "job-242" to be "released"
Nov 29 19:08:37.380: INFO: Pod "adopt-release-f8fbl": Phase="Running", Reason="", readiness=true. Elapsed: 5.125495ms
Nov 29 19:08:39.383: INFO: Pod "adopt-release-f8fbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.008196054s
Nov 29 19:08:39.383: INFO: Pod "adopt-release-f8fbl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:08:39.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-242" for this suite.

• [SLOW TEST:7.065 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":260,"skipped":4185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:08:39.389: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Nov 29 19:08:39.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 api-versions'
Nov 29 19:08:39.496: INFO: stderr: ""
Nov 29 19:08:39.496: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:08:39.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2326" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":261,"skipped":4228,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:08:39.503: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-4606149e-80f0-47c0-aefb-c22b6f65e492
STEP: Creating a pod to test consume secrets
Nov 29 19:08:39.594: INFO: Waiting up to 5m0s for pod "pod-secrets-a6f22d26-c280-497b-ae6d-ecf471d4fb94" in namespace "secrets-2337" to be "Succeeded or Failed"
Nov 29 19:08:39.615: INFO: Pod "pod-secrets-a6f22d26-c280-497b-ae6d-ecf471d4fb94": Phase="Pending", Reason="", readiness=false. Elapsed: 20.566431ms
Nov 29 19:08:41.618: INFO: Pod "pod-secrets-a6f22d26-c280-497b-ae6d-ecf471d4fb94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023744154s
STEP: Saw pod success
Nov 29 19:08:41.618: INFO: Pod "pod-secrets-a6f22d26-c280-497b-ae6d-ecf471d4fb94" satisfied condition "Succeeded or Failed"
Nov 29 19:08:41.620: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-a6f22d26-c280-497b-ae6d-ecf471d4fb94 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 19:08:41.631: INFO: Waiting for pod pod-secrets-a6f22d26-c280-497b-ae6d-ecf471d4fb94 to disappear
Nov 29 19:08:41.632: INFO: Pod pod-secrets-a6f22d26-c280-497b-ae6d-ecf471d4fb94 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:08:41.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2337" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":262,"skipped":4230,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:08:41.637: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Nov 29 19:08:41.660: INFO: created test-event-1
Nov 29 19:08:41.663: INFO: created test-event-2
Nov 29 19:08:41.664: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Nov 29 19:08:41.666: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Nov 29 19:08:41.676: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:08:41.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2397" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":263,"skipped":4241,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:08:41.683: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-b5074a21-08e8-4e46-b067-83cd8179e7f3 in namespace container-probe-1169
Nov 29 19:08:43.716: INFO: Started pod liveness-b5074a21-08e8-4e46-b067-83cd8179e7f3 in namespace container-probe-1169
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 19:08:43.718: INFO: Initial restart count of pod liveness-b5074a21-08e8-4e46-b067-83cd8179e7f3 is 0
Nov 29 19:09:05.752: INFO: Restart count of pod container-probe-1169/liveness-b5074a21-08e8-4e46-b067-83cd8179e7f3 is now 1 (22.033851115s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:09:05.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1169" for this suite.

• [SLOW TEST:24.098 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":264,"skipped":4242,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:09:05.783: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 19:09:06.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 19:09:09.979: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:09:10.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6158" for this suite.
STEP: Destroying namespace "webhook-6158-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":265,"skipped":4254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:09:10.048: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7582
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Nov 29 19:09:10.112: INFO: Found 0 stateful pods, waiting for 3
Nov 29 19:09:20.115: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 19:09:20.115: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 19:09:20.115: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 29 19:09:20.138: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 29 19:09:30.170: INFO: Updating stateful set ss2
Nov 29 19:09:30.189: INFO: Waiting for Pod statefulset-7582/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Nov 29 19:09:40.248: INFO: Found 2 stateful pods, waiting for 3
Nov 29 19:09:50.251: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 19:09:50.251: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 19:09:50.251: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 29 19:09:50.272: INFO: Updating stateful set ss2
Nov 29 19:09:50.284: INFO: Waiting for Pod statefulset-7582/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 19:10:00.305: INFO: Updating stateful set ss2
Nov 29 19:10:00.314: INFO: Waiting for StatefulSet statefulset-7582/ss2 to complete update
Nov 29 19:10:00.314: INFO: Waiting for Pod statefulset-7582/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 19:10:10.320: INFO: Waiting for StatefulSet statefulset-7582/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 29 19:10:20.319: INFO: Deleting all statefulset in ns statefulset-7582
Nov 29 19:10:20.321: INFO: Scaling statefulset ss2 to 0
Nov 29 19:10:30.331: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 19:10:30.333: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:10:30.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7582" for this suite.

• [SLOW TEST:80.301 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":266,"skipped":4277,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:10:30.350: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 29 19:10:30.371: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 19:10:30.377: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 19:10:30.379: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Nov 29 19:10:30.383: INFO: coredns-f9fd979d6-brw29 from kube-system started at 2020-11-29 17:56:10 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.383: INFO: 	Container coredns ready: true, restart count 0
Nov 29 19:10:30.383: INFO: etcd-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.383: INFO: 	Container etcd ready: true, restart count 0
Nov 29 19:10:30.383: INFO: kindnet-w9hf5 from kube-system started at 2020-11-29 17:56:09 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.383: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 29 19:10:30.383: INFO: kube-apiserver-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.383: INFO: 	Container kube-apiserver ready: true, restart count 0
Nov 29 19:10:30.383: INFO: kube-controller-manager-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.383: INFO: 	Container kube-controller-manager ready: true, restart count 0
Nov 29 19:10:30.383: INFO: kube-proxy-rk57s from kube-system started at 2020-11-29 17:56:09 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.383: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 19:10:30.383: INFO: kube-scheduler-k8sconformance from kube-system started at 2020-11-29 17:56:00 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.383: INFO: 	Container kube-scheduler ready: true, restart count 0
Nov 29 19:10:30.383: INFO: storage-provisioner from kube-system started at 2020-11-29 17:56:11 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.383: INFO: 	Container storage-provisioner ready: true, restart count 0
Nov 29 19:10:30.383: INFO: sonobuoy from sonobuoy started at 2020-11-29 17:57:02 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.383: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 29 19:10:30.383: INFO: sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-bfkxd from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 19:10:30.383: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 29 19:10:30.383: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 19:10:30.383: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Nov 29 19:10:30.386: INFO: kindnet-dcctx from kube-system started at 2020-11-29 18:59:04 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.386: INFO: 	Container kindnet-cni ready: true, restart count 0
Nov 29 19:10:30.386: INFO: kube-proxy-p6zkw from kube-system started at 2020-11-29 17:56:58 +0000 UTC (1 container statuses recorded)
Nov 29 19:10:30.386: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 19:10:30.386: INFO: sonobuoy-e2e-job-9d13ec04340c4e3d from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 19:10:30.386: INFO: 	Container e2e ready: true, restart count 0
Nov 29 19:10:30.386: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 19:10:30.386: INFO: sonobuoy-systemd-logs-daemon-set-ec8b2f7d05f94611-nkv9s from sonobuoy started at 2020-11-29 17:57:09 +0000 UTC (2 container statuses recorded)
Nov 29 19:10:30.386: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 29 19:10:30.386: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-55685c7c-2ab4-4995-9f81-2b53a28567fd 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-55685c7c-2ab4-4995-9f81-2b53a28567fd off the node k8sconformance-m02
STEP: verifying the node doesn't have the label kubernetes.io/e2e-55685c7c-2ab4-4995-9f81-2b53a28567fd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:10:34.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2272" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":267,"skipped":4280,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:10:34.444: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-1d529bb8-c1ac-47db-ad86-a71323ca0d71 in namespace container-probe-198
Nov 29 19:10:36.473: INFO: Started pod test-webserver-1d529bb8-c1ac-47db-ad86-a71323ca0d71 in namespace container-probe-198
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 19:10:36.475: INFO: Initial restart count of pod test-webserver-1d529bb8-c1ac-47db-ad86-a71323ca0d71 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:14:36.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-198" for this suite.

• [SLOW TEST:242.454 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":268,"skipped":4293,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:14:36.898: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:14:47.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4641" for this suite.

• [SLOW TEST:11.081 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":269,"skipped":4312,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:14:47.979: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-1f65092c-3513-4ac2-976b-00c8cbaf4202
STEP: Creating a pod to test consume configMaps
Nov 29 19:14:48.042: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f95de341-e34c-4f8b-b15d-de7912e8f53d" in namespace "projected-5480" to be "Succeeded or Failed"
Nov 29 19:14:48.044: INFO: Pod "pod-projected-configmaps-f95de341-e34c-4f8b-b15d-de7912e8f53d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.148264ms
Nov 29 19:14:50.049: INFO: Pod "pod-projected-configmaps-f95de341-e34c-4f8b-b15d-de7912e8f53d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006429754s
STEP: Saw pod success
Nov 29 19:14:50.049: INFO: Pod "pod-projected-configmaps-f95de341-e34c-4f8b-b15d-de7912e8f53d" satisfied condition "Succeeded or Failed"
Nov 29 19:14:50.051: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-f95de341-e34c-4f8b-b15d-de7912e8f53d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 19:14:50.074: INFO: Waiting for pod pod-projected-configmaps-f95de341-e34c-4f8b-b15d-de7912e8f53d to disappear
Nov 29 19:14:50.075: INFO: Pod pod-projected-configmaps-f95de341-e34c-4f8b-b15d-de7912e8f53d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:14:50.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5480" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:14:50.081: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 29 19:14:50.672: INFO: starting watch
STEP: patching
STEP: updating
Nov 29 19:14:50.679: INFO: waiting for watch events with expected annotations
Nov 29 19:14:50.679: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:14:50.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1502" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":271,"skipped":4377,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:14:50.727: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:14:50.783: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b9b45357-3661-405c-b97d-a9204b450f0b", Controller:(*bool)(0xc0048644ba), BlockOwnerDeletion:(*bool)(0xc0048644bb)}}
Nov 29 19:14:50.799: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"daa0afb8-0f73-4296-9ccd-a5711f4b3b74", Controller:(*bool)(0xc003e22946), BlockOwnerDeletion:(*bool)(0xc003e22947)}}
Nov 29 19:14:50.802: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8ba9bc6c-4f72-4014-9609-ba2d90761d49", Controller:(*bool)(0xc00486470a), BlockOwnerDeletion:(*bool)(0xc00486470b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:14:55.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3793" for this suite.

• [SLOW TEST:5.089 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":272,"skipped":4387,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:14:55.816: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:14:55.837: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:14:57.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9714" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":273,"skipped":4402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:14:57.932: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 19:14:57.959: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b29a61a-69a8-4432-9478-d0e78eff0007" in namespace "projected-2007" to be "Succeeded or Failed"
Nov 29 19:14:57.964: INFO: Pod "downwardapi-volume-4b29a61a-69a8-4432-9478-d0e78eff0007": Phase="Pending", Reason="", readiness=false. Elapsed: 4.348644ms
Nov 29 19:14:59.980: INFO: Pod "downwardapi-volume-4b29a61a-69a8-4432-9478-d0e78eff0007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020339407s
STEP: Saw pod success
Nov 29 19:14:59.980: INFO: Pod "downwardapi-volume-4b29a61a-69a8-4432-9478-d0e78eff0007" satisfied condition "Succeeded or Failed"
Nov 29 19:14:59.982: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-4b29a61a-69a8-4432-9478-d0e78eff0007 container client-container: <nil>
STEP: delete the pod
Nov 29 19:14:59.996: INFO: Waiting for pod downwardapi-volume-4b29a61a-69a8-4432-9478-d0e78eff0007 to disappear
Nov 29 19:14:59.998: INFO: Pod downwardapi-volume-4b29a61a-69a8-4432-9478-d0e78eff0007 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:14:59.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2007" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4424,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:15:00.004: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:15:00.038: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:15:01.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7278" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":275,"skipped":4431,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:15:01.057: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-4e7cf768-806f-4536-9245-d3941404d019 in namespace container-probe-5356
Nov 29 19:15:03.134: INFO: Started pod busybox-4e7cf768-806f-4536-9245-d3941404d019 in namespace container-probe-5356
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 19:15:03.135: INFO: Initial restart count of pod busybox-4e7cf768-806f-4536-9245-d3941404d019 is 0
Nov 29 19:15:57.214: INFO: Restart count of pod container-probe-5356/busybox-4e7cf768-806f-4536-9245-d3941404d019 is now 1 (54.078475334s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:15:57.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5356" for this suite.

• [SLOW TEST:56.177 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":276,"skipped":4438,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:15:57.233: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-2e5ac6a0-04f7-41dc-942a-79b466c0909c
STEP: Creating a pod to test consume secrets
Nov 29 19:15:57.262: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-09c909a6-5af5-4eef-af98-53d70d270b2f" in namespace "projected-1070" to be "Succeeded or Failed"
Nov 29 19:15:57.264: INFO: Pod "pod-projected-secrets-09c909a6-5af5-4eef-af98-53d70d270b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.799055ms
Nov 29 19:15:59.267: INFO: Pod "pod-projected-secrets-09c909a6-5af5-4eef-af98-53d70d270b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004730514s
STEP: Saw pod success
Nov 29 19:15:59.267: INFO: Pod "pod-projected-secrets-09c909a6-5af5-4eef-af98-53d70d270b2f" satisfied condition "Succeeded or Failed"
Nov 29 19:15:59.269: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-09c909a6-5af5-4eef-af98-53d70d270b2f container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 19:15:59.283: INFO: Waiting for pod pod-projected-secrets-09c909a6-5af5-4eef-af98-53d70d270b2f to disappear
Nov 29 19:15:59.285: INFO: Pod pod-projected-secrets-09c909a6-5af5-4eef-af98-53d70d270b2f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:15:59.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1070" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":277,"skipped":4440,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:15:59.290: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 29 19:16:01.329: INFO: &Pod{ObjectMeta:{send-events-dd31d7c1-f78d-41e2-9078-e240daaa8953  events-9384 /api/v1/namespaces/events-9384/pods/send-events-dd31d7c1-f78d-41e2-9078-e240daaa8953 20b37cab-5451-4b0d-8ccb-d7f9b7bac5c9 21166 0 2020-11-29 19:15:59 +0000 UTC <nil> <nil> map[name:foo time:312774431] map[] [] []  [{e2e.test Update v1 2020-11-29 19:15:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 19:16:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-w45hk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-w45hk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-w45hk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:15:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:16:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:16:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:15:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.86,StartTime:2020-11-29 19:15:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 19:16:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://e60d7474febd5e63d5dda7d4cb2be3d0026fe147942bd5dd53d147c52030670a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Nov 29 19:16:03.334: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 29 19:16:05.337: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:16:05.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9384" for this suite.

• [SLOW TEST:6.061 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":278,"skipped":4451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:16:05.352: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 19:16:05.769: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 19:16:08.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:16:08.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1697" for this suite.
STEP: Destroying namespace "webhook-1697-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":279,"skipped":4487,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:16:08.870: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4468
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4468
STEP: creating replication controller externalsvc in namespace services-4468
I1129 19:16:08.963241      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4468, replica count: 2
I1129 19:16:12.013686      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov 29 19:16:12.032: INFO: Creating new exec pod
Nov 29 19:16:14.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 exec --namespace=services-4468 execpod8mtk2 -- /bin/sh -x -c nslookup clusterip-service.services-4468.svc.cluster.local'
Nov 29 19:16:14.897: INFO: stderr: "+ nslookup clusterip-service.services-4468.svc.cluster.local\n"
Nov 29 19:16:14.897: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-4468.svc.cluster.local\tcanonical name = externalsvc.services-4468.svc.cluster.local.\nName:\texternalsvc.services-4468.svc.cluster.local\nAddress: 10.103.180.238\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4468, will wait for the garbage collector to delete the pods
Nov 29 19:16:14.955: INFO: Deleting ReplicationController externalsvc took: 4.945802ms
Nov 29 19:16:15.055: INFO: Terminating ReplicationController externalsvc pods took: 100.21762ms
Nov 29 19:16:28.068: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:16:28.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4468" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:19.213 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":280,"skipped":4490,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:16:28.083: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:16:39.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1511" for this suite.

• [SLOW TEST:11.103 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":281,"skipped":4508,"failed":0}
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:16:39.186: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-ade9102b-6123-4d06-932d-c4fafdfb549e
STEP: Creating secret with name secret-projected-all-test-volume-8521beae-5139-4a54-a805-bfad93bbc5a5
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 29 19:16:39.219: INFO: Waiting up to 5m0s for pod "projected-volume-6aed5abb-0fed-4dbb-9b64-04bd7361f85a" in namespace "projected-5244" to be "Succeeded or Failed"
Nov 29 19:16:39.223: INFO: Pod "projected-volume-6aed5abb-0fed-4dbb-9b64-04bd7361f85a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.114592ms
Nov 29 19:16:41.226: INFO: Pod "projected-volume-6aed5abb-0fed-4dbb-9b64-04bd7361f85a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0070474s
STEP: Saw pod success
Nov 29 19:16:41.226: INFO: Pod "projected-volume-6aed5abb-0fed-4dbb-9b64-04bd7361f85a" satisfied condition "Succeeded or Failed"
Nov 29 19:16:41.228: INFO: Trying to get logs from node k8sconformance-m02 pod projected-volume-6aed5abb-0fed-4dbb-9b64-04bd7361f85a container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 29 19:16:41.242: INFO: Waiting for pod projected-volume-6aed5abb-0fed-4dbb-9b64-04bd7361f85a to disappear
Nov 29 19:16:41.244: INFO: Pod projected-volume-6aed5abb-0fed-4dbb-9b64-04bd7361f85a no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:16:41.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5244" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":282,"skipped":4509,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:16:41.250: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Nov 29 19:16:41.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-5656'
Nov 29 19:16:41.558: INFO: stderr: ""
Nov 29 19:16:41.558: INFO: stdout: "pod/pause created\n"
Nov 29 19:16:41.558: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 29 19:16:41.558: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5656" to be "running and ready"
Nov 29 19:16:41.561: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.361535ms
Nov 29 19:16:43.564: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006315249s
Nov 29 19:16:43.564: INFO: Pod "pause" satisfied condition "running and ready"
Nov 29 19:16:43.564: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 29 19:16:43.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 label pods pause testing-label=testing-label-value --namespace=kubectl-5656'
Nov 29 19:16:43.639: INFO: stderr: ""
Nov 29 19:16:43.639: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 29 19:16:43.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pod pause -L testing-label --namespace=kubectl-5656'
Nov 29 19:16:43.706: INFO: stderr: ""
Nov 29 19:16:43.706: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 29 19:16:43.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 label pods pause testing-label- --namespace=kubectl-5656'
Nov 29 19:16:43.778: INFO: stderr: ""
Nov 29 19:16:43.778: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 29 19:16:43.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pod pause -L testing-label --namespace=kubectl-5656'
Nov 29 19:16:43.847: INFO: stderr: ""
Nov 29 19:16:43.847: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Nov 29 19:16:43.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete --grace-period=0 --force -f - --namespace=kubectl-5656'
Nov 29 19:16:43.924: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 19:16:43.924: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 29 19:16:43.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get rc,svc -l name=pause --no-headers --namespace=kubectl-5656'
Nov 29 19:16:44.023: INFO: stderr: "No resources found in kubectl-5656 namespace.\n"
Nov 29 19:16:44.023: INFO: stdout: ""
Nov 29 19:16:44.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -l name=pause --namespace=kubectl-5656 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 19:16:44.091: INFO: stderr: ""
Nov 29 19:16:44.091: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:16:44.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5656" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":283,"skipped":4542,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:16:44.098: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6850
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-6850
Nov 29 19:16:44.135: INFO: Found 0 stateful pods, waiting for 1
Nov 29 19:16:54.139: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 29 19:16:54.149: INFO: Deleting all statefulset in ns statefulset-6850
Nov 29 19:16:54.151: INFO: Scaling statefulset ss to 0
Nov 29 19:17:04.173: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 19:17:04.175: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:17:04.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6850" for this suite.

• [SLOW TEST:20.093 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":284,"skipped":4548,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:17:04.191: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:17:04.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7114" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":285,"skipped":4596,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:17:04.247: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:17:11.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9265" for this suite.

• [SLOW TEST:7.076 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":286,"skipped":4602,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:17:11.323: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Nov 29 19:17:11.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 create -f - --namespace=kubectl-1865'
Nov 29 19:17:11.553: INFO: stderr: ""
Nov 29 19:17:11.553: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 19:17:11.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1865'
Nov 29 19:17:11.628: INFO: stderr: ""
Nov 29 19:17:11.628: INFO: stdout: "update-demo-nautilus-mclzn update-demo-nautilus-xh4bd "
Nov 29 19:17:11.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-mclzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1865'
Nov 29 19:17:11.696: INFO: stderr: ""
Nov 29 19:17:11.696: INFO: stdout: ""
Nov 29 19:17:11.696: INFO: update-demo-nautilus-mclzn is created but not running
Nov 29 19:17:16.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1865'
Nov 29 19:17:16.767: INFO: stderr: ""
Nov 29 19:17:16.767: INFO: stdout: "update-demo-nautilus-mclzn update-demo-nautilus-xh4bd "
Nov 29 19:17:16.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-mclzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1865'
Nov 29 19:17:16.835: INFO: stderr: ""
Nov 29 19:17:16.836: INFO: stdout: "true"
Nov 29 19:17:16.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-mclzn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1865'
Nov 29 19:17:16.904: INFO: stderr: ""
Nov 29 19:17:16.904: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 19:17:16.904: INFO: validating pod update-demo-nautilus-mclzn
Nov 29 19:17:16.908: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 19:17:16.908: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 19:17:16.908: INFO: update-demo-nautilus-mclzn is verified up and running
Nov 29 19:17:16.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-xh4bd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1865'
Nov 29 19:17:16.979: INFO: stderr: ""
Nov 29 19:17:16.979: INFO: stdout: "true"
Nov 29 19:17:16.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods update-demo-nautilus-xh4bd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1865'
Nov 29 19:17:17.048: INFO: stderr: ""
Nov 29 19:17:17.048: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 19:17:17.048: INFO: validating pod update-demo-nautilus-xh4bd
Nov 29 19:17:17.051: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 19:17:17.051: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 19:17:17.051: INFO: update-demo-nautilus-xh4bd is verified up and running
STEP: using delete to clean up resources
Nov 29 19:17:17.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 delete --grace-period=0 --force -f - --namespace=kubectl-1865'
Nov 29 19:17:17.125: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 19:17:17.125: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 29 19:17:17.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1865'
Nov 29 19:17:17.247: INFO: stderr: "No resources found in kubectl-1865 namespace.\n"
Nov 29 19:17:17.247: INFO: stdout: ""
Nov 29 19:17:17.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-120018680 get pods -l name=update-demo --namespace=kubectl-1865 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 19:17:17.340: INFO: stderr: ""
Nov 29 19:17:17.340: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:17:17.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1865" for this suite.

• [SLOW TEST:6.023 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":287,"skipped":4609,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:17:17.346: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:17:33.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1798" for this suite.

• [SLOW TEST:16.138 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":288,"skipped":4610,"failed":0}
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:17:33.485: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-31447d5c-c3af-4929-bb48-323e023cd43e in namespace container-probe-4406
Nov 29 19:17:35.514: INFO: Started pod busybox-31447d5c-c3af-4929-bb48-323e023cd43e in namespace container-probe-4406
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 19:17:35.516: INFO: Initial restart count of pod busybox-31447d5c-c3af-4929-bb48-323e023cd43e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:21:35.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4406" for this suite.

• [SLOW TEST:242.424 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":289,"skipped":4610,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:21:35.909: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-qjrtj in namespace proxy-5993
I1129 19:21:35.944591      22 runners.go:190] Created replication controller with name: proxy-service-qjrtj, namespace: proxy-5993, replica count: 1
I1129 19:21:36.995115      22 runners.go:190] proxy-service-qjrtj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 19:21:37.995385      22 runners.go:190] proxy-service-qjrtj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 19:21:38.995655      22 runners.go:190] proxy-service-qjrtj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 19:21:39.995883      22 runners.go:190] proxy-service-qjrtj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 19:21:40.996197      22 runners.go:190] proxy-service-qjrtj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 19:21:41.996452      22 runners.go:190] proxy-service-qjrtj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 19:21:41.999: INFO: setup took 6.069025341s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 29 19:21:42.005: INFO: (0) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 6.289059ms)
Nov 29 19:21:42.005: INFO: (0) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 6.446645ms)
Nov 29 19:21:42.006: INFO: (0) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 7.236067ms)
Nov 29 19:21:42.006: INFO: (0) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 7.472173ms)
Nov 29 19:21:42.006: INFO: (0) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 7.43911ms)
Nov 29 19:21:42.006: INFO: (0) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 7.561415ms)
Nov 29 19:21:42.006: INFO: (0) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 7.454229ms)
Nov 29 19:21:42.006: INFO: (0) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 7.551353ms)
Nov 29 19:21:42.008: INFO: (0) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 8.664594ms)
Nov 29 19:21:42.008: INFO: (0) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 8.539218ms)
Nov 29 19:21:42.008: INFO: (0) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 8.672919ms)
Nov 29 19:21:42.008: INFO: (0) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 9.370211ms)
Nov 29 19:21:42.009: INFO: (0) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 9.346772ms)
Nov 29 19:21:42.009: INFO: (0) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 9.542356ms)
Nov 29 19:21:42.010: INFO: (0) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 11.177959ms)
Nov 29 19:21:42.010: INFO: (0) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 11.310244ms)
Nov 29 19:21:42.014: INFO: (1) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 3.598625ms)
Nov 29 19:21:42.014: INFO: (1) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 3.597963ms)
Nov 29 19:21:42.014: INFO: (1) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.732341ms)
Nov 29 19:21:42.014: INFO: (1) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 3.887212ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 5.632262ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 5.69718ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 5.800857ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 5.92427ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 5.689904ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 6.320406ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 6.078202ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 6.09131ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 6.119208ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 6.401358ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 5.994538ms)
Nov 29 19:21:42.017: INFO: (1) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 6.088792ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 3.323544ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 4.082754ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.572241ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.645079ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 4.045763ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 3.723918ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 3.850363ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.801656ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 3.653349ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.018713ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 4.085245ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 3.925451ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 3.978045ms)
Nov 29 19:21:42.021: INFO: (2) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 4.278264ms)
Nov 29 19:21:42.022: INFO: (2) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 4.298268ms)
Nov 29 19:21:42.022: INFO: (2) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 4.458332ms)
Nov 29 19:21:42.024: INFO: (3) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 2.261314ms)
Nov 29 19:21:42.024: INFO: (3) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 2.220435ms)
Nov 29 19:21:42.025: INFO: (3) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 2.983061ms)
Nov 29 19:21:42.025: INFO: (3) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 3.202867ms)
Nov 29 19:21:42.025: INFO: (3) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.340249ms)
Nov 29 19:21:42.025: INFO: (3) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.262793ms)
Nov 29 19:21:42.025: INFO: (3) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 3.180558ms)
Nov 29 19:21:42.026: INFO: (3) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.906638ms)
Nov 29 19:21:42.026: INFO: (3) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 3.95608ms)
Nov 29 19:21:42.026: INFO: (3) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 4.040761ms)
Nov 29 19:21:42.027: INFO: (3) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 4.948458ms)
Nov 29 19:21:42.027: INFO: (3) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 4.857438ms)
Nov 29 19:21:42.027: INFO: (3) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 4.807969ms)
Nov 29 19:21:42.027: INFO: (3) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.901315ms)
Nov 29 19:21:42.027: INFO: (3) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 4.972455ms)
Nov 29 19:21:42.027: INFO: (3) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 5.281109ms)
Nov 29 19:21:42.030: INFO: (4) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.423416ms)
Nov 29 19:21:42.031: INFO: (4) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 3.292048ms)
Nov 29 19:21:42.031: INFO: (4) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.464487ms)
Nov 29 19:21:42.031: INFO: (4) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 3.382912ms)
Nov 29 19:21:42.031: INFO: (4) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.419904ms)
Nov 29 19:21:42.034: INFO: (4) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 6.804956ms)
Nov 29 19:21:42.034: INFO: (4) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 6.753702ms)
Nov 29 19:21:42.034: INFO: (4) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 6.741295ms)
Nov 29 19:21:42.034: INFO: (4) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 6.743484ms)
Nov 29 19:21:42.034: INFO: (4) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 6.878011ms)
Nov 29 19:21:42.034: INFO: (4) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 6.812713ms)
Nov 29 19:21:42.034: INFO: (4) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 6.788681ms)
Nov 29 19:21:42.034: INFO: (4) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 6.96389ms)
Nov 29 19:21:42.034: INFO: (4) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 6.971184ms)
Nov 29 19:21:42.034: INFO: (4) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 6.93502ms)
Nov 29 19:21:42.035: INFO: (4) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 7.300131ms)
Nov 29 19:21:42.037: INFO: (5) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 1.871118ms)
Nov 29 19:21:42.037: INFO: (5) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 2.642457ms)
Nov 29 19:21:42.038: INFO: (5) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.241698ms)
Nov 29 19:21:42.038: INFO: (5) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 3.336649ms)
Nov 29 19:21:42.038: INFO: (5) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.133923ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 5.153852ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 5.399979ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 5.317778ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 5.67664ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 5.85168ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 5.832414ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 5.770148ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 5.738571ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 5.739501ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 5.668456ms)
Nov 29 19:21:42.040: INFO: (5) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 5.802918ms)
Nov 29 19:21:42.044: INFO: (6) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.302197ms)
Nov 29 19:21:42.044: INFO: (6) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 3.380542ms)
Nov 29 19:21:42.044: INFO: (6) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 3.388782ms)
Nov 29 19:21:42.044: INFO: (6) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 3.438833ms)
Nov 29 19:21:42.044: INFO: (6) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 3.904827ms)
Nov 29 19:21:42.045: INFO: (6) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 4.738588ms)
Nov 29 19:21:42.045: INFO: (6) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 4.92025ms)
Nov 29 19:21:42.045: INFO: (6) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.92017ms)
Nov 29 19:21:42.046: INFO: (6) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 5.026409ms)
Nov 29 19:21:42.046: INFO: (6) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 5.076029ms)
Nov 29 19:21:42.046: INFO: (6) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 5.040075ms)
Nov 29 19:21:42.046: INFO: (6) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 5.058552ms)
Nov 29 19:21:42.046: INFO: (6) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 5.050514ms)
Nov 29 19:21:42.046: INFO: (6) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 5.039608ms)
Nov 29 19:21:42.046: INFO: (6) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 5.082685ms)
Nov 29 19:21:42.046: INFO: (6) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 5.169103ms)
Nov 29 19:21:42.049: INFO: (7) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.682221ms)
Nov 29 19:21:42.049: INFO: (7) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.793553ms)
Nov 29 19:21:42.049: INFO: (7) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 3.640278ms)
Nov 29 19:21:42.049: INFO: (7) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 3.726624ms)
Nov 29 19:21:42.050: INFO: (7) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 3.801276ms)
Nov 29 19:21:42.050: INFO: (7) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 4.307898ms)
Nov 29 19:21:42.050: INFO: (7) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 4.71774ms)
Nov 29 19:21:42.050: INFO: (7) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.65197ms)
Nov 29 19:21:42.051: INFO: (7) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 4.860922ms)
Nov 29 19:21:42.051: INFO: (7) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 4.912276ms)
Nov 29 19:21:42.051: INFO: (7) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 5.056774ms)
Nov 29 19:21:42.051: INFO: (7) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 4.945063ms)
Nov 29 19:21:42.051: INFO: (7) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 4.911276ms)
Nov 29 19:21:42.051: INFO: (7) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 5.024998ms)
Nov 29 19:21:42.051: INFO: (7) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 5.012952ms)
Nov 29 19:21:42.051: INFO: (7) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 5.02988ms)
Nov 29 19:21:42.054: INFO: (8) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 3.267455ms)
Nov 29 19:21:42.054: INFO: (8) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 3.575628ms)
Nov 29 19:21:42.054: INFO: (8) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 3.496027ms)
Nov 29 19:21:42.055: INFO: (8) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.556724ms)
Nov 29 19:21:42.055: INFO: (8) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.646618ms)
Nov 29 19:21:42.055: INFO: (8) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 3.581371ms)
Nov 29 19:21:42.055: INFO: (8) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 3.533697ms)
Nov 29 19:21:42.055: INFO: (8) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.578343ms)
Nov 29 19:21:42.055: INFO: (8) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 3.562372ms)
Nov 29 19:21:42.055: INFO: (8) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.606294ms)
Nov 29 19:21:42.056: INFO: (8) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 4.820297ms)
Nov 29 19:21:42.056: INFO: (8) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.88735ms)
Nov 29 19:21:42.056: INFO: (8) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 5.002921ms)
Nov 29 19:21:42.056: INFO: (8) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 5.002221ms)
Nov 29 19:21:42.056: INFO: (8) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 5.214066ms)
Nov 29 19:21:42.056: INFO: (8) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 5.267534ms)
Nov 29 19:21:42.110: INFO: (9) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 53.544477ms)
Nov 29 19:21:42.110: INFO: (9) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 53.550528ms)
Nov 29 19:21:42.110: INFO: (9) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 53.472715ms)
Nov 29 19:21:42.110: INFO: (9) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 53.502885ms)
Nov 29 19:21:42.110: INFO: (9) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 53.486717ms)
Nov 29 19:21:42.111: INFO: (9) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 54.619524ms)
Nov 29 19:21:42.112: INFO: (9) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 55.715238ms)
Nov 29 19:21:42.113: INFO: (9) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 56.18125ms)
Nov 29 19:21:42.113: INFO: (9) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 56.221528ms)
Nov 29 19:21:42.113: INFO: (9) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 56.123121ms)
Nov 29 19:21:42.113: INFO: (9) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 56.569309ms)
Nov 29 19:21:42.113: INFO: (9) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 56.643915ms)
Nov 29 19:21:42.113: INFO: (9) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 56.560813ms)
Nov 29 19:21:42.113: INFO: (9) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 56.644174ms)
Nov 29 19:21:42.113: INFO: (9) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 56.836596ms)
Nov 29 19:21:42.113: INFO: (9) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 56.686191ms)
Nov 29 19:21:42.117: INFO: (10) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.138142ms)
Nov 29 19:21:42.118: INFO: (10) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 4.546109ms)
Nov 29 19:21:42.118: INFO: (10) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 4.646696ms)
Nov 29 19:21:42.118: INFO: (10) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 4.730312ms)
Nov 29 19:21:42.118: INFO: (10) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 4.780593ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 5.329543ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 5.394887ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 5.303112ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 5.473925ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 5.37178ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 5.404875ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 5.410132ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 5.367188ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 5.434633ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 5.554283ms)
Nov 29 19:21:42.119: INFO: (10) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 5.898782ms)
Nov 29 19:21:42.123: INFO: (11) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.502271ms)
Nov 29 19:21:42.123: INFO: (11) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.788352ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 4.262506ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 4.412293ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.292393ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 4.269287ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 4.225348ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 4.229663ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 4.433424ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 4.284383ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 4.488767ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 4.870354ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 4.690291ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 4.677442ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 4.866112ms)
Nov 29 19:21:42.124: INFO: (11) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 4.830841ms)
Nov 29 19:21:42.126: INFO: (12) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 1.723359ms)
Nov 29 19:21:42.126: INFO: (12) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 1.978363ms)
Nov 29 19:21:42.128: INFO: (12) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 3.029763ms)
Nov 29 19:21:42.128: INFO: (12) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.054274ms)
Nov 29 19:21:42.128: INFO: (12) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 3.11984ms)
Nov 29 19:21:42.128: INFO: (12) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 3.413912ms)
Nov 29 19:21:42.128: INFO: (12) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.449551ms)
Nov 29 19:21:42.128: INFO: (12) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 3.529515ms)
Nov 29 19:21:42.128: INFO: (12) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 3.544709ms)
Nov 29 19:21:42.128: INFO: (12) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.513077ms)
Nov 29 19:21:42.129: INFO: (12) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 4.308554ms)
Nov 29 19:21:42.129: INFO: (12) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 4.309914ms)
Nov 29 19:21:42.129: INFO: (12) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 4.847538ms)
Nov 29 19:21:42.129: INFO: (12) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 4.96632ms)
Nov 29 19:21:42.129: INFO: (12) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.935767ms)
Nov 29 19:21:42.130: INFO: (12) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 4.976206ms)
Nov 29 19:21:42.133: INFO: (13) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 3.345046ms)
Nov 29 19:21:42.133: INFO: (13) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.389497ms)
Nov 29 19:21:42.133: INFO: (13) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.476719ms)
Nov 29 19:21:42.133: INFO: (13) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 3.525442ms)
Nov 29 19:21:42.133: INFO: (13) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 3.485323ms)
Nov 29 19:21:42.133: INFO: (13) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 3.526149ms)
Nov 29 19:21:42.133: INFO: (13) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 3.556632ms)
Nov 29 19:21:42.133: INFO: (13) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.575902ms)
Nov 29 19:21:42.133: INFO: (13) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 3.672466ms)
Nov 29 19:21:42.133: INFO: (13) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.747781ms)
Nov 29 19:21:42.134: INFO: (13) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 4.807574ms)
Nov 29 19:21:42.134: INFO: (13) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 4.92607ms)
Nov 29 19:21:42.134: INFO: (13) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 4.878499ms)
Nov 29 19:21:42.134: INFO: (13) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.947741ms)
Nov 29 19:21:42.135: INFO: (13) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 4.835654ms)
Nov 29 19:21:42.135: INFO: (13) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 5.268475ms)
Nov 29 19:21:42.138: INFO: (14) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 2.762737ms)
Nov 29 19:21:42.138: INFO: (14) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 2.827845ms)
Nov 29 19:21:42.138: INFO: (14) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 2.915626ms)
Nov 29 19:21:42.138: INFO: (14) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 2.915632ms)
Nov 29 19:21:42.138: INFO: (14) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 3.218362ms)
Nov 29 19:21:42.138: INFO: (14) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.373008ms)
Nov 29 19:21:42.138: INFO: (14) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.425976ms)
Nov 29 19:21:42.138: INFO: (14) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 3.470249ms)
Nov 29 19:21:42.138: INFO: (14) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 3.440704ms)
Nov 29 19:21:42.138: INFO: (14) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 3.567363ms)
Nov 29 19:21:42.140: INFO: (14) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 5.096057ms)
Nov 29 19:21:42.140: INFO: (14) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 5.174344ms)
Nov 29 19:21:42.140: INFO: (14) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 5.15706ms)
Nov 29 19:21:42.140: INFO: (14) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 5.224019ms)
Nov 29 19:21:42.140: INFO: (14) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 5.247859ms)
Nov 29 19:21:42.140: INFO: (14) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 5.365591ms)
Nov 29 19:21:42.143: INFO: (15) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 2.657645ms)
Nov 29 19:21:42.144: INFO: (15) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 3.39368ms)
Nov 29 19:21:42.144: INFO: (15) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 3.83839ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 4.11418ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 3.707943ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 4.24867ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 4.261436ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 4.500015ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 4.733354ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 4.083194ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 4.685606ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 4.4402ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 4.343479ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 4.648435ms)
Nov 29 19:21:42.145: INFO: (15) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.397403ms)
Nov 29 19:21:42.146: INFO: (15) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 4.817552ms)
Nov 29 19:21:42.148: INFO: (16) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 2.337824ms)
Nov 29 19:21:42.150: INFO: (16) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 4.739316ms)
Nov 29 19:21:42.150: INFO: (16) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.754303ms)
Nov 29 19:21:42.151: INFO: (16) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 4.861788ms)
Nov 29 19:21:42.151: INFO: (16) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 4.838376ms)
Nov 29 19:21:42.151: INFO: (16) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 4.918268ms)
Nov 29 19:21:42.151: INFO: (16) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 4.918324ms)
Nov 29 19:21:42.151: INFO: (16) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 5.362245ms)
Nov 29 19:21:42.151: INFO: (16) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 5.654169ms)
Nov 29 19:21:42.152: INFO: (16) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 5.944762ms)
Nov 29 19:21:42.152: INFO: (16) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 5.856732ms)
Nov 29 19:21:42.152: INFO: (16) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 5.85968ms)
Nov 29 19:21:42.152: INFO: (16) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 5.961068ms)
Nov 29 19:21:42.152: INFO: (16) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 5.952876ms)
Nov 29 19:21:42.152: INFO: (16) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 6.072638ms)
Nov 29 19:21:42.210: INFO: (16) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 63.90523ms)
Nov 29 19:21:42.218: INFO: (17) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 8.718577ms)
Nov 29 19:21:42.218: INFO: (17) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 8.612426ms)
Nov 29 19:21:42.218: INFO: (17) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 8.753745ms)
Nov 29 19:21:42.218: INFO: (17) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 8.676032ms)
Nov 29 19:21:42.218: INFO: (17) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 8.800604ms)
Nov 29 19:21:42.218: INFO: (17) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 8.882401ms)
Nov 29 19:21:42.218: INFO: (17) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 8.745962ms)
Nov 29 19:21:42.218: INFO: (17) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 8.72128ms)
Nov 29 19:21:42.218: INFO: (17) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 8.835139ms)
Nov 29 19:21:42.220: INFO: (17) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 10.053312ms)
Nov 29 19:21:42.220: INFO: (17) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 9.971924ms)
Nov 29 19:21:42.220: INFO: (17) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 10.125578ms)
Nov 29 19:21:42.220: INFO: (17) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 10.322492ms)
Nov 29 19:21:42.220: INFO: (17) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 10.146926ms)
Nov 29 19:21:42.220: INFO: (17) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 10.115454ms)
Nov 29 19:21:42.220: INFO: (17) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 10.249698ms)
Nov 29 19:21:42.223: INFO: (18) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 3.343512ms)
Nov 29 19:21:42.224: INFO: (18) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 4.14457ms)
Nov 29 19:21:42.224: INFO: (18) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 4.114327ms)
Nov 29 19:21:42.224: INFO: (18) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 4.09064ms)
Nov 29 19:21:42.224: INFO: (18) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 4.213304ms)
Nov 29 19:21:42.224: INFO: (18) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 4.165464ms)
Nov 29 19:21:42.224: INFO: (18) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 4.105498ms)
Nov 29 19:21:42.224: INFO: (18) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 4.080615ms)
Nov 29 19:21:42.224: INFO: (18) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 4.118581ms)
Nov 29 19:21:42.225: INFO: (18) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 4.449242ms)
Nov 29 19:21:42.225: INFO: (18) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 4.452862ms)
Nov 29 19:21:42.225: INFO: (18) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 4.465939ms)
Nov 29 19:21:42.225: INFO: (18) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 4.511575ms)
Nov 29 19:21:42.225: INFO: (18) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 4.842416ms)
Nov 29 19:21:42.225: INFO: (18) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 5.233885ms)
Nov 29 19:21:42.226: INFO: (18) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 5.596157ms)
Nov 29 19:21:42.230: INFO: (19) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">test<... (200; 4.03193ms)
Nov 29 19:21:42.230: INFO: (19) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 4.150876ms)
Nov 29 19:21:42.230: INFO: (19) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm/proxy/rewriteme">test</a> (200; 4.121883ms)
Nov 29 19:21:42.230: INFO: (19) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:162/proxy/: bar (200; 4.327685ms)
Nov 29 19:21:42.231: INFO: (19) /api/v1/namespaces/proxy-5993/pods/proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 4.634059ms)
Nov 29 19:21:42.231: INFO: (19) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:462/proxy/: tls qux (200; 4.686418ms)
Nov 29 19:21:42.231: INFO: (19) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:443/proxy/tlsrewritem... (200; 4.742092ms)
Nov 29 19:21:42.231: INFO: (19) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname1/proxy/: tls baz (200; 4.835628ms)
Nov 29 19:21:42.231: INFO: (19) /api/v1/namespaces/proxy-5993/pods/https:proxy-service-qjrtj-blqnm:460/proxy/: tls baz (200; 4.928493ms)
Nov 29 19:21:42.231: INFO: (19) /api/v1/namespaces/proxy-5993/services/https:proxy-service-qjrtj:tlsportname2/proxy/: tls qux (200; 4.982152ms)
Nov 29 19:21:42.231: INFO: (19) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:1080/proxy/rewriteme">... (200; 5.050305ms)
Nov 29 19:21:42.231: INFO: (19) /api/v1/namespaces/proxy-5993/pods/http:proxy-service-qjrtj-blqnm:160/proxy/: foo (200; 5.017156ms)
Nov 29 19:21:42.231: INFO: (19) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname2/proxy/: bar (200; 5.062793ms)
Nov 29 19:21:42.232: INFO: (19) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname2/proxy/: bar (200; 5.663538ms)
Nov 29 19:21:42.232: INFO: (19) /api/v1/namespaces/proxy-5993/services/proxy-service-qjrtj:portname1/proxy/: foo (200; 5.695165ms)
Nov 29 19:21:42.232: INFO: (19) /api/v1/namespaces/proxy-5993/services/http:proxy-service-qjrtj:portname1/proxy/: foo (200; 5.726197ms)
STEP: deleting ReplicationController proxy-service-qjrtj in namespace proxy-5993, will wait for the garbage collector to delete the pods
Nov 29 19:21:42.289: INFO: Deleting ReplicationController proxy-service-qjrtj took: 5.210776ms
Nov 29 19:21:42.689: INFO: Terminating ReplicationController proxy-service-qjrtj pods took: 400.199952ms
[AfterEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:21:44.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5993" for this suite.

• [SLOW TEST:8.787 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":290,"skipped":4618,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:21:44.696: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 29 19:21:45.002: INFO: Pod name wrapped-volume-race-ce0beba0-7375-460c-8989-40701b472089: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ce0beba0-7375-460c-8989-40701b472089 in namespace emptydir-wrapper-7249, will wait for the garbage collector to delete the pods
Nov 29 19:22:01.124: INFO: Deleting ReplicationController wrapped-volume-race-ce0beba0-7375-460c-8989-40701b472089 took: 4.857617ms
Nov 29 19:22:01.524: INFO: Terminating ReplicationController wrapped-volume-race-ce0beba0-7375-460c-8989-40701b472089 pods took: 400.278232ms
STEP: Creating RC which spawns configmap-volume pods
Nov 29 19:22:08.173: INFO: Pod name wrapped-volume-race-6958342d-41ba-415d-886e-4838b73e2e80: Found 0 pods out of 5
Nov 29 19:22:13.179: INFO: Pod name wrapped-volume-race-6958342d-41ba-415d-886e-4838b73e2e80: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6958342d-41ba-415d-886e-4838b73e2e80 in namespace emptydir-wrapper-7249, will wait for the garbage collector to delete the pods
Nov 29 19:22:23.253: INFO: Deleting ReplicationController wrapped-volume-race-6958342d-41ba-415d-886e-4838b73e2e80 took: 5.326735ms
Nov 29 19:22:23.353: INFO: Terminating ReplicationController wrapped-volume-race-6958342d-41ba-415d-886e-4838b73e2e80 pods took: 100.106146ms
STEP: Creating RC which spawns configmap-volume pods
Nov 29 19:22:30.368: INFO: Pod name wrapped-volume-race-78874c12-3593-47ca-bd88-63360b8f41b8: Found 0 pods out of 5
Nov 29 19:22:35.373: INFO: Pod name wrapped-volume-race-78874c12-3593-47ca-bd88-63360b8f41b8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-78874c12-3593-47ca-bd88-63360b8f41b8 in namespace emptydir-wrapper-7249, will wait for the garbage collector to delete the pods
Nov 29 19:22:45.446: INFO: Deleting ReplicationController wrapped-volume-race-78874c12-3593-47ca-bd88-63360b8f41b8 took: 5.061458ms
Nov 29 19:22:45.846: INFO: Terminating ReplicationController wrapped-volume-race-78874c12-3593-47ca-bd88-63360b8f41b8 pods took: 400.29446ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:22:58.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7249" for this suite.

• [SLOW TEST:73.665 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":291,"skipped":4625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:22:58.362: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:23:15.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4900" for this suite.

• [SLOW TEST:17.057 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":292,"skipped":4647,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:23:15.419: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-1a7bcb0c-c29f-47c6-8687-a57ea3035089
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:23:15.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3731" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":293,"skipped":4652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:23:15.449: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 29 19:23:15.484: INFO: starting watch
STEP: patching
STEP: updating
Nov 29 19:23:15.490: INFO: waiting for watch events with expected annotations
Nov 29 19:23:15.490: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:23:15.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5842" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":294,"skipped":4706,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:23:15.519: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-6ea67872-4428-4673-a9f3-9db248140438
STEP: Creating a pod to test consume configMaps
Nov 29 19:23:15.545: INFO: Waiting up to 5m0s for pod "pod-configmaps-6aa183c7-9c5f-4e5f-8145-796211d1ea0e" in namespace "configmap-9198" to be "Succeeded or Failed"
Nov 29 19:23:15.557: INFO: Pod "pod-configmaps-6aa183c7-9c5f-4e5f-8145-796211d1ea0e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.636151ms
Nov 29 19:23:17.560: INFO: Pod "pod-configmaps-6aa183c7-9c5f-4e5f-8145-796211d1ea0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014678737s
Nov 29 19:23:19.563: INFO: Pod "pod-configmaps-6aa183c7-9c5f-4e5f-8145-796211d1ea0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017750206s
STEP: Saw pod success
Nov 29 19:23:19.563: INFO: Pod "pod-configmaps-6aa183c7-9c5f-4e5f-8145-796211d1ea0e" satisfied condition "Succeeded or Failed"
Nov 29 19:23:19.565: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-6aa183c7-9c5f-4e5f-8145-796211d1ea0e container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 19:23:19.591: INFO: Waiting for pod pod-configmaps-6aa183c7-9c5f-4e5f-8145-796211d1ea0e to disappear
Nov 29 19:23:19.593: INFO: Pod pod-configmaps-6aa183c7-9c5f-4e5f-8145-796211d1ea0e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:23:19.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9198" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":295,"skipped":4718,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:23:19.598: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1129 19:23:20.721428      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 29 19:24:22.733: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:24:22.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-924" for this suite.

• [SLOW TEST:63.141 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":296,"skipped":4719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:24:22.740: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 29 19:24:22.766: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33858aa2-28db-4040-ac94-f31397b01a3e" in namespace "projected-2555" to be "Succeeded or Failed"
Nov 29 19:24:22.770: INFO: Pod "downwardapi-volume-33858aa2-28db-4040-ac94-f31397b01a3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2237ms
Nov 29 19:24:24.774: INFO: Pod "downwardapi-volume-33858aa2-28db-4040-ac94-f31397b01a3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.007676517s
Nov 29 19:24:26.777: INFO: Pod "downwardapi-volume-33858aa2-28db-4040-ac94-f31397b01a3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011057507s
STEP: Saw pod success
Nov 29 19:24:26.777: INFO: Pod "downwardapi-volume-33858aa2-28db-4040-ac94-f31397b01a3e" satisfied condition "Succeeded or Failed"
Nov 29 19:24:26.779: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-33858aa2-28db-4040-ac94-f31397b01a3e container client-container: <nil>
STEP: delete the pod
Nov 29 19:24:26.793: INFO: Waiting for pod downwardapi-volume-33858aa2-28db-4040-ac94-f31397b01a3e to disappear
Nov 29 19:24:26.794: INFO: Pod downwardapi-volume-33858aa2-28db-4040-ac94-f31397b01a3e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:24:26.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2555" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":297,"skipped":4776,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:24:26.800: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Nov 29 19:24:26.823: INFO: Waiting up to 5m0s for pod "client-containers-c79bfab8-6248-4f57-bac6-ef35823de8a1" in namespace "containers-5701" to be "Succeeded or Failed"
Nov 29 19:24:26.827: INFO: Pod "client-containers-c79bfab8-6248-4f57-bac6-ef35823de8a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087735ms
Nov 29 19:24:28.830: INFO: Pod "client-containers-c79bfab8-6248-4f57-bac6-ef35823de8a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007156413s
Nov 29 19:24:30.833: INFO: Pod "client-containers-c79bfab8-6248-4f57-bac6-ef35823de8a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010354004s
STEP: Saw pod success
Nov 29 19:24:30.834: INFO: Pod "client-containers-c79bfab8-6248-4f57-bac6-ef35823de8a1" satisfied condition "Succeeded or Failed"
Nov 29 19:24:30.835: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-c79bfab8-6248-4f57-bac6-ef35823de8a1 container test-container: <nil>
STEP: delete the pod
Nov 29 19:24:30.849: INFO: Waiting for pod client-containers-c79bfab8-6248-4f57-bac6-ef35823de8a1 to disappear
Nov 29 19:24:30.851: INFO: Pod client-containers-c79bfab8-6248-4f57-bac6-ef35823de8a1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:24:30.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5701" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":298,"skipped":4789,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:24:30.856: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-314.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-314.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-314.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 19:24:34.897: INFO: DNS probes using dns-test-9b276e39-809d-49c0-8b61-2753dfcba85e succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-314.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-314.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-314.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 19:24:36.954: INFO: File wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:24:36.956: INFO: File jessie_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:24:36.956: INFO: Lookups using dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 failed for: [wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local jessie_udp@dns-test-service-3.dns-314.svc.cluster.local]

Nov 29 19:24:41.960: INFO: File wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:24:41.962: INFO: File jessie_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:24:41.962: INFO: Lookups using dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 failed for: [wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local jessie_udp@dns-test-service-3.dns-314.svc.cluster.local]

Nov 29 19:24:46.960: INFO: File wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:24:46.962: INFO: File jessie_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:24:46.962: INFO: Lookups using dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 failed for: [wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local jessie_udp@dns-test-service-3.dns-314.svc.cluster.local]

Nov 29 19:24:51.960: INFO: File wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:24:51.962: INFO: File jessie_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:24:51.963: INFO: Lookups using dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 failed for: [wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local jessie_udp@dns-test-service-3.dns-314.svc.cluster.local]

Nov 29 19:24:56.960: INFO: File wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:24:56.962: INFO: File jessie_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:24:56.962: INFO: Lookups using dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 failed for: [wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local jessie_udp@dns-test-service-3.dns-314.svc.cluster.local]

Nov 29 19:25:01.960: INFO: File wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:25:01.963: INFO: File jessie_udp@dns-test-service-3.dns-314.svc.cluster.local from pod  dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 19:25:01.963: INFO: Lookups using dns-314/dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 failed for: [wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local jessie_udp@dns-test-service-3.dns-314.svc.cluster.local]

Nov 29 19:25:06.962: INFO: DNS probes using dns-test-d7f213a5-ad46-45b5-8399-2abeae20c312 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-314.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-314.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-314.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-314.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 19:25:09.030: INFO: DNS probes using dns-test-12598c13-0e2a-42d9-816f-85aab52f8617 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:25:09.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-314" for this suite.

• [SLOW TEST:38.212 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":299,"skipped":4810,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:25:09.068: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:25:33.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3930" for this suite.

• [SLOW TEST:24.382 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4811,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:25:33.450: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:25:33.475: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:25:34.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5744" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":301,"skipped":4812,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:25:34.607: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 19:25:35.314: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 19:25:38.334: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:25:38.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1147" for this suite.
STEP: Destroying namespace "webhook-1147-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":302,"skipped":4873,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:25:38.519: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Nov 29 19:25:40.559: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4697 PodName:var-expansion-6a53b8a8-2f48-4cef-815c-a4a83361b7a6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 19:25:40.559: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: test for file in mounted path
Nov 29 19:25:40.641: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4697 PodName:var-expansion-6a53b8a8-2f48-4cef-815c-a4a83361b7a6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 19:25:40.641: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: updating the annotation value
Nov 29 19:25:41.215: INFO: Successfully updated pod "var-expansion-6a53b8a8-2f48-4cef-815c-a4a83361b7a6"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Nov 29 19:25:41.221: INFO: Deleting pod "var-expansion-6a53b8a8-2f48-4cef-815c-a4a83361b7a6" in namespace "var-expansion-4697"
Nov 29 19:25:41.224: INFO: Wait up to 5m0s for pod "var-expansion-6a53b8a8-2f48-4cef-815c-a4a83361b7a6" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:26:19.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4697" for this suite.

• [SLOW TEST:40.717 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":303,"skipped":4889,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:26:19.237: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 29 19:26:19.257: INFO: Creating deployment "test-recreate-deployment"
Nov 29 19:26:19.260: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 29 19:26:19.264: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 29 19:26:21.269: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 29 19:26:21.271: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 29 19:26:21.275: INFO: Updating deployment test-recreate-deployment
Nov 29 19:26:21.275: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 29 19:26:21.403: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-88 /apis/apps/v1/namespaces/deployment-88/deployments/test-recreate-deployment ee370f11-2d26-4dc3-b369-6af4045d0298 23748 2 2020-11-29 19:26:19 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-29 19:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-29 19:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004890ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-29 19:26:21 +0000 UTC,LastTransitionTime:2020-11-29 19:26:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-11-29 19:26:21 +0000 UTC,LastTransitionTime:2020-11-29 19:26:19 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 29 19:26:21.406: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-88 /apis/apps/v1/namespaces/deployment-88/replicasets/test-recreate-deployment-f79dd4667 01672fe8-4529-4ef4-be10-dd3b1ef9734b 23746 1 2020-11-29 19:26:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ee370f11-2d26-4dc3-b369-6af4045d0298 0xc0048913f0 0xc0048913f1}] []  [{kube-controller-manager Update apps/v1 2020-11-29 19:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee370f11-2d26-4dc3-b369-6af4045d0298\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004891468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 19:26:21.406: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 29 19:26:21.406: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-88 /apis/apps/v1/namespaces/deployment-88/replicasets/test-recreate-deployment-c96cf48f 65708298-6a1d-4182-8d47-a1c15674a7ba 23738 2 2020-11-29 19:26:19 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ee370f11-2d26-4dc3-b369-6af4045d0298 0xc0048912ef 0xc004891300}] []  [{kube-controller-manager Update apps/v1 2020-11-29 19:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee370f11-2d26-4dc3-b369-6af4045d0298\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004891378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 19:26:21.409: INFO: Pod "test-recreate-deployment-f79dd4667-d794j" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-d794j test-recreate-deployment-f79dd4667- deployment-88 /api/v1/namespaces/deployment-88/pods/test-recreate-deployment-f79dd4667-d794j a5183f29-8a2b-48b0-b747-0fc3279c67d2 23750 0 2020-11-29 19:26:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 01672fe8-4529-4ef4-be10-dd3b1ef9734b 0xc004891990 0xc004891991}] []  [{kube-controller-manager Update v1 2020-11-29 19:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01672fe8-4529-4ef4-be10-dd3b1ef9734b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-29 19:26:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nj8jm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nj8jm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nj8jm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:26:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:26:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:26:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 19:26:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2020-11-29 19:26:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:26:21.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-88" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":304,"skipped":4890,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 29 19:26:21.414: INFO: >>> kubeConfig: /tmp/kubeconfig-120018680
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 19:26:21.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 19:26:23.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742274781, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742274781, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742274781, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742274781, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 19:26:26.728: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 29 19:26:26.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6899" for this suite.
STEP: Destroying namespace "webhook-6899-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.503 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":305,"skipped":4911,"failed":0}
SSSSSSSSSSSSSSSSSSNov 29 19:26:26.917: INFO: Running AfterSuite actions on all nodes
Nov 29 19:26:26.917: INFO: Running AfterSuite actions on node 1
Nov 29 19:26:26.917: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4929,"failed":0}

Ran 305 of 5234 Specs in 5335.186 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4929 Skipped
PASS

Ginkgo ran 1 suite in 1h28m57.433999631s
Test Suite Passed
