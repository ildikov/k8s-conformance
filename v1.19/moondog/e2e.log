I0911 17:37:32.977193      19 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-107083211
I0911 17:37:32.977217      19 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0911 17:37:32.977370      19 e2e.go:129] Starting e2e run "3f89ce03-4277-47b6-941c-d01a38cf0285" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1599845851 - Will randomize all specs
Will run 305 of 5232 specs

Sep 11 17:37:33.000: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 17:37:33.003: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 11 17:37:33.027: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 11 17:37:33.070: INFO: 28 / 28 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 11 17:37:33.070: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Sep 11 17:37:33.071: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 11 17:37:33.081: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep 11 17:37:33.081: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 11 17:37:33.081: INFO: e2e test version: v1.19.0
Sep 11 17:37:33.082: INFO: kube-apiserver version: v1.19.0
Sep 11 17:37:33.082: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 17:37:33.089: INFO: Cluster IP family: ipv4
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:37:33.089: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
Sep 11 17:37:33.135: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Sep 11 17:37:33.149: INFO: PSP annotation exists on dry run pod: "admin"; assuming PodSecurityPolicy is enabled
Sep 11 17:37:33.162: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8332
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8332
I0911 17:37:33.346973      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8332, replica count: 2
I0911 17:37:36.398282      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 17:37:36.398: INFO: Creating new exec pod
Sep 11 17:37:41.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-8332 execpodr8fkt -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 11 17:37:42.027: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 11 17:37:42.027: INFO: stdout: ""
Sep 11 17:37:42.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-8332 execpodr8fkt -- /bin/sh -x -c nc -zv -t -w 2 10.102.133.38 80'
Sep 11 17:37:42.290: INFO: stderr: "+ nc -zv -t -w 2 10.102.133.38 80\nConnection to 10.102.133.38 80 port [tcp/http] succeeded!\n"
Sep 11 17:37:42.290: INFO: stdout: ""
Sep 11 17:37:42.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-8332 execpodr8fkt -- /bin/sh -x -c nc -zv -t -w 2 10.10.2.19 31296'
Sep 11 17:37:42.541: INFO: stderr: "+ nc -zv -t -w 2 10.10.2.19 31296\nConnection to 10.10.2.19 31296 port [tcp/31296] succeeded!\n"
Sep 11 17:37:42.541: INFO: stdout: ""
Sep 11 17:37:42.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-8332 execpodr8fkt -- /bin/sh -x -c nc -zv -t -w 2 10.10.3.166 31296'
Sep 11 17:37:42.815: INFO: stderr: "+ nc -zv -t -w 2 10.10.3.166 31296\nConnection to 10.10.3.166 31296 port [tcp/31296] succeeded!\n"
Sep 11 17:37:42.815: INFO: stdout: ""
Sep 11 17:37:42.815: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:37:42.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8332" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:9.810 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":1,"skipped":6,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:37:42.900: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6002
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:37:43.066: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 11 17:37:48.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-6002 create -f -'
Sep 11 17:37:49.556: INFO: stderr: ""
Sep 11 17:37:49.556: INFO: stdout: "e2e-test-crd-publish-openapi-8297-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 11 17:37:49.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-6002 delete e2e-test-crd-publish-openapi-8297-crds test-cr'
Sep 11 17:37:49.691: INFO: stderr: ""
Sep 11 17:37:49.692: INFO: stdout: "e2e-test-crd-publish-openapi-8297-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 11 17:37:49.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-6002 apply -f -'
Sep 11 17:37:50.032: INFO: stderr: ""
Sep 11 17:37:50.032: INFO: stdout: "e2e-test-crd-publish-openapi-8297-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 11 17:37:50.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-6002 delete e2e-test-crd-publish-openapi-8297-crds test-cr'
Sep 11 17:37:50.147: INFO: stderr: ""
Sep 11 17:37:50.147: INFO: stdout: "e2e-test-crd-publish-openapi-8297-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 11 17:37:50.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 explain e2e-test-crd-publish-openapi-8297-crds'
Sep 11 17:37:50.450: INFO: stderr: ""
Sep 11 17:37:50.450: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8297-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:37:55.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6002" for this suite.

â€¢ [SLOW TEST:12.879 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":2,"skipped":16,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:37:55.781: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2793
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:37:55.930: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:37:56.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2793" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":3,"skipped":28,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:37:57.514: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 11 17:37:57.701: INFO: Waiting up to 5m0s for pod "pod-7683d7c6-91ea-4c1d-88e3-2b602c59e3e9" in namespace "emptydir-3255" to be "Succeeded or Failed"
Sep 11 17:37:57.705: INFO: Pod "pod-7683d7c6-91ea-4c1d-88e3-2b602c59e3e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.639204ms
Sep 11 17:37:59.710: INFO: Pod "pod-7683d7c6-91ea-4c1d-88e3-2b602c59e3e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008674992s
STEP: Saw pod success
Sep 11 17:37:59.710: INFO: Pod "pod-7683d7c6-91ea-4c1d-88e3-2b602c59e3e9" satisfied condition "Succeeded or Failed"
Sep 11 17:37:59.718: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-7683d7c6-91ea-4c1d-88e3-2b602c59e3e9 container test-container: <nil>
STEP: delete the pod
Sep 11 17:37:59.765: INFO: Waiting for pod pod-7683d7c6-91ea-4c1d-88e3-2b602c59e3e9 to disappear
Sep 11 17:37:59.771: INFO: Pod pod-7683d7c6-91ea-4c1d-88e3-2b602c59e3e9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:37:59.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3255" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":4,"skipped":48,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:37:59.800: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4590
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 17:38:00.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4386271-4a8f-4ea0-afb8-d5ff440a85eb" in namespace "projected-4590" to be "Succeeded or Failed"
Sep 11 17:38:00.038: INFO: Pod "downwardapi-volume-f4386271-4a8f-4ea0-afb8-d5ff440a85eb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979115ms
Sep 11 17:38:02.042: INFO: Pod "downwardapi-volume-f4386271-4a8f-4ea0-afb8-d5ff440a85eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008441127s
STEP: Saw pod success
Sep 11 17:38:02.042: INFO: Pod "downwardapi-volume-f4386271-4a8f-4ea0-afb8-d5ff440a85eb" satisfied condition "Succeeded or Failed"
Sep 11 17:38:02.046: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-f4386271-4a8f-4ea0-afb8-d5ff440a85eb container client-container: <nil>
STEP: delete the pod
Sep 11 17:38:02.077: INFO: Waiting for pod downwardapi-volume-f4386271-4a8f-4ea0-afb8-d5ff440a85eb to disappear
Sep 11 17:38:02.081: INFO: Pod downwardapi-volume-f4386271-4a8f-4ea0-afb8-d5ff440a85eb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:38:02.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4590" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":5,"skipped":57,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:38:02.109: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:38:04.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2176" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":6,"skipped":70,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:38:04.351: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Sep 11 17:38:04.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-6515'
Sep 11 17:38:04.924: INFO: stderr: ""
Sep 11 17:38:04.924: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 11 17:38:05.928: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 11 17:38:05.928: INFO: Found 0 / 1
Sep 11 17:38:06.929: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 11 17:38:06.929: INFO: Found 1 / 1
Sep 11 17:38:06.929: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 11 17:38:06.933: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 11 17:38:06.933: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 11 17:38:06.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 patch pod agnhost-primary-qnqsr --namespace=kubectl-6515 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 11 17:38:07.046: INFO: stderr: ""
Sep 11 17:38:07.046: INFO: stdout: "pod/agnhost-primary-qnqsr patched\n"
STEP: checking annotations
Sep 11 17:38:07.051: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 11 17:38:07.051: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:38:07.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6515" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":7,"skipped":89,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:38:07.101: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:38:07.424: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-43a04c56-36ba-44c5-b1d9-020189c7c2ce" in namespace "security-context-test-3608" to be "Succeeded or Failed"
Sep 11 17:38:07.466: INFO: Pod "busybox-privileged-false-43a04c56-36ba-44c5-b1d9-020189c7c2ce": Phase="Pending", Reason="", readiness=false. Elapsed: 41.27443ms
Sep 11 17:38:09.471: INFO: Pod "busybox-privileged-false-43a04c56-36ba-44c5-b1d9-020189c7c2ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046822072s
Sep 11 17:38:11.477: INFO: Pod "busybox-privileged-false-43a04c56-36ba-44c5-b1d9-020189c7c2ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053056851s
Sep 11 17:38:11.477: INFO: Pod "busybox-privileged-false-43a04c56-36ba-44c5-b1d9-020189c7c2ce" satisfied condition "Succeeded or Failed"
Sep 11 17:38:11.508: INFO: Got logs for pod "busybox-privileged-false-43a04c56-36ba-44c5-b1d9-020189c7c2ce": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:38:11.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3608" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":118,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:38:11.538: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 11 17:38:11.704: INFO: Waiting up to 5m0s for pod "pod-7c09b506-8b24-442e-8dbf-40a06b46059e" in namespace "emptydir-4423" to be "Succeeded or Failed"
Sep 11 17:38:11.710: INFO: Pod "pod-7c09b506-8b24-442e-8dbf-40a06b46059e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.335101ms
Sep 11 17:38:13.717: INFO: Pod "pod-7c09b506-8b24-442e-8dbf-40a06b46059e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012575721s
Sep 11 17:38:15.721: INFO: Pod "pod-7c09b506-8b24-442e-8dbf-40a06b46059e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016618787s
STEP: Saw pod success
Sep 11 17:38:15.721: INFO: Pod "pod-7c09b506-8b24-442e-8dbf-40a06b46059e" satisfied condition "Succeeded or Failed"
Sep 11 17:38:15.725: INFO: Trying to get logs from node ip-10-10-1-141.ec2.internal pod pod-7c09b506-8b24-442e-8dbf-40a06b46059e container test-container: <nil>
STEP: delete the pod
Sep 11 17:38:15.767: INFO: Waiting for pod pod-7c09b506-8b24-442e-8dbf-40a06b46059e to disappear
Sep 11 17:38:15.770: INFO: Pod pod-7c09b506-8b24-442e-8dbf-40a06b46059e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:38:15.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4423" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":9,"skipped":120,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:38:15.799: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Sep 11 17:38:15.963: INFO: Waiting up to 5m0s for pod "var-expansion-2603c27b-5869-4a63-b19d-bbae0845a72d" in namespace "var-expansion-2771" to be "Succeeded or Failed"
Sep 11 17:38:15.978: INFO: Pod "var-expansion-2603c27b-5869-4a63-b19d-bbae0845a72d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.232498ms
Sep 11 17:38:17.982: INFO: Pod "var-expansion-2603c27b-5869-4a63-b19d-bbae0845a72d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019797374s
STEP: Saw pod success
Sep 11 17:38:17.982: INFO: Pod "var-expansion-2603c27b-5869-4a63-b19d-bbae0845a72d" satisfied condition "Succeeded or Failed"
Sep 11 17:38:17.993: INFO: Trying to get logs from node ip-10-10-1-141.ec2.internal pod var-expansion-2603c27b-5869-4a63-b19d-bbae0845a72d container dapi-container: <nil>
STEP: delete the pod
Sep 11 17:38:18.029: INFO: Waiting for pod var-expansion-2603c27b-5869-4a63-b19d-bbae0845a72d to disappear
Sep 11 17:38:18.033: INFO: Pod var-expansion-2603c27b-5869-4a63-b19d-bbae0845a72d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:38:18.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2771" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":10,"skipped":123,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:38:18.059: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3967
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 11 17:38:18.212: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 11 17:38:18.308: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 11 17:38:20.313: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:38:22.313: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:38:24.313: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:38:26.314: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:38:28.313: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:38:30.313: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:38:32.315: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:38:34.313: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:38:36.313: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:38:38.313: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 11 17:38:38.321: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 11 17:38:38.328: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 11 17:38:40.373: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.134.93 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3967 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 17:38:40.373: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 17:38:41.542: INFO: Found all expected endpoints: [netserver-0]
Sep 11 17:38:41.546: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.123.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3967 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 17:38:41.546: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 17:38:42.757: INFO: Found all expected endpoints: [netserver-1]
Sep 11 17:38:42.762: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.249.228 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3967 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 17:38:42.762: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 17:38:44.005: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:38:44.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3967" for this suite.

â€¢ [SLOW TEST:25.975 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":11,"skipped":123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:38:44.035: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 17:38:44.209: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48c4481f-3ca4-4977-bc60-0ecb0c5aa41b" in namespace "projected-3125" to be "Succeeded or Failed"
Sep 11 17:38:44.216: INFO: Pod "downwardapi-volume-48c4481f-3ca4-4977-bc60-0ecb0c5aa41b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.203895ms
Sep 11 17:38:46.221: INFO: Pod "downwardapi-volume-48c4481f-3ca4-4977-bc60-0ecb0c5aa41b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012099213s
STEP: Saw pod success
Sep 11 17:38:46.221: INFO: Pod "downwardapi-volume-48c4481f-3ca4-4977-bc60-0ecb0c5aa41b" satisfied condition "Succeeded or Failed"
Sep 11 17:38:46.225: INFO: Trying to get logs from node ip-10-10-3-166.ec2.internal pod downwardapi-volume-48c4481f-3ca4-4977-bc60-0ecb0c5aa41b container client-container: <nil>
STEP: delete the pod
Sep 11 17:38:46.265: INFO: Waiting for pod downwardapi-volume-48c4481f-3ca4-4977-bc60-0ecb0c5aa41b to disappear
Sep 11 17:38:46.270: INFO: Pod downwardapi-volume-48c4481f-3ca4-4977-bc60-0ecb0c5aa41b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:38:46.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3125" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":12,"skipped":159,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:38:46.296: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 17:38:47.073: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 17:38:50.310: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:38:50.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6180" for this suite.
STEP: Destroying namespace "webhook-6180-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":13,"skipped":160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:38:50.660: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 17:38:51.422: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 17:38:53.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442731, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442731, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442731, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442731, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 17:38:56.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 11 17:38:56.514: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:38:56.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5541" for this suite.
STEP: Destroying namespace "webhook-5541-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.036 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":14,"skipped":198,"failed":0}
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:38:56.696: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:38:56.874: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: creating replication controller svc-latency-rc in namespace svc-latency-108
I0911 17:38:56.888856      19 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-108, replica count: 1
I0911 17:38:57.939359      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0911 17:38:58.939604      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0911 17:38:59.939817      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 17:39:00.062: INFO: Created: latency-svc-6rfb8
Sep 11 17:39:00.073: INFO: Got endpoints: latency-svc-6rfb8 [33.386907ms]
Sep 11 17:39:00.105: INFO: Created: latency-svc-nqjpw
Sep 11 17:39:00.118: INFO: Got endpoints: latency-svc-nqjpw [44.583834ms]
Sep 11 17:39:00.127: INFO: Created: latency-svc-l8bbf
Sep 11 17:39:00.137: INFO: Got endpoints: latency-svc-l8bbf [63.759496ms]
Sep 11 17:39:00.155: INFO: Created: latency-svc-x6g2p
Sep 11 17:39:00.171: INFO: Got endpoints: latency-svc-x6g2p [97.312605ms]
Sep 11 17:39:00.183: INFO: Created: latency-svc-n4h5x
Sep 11 17:39:00.198: INFO: Got endpoints: latency-svc-n4h5x [123.759137ms]
Sep 11 17:39:00.204: INFO: Created: latency-svc-xmwsv
Sep 11 17:39:00.215: INFO: Got endpoints: latency-svc-xmwsv [139.338087ms]
Sep 11 17:39:00.222: INFO: Created: latency-svc-d8trl
Sep 11 17:39:00.232: INFO: Got endpoints: latency-svc-d8trl [155.87075ms]
Sep 11 17:39:00.243: INFO: Created: latency-svc-q78wj
Sep 11 17:39:00.254: INFO: Got endpoints: latency-svc-q78wj [177.551222ms]
Sep 11 17:39:00.261: INFO: Created: latency-svc-zdcr9
Sep 11 17:39:00.273: INFO: Got endpoints: latency-svc-zdcr9 [196.619791ms]
Sep 11 17:39:00.276: INFO: Created: latency-svc-whnmp
Sep 11 17:39:00.289: INFO: Got endpoints: latency-svc-whnmp [212.638296ms]
Sep 11 17:39:00.299: INFO: Created: latency-svc-lpgj8
Sep 11 17:39:00.314: INFO: Got endpoints: latency-svc-lpgj8 [237.355585ms]
Sep 11 17:39:00.320: INFO: Created: latency-svc-jn4ml
Sep 11 17:39:00.328: INFO: Got endpoints: latency-svc-jn4ml [251.848654ms]
Sep 11 17:39:00.342: INFO: Created: latency-svc-qkt8b
Sep 11 17:39:00.350: INFO: Got endpoints: latency-svc-qkt8b [273.40584ms]
Sep 11 17:39:00.362: INFO: Created: latency-svc-9jvpf
Sep 11 17:39:00.375: INFO: Got endpoints: latency-svc-9jvpf [298.205259ms]
Sep 11 17:39:00.385: INFO: Created: latency-svc-hf5wv
Sep 11 17:39:00.391: INFO: Got endpoints: latency-svc-hf5wv [313.993113ms]
Sep 11 17:39:00.409: INFO: Created: latency-svc-wzcx2
Sep 11 17:39:00.424: INFO: Got endpoints: latency-svc-wzcx2 [351.269143ms]
Sep 11 17:39:00.442: INFO: Created: latency-svc-4l7w7
Sep 11 17:39:00.454: INFO: Got endpoints: latency-svc-4l7w7 [335.697093ms]
Sep 11 17:39:00.460: INFO: Created: latency-svc-vnb9j
Sep 11 17:39:00.470: INFO: Got endpoints: latency-svc-vnb9j [332.622209ms]
Sep 11 17:39:00.478: INFO: Created: latency-svc-t7gw7
Sep 11 17:39:00.486: INFO: Got endpoints: latency-svc-t7gw7 [315.288466ms]
Sep 11 17:39:00.498: INFO: Created: latency-svc-vv88w
Sep 11 17:39:00.515: INFO: Got endpoints: latency-svc-vv88w [317.19742ms]
Sep 11 17:39:00.521: INFO: Created: latency-svc-652rp
Sep 11 17:39:00.542: INFO: Got endpoints: latency-svc-652rp [326.492338ms]
Sep 11 17:39:00.550: INFO: Created: latency-svc-s6djh
Sep 11 17:39:00.564: INFO: Got endpoints: latency-svc-s6djh [331.801035ms]
Sep 11 17:39:00.570: INFO: Created: latency-svc-mr45t
Sep 11 17:39:00.583: INFO: Got endpoints: latency-svc-mr45t [328.918228ms]
Sep 11 17:39:00.588: INFO: Created: latency-svc-6sh27
Sep 11 17:39:00.595: INFO: Got endpoints: latency-svc-6sh27 [321.673715ms]
Sep 11 17:39:00.628: INFO: Created: latency-svc-nmqzv
Sep 11 17:39:00.646: INFO: Got endpoints: latency-svc-nmqzv [356.336031ms]
Sep 11 17:39:00.659: INFO: Created: latency-svc-77k2z
Sep 11 17:39:00.671: INFO: Got endpoints: latency-svc-77k2z [357.308936ms]
Sep 11 17:39:00.675: INFO: Created: latency-svc-44rq7
Sep 11 17:39:00.698: INFO: Got endpoints: latency-svc-44rq7 [369.947107ms]
Sep 11 17:39:00.710: INFO: Created: latency-svc-rdgj2
Sep 11 17:39:00.721: INFO: Got endpoints: latency-svc-rdgj2 [371.580909ms]
Sep 11 17:39:00.731: INFO: Created: latency-svc-vrx9s
Sep 11 17:39:00.744: INFO: Got endpoints: latency-svc-vrx9s [368.812689ms]
Sep 11 17:39:00.749: INFO: Created: latency-svc-fqtcr
Sep 11 17:39:00.761: INFO: Got endpoints: latency-svc-fqtcr [370.460027ms]
Sep 11 17:39:00.767: INFO: Created: latency-svc-9rm2p
Sep 11 17:39:00.782: INFO: Got endpoints: latency-svc-9rm2p [357.082613ms]
Sep 11 17:39:00.787: INFO: Created: latency-svc-jkzbp
Sep 11 17:39:00.795: INFO: Got endpoints: latency-svc-jkzbp [340.872505ms]
Sep 11 17:39:00.814: INFO: Created: latency-svc-rvwv5
Sep 11 17:39:00.829: INFO: Got endpoints: latency-svc-rvwv5 [358.823892ms]
Sep 11 17:39:00.838: INFO: Created: latency-svc-5ntrx
Sep 11 17:39:00.850: INFO: Got endpoints: latency-svc-5ntrx [364.030025ms]
Sep 11 17:39:00.860: INFO: Created: latency-svc-gst6n
Sep 11 17:39:00.880: INFO: Got endpoints: latency-svc-gst6n [364.524201ms]
Sep 11 17:39:00.898: INFO: Created: latency-svc-jdvlv
Sep 11 17:39:00.909: INFO: Got endpoints: latency-svc-jdvlv [367.505078ms]
Sep 11 17:39:00.915: INFO: Created: latency-svc-9bshm
Sep 11 17:39:00.927: INFO: Got endpoints: latency-svc-9bshm [363.297994ms]
Sep 11 17:39:00.934: INFO: Created: latency-svc-4bmqr
Sep 11 17:39:00.942: INFO: Got endpoints: latency-svc-4bmqr [359.061362ms]
Sep 11 17:39:00.958: INFO: Created: latency-svc-m5zgf
Sep 11 17:39:00.966: INFO: Got endpoints: latency-svc-m5zgf [370.886598ms]
Sep 11 17:39:00.984: INFO: Created: latency-svc-9gbgz
Sep 11 17:39:00.995: INFO: Got endpoints: latency-svc-9gbgz [348.889264ms]
Sep 11 17:39:01.000: INFO: Created: latency-svc-bm65f
Sep 11 17:39:01.012: INFO: Got endpoints: latency-svc-bm65f [340.796288ms]
Sep 11 17:39:01.016: INFO: Created: latency-svc-4l4c4
Sep 11 17:39:01.030: INFO: Got endpoints: latency-svc-4l4c4 [331.917835ms]
Sep 11 17:39:01.038: INFO: Created: latency-svc-6hgch
Sep 11 17:39:01.048: INFO: Got endpoints: latency-svc-6hgch [326.308906ms]
Sep 11 17:39:01.054: INFO: Created: latency-svc-t88hm
Sep 11 17:39:01.075: INFO: Got endpoints: latency-svc-t88hm [331.180539ms]
Sep 11 17:39:01.095: INFO: Created: latency-svc-v7c9b
Sep 11 17:39:01.112: INFO: Got endpoints: latency-svc-v7c9b [351.339654ms]
Sep 11 17:39:01.129: INFO: Created: latency-svc-fpx9n
Sep 11 17:39:01.155: INFO: Got endpoints: latency-svc-fpx9n [373.764919ms]
Sep 11 17:39:01.163: INFO: Created: latency-svc-9snlv
Sep 11 17:39:01.175: INFO: Got endpoints: latency-svc-9snlv [380.249576ms]
Sep 11 17:39:01.187: INFO: Created: latency-svc-9db52
Sep 11 17:39:01.202: INFO: Got endpoints: latency-svc-9db52 [372.627809ms]
Sep 11 17:39:01.207: INFO: Created: latency-svc-qxj5l
Sep 11 17:39:01.220: INFO: Got endpoints: latency-svc-qxj5l [369.339789ms]
Sep 11 17:39:01.223: INFO: Created: latency-svc-s7jhv
Sep 11 17:39:01.237: INFO: Got endpoints: latency-svc-s7jhv [356.973011ms]
Sep 11 17:39:01.240: INFO: Created: latency-svc-xtsx6
Sep 11 17:39:01.256: INFO: Got endpoints: latency-svc-xtsx6 [346.345497ms]
Sep 11 17:39:01.261: INFO: Created: latency-svc-fhrfx
Sep 11 17:39:01.271: INFO: Got endpoints: latency-svc-fhrfx [343.895224ms]
Sep 11 17:39:01.277: INFO: Created: latency-svc-9r6vv
Sep 11 17:39:01.293: INFO: Got endpoints: latency-svc-9r6vv [351.63232ms]
Sep 11 17:39:01.325: INFO: Created: latency-svc-l7vfh
Sep 11 17:39:01.337: INFO: Got endpoints: latency-svc-l7vfh [370.609621ms]
Sep 11 17:39:01.343: INFO: Created: latency-svc-x922w
Sep 11 17:39:01.355: INFO: Got endpoints: latency-svc-x922w [360.177495ms]
Sep 11 17:39:01.361: INFO: Created: latency-svc-pr4n8
Sep 11 17:39:01.373: INFO: Got endpoints: latency-svc-pr4n8 [361.625788ms]
Sep 11 17:39:01.380: INFO: Created: latency-svc-pghhq
Sep 11 17:39:01.397: INFO: Created: latency-svc-54qwf
Sep 11 17:39:01.450: INFO: Got endpoints: latency-svc-pghhq [419.760894ms]
Sep 11 17:39:01.456: INFO: Created: latency-svc-jsn4n
Sep 11 17:39:01.468: INFO: Created: latency-svc-n49gc
Sep 11 17:39:01.481: INFO: Got endpoints: latency-svc-54qwf [432.755898ms]
Sep 11 17:39:01.495: INFO: Created: latency-svc-xq7wp
Sep 11 17:39:01.510: INFO: Created: latency-svc-g4wz2
Sep 11 17:39:01.524: INFO: Got endpoints: latency-svc-jsn4n [448.320673ms]
Sep 11 17:39:01.530: INFO: Created: latency-svc-gmkrv
Sep 11 17:39:01.542: INFO: Created: latency-svc-qh28s
Sep 11 17:39:01.562: INFO: Created: latency-svc-4w4ht
Sep 11 17:39:01.581: INFO: Got endpoints: latency-svc-n49gc [468.006243ms]
Sep 11 17:39:01.585: INFO: Created: latency-svc-5f4dv
Sep 11 17:39:01.606: INFO: Created: latency-svc-nfbk7
Sep 11 17:39:01.631: INFO: Got endpoints: latency-svc-xq7wp [475.877581ms]
Sep 11 17:39:01.648: INFO: Created: latency-svc-87zdt
Sep 11 17:39:01.665: INFO: Created: latency-svc-qr5gl
Sep 11 17:39:01.684: INFO: Got endpoints: latency-svc-g4wz2 [508.697094ms]
Sep 11 17:39:01.697: INFO: Created: latency-svc-9t5rx
Sep 11 17:39:01.725: INFO: Got endpoints: latency-svc-gmkrv [523.869638ms]
Sep 11 17:39:01.735: INFO: Created: latency-svc-dts28
Sep 11 17:39:01.799: INFO: Got endpoints: latency-svc-qh28s [579.385297ms]
Sep 11 17:39:01.806: INFO: Created: latency-svc-2sm9q
Sep 11 17:39:01.839: INFO: Got endpoints: latency-svc-4w4ht [602.217852ms]
Sep 11 17:39:01.849: INFO: Created: latency-svc-qxmhs
Sep 11 17:39:01.887: INFO: Got endpoints: latency-svc-5f4dv [631.350825ms]
Sep 11 17:39:01.903: INFO: Created: latency-svc-77sqb
Sep 11 17:39:01.927: INFO: Got endpoints: latency-svc-nfbk7 [655.435957ms]
Sep 11 17:39:01.927: INFO: Created: latency-svc-nb2vx
Sep 11 17:39:01.949: INFO: Created: latency-svc-ssbjq
Sep 11 17:39:01.967: INFO: Created: latency-svc-zk74s
Sep 11 17:39:01.974: INFO: Got endpoints: latency-svc-87zdt [680.989429ms]
Sep 11 17:39:01.989: INFO: Created: latency-svc-4ngxs
Sep 11 17:39:02.009: INFO: Created: latency-svc-4qgf8
Sep 11 17:39:02.026: INFO: Got endpoints: latency-svc-qr5gl [688.831962ms]
Sep 11 17:39:02.034: INFO: Created: latency-svc-r9mfp
Sep 11 17:39:02.057: INFO: Created: latency-svc-drppz
Sep 11 17:39:02.073: INFO: Got endpoints: latency-svc-9t5rx [718.34296ms]
Sep 11 17:39:02.080: INFO: Created: latency-svc-cn562
Sep 11 17:39:02.103: INFO: Created: latency-svc-bsncd
Sep 11 17:39:02.148: INFO: Got endpoints: latency-svc-dts28 [774.684328ms]
Sep 11 17:39:02.163: INFO: Created: latency-svc-gsqxv
Sep 11 17:39:02.180: INFO: Got endpoints: latency-svc-2sm9q [729.817415ms]
Sep 11 17:39:02.188: INFO: Created: latency-svc-gt2d4
Sep 11 17:39:02.205: INFO: Created: latency-svc-cltj9
Sep 11 17:39:02.228: INFO: Got endpoints: latency-svc-qxmhs [746.989079ms]
Sep 11 17:39:02.235: INFO: Created: latency-svc-pvwcw
Sep 11 17:39:02.262: INFO: Created: latency-svc-rtw9f
Sep 11 17:39:02.275: INFO: Got endpoints: latency-svc-77sqb [751.613357ms]
Sep 11 17:39:02.298: INFO: Created: latency-svc-d84nc
Sep 11 17:39:02.323: INFO: Got endpoints: latency-svc-nb2vx [742.268866ms]
Sep 11 17:39:02.348: INFO: Created: latency-svc-jw6q5
Sep 11 17:39:02.372: INFO: Got endpoints: latency-svc-ssbjq [740.462394ms]
Sep 11 17:39:02.396: INFO: Created: latency-svc-rqlcp
Sep 11 17:39:02.423: INFO: Got endpoints: latency-svc-zk74s [738.872743ms]
Sep 11 17:39:02.470: INFO: Created: latency-svc-std7c
Sep 11 17:39:02.473: INFO: Got endpoints: latency-svc-4ngxs [747.50566ms]
Sep 11 17:39:02.499: INFO: Created: latency-svc-rzfj2
Sep 11 17:39:02.526: INFO: Got endpoints: latency-svc-4qgf8 [726.249218ms]
Sep 11 17:39:02.553: INFO: Created: latency-svc-7qk6v
Sep 11 17:39:02.575: INFO: Got endpoints: latency-svc-r9mfp [736.006213ms]
Sep 11 17:39:02.603: INFO: Created: latency-svc-n57px
Sep 11 17:39:02.623: INFO: Got endpoints: latency-svc-drppz [735.604116ms]
Sep 11 17:39:02.647: INFO: Created: latency-svc-r9xvr
Sep 11 17:39:02.673: INFO: Got endpoints: latency-svc-cn562 [746.09976ms]
Sep 11 17:39:02.698: INFO: Created: latency-svc-wlhr5
Sep 11 17:39:02.722: INFO: Got endpoints: latency-svc-bsncd [747.512253ms]
Sep 11 17:39:02.750: INFO: Created: latency-svc-bffjl
Sep 11 17:39:02.773: INFO: Got endpoints: latency-svc-gsqxv [746.821635ms]
Sep 11 17:39:02.802: INFO: Created: latency-svc-wprtx
Sep 11 17:39:02.819: INFO: Got endpoints: latency-svc-gt2d4 [746.039387ms]
Sep 11 17:39:02.849: INFO: Created: latency-svc-44wlx
Sep 11 17:39:02.871: INFO: Got endpoints: latency-svc-cltj9 [721.933126ms]
Sep 11 17:39:02.898: INFO: Created: latency-svc-p47g5
Sep 11 17:39:02.928: INFO: Got endpoints: latency-svc-pvwcw [748.066257ms]
Sep 11 17:39:02.959: INFO: Created: latency-svc-8crf2
Sep 11 17:39:02.972: INFO: Got endpoints: latency-svc-rtw9f [743.978962ms]
Sep 11 17:39:02.998: INFO: Created: latency-svc-2l8gq
Sep 11 17:39:03.021: INFO: Got endpoints: latency-svc-d84nc [745.850232ms]
Sep 11 17:39:03.046: INFO: Created: latency-svc-bnhdh
Sep 11 17:39:03.071: INFO: Got endpoints: latency-svc-jw6q5 [748.476901ms]
Sep 11 17:39:03.103: INFO: Created: latency-svc-24fn9
Sep 11 17:39:03.121: INFO: Got endpoints: latency-svc-rqlcp [749.568787ms]
Sep 11 17:39:03.155: INFO: Created: latency-svc-9zqdd
Sep 11 17:39:03.173: INFO: Got endpoints: latency-svc-std7c [750.376575ms]
Sep 11 17:39:03.201: INFO: Created: latency-svc-2nlzw
Sep 11 17:39:03.226: INFO: Got endpoints: latency-svc-rzfj2 [752.569069ms]
Sep 11 17:39:03.255: INFO: Created: latency-svc-4wjjh
Sep 11 17:39:03.272: INFO: Got endpoints: latency-svc-7qk6v [745.926106ms]
Sep 11 17:39:03.297: INFO: Created: latency-svc-wqsg4
Sep 11 17:39:03.322: INFO: Got endpoints: latency-svc-n57px [746.719273ms]
Sep 11 17:39:03.346: INFO: Created: latency-svc-vqr6n
Sep 11 17:39:03.374: INFO: Got endpoints: latency-svc-r9xvr [750.95782ms]
Sep 11 17:39:03.396: INFO: Created: latency-svc-26r4l
Sep 11 17:39:03.421: INFO: Got endpoints: latency-svc-wlhr5 [748.369815ms]
Sep 11 17:39:03.443: INFO: Created: latency-svc-2f2jk
Sep 11 17:39:03.472: INFO: Got endpoints: latency-svc-bffjl [749.979152ms]
Sep 11 17:39:03.493: INFO: Created: latency-svc-lwlqk
Sep 11 17:39:03.521: INFO: Got endpoints: latency-svc-wprtx [748.741779ms]
Sep 11 17:39:03.544: INFO: Created: latency-svc-b6gn4
Sep 11 17:39:03.575: INFO: Got endpoints: latency-svc-44wlx [755.536236ms]
Sep 11 17:39:03.597: INFO: Created: latency-svc-h6bhf
Sep 11 17:39:03.623: INFO: Got endpoints: latency-svc-p47g5 [751.016849ms]
Sep 11 17:39:03.648: INFO: Created: latency-svc-248zr
Sep 11 17:39:03.672: INFO: Got endpoints: latency-svc-8crf2 [744.325994ms]
Sep 11 17:39:03.695: INFO: Created: latency-svc-fxg5l
Sep 11 17:39:03.722: INFO: Got endpoints: latency-svc-2l8gq [749.984997ms]
Sep 11 17:39:03.746: INFO: Created: latency-svc-w72ht
Sep 11 17:39:03.772: INFO: Got endpoints: latency-svc-bnhdh [750.381966ms]
Sep 11 17:39:03.796: INFO: Created: latency-svc-flp75
Sep 11 17:39:03.821: INFO: Got endpoints: latency-svc-24fn9 [750.028724ms]
Sep 11 17:39:03.843: INFO: Created: latency-svc-sgwh7
Sep 11 17:39:03.870: INFO: Got endpoints: latency-svc-9zqdd [749.027476ms]
Sep 11 17:39:03.895: INFO: Created: latency-svc-khx22
Sep 11 17:39:03.921: INFO: Got endpoints: latency-svc-2nlzw [748.15329ms]
Sep 11 17:39:03.943: INFO: Created: latency-svc-44sw9
Sep 11 17:39:03.973: INFO: Got endpoints: latency-svc-4wjjh [747.383239ms]
Sep 11 17:39:04.002: INFO: Created: latency-svc-nrrt5
Sep 11 17:39:04.021: INFO: Got endpoints: latency-svc-wqsg4 [749.121406ms]
Sep 11 17:39:04.043: INFO: Created: latency-svc-fxpq5
Sep 11 17:39:04.074: INFO: Got endpoints: latency-svc-vqr6n [751.960944ms]
Sep 11 17:39:04.101: INFO: Created: latency-svc-rw4qn
Sep 11 17:39:04.124: INFO: Got endpoints: latency-svc-26r4l [749.759726ms]
Sep 11 17:39:04.158: INFO: Created: latency-svc-fqnlh
Sep 11 17:39:04.170: INFO: Got endpoints: latency-svc-2f2jk [749.24776ms]
Sep 11 17:39:04.193: INFO: Created: latency-svc-5npfw
Sep 11 17:39:04.221: INFO: Got endpoints: latency-svc-lwlqk [748.816643ms]
Sep 11 17:39:04.254: INFO: Created: latency-svc-gknm7
Sep 11 17:39:04.269: INFO: Got endpoints: latency-svc-b6gn4 [747.97635ms]
Sep 11 17:39:04.292: INFO: Created: latency-svc-jttt7
Sep 11 17:39:04.320: INFO: Got endpoints: latency-svc-h6bhf [745.009943ms]
Sep 11 17:39:04.347: INFO: Created: latency-svc-fl7zj
Sep 11 17:39:04.373: INFO: Got endpoints: latency-svc-248zr [750.73656ms]
Sep 11 17:39:04.395: INFO: Created: latency-svc-cdjc9
Sep 11 17:39:04.421: INFO: Got endpoints: latency-svc-fxg5l [748.808835ms]
Sep 11 17:39:04.443: INFO: Created: latency-svc-jdmxd
Sep 11 17:39:04.470: INFO: Got endpoints: latency-svc-w72ht [748.164839ms]
Sep 11 17:39:04.496: INFO: Created: latency-svc-pfs8b
Sep 11 17:39:04.523: INFO: Got endpoints: latency-svc-flp75 [750.680541ms]
Sep 11 17:39:04.572: INFO: Got endpoints: latency-svc-sgwh7 [750.944675ms]
Sep 11 17:39:04.603: INFO: Created: latency-svc-9dmwf
Sep 11 17:39:04.820: INFO: Got endpoints: latency-svc-fxpq5 [799.203777ms]
Sep 11 17:39:04.831: INFO: Got endpoints: latency-svc-nrrt5 [857.539659ms]
Sep 11 17:39:04.831: INFO: Got endpoints: latency-svc-khx22 [960.358377ms]
Sep 11 17:39:04.839: INFO: Got endpoints: latency-svc-44sw9 [918.21723ms]
Sep 11 17:39:04.856: INFO: Created: latency-svc-8bmkb
Sep 11 17:39:04.856: INFO: Got endpoints: latency-svc-rw4qn [782.665374ms]
Sep 11 17:39:04.878: INFO: Created: latency-svc-v5jnc
Sep 11 17:39:04.882: INFO: Got endpoints: latency-svc-fqnlh [757.973333ms]
Sep 11 17:39:04.918: INFO: Created: latency-svc-p45ss
Sep 11 17:39:04.940: INFO: Got endpoints: latency-svc-5npfw [769.673024ms]
Sep 11 17:39:04.950: INFO: Created: latency-svc-lds58
Sep 11 17:39:04.973: INFO: Created: latency-svc-nw9sq
Sep 11 17:39:04.975: INFO: Got endpoints: latency-svc-gknm7 [753.943111ms]
Sep 11 17:39:05.000: INFO: Created: latency-svc-zdn5b
Sep 11 17:39:05.015: INFO: Created: latency-svc-2lnc9
Sep 11 17:39:05.026: INFO: Got endpoints: latency-svc-jttt7 [756.418862ms]
Sep 11 17:39:05.036: INFO: Created: latency-svc-sk8wt
Sep 11 17:39:05.052: INFO: Created: latency-svc-ht8n2
Sep 11 17:39:05.067: INFO: Created: latency-svc-clktp
Sep 11 17:39:05.074: INFO: Got endpoints: latency-svc-fl7zj [753.742621ms]
Sep 11 17:39:05.096: INFO: Created: latency-svc-kqwth
Sep 11 17:39:05.130: INFO: Got endpoints: latency-svc-cdjc9 [756.212299ms]
Sep 11 17:39:05.158: INFO: Created: latency-svc-gbjwl
Sep 11 17:39:05.173: INFO: Got endpoints: latency-svc-jdmxd [751.464867ms]
Sep 11 17:39:05.193: INFO: Created: latency-svc-lf7q7
Sep 11 17:39:05.221: INFO: Got endpoints: latency-svc-pfs8b [751.565856ms]
Sep 11 17:39:05.241: INFO: Created: latency-svc-k94xm
Sep 11 17:39:05.271: INFO: Got endpoints: latency-svc-9dmwf [747.997482ms]
Sep 11 17:39:05.292: INFO: Created: latency-svc-6x2sd
Sep 11 17:39:05.321: INFO: Got endpoints: latency-svc-8bmkb [748.674506ms]
Sep 11 17:39:05.353: INFO: Created: latency-svc-fmqbc
Sep 11 17:39:05.371: INFO: Got endpoints: latency-svc-v5jnc [550.561586ms]
Sep 11 17:39:05.390: INFO: Created: latency-svc-t7nt9
Sep 11 17:39:05.421: INFO: Got endpoints: latency-svc-p45ss [590.239078ms]
Sep 11 17:39:05.443: INFO: Created: latency-svc-kdcx5
Sep 11 17:39:05.472: INFO: Got endpoints: latency-svc-lds58 [640.790467ms]
Sep 11 17:39:05.491: INFO: Created: latency-svc-mlf4m
Sep 11 17:39:05.521: INFO: Got endpoints: latency-svc-nw9sq [681.873707ms]
Sep 11 17:39:05.541: INFO: Created: latency-svc-mm6d9
Sep 11 17:39:05.570: INFO: Got endpoints: latency-svc-zdn5b [714.004173ms]
Sep 11 17:39:05.591: INFO: Created: latency-svc-mvnf2
Sep 11 17:39:05.621: INFO: Got endpoints: latency-svc-2lnc9 [739.065075ms]
Sep 11 17:39:05.640: INFO: Created: latency-svc-979gx
Sep 11 17:39:05.671: INFO: Got endpoints: latency-svc-sk8wt [730.944072ms]
Sep 11 17:39:05.691: INFO: Created: latency-svc-qtxpx
Sep 11 17:39:05.723: INFO: Got endpoints: latency-svc-ht8n2 [748.195628ms]
Sep 11 17:39:05.749: INFO: Created: latency-svc-qb6ws
Sep 11 17:39:05.772: INFO: Got endpoints: latency-svc-clktp [745.672598ms]
Sep 11 17:39:05.796: INFO: Created: latency-svc-ntszs
Sep 11 17:39:05.820: INFO: Got endpoints: latency-svc-kqwth [746.552054ms]
Sep 11 17:39:05.850: INFO: Created: latency-svc-ghzqc
Sep 11 17:39:05.871: INFO: Got endpoints: latency-svc-gbjwl [740.947757ms]
Sep 11 17:39:05.890: INFO: Created: latency-svc-r2h2f
Sep 11 17:39:05.922: INFO: Got endpoints: latency-svc-lf7q7 [749.763068ms]
Sep 11 17:39:05.944: INFO: Created: latency-svc-dpzvj
Sep 11 17:39:05.970: INFO: Got endpoints: latency-svc-k94xm [748.838189ms]
Sep 11 17:39:05.991: INFO: Created: latency-svc-g5rlk
Sep 11 17:39:06.022: INFO: Got endpoints: latency-svc-6x2sd [751.217737ms]
Sep 11 17:39:06.042: INFO: Created: latency-svc-69pqp
Sep 11 17:39:06.071: INFO: Got endpoints: latency-svc-fmqbc [750.196329ms]
Sep 11 17:39:06.094: INFO: Created: latency-svc-g9sdt
Sep 11 17:39:06.123: INFO: Got endpoints: latency-svc-t7nt9 [752.709195ms]
Sep 11 17:39:06.149: INFO: Created: latency-svc-9xmmg
Sep 11 17:39:06.171: INFO: Got endpoints: latency-svc-kdcx5 [749.892841ms]
Sep 11 17:39:06.197: INFO: Created: latency-svc-ls9z7
Sep 11 17:39:06.222: INFO: Got endpoints: latency-svc-mlf4m [749.992258ms]
Sep 11 17:39:06.243: INFO: Created: latency-svc-7l7x4
Sep 11 17:39:06.275: INFO: Got endpoints: latency-svc-mm6d9 [753.458736ms]
Sep 11 17:39:06.295: INFO: Created: latency-svc-6bx8b
Sep 11 17:39:06.320: INFO: Got endpoints: latency-svc-mvnf2 [749.367614ms]
Sep 11 17:39:06.343: INFO: Created: latency-svc-mj4fh
Sep 11 17:39:06.371: INFO: Got endpoints: latency-svc-979gx [749.846666ms]
Sep 11 17:39:06.396: INFO: Created: latency-svc-wzvhx
Sep 11 17:39:06.422: INFO: Got endpoints: latency-svc-qtxpx [751.17826ms]
Sep 11 17:39:06.447: INFO: Created: latency-svc-g5445
Sep 11 17:39:06.470: INFO: Got endpoints: latency-svc-qb6ws [746.947812ms]
Sep 11 17:39:06.510: INFO: Created: latency-svc-7pqwm
Sep 11 17:39:06.520: INFO: Got endpoints: latency-svc-ntszs [748.278201ms]
Sep 11 17:39:06.549: INFO: Created: latency-svc-df8w6
Sep 11 17:39:06.571: INFO: Got endpoints: latency-svc-ghzqc [750.137998ms]
Sep 11 17:39:06.590: INFO: Created: latency-svc-t4ng8
Sep 11 17:39:06.621: INFO: Got endpoints: latency-svc-r2h2f [750.429028ms]
Sep 11 17:39:06.641: INFO: Created: latency-svc-m9qp9
Sep 11 17:39:06.671: INFO: Got endpoints: latency-svc-dpzvj [748.614964ms]
Sep 11 17:39:06.691: INFO: Created: latency-svc-j2564
Sep 11 17:39:06.721: INFO: Got endpoints: latency-svc-g5rlk [751.020948ms]
Sep 11 17:39:06.741: INFO: Created: latency-svc-sx67s
Sep 11 17:39:06.771: INFO: Got endpoints: latency-svc-69pqp [749.062229ms]
Sep 11 17:39:06.792: INFO: Created: latency-svc-tcbzb
Sep 11 17:39:06.837: INFO: Got endpoints: latency-svc-g9sdt [765.096036ms]
Sep 11 17:39:06.856: INFO: Created: latency-svc-dwp48
Sep 11 17:39:06.871: INFO: Got endpoints: latency-svc-9xmmg [747.565253ms]
Sep 11 17:39:06.891: INFO: Created: latency-svc-hwnq4
Sep 11 17:39:06.929: INFO: Got endpoints: latency-svc-ls9z7 [758.290795ms]
Sep 11 17:39:06.954: INFO: Created: latency-svc-9m9zx
Sep 11 17:39:06.970: INFO: Got endpoints: latency-svc-7l7x4 [748.400867ms]
Sep 11 17:39:06.990: INFO: Created: latency-svc-4lknc
Sep 11 17:39:07.027: INFO: Got endpoints: latency-svc-6bx8b [751.616056ms]
Sep 11 17:39:07.047: INFO: Created: latency-svc-bzn4h
Sep 11 17:39:07.072: INFO: Got endpoints: latency-svc-mj4fh [752.019763ms]
Sep 11 17:39:07.096: INFO: Created: latency-svc-vbgfg
Sep 11 17:39:07.124: INFO: Got endpoints: latency-svc-wzvhx [752.91865ms]
Sep 11 17:39:07.146: INFO: Created: latency-svc-kjncb
Sep 11 17:39:07.172: INFO: Got endpoints: latency-svc-g5445 [749.405701ms]
Sep 11 17:39:07.195: INFO: Created: latency-svc-gxb76
Sep 11 17:39:07.220: INFO: Got endpoints: latency-svc-7pqwm [749.37844ms]
Sep 11 17:39:07.242: INFO: Created: latency-svc-jpn5k
Sep 11 17:39:07.272: INFO: Got endpoints: latency-svc-df8w6 [752.113928ms]
Sep 11 17:39:07.292: INFO: Created: latency-svc-tgktk
Sep 11 17:39:07.320: INFO: Got endpoints: latency-svc-t4ng8 [749.513651ms]
Sep 11 17:39:07.340: INFO: Created: latency-svc-qmnmm
Sep 11 17:39:07.370: INFO: Got endpoints: latency-svc-m9qp9 [749.040936ms]
Sep 11 17:39:07.390: INFO: Created: latency-svc-w8z87
Sep 11 17:39:07.420: INFO: Got endpoints: latency-svc-j2564 [748.959938ms]
Sep 11 17:39:07.441: INFO: Created: latency-svc-sxssz
Sep 11 17:39:07.471: INFO: Got endpoints: latency-svc-sx67s [749.390662ms]
Sep 11 17:39:07.490: INFO: Created: latency-svc-86hf9
Sep 11 17:39:07.523: INFO: Got endpoints: latency-svc-tcbzb [751.965592ms]
Sep 11 17:39:07.544: INFO: Created: latency-svc-6blgd
Sep 11 17:39:07.570: INFO: Got endpoints: latency-svc-dwp48 [733.586586ms]
Sep 11 17:39:07.591: INFO: Created: latency-svc-9s6vx
Sep 11 17:39:07.621: INFO: Got endpoints: latency-svc-hwnq4 [750.368632ms]
Sep 11 17:39:07.644: INFO: Created: latency-svc-xbxzb
Sep 11 17:39:07.673: INFO: Got endpoints: latency-svc-9m9zx [743.803218ms]
Sep 11 17:39:07.693: INFO: Created: latency-svc-npwtl
Sep 11 17:39:07.723: INFO: Got endpoints: latency-svc-4lknc [752.973379ms]
Sep 11 17:39:07.763: INFO: Created: latency-svc-8rx88
Sep 11 17:39:07.771: INFO: Got endpoints: latency-svc-bzn4h [744.849424ms]
Sep 11 17:39:07.802: INFO: Created: latency-svc-p4fpt
Sep 11 17:39:07.843: INFO: Got endpoints: latency-svc-vbgfg [770.943498ms]
Sep 11 17:39:07.894: INFO: Got endpoints: latency-svc-kjncb [770.195033ms]
Sep 11 17:39:07.906: INFO: Created: latency-svc-ncvcl
Sep 11 17:39:07.930: INFO: Got endpoints: latency-svc-gxb76 [758.122866ms]
Sep 11 17:39:07.930: INFO: Created: latency-svc-64llx
Sep 11 17:39:07.969: INFO: Got endpoints: latency-svc-jpn5k [749.688728ms]
Sep 11 17:39:08.022: INFO: Got endpoints: latency-svc-tgktk [749.529444ms]
Sep 11 17:39:08.071: INFO: Got endpoints: latency-svc-qmnmm [751.20654ms]
Sep 11 17:39:08.126: INFO: Got endpoints: latency-svc-w8z87 [755.674241ms]
Sep 11 17:39:08.173: INFO: Got endpoints: latency-svc-sxssz [752.519254ms]
Sep 11 17:39:08.221: INFO: Got endpoints: latency-svc-86hf9 [750.377969ms]
Sep 11 17:39:08.272: INFO: Got endpoints: latency-svc-6blgd [748.778793ms]
Sep 11 17:39:08.323: INFO: Got endpoints: latency-svc-9s6vx [752.637955ms]
Sep 11 17:39:08.370: INFO: Got endpoints: latency-svc-xbxzb [748.247833ms]
Sep 11 17:39:08.421: INFO: Got endpoints: latency-svc-npwtl [748.04869ms]
Sep 11 17:39:08.471: INFO: Got endpoints: latency-svc-8rx88 [748.062882ms]
Sep 11 17:39:08.521: INFO: Got endpoints: latency-svc-p4fpt [749.461135ms]
Sep 11 17:39:08.571: INFO: Got endpoints: latency-svc-ncvcl [728.437872ms]
Sep 11 17:39:08.621: INFO: Got endpoints: latency-svc-64llx [727.268515ms]
Sep 11 17:39:08.621: INFO: Latencies: [44.583834ms 63.759496ms 97.312605ms 123.759137ms 139.338087ms 155.87075ms 177.551222ms 196.619791ms 212.638296ms 237.355585ms 251.848654ms 273.40584ms 298.205259ms 313.993113ms 315.288466ms 317.19742ms 321.673715ms 326.308906ms 326.492338ms 328.918228ms 331.180539ms 331.801035ms 331.917835ms 332.622209ms 335.697093ms 340.796288ms 340.872505ms 343.895224ms 346.345497ms 348.889264ms 351.269143ms 351.339654ms 351.63232ms 356.336031ms 356.973011ms 357.082613ms 357.308936ms 358.823892ms 359.061362ms 360.177495ms 361.625788ms 363.297994ms 364.030025ms 364.524201ms 367.505078ms 368.812689ms 369.339789ms 369.947107ms 370.460027ms 370.609621ms 370.886598ms 371.580909ms 372.627809ms 373.764919ms 380.249576ms 419.760894ms 432.755898ms 448.320673ms 468.006243ms 475.877581ms 508.697094ms 523.869638ms 550.561586ms 579.385297ms 590.239078ms 602.217852ms 631.350825ms 640.790467ms 655.435957ms 680.989429ms 681.873707ms 688.831962ms 714.004173ms 718.34296ms 721.933126ms 726.249218ms 727.268515ms 728.437872ms 729.817415ms 730.944072ms 733.586586ms 735.604116ms 736.006213ms 738.872743ms 739.065075ms 740.462394ms 740.947757ms 742.268866ms 743.803218ms 743.978962ms 744.325994ms 744.849424ms 745.009943ms 745.672598ms 745.850232ms 745.926106ms 746.039387ms 746.09976ms 746.552054ms 746.719273ms 746.821635ms 746.947812ms 746.989079ms 747.383239ms 747.50566ms 747.512253ms 747.565253ms 747.97635ms 747.997482ms 748.04869ms 748.062882ms 748.066257ms 748.15329ms 748.164839ms 748.195628ms 748.247833ms 748.278201ms 748.369815ms 748.400867ms 748.476901ms 748.614964ms 748.674506ms 748.741779ms 748.778793ms 748.808835ms 748.816643ms 748.838189ms 748.959938ms 749.027476ms 749.040936ms 749.062229ms 749.121406ms 749.24776ms 749.367614ms 749.37844ms 749.390662ms 749.405701ms 749.461135ms 749.513651ms 749.529444ms 749.568787ms 749.688728ms 749.759726ms 749.763068ms 749.846666ms 749.892841ms 749.979152ms 749.984997ms 749.992258ms 750.028724ms 750.137998ms 750.196329ms 750.368632ms 750.376575ms 750.377969ms 750.381966ms 750.429028ms 750.680541ms 750.73656ms 750.944675ms 750.95782ms 751.016849ms 751.020948ms 751.17826ms 751.20654ms 751.217737ms 751.464867ms 751.565856ms 751.613357ms 751.616056ms 751.960944ms 751.965592ms 752.019763ms 752.113928ms 752.519254ms 752.569069ms 752.637955ms 752.709195ms 752.91865ms 752.973379ms 753.458736ms 753.742621ms 753.943111ms 755.536236ms 755.674241ms 756.212299ms 756.418862ms 757.973333ms 758.122866ms 758.290795ms 765.096036ms 769.673024ms 770.195033ms 770.943498ms 774.684328ms 782.665374ms 799.203777ms 857.539659ms 918.21723ms 960.358377ms]
Sep 11 17:39:08.621: INFO: 50 %ile: 746.821635ms
Sep 11 17:39:08.621: INFO: 90 %ile: 753.458736ms
Sep 11 17:39:08.621: INFO: 99 %ile: 918.21723ms
Sep 11 17:39:08.621: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:39:08.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-108" for this suite.

â€¢ [SLOW TEST:11.960 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":15,"skipped":204,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:39:08.656: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7668
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:39:25.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7668" for this suite.

â€¢ [SLOW TEST:16.404 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":16,"skipped":210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:39:25.062: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4213
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4213
Sep 11 17:39:27.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-4213 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 11 17:39:27.612: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep 11 17:39:27.612: INFO: stdout: "iptables"
Sep 11 17:39:27.612: INFO: proxyMode: iptables
Sep 11 17:39:27.622: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 11 17:39:27.626: INFO: Pod kube-proxy-mode-detector still exists
Sep 11 17:39:29.626: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 11 17:39:29.631: INFO: Pod kube-proxy-mode-detector still exists
Sep 11 17:39:31.626: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 11 17:39:31.631: INFO: Pod kube-proxy-mode-detector still exists
Sep 11 17:39:33.626: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 11 17:39:33.631: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-4213
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4213
I0911 17:39:33.677426      19 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4213, replica count: 3
I0911 17:39:36.727913      19 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 17:39:36.740: INFO: Creating new exec pod
Sep 11 17:39:41.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-4213 execpod-affinityf4h5t -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Sep 11 17:39:42.081: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Sep 11 17:39:42.081: INFO: stdout: ""
Sep 11 17:39:42.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-4213 execpod-affinityf4h5t -- /bin/sh -x -c nc -zv -t -w 2 10.104.165.82 80'
Sep 11 17:39:42.432: INFO: stderr: "+ nc -zv -t -w 2 10.104.165.82 80\nConnection to 10.104.165.82 80 port [tcp/http] succeeded!\n"
Sep 11 17:39:42.433: INFO: stdout: ""
Sep 11 17:39:42.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-4213 execpod-affinityf4h5t -- /bin/sh -x -c nc -zv -t -w 2 10.10.2.19 30321'
Sep 11 17:39:42.741: INFO: stderr: "+ nc -zv -t -w 2 10.10.2.19 30321\nConnection to 10.10.2.19 30321 port [tcp/30321] succeeded!\n"
Sep 11 17:39:42.741: INFO: stdout: ""
Sep 11 17:39:42.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-4213 execpod-affinityf4h5t -- /bin/sh -x -c nc -zv -t -w 2 10.10.3.166 30321'
Sep 11 17:39:43.319: INFO: stderr: "+ nc -zv -t -w 2 10.10.3.166 30321\nConnection to 10.10.3.166 30321 port [tcp/30321] succeeded!\n"
Sep 11 17:39:43.319: INFO: stdout: ""
Sep 11 17:39:43.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-4213 execpod-affinityf4h5t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.141:30321/ ; done'
Sep 11 17:39:44.742: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n"
Sep 11 17:39:44.742: INFO: stdout: "\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx\naffinity-nodeport-timeout-sfnnx"
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Received response from host: affinity-nodeport-timeout-sfnnx
Sep 11 17:39:44.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-4213 execpod-affinityf4h5t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.1.141:30321/'
Sep 11 17:39:45.154: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n"
Sep 11 17:39:45.154: INFO: stdout: "affinity-nodeport-timeout-sfnnx"
Sep 11 17:40:00.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-4213 execpod-affinityf4h5t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.1.141:30321/'
Sep 11 17:40:00.421: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.1.141:30321/\n"
Sep 11 17:40:00.421: INFO: stdout: "affinity-nodeport-timeout-jvtth"
Sep 11 17:40:00.421: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4213, will wait for the garbage collector to delete the pods
Sep 11 17:40:00.510: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 12.487707ms
Sep 11 17:40:01.611: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 1.100245275s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:40:11.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4213" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:46.054 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":17,"skipped":256,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:40:11.117: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1808
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:40:13.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1808" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":18,"skipped":283,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:40:13.368: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5371.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5371.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5371.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5371.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5371.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5371.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 11 17:40:17.597: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:17.602: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:17.607: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:17.612: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:17.626: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:17.632: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:17.637: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:17.642: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:17.652: INFO: Lookups using dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local]

Sep 11 17:40:22.658: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:22.664: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:22.670: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:22.675: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:22.694: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:22.703: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:22.708: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:22.713: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:22.723: INFO: Lookups using dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local]

Sep 11 17:40:27.660: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:27.669: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:27.675: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:28.108: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:28.129: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:28.135: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:28.139: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:28.147: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:28.158: INFO: Lookups using dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local]

Sep 11 17:40:32.657: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:32.662: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:32.667: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:32.672: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:32.685: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:32.690: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:32.694: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:32.699: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:32.708: INFO: Lookups using dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local]

Sep 11 17:40:37.657: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:37.661: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:37.666: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:37.670: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:37.683: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:37.688: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:37.692: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:37.697: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:37.706: INFO: Lookups using dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local]

Sep 11 17:40:42.659: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:42.666: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:42.672: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:42.677: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:42.691: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:42.696: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:42.702: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:42.707: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local from pod dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433: the server could not find the requested resource (get pods dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433)
Sep 11 17:40:42.716: INFO: Lookups using dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5371.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5371.svc.cluster.local jessie_udp@dns-test-service-2.dns-5371.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5371.svc.cluster.local]

Sep 11 17:40:47.714: INFO: DNS probes using dns-5371/dns-test-013c3d22-4ac6-4ad8-a43c-89bb81d55433 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:40:47.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5371" for this suite.

â€¢ [SLOW TEST:34.575 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":19,"skipped":293,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:40:47.944: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:40:48.110: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 11 17:40:53.114: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 11 17:40:53.114: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 11 17:40:55.119: INFO: Creating deployment "test-rollover-deployment"
Sep 11 17:40:55.131: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 11 17:40:57.140: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 11 17:40:57.147: INFO: Ensure that both replica sets have 1 created replica
Sep 11 17:40:57.155: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 11 17:40:57.166: INFO: Updating deployment test-rollover-deployment
Sep 11 17:40:57.166: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 11 17:40:59.175: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 11 17:40:59.184: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 11 17:40:59.202: INFO: all replica sets need to contain the pod-template-hash label
Sep 11 17:40:59.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442859, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 11 17:41:01.212: INFO: all replica sets need to contain the pod-template-hash label
Sep 11 17:41:01.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442859, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 11 17:41:03.212: INFO: all replica sets need to contain the pod-template-hash label
Sep 11 17:41:03.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442859, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 11 17:41:05.212: INFO: all replica sets need to contain the pod-template-hash label
Sep 11 17:41:05.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442859, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 11 17:41:07.215: INFO: all replica sets need to contain the pod-template-hash label
Sep 11 17:41:07.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442859, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 11 17:41:09.223: INFO: 
Sep 11 17:41:09.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442869, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735442855, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 11 17:41:11.212: INFO: 
Sep 11 17:41:11.212: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep 11 17:41:11.224: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3869 /apis/apps/v1/namespaces/deployment-3869/deployments/test-rollover-deployment b20163bd-9dd8-417e-95d6-15b2d9e7dc9c 2749598 2 2020-09-11 17:40:55 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-09-11 17:40:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-09-11 17:41:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b21d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-09-11 17:40:55 +0000 UTC,LastTransitionTime:2020-09-11 17:40:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-09-11 17:41:09 +0000 UTC,LastTransitionTime:2020-09-11 17:40:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 11 17:41:11.228: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-3869 /apis/apps/v1/namespaces/deployment-3869/replicasets/test-rollover-deployment-5797c7764 ed6cae23-bffe-435c-8520-54805d1b01a3 2749588 2 2020-09-11 17:40:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b20163bd-9dd8-417e-95d6-15b2d9e7dc9c 0xc003b19b70 0xc003b19b71}] []  [{kube-controller-manager Update apps/v1 2020-09-11 17:41:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b20163bd-9dd8-417e-95d6-15b2d9e7dc9c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b19be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 11 17:41:11.228: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 11 17:41:11.228: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3869 /apis/apps/v1/namespaces/deployment-3869/replicasets/test-rollover-controller d22009a2-9405-4c30-8cee-40fc04100627 2749597 2 2020-09-11 17:40:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b20163bd-9dd8-417e-95d6-15b2d9e7dc9c 0xc003b19a67 0xc003b19a68}] []  [{e2e.test Update apps/v1 2020-09-11 17:40:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-09-11 17:41:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b20163bd-9dd8-417e-95d6-15b2d9e7dc9c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003b19b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 11 17:41:11.229: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-3869 /apis/apps/v1/namespaces/deployment-3869/replicasets/test-rollover-deployment-78bc8b888c a672c0bc-ad1d-41ba-a8ac-6aff60d32e48 2749509 2 2020-09-11 17:40:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b20163bd-9dd8-417e-95d6-15b2d9e7dc9c 0xc003b19c57 0xc003b19c58}] []  [{kube-controller-manager Update apps/v1 2020-09-11 17:40:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b20163bd-9dd8-417e-95d6-15b2d9e7dc9c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b19ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 11 17:41:11.233: INFO: Pod "test-rollover-deployment-5797c7764-dsrn7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-dsrn7 test-rollover-deployment-5797c7764- deployment-3869 /api/v1/namespaces/deployment-3869/pods/test-rollover-deployment-5797c7764-dsrn7 ebbfaa46-e7f3-415a-93fd-c97b8418dc34 2749538 0 2020-09-11 17:40:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:192.168.123.40/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 ed6cae23-bffe-435c-8520-54805d1b01a3 0xc001d34180 0xc001d34181}] []  [{kube-controller-manager Update v1 2020-09-11 17:40:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed6cae23-bffe-435c-8520-54805d1b01a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 17:40:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 17:40:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.123.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-thnzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-thnzm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-thnzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-2-19.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:40:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:40:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:40:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:40:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.2.19,PodIP:192.168.123.40,StartTime:2020-09-11 17:40:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 17:40:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://2db6066517e72fbd9e78824ee8dd861ede56e099668da21ee522e9ae9de6b022,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.123.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:41:11.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3869" for this suite.

â€¢ [SLOW TEST:23.314 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":20,"skipped":340,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:41:11.259: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:41:11.420: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 11 17:41:16.425: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 11 17:41:16.425: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep 11 17:41:16.466: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2990 /apis/apps/v1/namespaces/deployment-2990/deployments/test-cleanup-deployment 96f5268a-3b9c-402e-8fca-f1567154f147 2749663 1 2020-09-11 17:41:16 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-09-11 17:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002372178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep 11 17:41:16.476: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-2990 /apis/apps/v1/namespaces/deployment-2990/replicasets/test-cleanup-deployment-5d446bdd47 ba591415-881e-473f-956a-9d8b09a41fc4 2749667 1 2020-09-11 17:41:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 96f5268a-3b9c-402e-8fca-f1567154f147 0xc002373387 0xc002373388}] []  [{kube-controller-manager Update apps/v1 2020-09-11 17:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96f5268a-3b9c-402e-8fca-f1567154f147\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023734d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 11 17:41:16.476: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 11 17:41:16.476: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2990 /apis/apps/v1/namespaces/deployment-2990/replicasets/test-cleanup-controller b41362b3-102f-459d-ab4b-e54f8b878ba1 2749665 1 2020-09-11 17:41:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 96f5268a-3b9c-402e-8fca-f1567154f147 0xc002373187 0xc002373188}] []  [{e2e.test Update apps/v1 2020-09-11 17:41:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-09-11 17:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"96f5268a-3b9c-402e-8fca-f1567154f147\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0023732e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 11 17:41:16.488: INFO: Pod "test-cleanup-controller-lglhw" is available:
&Pod{ObjectMeta:{test-cleanup-controller-lglhw test-cleanup-controller- deployment-2990 /api/v1/namespaces/deployment-2990/pods/test-cleanup-controller-lglhw 6035bb51-4e05-41e0-b668-b66f8e993d7f 2749640 0 2020-09-11 17:41:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:192.168.134.92/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller b41362b3-102f-459d-ab4b-e54f8b878ba1 0xc001b8b237 0xc001b8b238}] []  [{kube-controller-manager Update v1 2020-09-11 17:41:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b41362b3-102f-459d-ab4b-e54f8b878ba1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 17:41:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 17:41:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.134.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h8q6f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h8q6f,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h8q6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-1-141.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:41:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:41:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:41:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:41:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.141,PodIP:192.168.134.92,StartTime:2020-09-11 17:41:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 17:41:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f677ed976f16e7196a01fc9a339b10a3c0ba183761ed270383dfea283636457f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.134.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 17:41:16.488: INFO: Pod "test-cleanup-deployment-5d446bdd47-f7vzp" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-f7vzp test-cleanup-deployment-5d446bdd47- deployment-2990 /api/v1/namespaces/deployment-2990/pods/test-cleanup-deployment-5d446bdd47-f7vzp 1616cfb9-701a-4189-bf68-055cf929ca59 2749669 0 2020-09-11 17:41:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 ba591415-881e-473f-956a-9d8b09a41fc4 0xc001b8b417 0xc001b8b418}] []  [{kube-controller-manager Update v1 2020-09-11 17:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba591415-881e-473f-956a-9d8b09a41fc4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h8q6f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h8q6f,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h8q6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:41:16.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2990" for this suite.

â€¢ [SLOW TEST:5.263 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":21,"skipped":342,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:41:16.522: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6452
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-da5f4599-3844-45c8-875f-6249e237b217
STEP: Creating a pod to test consume secrets
Sep 11 17:41:16.698: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1a2df3c3-0a35-4653-ba17-4c717e3b5a85" in namespace "projected-6452" to be "Succeeded or Failed"
Sep 11 17:41:16.703: INFO: Pod "pod-projected-secrets-1a2df3c3-0a35-4653-ba17-4c717e3b5a85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.167693ms
Sep 11 17:41:18.708: INFO: Pod "pod-projected-secrets-1a2df3c3-0a35-4653-ba17-4c717e3b5a85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009425637s
Sep 11 17:41:20.712: INFO: Pod "pod-projected-secrets-1a2df3c3-0a35-4653-ba17-4c717e3b5a85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013981806s
STEP: Saw pod success
Sep 11 17:41:20.712: INFO: Pod "pod-projected-secrets-1a2df3c3-0a35-4653-ba17-4c717e3b5a85" satisfied condition "Succeeded or Failed"
Sep 11 17:41:20.716: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-secrets-1a2df3c3-0a35-4653-ba17-4c717e3b5a85 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 11 17:41:20.745: INFO: Waiting for pod pod-projected-secrets-1a2df3c3-0a35-4653-ba17-4c717e3b5a85 to disappear
Sep 11 17:41:20.748: INFO: Pod pod-projected-secrets-1a2df3c3-0a35-4653-ba17-4c717e3b5a85 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:41:20.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6452" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":22,"skipped":352,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:41:20.777: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:41:32.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1536" for this suite.

â€¢ [SLOW TEST:11.252 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":23,"skipped":359,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:41:32.029: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 17:41:32.195: INFO: Waiting up to 5m0s for pod "downwardapi-volume-195bb019-7ea9-4472-a15f-d14039b078c9" in namespace "downward-api-7736" to be "Succeeded or Failed"
Sep 11 17:41:32.199: INFO: Pod "downwardapi-volume-195bb019-7ea9-4472-a15f-d14039b078c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.745918ms
Sep 11 17:41:34.204: INFO: Pod "downwardapi-volume-195bb019-7ea9-4472-a15f-d14039b078c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008957763s
STEP: Saw pod success
Sep 11 17:41:34.204: INFO: Pod "downwardapi-volume-195bb019-7ea9-4472-a15f-d14039b078c9" satisfied condition "Succeeded or Failed"
Sep 11 17:41:34.208: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-195bb019-7ea9-4472-a15f-d14039b078c9 container client-container: <nil>
STEP: delete the pod
Sep 11 17:41:34.247: INFO: Waiting for pod downwardapi-volume-195bb019-7ea9-4472-a15f-d14039b078c9 to disappear
Sep 11 17:41:34.251: INFO: Pod downwardapi-volume-195bb019-7ea9-4472-a15f-d14039b078c9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:41:34.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7736" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":24,"skipped":375,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:41:34.277: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3152
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep 11 17:41:36.474: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3152 PodName:pod-sharedvolume-9a542678-18a4-43a6-877a-286c296e5848 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 17:41:36.474: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 17:41:36.629: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:41:36.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3152" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":25,"skipped":396,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:41:36.654: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:41:36.825: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 11 17:41:36.836: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 11 17:41:41.840: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 11 17:41:41.840: INFO: Creating deployment "test-rolling-update-deployment"
Sep 11 17:41:41.846: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 11 17:41:41.856: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 11 17:41:43.865: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 11 17:41:43.871: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep 11 17:41:43.884: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8723 /apis/apps/v1/namespaces/deployment-8723/deployments/test-rolling-update-deployment 4286d42a-869a-4f97-90a3-e875e5fcaeb1 2750053 1 2020-09-11 17:41:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-09-11 17:41:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-09-11 17:41:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007f1e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-09-11 17:41:41 +0000 UTC,LastTransitionTime:2020-09-11 17:41:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-09-11 17:41:43 +0000 UTC,LastTransitionTime:2020-09-11 17:41:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 11 17:41:43.888: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-8723 /apis/apps/v1/namespaces/deployment-8723/replicasets/test-rolling-update-deployment-c4cb8d6d9 62e78f09-9700-48ac-aa11-d8da7ea46296 2750042 1 2020-09-11 17:41:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 4286d42a-869a-4f97-90a3-e875e5fcaeb1 0xc000a8b8e0 0xc000a8b8e1}] []  [{kube-controller-manager Update apps/v1 2020-09-11 17:41:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4286d42a-869a-4f97-90a3-e875e5fcaeb1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a8bc88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 11 17:41:43.888: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 11 17:41:43.889: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8723 /apis/apps/v1/namespaces/deployment-8723/replicasets/test-rolling-update-controller 05c7ac1f-e3fd-49b5-84b7-7fcd29ee1562 2750052 2 2020-09-11 17:41:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 4286d42a-869a-4f97-90a3-e875e5fcaeb1 0xc000a8b587 0xc000a8b588}] []  [{e2e.test Update apps/v1 2020-09-11 17:41:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-09-11 17:41:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4286d42a-869a-4f97-90a3-e875e5fcaeb1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000a8b7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 11 17:41:43.892: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-l9pm5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-l9pm5 test-rolling-update-deployment-c4cb8d6d9- deployment-8723 /api/v1/namespaces/deployment-8723/pods/test-rolling-update-deployment-c4cb8d6d9-l9pm5 fade5364-a5bc-4038-8fd5-5c318b65e73e 2750041 0 2020-09-11 17:41:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:192.168.249.231/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 62e78f09-9700-48ac-aa11-d8da7ea46296 0xc00086e4c0 0xc00086e4c1}] []  [{kube-controller-manager Update v1 2020-09-11 17:41:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62e78f09-9700-48ac-aa11-d8da7ea46296\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 17:41:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.249.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8wcqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8wcqx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8wcqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-3-166.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:41:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:41:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:41:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:41:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.3.166,PodIP:192.168.249.231,StartTime:2020-09-11 17:41:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 17:41:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://84de14176a2d4fbb927315b0a3bde41b52dda0c032e7515d52edb67abaaefff0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.249.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:41:43.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8723" for this suite.

â€¢ [SLOW TEST:7.264 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":26,"skipped":398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:41:43.918: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8070
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 11 17:41:44.069: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 11 17:41:44.128: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 11 17:41:46.132: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 11 17:41:48.133: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:41:50.133: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:41:52.777: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:41:54.133: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:41:56.133: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:41:58.132: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 17:42:00.133: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 11 17:42:00.140: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 11 17:42:02.145: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 11 17:42:02.153: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 11 17:42:06.181: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.123.37:8080/dial?request=hostname&protocol=udp&host=192.168.134.97&port=8081&tries=1'] Namespace:pod-network-test-8070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 17:42:06.181: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 17:42:06.399: INFO: Waiting for responses: map[]
Sep 11 17:42:06.404: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.123.37:8080/dial?request=hostname&protocol=udp&host=192.168.123.48&port=8081&tries=1'] Namespace:pod-network-test-8070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 17:42:06.404: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 17:42:06.634: INFO: Waiting for responses: map[]
Sep 11 17:42:06.639: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.123.37:8080/dial?request=hostname&protocol=udp&host=192.168.249.232&port=8081&tries=1'] Namespace:pod-network-test-8070 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 17:42:06.639: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 17:42:06.797: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:42:06.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8070" for this suite.

â€¢ [SLOW TEST:22.911 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":27,"skipped":425,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:42:06.829: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 11 17:42:06.979: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 11 17:42:06.995: INFO: Waiting for terminating namespaces to be deleted...
Sep 11 17:42:07.000: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-1-141.ec2.internal before test
Sep 11 17:42:07.012: INFO: calico-node-jgr6s from kube-system started at 2020-08-31 21:04:41 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 17:42:07.012: INFO: kube-proxy-72jmn from kube-system started at 2020-08-31 21:04:41 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-cert-manager-7b65d9c9d6-jst49 from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-flux-memcached-d75b6d9d6-vpnqm from md-flux started at 2020-09-11 16:47:51 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container memcached ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-gangway-66c5d44b4c-f7psj from md-gangway started at 2020-09-11 16:47:59 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container gangway ready: true, restart count 0
Sep 11 17:42:07.012: INFO: harbor-pg-1 from md-harbor started at 2020-09-11 16:49:13 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container postgres ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-harbor-harbor-notary-server-ddbf4b74b-69jlj from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container notary-server ready: true, restart count 1
Sep 11 17:42:07.012: INFO: md-harbor-harbor-registry-9cdb888bc-97n4b from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (2 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container registry ready: true, restart count 0
Sep 11 17:42:07.012: INFO: 	Container registryctl ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-mlqtr from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-harbor-redis-redis-ha-server-2 from md-harbor started at 2020-09-11 16:49:22 +0000 UTC (2 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container redis ready: true, restart count 0
Sep 11 17:42:07.012: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-loki-stack-0 from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container loki ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-loki-stack-promtail-hm9cx from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container promtail ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-nginx-ingress-controller-dfxrw from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 17:42:07.012: INFO: alertmanager-md-prometheus-operator-alertmanager-0 from md-prometheus-operator started at 2020-09-11 16:49:13 +0000 UTC (2 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container alertmanager ready: true, restart count 0
Sep 11 17:42:07.012: INFO: 	Container config-reloader ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-prometheus-operator-operator-7d5c6f8689-tfzwv from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (2 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 11 17:42:07.012: INFO: 	Container tls-proxy ready: true, restart count 0
Sep 11 17:42:07.012: INFO: md-prometheus-operator-prometheus-node-exporter-6sz5n from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 17:42:07.012: INFO: netserver-0 from pod-network-test-8070 started at 2020-09-11 17:41:44 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container webserver ready: true, restart count 0
Sep 11 17:42:07.012: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-ps57h from sonobuoy started at 2020-09-11 17:37:25 +0000 UTC (2 container statuses recorded)
Sep 11 17:42:07.012: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 17:42:07.012: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 11 17:42:07.012: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-2-19.ec2.internal before test
Sep 11 17:42:07.024: INFO: calico-node-w9tmq from kube-system started at 2020-08-31 21:04:43 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.024: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 17:42:07.024: INFO: kube-proxy-djv8h from kube-system started at 2020-08-31 21:04:43 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.024: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 17:42:07.024: INFO: md-cert-manager-cainjector-76b464f9f7-cqsld from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.024: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-harbor-harbor-notary-signer-6789c9d4bc-wtbs5 from md-harbor started at 2020-09-11 16:48:09 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container notary-signer ready: true, restart count 1
Sep 11 17:42:07.025: INFO: md-harbor-harbor-portal-7976969b-cp7xt from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container portal ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-bgttn from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-harbor-redis-redis-ha-server-0 from md-harbor started at 2020-09-11 16:48:07 +0000 UTC (2 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container redis ready: true, restart count 0
Sep 11 17:42:07.025: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-helm-operator-557b9b587c-gdvck from md-helm-operator started at 2020-09-11 16:30:29 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container flux-helm-operator ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-kubedb-84d9fff66c-77kr5 from md-kubedb started at 2020-09-11 16:31:16 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container operator ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-loki-stack-promtail-fkrtb from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container promtail ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-nginx-ingress-controller-zdxvh from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-nginx-ingress-default-backend-74c98f4684-clcml from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-oauth2-proxy-5c876db75d-7m69g from md-oauth2-proxy started at 2020-09-11 16:54:02 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container oauth2-proxy ready: true, restart count 6
Sep 11 17:42:07.025: INFO: md-prometheus-operator-kube-state-metrics-6f5d54b49-nsw4z from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-prometheus-operator-prometheus-node-exporter-pvvtd from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 17:42:07.025: INFO: prometheus-md-prometheus-operator-prometheus-0 from md-prometheus-operator started at 2020-09-11 16:49:23 +0000 UTC (3 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container prometheus ready: true, restart count 1
Sep 11 17:42:07.025: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 11 17:42:07.025: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 11 17:42:07.025: INFO: md-velero-6cc8bd66b5-vltqx from md-velero started at 2020-09-11 16:48:30 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container velero ready: true, restart count 0
Sep 11 17:42:07.025: INFO: netserver-1 from pod-network-test-8070 started at 2020-09-11 17:41:44 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container webserver ready: true, restart count 0
Sep 11 17:42:07.025: INFO: test-container-pod from pod-network-test-8070 started at 2020-09-11 17:42:02 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container webserver ready: true, restart count 0
Sep 11 17:42:07.025: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-jnktw from sonobuoy started at 2020-09-11 17:37:24 +0000 UTC (2 container statuses recorded)
Sep 11 17:42:07.025: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 17:42:07.025: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 11 17:42:07.025: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-3-166.ec2.internal before test
Sep 11 17:42:07.040: INFO: calico-node-wfxpk from kube-system started at 2020-08-31 21:04:40 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 17:42:07.040: INFO: kube-proxy-9vh4x from kube-system started at 2020-08-31 21:04:40 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 17:42:07.040: INFO: sealed-secrets-controller-7dfd7dcc6b-g8r5f from kube-system started at 2020-09-11 16:30:49 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container sealed-secrets-controller ready: true, restart count 0
Sep 11 17:42:07.040: INFO: md-cert-manager-webhook-656744686b-gch9j from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 17:42:07.040: INFO: md-dex-5b4f679bd4-dmmrq from md-dex started at 2020-09-11 16:47:54 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container main ready: true, restart count 1
Sep 11 17:42:07.040: INFO: md-flux-5465b59cbd-dd4fp from md-flux started at 2020-09-11 16:47:51 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container flux ready: true, restart count 0
Sep 11 17:42:07.040: INFO: harbor-pg-0 from md-harbor started at 2020-09-11 16:48:15 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container postgres ready: true, restart count 0
Sep 11 17:42:07.040: INFO: md-harbor-harbor-chartmuseum-55b4567ff8-cx2ch from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container chartmuseum ready: true, restart count 0
Sep 11 17:42:07.040: INFO: md-harbor-harbor-clair-6479d48569-vd6wm from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (2 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container adapter ready: true, restart count 0
Sep 11 17:42:07.040: INFO: 	Container clair ready: true, restart count 1
Sep 11 17:42:07.040: INFO: md-harbor-harbor-core-7999f49dc-vpk6g from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container core ready: true, restart count 0
Sep 11 17:42:07.040: INFO: md-harbor-harbor-jobservice-556554c4c7-gqzhp from md-harbor started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container jobservice ready: true, restart count 0
Sep 11 17:42:07.040: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-tdbqd from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 17:42:07.040: INFO: md-harbor-redis-redis-ha-server-1 from md-harbor started at 2020-09-11 16:48:53 +0000 UTC (2 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container redis ready: true, restart count 0
Sep 11 17:42:07.040: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 17:42:07.040: INFO: md-loki-stack-promtail-94946 from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container promtail ready: true, restart count 0
Sep 11 17:42:07.040: INFO: md-nginx-ingress-controller-lhjv9 from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 17:42:07.040: INFO: md-prometheus-operator-prometheus-node-exporter-mjvrq from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 17:42:07.040: INFO: netserver-2 from pod-network-test-8070 started at 2020-09-11 17:41:44 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container webserver ready: true, restart count 0
Sep 11 17:42:07.040: INFO: sonobuoy from sonobuoy started at 2020-09-11 17:37:23 +0000 UTC (1 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 11 17:42:07.040: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-lb975 from sonobuoy started at 2020-09-11 17:37:25 +0000 UTC (2 container statuses recorded)
Sep 11 17:42:07.040: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 17:42:07.040: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-401ae9b8-f289-4865-9b94-cafefc02cada 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-401ae9b8-f289-4865-9b94-cafefc02cada off the node ip-10-10-1-141.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-401ae9b8-f289-4865-9b94-cafefc02cada
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:42:13.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6897" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:6.357 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":28,"skipped":445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:42:13.187: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9805
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4268
STEP: Creating secret with name secret-test-2a0d656e-e188-4a2e-8ae4-3aad9975f2c3
STEP: Creating a pod to test consume secrets
Sep 11 17:42:13.515: INFO: Waiting up to 5m0s for pod "pod-secrets-cd96ec31-1b6b-4494-844e-9fd71c61bac5" in namespace "secrets-9805" to be "Succeeded or Failed"
Sep 11 17:42:13.519: INFO: Pod "pod-secrets-cd96ec31-1b6b-4494-844e-9fd71c61bac5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.682384ms
Sep 11 17:42:15.523: INFO: Pod "pod-secrets-cd96ec31-1b6b-4494-844e-9fd71c61bac5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007808651s
Sep 11 17:42:17.528: INFO: Pod "pod-secrets-cd96ec31-1b6b-4494-844e-9fd71c61bac5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012239004s
STEP: Saw pod success
Sep 11 17:42:17.528: INFO: Pod "pod-secrets-cd96ec31-1b6b-4494-844e-9fd71c61bac5" satisfied condition "Succeeded or Failed"
Sep 11 17:42:17.531: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-secrets-cd96ec31-1b6b-4494-844e-9fd71c61bac5 container secret-volume-test: <nil>
STEP: delete the pod
Sep 11 17:42:17.562: INFO: Waiting for pod pod-secrets-cd96ec31-1b6b-4494-844e-9fd71c61bac5 to disappear
Sep 11 17:42:17.566: INFO: Pod pod-secrets-cd96ec31-1b6b-4494-844e-9fd71c61bac5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:42:17.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9805" for this suite.
STEP: Destroying namespace "secret-namespace-4268" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":29,"skipped":478,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:42:17.610: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-330e6565-60a2-40e4-bbff-62ae9730a2ba
STEP: Creating a pod to test consume secrets
Sep 11 17:42:17.799: INFO: Waiting up to 5m0s for pod "pod-secrets-37523869-7681-497b-9892-184647b8f707" in namespace "secrets-3997" to be "Succeeded or Failed"
Sep 11 17:42:17.832: INFO: Pod "pod-secrets-37523869-7681-497b-9892-184647b8f707": Phase="Pending", Reason="", readiness=false. Elapsed: 32.989665ms
Sep 11 17:42:19.837: INFO: Pod "pod-secrets-37523869-7681-497b-9892-184647b8f707": Phase="Running", Reason="", readiness=true. Elapsed: 2.037955431s
Sep 11 17:42:21.843: INFO: Pod "pod-secrets-37523869-7681-497b-9892-184647b8f707": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044666015s
STEP: Saw pod success
Sep 11 17:42:21.843: INFO: Pod "pod-secrets-37523869-7681-497b-9892-184647b8f707" satisfied condition "Succeeded or Failed"
Sep 11 17:42:21.849: INFO: Trying to get logs from node ip-10-10-3-166.ec2.internal pod pod-secrets-37523869-7681-497b-9892-184647b8f707 container secret-volume-test: <nil>
STEP: delete the pod
Sep 11 17:42:21.889: INFO: Waiting for pod pod-secrets-37523869-7681-497b-9892-184647b8f707 to disappear
Sep 11 17:42:21.894: INFO: Pod pod-secrets-37523869-7681-497b-9892-184647b8f707 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:42:21.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3997" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":30,"skipped":494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:42:21.920: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8807
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-416c5bd5-def2-4d2a-b056-b50b8232e06e
STEP: Creating a pod to test consume configMaps
Sep 11 17:42:22.103: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-030500ad-7eab-48c7-96ed-b9344a97e97e" in namespace "projected-8807" to be "Succeeded or Failed"
Sep 11 17:42:22.114: INFO: Pod "pod-projected-configmaps-030500ad-7eab-48c7-96ed-b9344a97e97e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.995911ms
Sep 11 17:42:24.119: INFO: Pod "pod-projected-configmaps-030500ad-7eab-48c7-96ed-b9344a97e97e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015237109s
STEP: Saw pod success
Sep 11 17:42:24.119: INFO: Pod "pod-projected-configmaps-030500ad-7eab-48c7-96ed-b9344a97e97e" satisfied condition "Succeeded or Failed"
Sep 11 17:42:24.124: INFO: Trying to get logs from node ip-10-10-1-141.ec2.internal pod pod-projected-configmaps-030500ad-7eab-48c7-96ed-b9344a97e97e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 17:42:24.169: INFO: Waiting for pod pod-projected-configmaps-030500ad-7eab-48c7-96ed-b9344a97e97e to disappear
Sep 11 17:42:24.176: INFO: Pod pod-projected-configmaps-030500ad-7eab-48c7-96ed-b9344a97e97e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:42:24.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8807" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":31,"skipped":516,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:42:24.202: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8317 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8317;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8317 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8317;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8317.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8317.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8317.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8317.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8317.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8317.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8317.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8317.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8317.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8317.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8317.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 162.199.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.199.162_udp@PTR;check="$$(dig +tcp +noall +answer +search 162.199.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.199.162_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8317 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8317;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8317 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8317;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8317.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8317.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8317.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8317.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8317.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8317.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8317.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8317.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8317.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8317.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8317.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8317.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 162.199.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.199.162_udp@PTR;check="$$(dig +tcp +noall +answer +search 162.199.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.199.162_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 11 17:42:28.617: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.622: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.628: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.632: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.637: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.642: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.646: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.652: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.726: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.731: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.738: INFO: Unable to read jessie_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.743: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.748: INFO: Unable to read jessie_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.753: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.758: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.765: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:28.800: INFO: Lookups using dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8317 wheezy_tcp@dns-test-service.dns-8317 wheezy_udp@dns-test-service.dns-8317.svc wheezy_tcp@dns-test-service.dns-8317.svc wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8317 jessie_tcp@dns-test-service.dns-8317 jessie_udp@dns-test-service.dns-8317.svc jessie_tcp@dns-test-service.dns-8317.svc jessie_udp@_http._tcp.dns-test-service.dns-8317.svc jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc]

Sep 11 17:42:33.806: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.810: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.815: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.825: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.829: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.834: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.839: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.873: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.879: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.883: INFO: Unable to read jessie_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.888: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.893: INFO: Unable to read jessie_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.902: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.911: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.917: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:33.947: INFO: Lookups using dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8317 wheezy_tcp@dns-test-service.dns-8317 wheezy_udp@dns-test-service.dns-8317.svc wheezy_tcp@dns-test-service.dns-8317.svc wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8317 jessie_tcp@dns-test-service.dns-8317 jessie_udp@dns-test-service.dns-8317.svc jessie_tcp@dns-test-service.dns-8317.svc jessie_udp@_http._tcp.dns-test-service.dns-8317.svc jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc]

Sep 11 17:42:38.813: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.818: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.836: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.859: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.865: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.899: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.905: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.915: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.978: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.983: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.989: INFO: Unable to read jessie_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:38.996: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:39.001: INFO: Unable to read jessie_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:39.006: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:39.013: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:39.041: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:39.114: INFO: Lookups using dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8317 wheezy_tcp@dns-test-service.dns-8317 wheezy_udp@dns-test-service.dns-8317.svc wheezy_tcp@dns-test-service.dns-8317.svc wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8317 jessie_tcp@dns-test-service.dns-8317 jessie_udp@dns-test-service.dns-8317.svc jessie_tcp@dns-test-service.dns-8317.svc jessie_udp@_http._tcp.dns-test-service.dns-8317.svc jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc]

Sep 11 17:42:43.805: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:43.820: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:43.825: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:43.830: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:43.835: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:43.841: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:43.851: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:43.866: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:43.966: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:43.983: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:43.992: INFO: Unable to read jessie_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:44.006: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:44.020: INFO: Unable to read jessie_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:44.033: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:44.038: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:44.045: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:44.110: INFO: Lookups using dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8317 wheezy_tcp@dns-test-service.dns-8317 wheezy_udp@dns-test-service.dns-8317.svc wheezy_tcp@dns-test-service.dns-8317.svc wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8317 jessie_tcp@dns-test-service.dns-8317 jessie_udp@dns-test-service.dns-8317.svc jessie_tcp@dns-test-service.dns-8317.svc jessie_udp@_http._tcp.dns-test-service.dns-8317.svc jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc]

Sep 11 17:42:48.807: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.812: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.817: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.821: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.826: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.831: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.835: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.840: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.872: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.877: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.882: INFO: Unable to read jessie_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.886: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.891: INFO: Unable to read jessie_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.895: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.900: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.907: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:48.936: INFO: Lookups using dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8317 wheezy_tcp@dns-test-service.dns-8317 wheezy_udp@dns-test-service.dns-8317.svc wheezy_tcp@dns-test-service.dns-8317.svc wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8317 jessie_tcp@dns-test-service.dns-8317 jessie_udp@dns-test-service.dns-8317.svc jessie_tcp@dns-test-service.dns-8317.svc jessie_udp@_http._tcp.dns-test-service.dns-8317.svc jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc]

Sep 11 17:42:53.806: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.810: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.815: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.826: INFO: Unable to read wheezy_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.830: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.838: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.860: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.971: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.978: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.983: INFO: Unable to read jessie_udp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:53.994: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317 from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:54.027: INFO: Unable to read jessie_udp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:54.096: INFO: Unable to read jessie_tcp@dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:54.113: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:54.134: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc from pod dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e: the server could not find the requested resource (get pods dns-test-c6e0aa98-d97c-4029-9401-6354757f580e)
Sep 11 17:42:54.266: INFO: Lookups using dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8317 wheezy_tcp@dns-test-service.dns-8317 wheezy_udp@dns-test-service.dns-8317.svc wheezy_tcp@dns-test-service.dns-8317.svc wheezy_udp@_http._tcp.dns-test-service.dns-8317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8317 jessie_tcp@dns-test-service.dns-8317 jessie_udp@dns-test-service.dns-8317.svc jessie_tcp@dns-test-service.dns-8317.svc jessie_udp@_http._tcp.dns-test-service.dns-8317.svc jessie_tcp@_http._tcp.dns-test-service.dns-8317.svc]

Sep 11 17:42:58.948: INFO: DNS probes using dns-8317/dns-test-c6e0aa98-d97c-4029-9401-6354757f580e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:42:59.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8317" for this suite.

â€¢ [SLOW TEST:34.947 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":32,"skipped":536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:42:59.150: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1477
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1477
STEP: creating replication controller externalsvc in namespace services-1477
I0911 17:42:59.379716      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1477, replica count: 2
I0911 17:43:02.430168      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 11 17:43:02.462: INFO: Creating new exec pod
Sep 11 17:43:04.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1477 execpodfbgv9 -- /bin/sh -x -c nslookup clusterip-service.services-1477.svc.cluster.local'
Sep 11 17:43:04.782: INFO: stderr: "+ nslookup clusterip-service.services-1477.svc.cluster.local\n"
Sep 11 17:43:04.782: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-1477.svc.cluster.local\tcanonical name = externalsvc.services-1477.svc.cluster.local.\nName:\texternalsvc.services-1477.svc.cluster.local\nAddress: 10.107.219.22\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1477, will wait for the garbage collector to delete the pods
Sep 11 17:43:04.852: INFO: Deleting ReplicationController externalsvc took: 14.349346ms
Sep 11 17:43:04.952: INFO: Terminating ReplicationController externalsvc pods took: 100.17209ms
Sep 11 17:43:10.905: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:10.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1477" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:11.836 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":33,"skipped":582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:10.986: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep 11 17:43:11.157: INFO: Waiting up to 5m0s for pod "downward-api-8bc2076d-b6ee-46c9-8da3-902b849bb871" in namespace "downward-api-6782" to be "Succeeded or Failed"
Sep 11 17:43:11.161: INFO: Pod "downward-api-8bc2076d-b6ee-46c9-8da3-902b849bb871": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131121ms
Sep 11 17:43:13.166: INFO: Pod "downward-api-8bc2076d-b6ee-46c9-8da3-902b849bb871": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009254396s
STEP: Saw pod success
Sep 11 17:43:13.166: INFO: Pod "downward-api-8bc2076d-b6ee-46c9-8da3-902b849bb871" satisfied condition "Succeeded or Failed"
Sep 11 17:43:13.170: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downward-api-8bc2076d-b6ee-46c9-8da3-902b849bb871 container dapi-container: <nil>
STEP: delete the pod
Sep 11 17:43:13.208: INFO: Waiting for pod downward-api-8bc2076d-b6ee-46c9-8da3-902b849bb871 to disappear
Sep 11 17:43:13.212: INFO: Pod downward-api-8bc2076d-b6ee-46c9-8da3-902b849bb871 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:13.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6782" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":34,"skipped":605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:13.238: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-72e84c66-9639-438e-9da6-39e57b00ed1c
STEP: Creating a pod to test consume configMaps
Sep 11 17:43:13.408: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a4fa20b-66ea-4dde-a121-62db1e029c19" in namespace "projected-8769" to be "Succeeded or Failed"
Sep 11 17:43:13.412: INFO: Pod "pod-projected-configmaps-9a4fa20b-66ea-4dde-a121-62db1e029c19": Phase="Pending", Reason="", readiness=false. Elapsed: 3.933335ms
Sep 11 17:43:15.420: INFO: Pod "pod-projected-configmaps-9a4fa20b-66ea-4dde-a121-62db1e029c19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011927773s
STEP: Saw pod success
Sep 11 17:43:15.420: INFO: Pod "pod-projected-configmaps-9a4fa20b-66ea-4dde-a121-62db1e029c19" satisfied condition "Succeeded or Failed"
Sep 11 17:43:15.424: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-configmaps-9a4fa20b-66ea-4dde-a121-62db1e029c19 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 17:43:15.651: INFO: Waiting for pod pod-projected-configmaps-9a4fa20b-66ea-4dde-a121-62db1e029c19 to disappear
Sep 11 17:43:15.673: INFO: Pod pod-projected-configmaps-9a4fa20b-66ea-4dde-a121-62db1e029c19 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:15.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8769" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":35,"skipped":628,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:15.713: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3451
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-d6a1237d-a17e-4167-b582-a8a1b9238970
STEP: Creating configMap with name cm-test-opt-upd-9fbbd594-1726-49fd-8dfa-fbf26be11912
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d6a1237d-a17e-4167-b582-a8a1b9238970
STEP: Updating configmap cm-test-opt-upd-9fbbd594-1726-49fd-8dfa-fbf26be11912
STEP: Creating configMap with name cm-test-opt-create-c611ca81-a943-4add-a07b-b348d5df3026
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:20.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3451" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":36,"skipped":634,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:20.087: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:20.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7123" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":37,"skipped":651,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:20.363: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Sep 11 17:43:20.526: INFO: Waiting up to 5m0s for pod "client-containers-a6927dee-1c3a-4cd3-89ab-66fbc02a0691" in namespace "containers-5553" to be "Succeeded or Failed"
Sep 11 17:43:20.533: INFO: Pod "client-containers-a6927dee-1c3a-4cd3-89ab-66fbc02a0691": Phase="Pending", Reason="", readiness=false. Elapsed: 7.392655ms
Sep 11 17:43:22.538: INFO: Pod "client-containers-a6927dee-1c3a-4cd3-89ab-66fbc02a0691": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011908023s
STEP: Saw pod success
Sep 11 17:43:22.538: INFO: Pod "client-containers-a6927dee-1c3a-4cd3-89ab-66fbc02a0691" satisfied condition "Succeeded or Failed"
Sep 11 17:43:22.542: INFO: Trying to get logs from node ip-10-10-3-166.ec2.internal pod client-containers-a6927dee-1c3a-4cd3-89ab-66fbc02a0691 container test-container: <nil>
STEP: delete the pod
Sep 11 17:43:22.571: INFO: Waiting for pod client-containers-a6927dee-1c3a-4cd3-89ab-66fbc02a0691 to disappear
Sep 11 17:43:22.575: INFO: Pod client-containers-a6927dee-1c3a-4cd3-89ab-66fbc02a0691 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:22.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5553" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":38,"skipped":672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:22.605: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2931.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2931.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2931.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2931.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2931.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2931.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 11 17:43:32.853: INFO: DNS probes using dns-2931/dns-test-29b8b95f-5a4f-4f4a-a2fe-1b2d2c96c865 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:32.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2931" for this suite.

â€¢ [SLOW TEST:10.293 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":39,"skipped":697,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:32.898: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-58de50fa-e2b1-42fb-b007-1ea80e594e7e
STEP: Creating a pod to test consume secrets
Sep 11 17:43:33.071: INFO: Waiting up to 5m0s for pod "pod-secrets-aab0dcf5-401d-4176-a3e7-fae1f40e0651" in namespace "secrets-9861" to be "Succeeded or Failed"
Sep 11 17:43:33.076: INFO: Pod "pod-secrets-aab0dcf5-401d-4176-a3e7-fae1f40e0651": Phase="Pending", Reason="", readiness=false. Elapsed: 4.396207ms
Sep 11 17:43:35.081: INFO: Pod "pod-secrets-aab0dcf5-401d-4176-a3e7-fae1f40e0651": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009545448s
Sep 11 17:43:37.086: INFO: Pod "pod-secrets-aab0dcf5-401d-4176-a3e7-fae1f40e0651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014628187s
STEP: Saw pod success
Sep 11 17:43:37.086: INFO: Pod "pod-secrets-aab0dcf5-401d-4176-a3e7-fae1f40e0651" satisfied condition "Succeeded or Failed"
Sep 11 17:43:37.090: INFO: Trying to get logs from node ip-10-10-1-141.ec2.internal pod pod-secrets-aab0dcf5-401d-4176-a3e7-fae1f40e0651 container secret-volume-test: <nil>
STEP: delete the pod
Sep 11 17:43:37.124: INFO: Waiting for pod pod-secrets-aab0dcf5-401d-4176-a3e7-fae1f40e0651 to disappear
Sep 11 17:43:37.129: INFO: Pod pod-secrets-aab0dcf5-401d-4176-a3e7-fae1f40e0651 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:37.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9861" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":40,"skipped":717,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:37.155: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-2875
STEP: creating replication controller nodeport-test in namespace services-2875
I0911 17:43:37.374411      19 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-2875, replica count: 2
Sep 11 17:43:40.425: INFO: Creating new exec pod
I0911 17:43:40.425731      19 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 17:43:45.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-2875 execpod2v62p -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep 11 17:43:45.715: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 11 17:43:45.715: INFO: stdout: ""
Sep 11 17:43:45.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-2875 execpod2v62p -- /bin/sh -x -c nc -zv -t -w 2 10.96.40.159 80'
Sep 11 17:43:45.954: INFO: stderr: "+ nc -zv -t -w 2 10.96.40.159 80\nConnection to 10.96.40.159 80 port [tcp/http] succeeded!\n"
Sep 11 17:43:45.954: INFO: stdout: ""
Sep 11 17:43:45.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-2875 execpod2v62p -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.141 32047'
Sep 11 17:43:46.235: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.141 32047\nConnection to 10.10.1.141 32047 port [tcp/32047] succeeded!\n"
Sep 11 17:43:46.235: INFO: stdout: ""
Sep 11 17:43:46.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-2875 execpod2v62p -- /bin/sh -x -c nc -zv -t -w 2 10.10.2.19 32047'
Sep 11 17:43:46.476: INFO: stderr: "+ nc -zv -t -w 2 10.10.2.19 32047\nConnection to 10.10.2.19 32047 port [tcp/32047] succeeded!\n"
Sep 11 17:43:46.476: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:46.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2875" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:9.349 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":41,"skipped":728,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:46.504: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8913
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-897c9214-a051-481a-b741-ff114cd59be4
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-897c9214-a051-481a-b741-ff114cd59be4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:50.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8913" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":42,"skipped":742,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:50.807: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-938052fe-47c7-4b5a-9715-c2966e42f2d6
STEP: Creating a pod to test consume configMaps
Sep 11 17:43:50.979: INFO: Waiting up to 5m0s for pod "pod-configmaps-943a7850-5f34-4a16-af40-9e8584b05c2f" in namespace "configmap-392" to be "Succeeded or Failed"
Sep 11 17:43:50.983: INFO: Pod "pod-configmaps-943a7850-5f34-4a16-af40-9e8584b05c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707101ms
Sep 11 17:43:52.987: INFO: Pod "pod-configmaps-943a7850-5f34-4a16-af40-9e8584b05c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007960573s
STEP: Saw pod success
Sep 11 17:43:52.987: INFO: Pod "pod-configmaps-943a7850-5f34-4a16-af40-9e8584b05c2f" satisfied condition "Succeeded or Failed"
Sep 11 17:43:52.992: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-configmaps-943a7850-5f34-4a16-af40-9e8584b05c2f container configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 17:43:53.026: INFO: Waiting for pod pod-configmaps-943a7850-5f34-4a16-af40-9e8584b05c2f to disappear
Sep 11 17:43:53.030: INFO: Pod pod-configmaps-943a7850-5f34-4a16-af40-9e8584b05c2f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:43:53.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-392" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":43,"skipped":744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:43:53.056: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Sep 11 17:43:53.257: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 11 17:44:53.315: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Sep 11 17:44:53.342: INFO: Created pod: pod0-sched-preemption-low-priority
Sep 11 17:44:53.360: INFO: Created pod: pod1-sched-preemption-medium-priority
Sep 11 17:44:53.400: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:45:01.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4440" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

â€¢ [SLOW TEST:68.518 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":44,"skipped":830,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:45:01.574: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Sep 11 17:45:01.755: INFO: Waiting up to 5m0s for pod "client-containers-ef00d656-2cab-4052-83e8-0d202d69b326" in namespace "containers-3602" to be "Succeeded or Failed"
Sep 11 17:45:01.771: INFO: Pod "client-containers-ef00d656-2cab-4052-83e8-0d202d69b326": Phase="Pending", Reason="", readiness=false. Elapsed: 15.727032ms
Sep 11 17:45:03.776: INFO: Pod "client-containers-ef00d656-2cab-4052-83e8-0d202d69b326": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021026396s
STEP: Saw pod success
Sep 11 17:45:03.776: INFO: Pod "client-containers-ef00d656-2cab-4052-83e8-0d202d69b326" satisfied condition "Succeeded or Failed"
Sep 11 17:45:03.780: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod client-containers-ef00d656-2cab-4052-83e8-0d202d69b326 container test-container: <nil>
STEP: delete the pod
Sep 11 17:45:03.816: INFO: Waiting for pod client-containers-ef00d656-2cab-4052-83e8-0d202d69b326 to disappear
Sep 11 17:45:03.821: INFO: Pod client-containers-ef00d656-2cab-4052-83e8-0d202d69b326 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:45:03.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3602" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":45,"skipped":836,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:45:03.848: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-d7c182c0-6d13-468c-8a28-1aa3dcfdbe87 in namespace container-probe-6903
Sep 11 17:45:06.038: INFO: Started pod busybox-d7c182c0-6d13-468c-8a28-1aa3dcfdbe87 in namespace container-probe-6903
STEP: checking the pod's current state and verifying that restartCount is present
Sep 11 17:45:06.042: INFO: Initial restart count of pod busybox-d7c182c0-6d13-468c-8a28-1aa3dcfdbe87 is 0
Sep 11 17:46:00.185: INFO: Restart count of pod container-probe-6903/busybox-d7c182c0-6d13-468c-8a28-1aa3dcfdbe87 is now 1 (54.143274417s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:46:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6903" for this suite.

â€¢ [SLOW TEST:56.397 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":46,"skipped":840,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:46:00.246: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3150
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:46:00.398: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:46:14.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3150" for this suite.

â€¢ [SLOW TEST:17.997 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":47,"skipped":843,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:46:18.244: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Sep 11 17:46:18.551: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 11 17:47:18.615: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Sep 11 17:47:18.653: INFO: Created pod: pod0-sched-preemption-low-priority
Sep 11 17:47:18.696: INFO: Created pod: pod1-sched-preemption-medium-priority
Sep 11 17:47:18.737: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:47:44.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3359" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

â€¢ [SLOW TEST:86.716 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":48,"skipped":865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:47:44.960: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8784
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8784
STEP: creating replication controller externalsvc in namespace services-8784
I0911 17:47:45.396472      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8784, replica count: 2
I0911 17:47:48.446816      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 11 17:47:48.488: INFO: Creating new exec pod
Sep 11 17:47:50.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-8784 execpodddcl9 -- /bin/sh -x -c nslookup nodeport-service.services-8784.svc.cluster.local'
Sep 11 17:47:51.004: INFO: stderr: "+ nslookup nodeport-service.services-8784.svc.cluster.local\n"
Sep 11 17:47:51.004: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8784.svc.cluster.local\tcanonical name = externalsvc.services-8784.svc.cluster.local.\nName:\texternalsvc.services-8784.svc.cluster.local\nAddress: 10.103.78.217\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8784, will wait for the garbage collector to delete the pods
Sep 11 17:47:51.085: INFO: Deleting ReplicationController externalsvc took: 25.673416ms
Sep 11 17:47:52.185: INFO: Terminating ReplicationController externalsvc pods took: 1.100472982s
Sep 11 17:48:01.170: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:48:01.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8784" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:16.268 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":49,"skipped":915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:48:01.229: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:48:01.412: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-346f19a9-0cfb-4f35-8bc2-892a417c3699" in namespace "security-context-test-7153" to be "Succeeded or Failed"
Sep 11 17:48:01.417: INFO: Pod "alpine-nnp-false-346f19a9-0cfb-4f35-8bc2-892a417c3699": Phase="Pending", Reason="", readiness=false. Elapsed: 4.708758ms
Sep 11 17:48:03.422: INFO: Pod "alpine-nnp-false-346f19a9-0cfb-4f35-8bc2-892a417c3699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00991761s
Sep 11 17:48:05.427: INFO: Pod "alpine-nnp-false-346f19a9-0cfb-4f35-8bc2-892a417c3699": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014996844s
Sep 11 17:48:05.427: INFO: Pod "alpine-nnp-false-346f19a9-0cfb-4f35-8bc2-892a417c3699" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:48:05.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7153" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":50,"skipped":965,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:48:05.477: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:48:05.678: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 11 17:48:05.688: INFO: Number of nodes with available pods: 0
Sep 11 17:48:05.688: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 11 17:48:05.715: INFO: Number of nodes with available pods: 0
Sep 11 17:48:05.715: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 17:48:06.720: INFO: Number of nodes with available pods: 0
Sep 11 17:48:06.720: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 17:48:07.720: INFO: Number of nodes with available pods: 0
Sep 11 17:48:07.720: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 17:48:08.722: INFO: Number of nodes with available pods: 0
Sep 11 17:48:08.722: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 17:48:09.720: INFO: Number of nodes with available pods: 1
Sep 11 17:48:09.720: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 11 17:48:09.743: INFO: Number of nodes with available pods: 1
Sep 11 17:48:09.743: INFO: Number of running nodes: 0, number of available pods: 1
Sep 11 17:48:10.749: INFO: Number of nodes with available pods: 0
Sep 11 17:48:10.749: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 11 17:48:10.761: INFO: Number of nodes with available pods: 0
Sep 11 17:48:10.761: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 17:48:11.767: INFO: Number of nodes with available pods: 0
Sep 11 17:48:11.767: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 17:48:12.766: INFO: Number of nodes with available pods: 0
Sep 11 17:48:12.766: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 17:48:13.771: INFO: Number of nodes with available pods: 0
Sep 11 17:48:13.771: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 17:48:14.766: INFO: Number of nodes with available pods: 0
Sep 11 17:48:14.766: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 17:48:15.766: INFO: Number of nodes with available pods: 1
Sep 11 17:48:15.766: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-732, will wait for the garbage collector to delete the pods
Sep 11 17:48:15.845: INFO: Deleting DaemonSet.extensions daemon-set took: 14.324913ms
Sep 11 17:48:15.945: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.24183ms
Sep 11 17:48:19.762: INFO: Number of nodes with available pods: 0
Sep 11 17:48:19.762: INFO: Number of running nodes: 0, number of available pods: 0
Sep 11 17:48:19.769: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-732/daemonsets","resourceVersion":"2753503"},"items":null}

Sep 11 17:48:19.773: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-732/pods","resourceVersion":"2753503"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:48:19.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-732" for this suite.

â€¢ [SLOW TEST:14.355 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":51,"skipped":972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:48:19.833: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-6154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep 11 17:48:19.997: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep 11 17:48:20.011: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 11 17:48:20.011: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep 11 17:48:20.027: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 11 17:48:20.027: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep 11 17:48:20.045: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep 11 17:48:20.045: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep 11 17:48:27.093: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:48:27.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6154" for this suite.

â€¢ [SLOW TEST:7.302 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":52,"skipped":995,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:48:27.135: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6727
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 17:48:27.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27ac6746-a1dd-4eec-9e32-57bd98c85d7e" in namespace "projected-6727" to be "Succeeded or Failed"
Sep 11 17:48:27.310: INFO: Pod "downwardapi-volume-27ac6746-a1dd-4eec-9e32-57bd98c85d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.111873ms
Sep 11 17:48:29.314: INFO: Pod "downwardapi-volume-27ac6746-a1dd-4eec-9e32-57bd98c85d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009510469s
STEP: Saw pod success
Sep 11 17:48:29.314: INFO: Pod "downwardapi-volume-27ac6746-a1dd-4eec-9e32-57bd98c85d7e" satisfied condition "Succeeded or Failed"
Sep 11 17:48:29.318: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-27ac6746-a1dd-4eec-9e32-57bd98c85d7e container client-container: <nil>
STEP: delete the pod
Sep 11 17:48:29.358: INFO: Waiting for pod downwardapi-volume-27ac6746-a1dd-4eec-9e32-57bd98c85d7e to disappear
Sep 11 17:48:29.362: INFO: Pod downwardapi-volume-27ac6746-a1dd-4eec-9e32-57bd98c85d7e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:48:29.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6727" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":53,"skipped":996,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:48:29.392: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7746
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 11 17:48:29.602: INFO: Waiting up to 5m0s for pod "pod-f641a5a4-bf40-4dcc-9893-823bfad08dc8" in namespace "emptydir-7746" to be "Succeeded or Failed"
Sep 11 17:48:29.608: INFO: Pod "pod-f641a5a4-bf40-4dcc-9893-823bfad08dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.639945ms
Sep 11 17:48:31.612: INFO: Pod "pod-f641a5a4-bf40-4dcc-9893-823bfad08dc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010450493s
STEP: Saw pod success
Sep 11 17:48:31.612: INFO: Pod "pod-f641a5a4-bf40-4dcc-9893-823bfad08dc8" satisfied condition "Succeeded or Failed"
Sep 11 17:48:31.616: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-f641a5a4-bf40-4dcc-9893-823bfad08dc8 container test-container: <nil>
STEP: delete the pod
Sep 11 17:48:31.647: INFO: Waiting for pod pod-f641a5a4-bf40-4dcc-9893-823bfad08dc8 to disappear
Sep 11 17:48:31.651: INFO: Pod pod-f641a5a4-bf40-4dcc-9893-823bfad08dc8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:48:31.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7746" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":54,"skipped":1000,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:48:31.677: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 11 17:48:31.853: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-a 2e7ed595-3aca-49e9-a771-942078d542d0 2753663 0 2020-09-11 17:48:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-11 17:48:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 17:48:31.853: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-a 2e7ed595-3aca-49e9-a771-942078d542d0 2753663 0 2020-09-11 17:48:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-11 17:48:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 11 17:48:41.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-a 2e7ed595-3aca-49e9-a771-942078d542d0 2753760 0 2020-09-11 17:48:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-11 17:48:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 17:48:41.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-a 2e7ed595-3aca-49e9-a771-942078d542d0 2753760 0 2020-09-11 17:48:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-11 17:48:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 11 17:48:51.928: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-a 2e7ed595-3aca-49e9-a771-942078d542d0 2753821 0 2020-09-11 17:48:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-11 17:48:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 17:48:51.928: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-a 2e7ed595-3aca-49e9-a771-942078d542d0 2753821 0 2020-09-11 17:48:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-11 17:48:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 11 17:49:01.942: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-a 2e7ed595-3aca-49e9-a771-942078d542d0 2753870 0 2020-09-11 17:48:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-11 17:48:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 17:49:01.942: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-a 2e7ed595-3aca-49e9-a771-942078d542d0 2753870 0 2020-09-11 17:48:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-11 17:48:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 11 17:49:11.954: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-b 101c5751-cf30-4524-8046-ea689dfbf8e6 2753919 0 2020-09-11 17:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-11 17:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 17:49:11.954: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-b 101c5751-cf30-4524-8046-ea689dfbf8e6 2753919 0 2020-09-11 17:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-11 17:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 11 17:49:21.966: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-b 101c5751-cf30-4524-8046-ea689dfbf8e6 2753971 0 2020-09-11 17:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-11 17:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 17:49:21.967: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2061 /api/v1/namespaces/watch-2061/configmaps/e2e-watch-test-configmap-b 101c5751-cf30-4524-8046-ea689dfbf8e6 2753971 0 2020-09-11 17:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-11 17:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:49:31.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2061" for this suite.

â€¢ [SLOW TEST:60.315 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":55,"skipped":1012,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:49:31.993: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7561
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 11 17:49:32.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7561'
Sep 11 17:49:32.253: INFO: stderr: ""
Sep 11 17:49:32.253: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Sep 11 17:49:32.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete pods e2e-test-httpd-pod --namespace=kubectl-7561'
Sep 11 17:49:43.216: INFO: stderr: ""
Sep 11 17:49:43.216: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:49:43.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7561" for this suite.

â€¢ [SLOW TEST:11.256 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":56,"skipped":1019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:49:43.249: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 17:49:43.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a77d1b9-3f52-453b-a417-bc6ee322fb4e" in namespace "downward-api-9468" to be "Succeeded or Failed"
Sep 11 17:49:43.559: INFO: Pod "downwardapi-volume-1a77d1b9-3f52-453b-a417-bc6ee322fb4e": Phase="Pending", Reason="", readiness=false. Elapsed: 109.643081ms
Sep 11 17:49:45.570: INFO: Pod "downwardapi-volume-1a77d1b9-3f52-453b-a417-bc6ee322fb4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.120920692s
STEP: Saw pod success
Sep 11 17:49:45.570: INFO: Pod "downwardapi-volume-1a77d1b9-3f52-453b-a417-bc6ee322fb4e" satisfied condition "Succeeded or Failed"
Sep 11 17:49:45.575: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-1a77d1b9-3f52-453b-a417-bc6ee322fb4e container client-container: <nil>
STEP: delete the pod
Sep 11 17:49:45.609: INFO: Waiting for pod downwardapi-volume-1a77d1b9-3f52-453b-a417-bc6ee322fb4e to disappear
Sep 11 17:49:45.615: INFO: Pod downwardapi-volume-1a77d1b9-3f52-453b-a417-bc6ee322fb4e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:49:45.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9468" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":57,"skipped":1045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:49:45.642: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-0a32f5bc-8620-4e8a-905b-d4e1e5a411c2
Sep 11 17:49:45.803: INFO: Pod name my-hostname-basic-0a32f5bc-8620-4e8a-905b-d4e1e5a411c2: Found 0 pods out of 1
Sep 11 17:49:50.807: INFO: Pod name my-hostname-basic-0a32f5bc-8620-4e8a-905b-d4e1e5a411c2: Found 1 pods out of 1
Sep 11 17:49:50.807: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0a32f5bc-8620-4e8a-905b-d4e1e5a411c2" are running
Sep 11 17:49:50.812: INFO: Pod "my-hostname-basic-0a32f5bc-8620-4e8a-905b-d4e1e5a411c2-8vjpg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-11 17:49:45 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-11 17:49:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-11 17:49:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-11 17:49:45 +0000 UTC Reason: Message:}])
Sep 11 17:49:50.812: INFO: Trying to dial the pod
Sep 11 17:49:55.826: INFO: Controller my-hostname-basic-0a32f5bc-8620-4e8a-905b-d4e1e5a411c2: Got expected result from replica 1 [my-hostname-basic-0a32f5bc-8620-4e8a-905b-d4e1e5a411c2-8vjpg]: "my-hostname-basic-0a32f5bc-8620-4e8a-905b-d4e1e5a411c2-8vjpg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:49:55.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7614" for this suite.

â€¢ [SLOW TEST:10.211 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":58,"skipped":1069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:49:55.854: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 11 17:49:58.037: INFO: &Pod{ObjectMeta:{send-events-f4f3f48c-6ebd-49aa-a4da-9d983fc2dd26  events-2003 /api/v1/namespaces/events-2003/pods/send-events-f4f3f48c-6ebd-49aa-a4da-9d983fc2dd26 c9a1a2b0-44e5-4401-851e-56b3ef3d01b1 2754247 0 2020-09-11 17:49:56 +0000 UTC <nil> <nil> map[name:foo time:6315564] map[cni.projectcalico.org/podIP:192.168.123.8/32 kubernetes.io/psp:admin] [] []  [{calico Update v1 2020-09-11 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {e2e.test Update v1 2020-09-11 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-09-11 17:49:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.123.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9cgql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9cgql,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9cgql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-2-19.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:49:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:49:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.2.19,PodIP:192.168.123.8,StartTime:2020-09-11 17:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 17:49:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://50596ba7f99da0cd032cda9c808d66286ef26f0492c89eb2b19c563ae539e321,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.123.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 11 17:50:00.043: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 11 17:50:02.048: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:50:02.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2003" for this suite.

â€¢ [SLOW TEST:6.239 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":59,"skipped":1116,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:50:02.094: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 17:50:02.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f9a784c-3b15-4b5f-a1ca-b2e1e4b1a707" in namespace "projected-4368" to be "Succeeded or Failed"
Sep 11 17:50:02.283: INFO: Pod "downwardapi-volume-6f9a784c-3b15-4b5f-a1ca-b2e1e4b1a707": Phase="Pending", Reason="", readiness=false. Elapsed: 12.967882ms
Sep 11 17:50:04.288: INFO: Pod "downwardapi-volume-6f9a784c-3b15-4b5f-a1ca-b2e1e4b1a707": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018208804s
STEP: Saw pod success
Sep 11 17:50:04.288: INFO: Pod "downwardapi-volume-6f9a784c-3b15-4b5f-a1ca-b2e1e4b1a707" satisfied condition "Succeeded or Failed"
Sep 11 17:50:04.295: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-6f9a784c-3b15-4b5f-a1ca-b2e1e4b1a707 container client-container: <nil>
STEP: delete the pod
Sep 11 17:50:04.332: INFO: Waiting for pod downwardapi-volume-6f9a784c-3b15-4b5f-a1ca-b2e1e4b1a707 to disappear
Sep 11 17:50:04.337: INFO: Pod downwardapi-volume-6f9a784c-3b15-4b5f-a1ca-b2e1e4b1a707 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:50:04.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4368" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":60,"skipped":1132,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:50:04.366: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6183
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-44a3a4a8-0af7-4e21-9fae-3217237d3822
STEP: Creating a pod to test consume configMaps
Sep 11 17:50:04.566: INFO: Waiting up to 5m0s for pod "pod-configmaps-0fb2a4f4-5384-4790-bf8f-e40b1c2ca6dc" in namespace "configmap-6183" to be "Succeeded or Failed"
Sep 11 17:50:04.570: INFO: Pod "pod-configmaps-0fb2a4f4-5384-4790-bf8f-e40b1c2ca6dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.196529ms
Sep 11 17:50:06.576: INFO: Pod "pod-configmaps-0fb2a4f4-5384-4790-bf8f-e40b1c2ca6dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010181452s
Sep 11 17:50:08.581: INFO: Pod "pod-configmaps-0fb2a4f4-5384-4790-bf8f-e40b1c2ca6dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0152208s
STEP: Saw pod success
Sep 11 17:50:08.581: INFO: Pod "pod-configmaps-0fb2a4f4-5384-4790-bf8f-e40b1c2ca6dc" satisfied condition "Succeeded or Failed"
Sep 11 17:50:08.586: INFO: Trying to get logs from node ip-10-10-1-141.ec2.internal pod pod-configmaps-0fb2a4f4-5384-4790-bf8f-e40b1c2ca6dc container configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 17:50:08.641: INFO: Waiting for pod pod-configmaps-0fb2a4f4-5384-4790-bf8f-e40b1c2ca6dc to disappear
Sep 11 17:50:08.645: INFO: Pod pod-configmaps-0fb2a4f4-5384-4790-bf8f-e40b1c2ca6dc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:50:08.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6183" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":61,"skipped":1205,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:50:08.676: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-d67b7cb7-aadc-4184-b4fe-b6ea8619e009 in namespace container-probe-1957
Sep 11 17:50:10.879: INFO: Started pod test-webserver-d67b7cb7-aadc-4184-b4fe-b6ea8619e009 in namespace container-probe-1957
STEP: checking the pod's current state and verifying that restartCount is present
Sep 11 17:50:10.883: INFO: Initial restart count of pod test-webserver-d67b7cb7-aadc-4184-b4fe-b6ea8619e009 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:54:11.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1957" for this suite.

â€¢ [SLOW TEST:242.976 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":62,"skipped":1209,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:54:11.653: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Sep 11 17:56:12.351: INFO: Successfully updated pod "var-expansion-c30696d5-7c0c-497d-b33d-7200c5751b3d"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Sep 11 17:56:14.360: INFO: Deleting pod "var-expansion-c30696d5-7c0c-497d-b33d-7200c5751b3d" in namespace "var-expansion-1689"
Sep 11 17:56:14.372: INFO: Wait up to 5m0s for pod "var-expansion-c30696d5-7c0c-497d-b33d-7200c5751b3d" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:56:54.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1689" for this suite.

â€¢ [SLOW TEST:162.781 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":63,"skipped":1220,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:56:54.435: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-6433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:56:55.047: INFO: Checking APIGroup: apiregistration.k8s.io
Sep 11 17:56:55.047: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep 11 17:56:55.047: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.047: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep 11 17:56:55.047: INFO: Checking APIGroup: extensions
Sep 11 17:56:55.048: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Sep 11 17:56:55.048: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Sep 11 17:56:55.048: INFO: extensions/v1beta1 matches extensions/v1beta1
Sep 11 17:56:55.048: INFO: Checking APIGroup: apps
Sep 11 17:56:55.049: INFO: PreferredVersion.GroupVersion: apps/v1
Sep 11 17:56:55.049: INFO: Versions found [{apps/v1 v1}]
Sep 11 17:56:55.049: INFO: apps/v1 matches apps/v1
Sep 11 17:56:55.049: INFO: Checking APIGroup: events.k8s.io
Sep 11 17:56:55.049: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep 11 17:56:55.049: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.049: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep 11 17:56:55.050: INFO: Checking APIGroup: authentication.k8s.io
Sep 11 17:56:55.050: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep 11 17:56:55.050: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.050: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep 11 17:56:55.050: INFO: Checking APIGroup: authorization.k8s.io
Sep 11 17:56:55.051: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep 11 17:56:55.051: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.051: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep 11 17:56:55.051: INFO: Checking APIGroup: autoscaling
Sep 11 17:56:55.051: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Sep 11 17:56:55.051: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Sep 11 17:56:55.051: INFO: autoscaling/v1 matches autoscaling/v1
Sep 11 17:56:55.051: INFO: Checking APIGroup: batch
Sep 11 17:56:55.052: INFO: PreferredVersion.GroupVersion: batch/v1
Sep 11 17:56:55.052: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Sep 11 17:56:55.052: INFO: batch/v1 matches batch/v1
Sep 11 17:56:55.052: INFO: Checking APIGroup: certificates.k8s.io
Sep 11 17:56:55.052: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep 11 17:56:55.052: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.052: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep 11 17:56:55.052: INFO: Checking APIGroup: networking.k8s.io
Sep 11 17:56:55.053: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep 11 17:56:55.053: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.053: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep 11 17:56:55.053: INFO: Checking APIGroup: policy
Sep 11 17:56:55.053: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Sep 11 17:56:55.053: INFO: Versions found [{policy/v1beta1 v1beta1}]
Sep 11 17:56:55.054: INFO: policy/v1beta1 matches policy/v1beta1
Sep 11 17:56:55.054: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep 11 17:56:55.054: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep 11 17:56:55.054: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.054: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep 11 17:56:55.054: INFO: Checking APIGroup: storage.k8s.io
Sep 11 17:56:55.055: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep 11 17:56:55.055: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.055: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep 11 17:56:55.055: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep 11 17:56:55.055: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep 11 17:56:55.055: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.055: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep 11 17:56:55.055: INFO: Checking APIGroup: apiextensions.k8s.io
Sep 11 17:56:55.056: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep 11 17:56:55.056: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.056: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep 11 17:56:55.056: INFO: Checking APIGroup: scheduling.k8s.io
Sep 11 17:56:55.057: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep 11 17:56:55.057: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.057: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep 11 17:56:55.057: INFO: Checking APIGroup: coordination.k8s.io
Sep 11 17:56:55.057: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep 11 17:56:55.057: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.057: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep 11 17:56:55.057: INFO: Checking APIGroup: node.k8s.io
Sep 11 17:56:55.058: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Sep 11 17:56:55.058: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.058: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Sep 11 17:56:55.058: INFO: Checking APIGroup: discovery.k8s.io
Sep 11 17:56:55.058: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Sep 11 17:56:55.058: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Sep 11 17:56:55.058: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Sep 11 17:56:55.058: INFO: Checking APIGroup: mutators.kubedb.com
Sep 11 17:56:55.059: INFO: PreferredVersion.GroupVersion: mutators.kubedb.com/v1alpha1
Sep 11 17:56:55.059: INFO: Versions found [{mutators.kubedb.com/v1alpha1 v1alpha1}]
Sep 11 17:56:55.059: INFO: mutators.kubedb.com/v1alpha1 matches mutators.kubedb.com/v1alpha1
Sep 11 17:56:55.059: INFO: Checking APIGroup: validators.kubedb.com
Sep 11 17:56:55.060: INFO: PreferredVersion.GroupVersion: validators.kubedb.com/v1alpha1
Sep 11 17:56:55.060: INFO: Versions found [{validators.kubedb.com/v1alpha1 v1alpha1}]
Sep 11 17:56:55.060: INFO: validators.kubedb.com/v1alpha1 matches validators.kubedb.com/v1alpha1
Sep 11 17:56:55.060: INFO: Checking APIGroup: crd.projectcalico.org
Sep 11 17:56:55.060: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Sep 11 17:56:55.060: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Sep 11 17:56:55.060: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Sep 11 17:56:55.060: INFO: Checking APIGroup: dex.coreos.com
Sep 11 17:56:55.061: INFO: PreferredVersion.GroupVersion: dex.coreos.com/v1
Sep 11 17:56:55.061: INFO: Versions found [{dex.coreos.com/v1 v1}]
Sep 11 17:56:55.061: INFO: dex.coreos.com/v1 matches dex.coreos.com/v1
Sep 11 17:56:55.061: INFO: Checking APIGroup: helm.fluxcd.io
Sep 11 17:56:55.062: INFO: PreferredVersion.GroupVersion: helm.fluxcd.io/v1
Sep 11 17:56:55.062: INFO: Versions found [{helm.fluxcd.io/v1 v1}]
Sep 11 17:56:55.062: INFO: helm.fluxcd.io/v1 matches helm.fluxcd.io/v1
Sep 11 17:56:55.062: INFO: Checking APIGroup: monitoring.coreos.com
Sep 11 17:56:55.062: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Sep 11 17:56:55.062: INFO: Versions found [{monitoring.coreos.com/v1 v1}]
Sep 11 17:56:55.062: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Sep 11 17:56:55.062: INFO: Checking APIGroup: velero.io
Sep 11 17:56:55.063: INFO: PreferredVersion.GroupVersion: velero.io/v1
Sep 11 17:56:55.063: INFO: Versions found [{velero.io/v1 v1}]
Sep 11 17:56:55.063: INFO: velero.io/v1 matches velero.io/v1
Sep 11 17:56:55.063: INFO: Checking APIGroup: appcatalog.appscode.com
Sep 11 17:56:55.064: INFO: PreferredVersion.GroupVersion: appcatalog.appscode.com/v1alpha1
Sep 11 17:56:55.064: INFO: Versions found [{appcatalog.appscode.com/v1alpha1 v1alpha1}]
Sep 11 17:56:55.064: INFO: appcatalog.appscode.com/v1alpha1 matches appcatalog.appscode.com/v1alpha1
Sep 11 17:56:55.064: INFO: Checking APIGroup: bitnami.com
Sep 11 17:56:55.064: INFO: PreferredVersion.GroupVersion: bitnami.com/v1alpha1
Sep 11 17:56:55.064: INFO: Versions found [{bitnami.com/v1alpha1 v1alpha1}]
Sep 11 17:56:55.064: INFO: bitnami.com/v1alpha1 matches bitnami.com/v1alpha1
Sep 11 17:56:55.064: INFO: Checking APIGroup: catalog.kubedb.com
Sep 11 17:56:55.065: INFO: PreferredVersion.GroupVersion: catalog.kubedb.com/v1alpha1
Sep 11 17:56:55.065: INFO: Versions found [{catalog.kubedb.com/v1alpha1 v1alpha1}]
Sep 11 17:56:55.065: INFO: catalog.kubedb.com/v1alpha1 matches catalog.kubedb.com/v1alpha1
Sep 11 17:56:55.065: INFO: Checking APIGroup: kubedb.com
Sep 11 17:56:55.065: INFO: PreferredVersion.GroupVersion: kubedb.com/v1alpha1
Sep 11 17:56:55.065: INFO: Versions found [{kubedb.com/v1alpha1 v1alpha1}]
Sep 11 17:56:55.065: INFO: kubedb.com/v1alpha1 matches kubedb.com/v1alpha1
Sep 11 17:56:55.065: INFO: Checking APIGroup: acme.cert-manager.io
Sep 11 17:56:55.066: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1alpha2
Sep 11 17:56:55.066: INFO: Versions found [{acme.cert-manager.io/v1alpha2 v1alpha2}]
Sep 11 17:56:55.066: INFO: acme.cert-manager.io/v1alpha2 matches acme.cert-manager.io/v1alpha2
Sep 11 17:56:55.066: INFO: Checking APIGroup: cert-manager.io
Sep 11 17:56:55.067: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1alpha2
Sep 11 17:56:55.067: INFO: Versions found [{cert-manager.io/v1alpha2 v1alpha2}]
Sep 11 17:56:55.067: INFO: cert-manager.io/v1alpha2 matches cert-manager.io/v1alpha2
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:56:55.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6433" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":64,"skipped":1233,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:56:55.093: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4933
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 11 17:56:55.264: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4933 /api/v1/namespaces/watch-4933/configmaps/e2e-watch-test-watch-closed 15a973c2-fd84-4d40-a050-eb2814a1eace 2756474 0 2020-09-11 17:56:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-11 17:56:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 17:56:55.264: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4933 /api/v1/namespaces/watch-4933/configmaps/e2e-watch-test-watch-closed 15a973c2-fd84-4d40-a050-eb2814a1eace 2756475 0 2020-09-11 17:56:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-11 17:56:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 11 17:56:55.286: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4933 /api/v1/namespaces/watch-4933/configmaps/e2e-watch-test-watch-closed 15a973c2-fd84-4d40-a050-eb2814a1eace 2756476 0 2020-09-11 17:56:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-11 17:56:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 17:56:55.287: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4933 /api/v1/namespaces/watch-4933/configmaps/e2e-watch-test-watch-closed 15a973c2-fd84-4d40-a050-eb2814a1eace 2756477 0 2020-09-11 17:56:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-11 17:56:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:56:55.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4933" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":65,"skipped":1248,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:56:55.315: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0911 17:56:57.539855      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 11 17:57:59.563: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:57:59.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7315" for this suite.

â€¢ [SLOW TEST:64.287 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":66,"skipped":1252,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:57:59.602: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-863
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 11 17:58:04.297: INFO: Successfully updated pod "adopt-release-vgsrg"
STEP: Checking that the Job readopts the Pod
Sep 11 17:58:04.297: INFO: Waiting up to 15m0s for pod "adopt-release-vgsrg" in namespace "job-863" to be "adopted"
Sep 11 17:58:04.308: INFO: Pod "adopt-release-vgsrg": Phase="Running", Reason="", readiness=true. Elapsed: 10.475701ms
Sep 11 17:58:06.312: INFO: Pod "adopt-release-vgsrg": Phase="Running", Reason="", readiness=true. Elapsed: 2.015027232s
Sep 11 17:58:06.312: INFO: Pod "adopt-release-vgsrg" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 11 17:58:06.841: INFO: Successfully updated pod "adopt-release-vgsrg"
STEP: Checking that the Job releases the Pod
Sep 11 17:58:06.841: INFO: Waiting up to 15m0s for pod "adopt-release-vgsrg" in namespace "job-863" to be "released"
Sep 11 17:58:06.848: INFO: Pod "adopt-release-vgsrg": Phase="Running", Reason="", readiness=true. Elapsed: 6.710326ms
Sep 11 17:58:06.848: INFO: Pod "adopt-release-vgsrg" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:58:06.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-863" for this suite.

â€¢ [SLOW TEST:7.286 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":67,"skipped":1291,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:58:06.888: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-c00b5b40-a29b-4a40-9be8-a40a2eab0e55
STEP: Creating a pod to test consume configMaps
Sep 11 17:58:07.065: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f366c07c-631e-4810-8f5c-90929a10a606" in namespace "projected-3119" to be "Succeeded or Failed"
Sep 11 17:58:07.069: INFO: Pod "pod-projected-configmaps-f366c07c-631e-4810-8f5c-90929a10a606": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990509ms
Sep 11 17:58:09.080: INFO: Pod "pod-projected-configmaps-f366c07c-631e-4810-8f5c-90929a10a606": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015110635s
STEP: Saw pod success
Sep 11 17:58:09.080: INFO: Pod "pod-projected-configmaps-f366c07c-631e-4810-8f5c-90929a10a606" satisfied condition "Succeeded or Failed"
Sep 11 17:58:09.085: INFO: Trying to get logs from node ip-10-10-1-141.ec2.internal pod pod-projected-configmaps-f366c07c-631e-4810-8f5c-90929a10a606 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 17:58:09.130: INFO: Waiting for pod pod-projected-configmaps-f366c07c-631e-4810-8f5c-90929a10a606 to disappear
Sep 11 17:58:09.135: INFO: Pod pod-projected-configmaps-f366c07c-631e-4810-8f5c-90929a10a606 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:58:09.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3119" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":68,"skipped":1294,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:58:09.159: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1341
Sep 11 17:58:11.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1341 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 11 17:58:11.780: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep 11 17:58:11.780: INFO: stdout: "iptables"
Sep 11 17:58:11.780: INFO: proxyMode: iptables
Sep 11 17:58:11.800: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 11 17:58:11.807: INFO: Pod kube-proxy-mode-detector still exists
Sep 11 17:58:13.807: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 11 17:58:13.812: INFO: Pod kube-proxy-mode-detector still exists
Sep 11 17:58:15.807: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 11 17:58:15.812: INFO: Pod kube-proxy-mode-detector still exists
Sep 11 17:58:17.807: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 11 17:58:17.813: INFO: Pod kube-proxy-mode-detector still exists
Sep 11 17:58:19.807: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 11 17:58:19.812: INFO: Pod kube-proxy-mode-detector still exists
Sep 11 17:58:21.807: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 11 17:58:21.811: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1341
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1341
I0911 17:58:21.853105      19 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1341, replica count: 3
I0911 17:58:24.904079      19 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 17:58:24.912: INFO: Creating new exec pod
Sep 11 17:58:29.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1341 execpod-affinityqzjkv -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Sep 11 17:58:30.456: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Sep 11 17:58:30.456: INFO: stdout: ""
Sep 11 17:58:30.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1341 execpod-affinityqzjkv -- /bin/sh -x -c nc -zv -t -w 2 10.96.171.188 80'
Sep 11 17:58:30.823: INFO: stderr: "+ nc -zv -t -w 2 10.96.171.188 80\nConnection to 10.96.171.188 80 port [tcp/http] succeeded!\n"
Sep 11 17:58:30.823: INFO: stdout: ""
Sep 11 17:58:30.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1341 execpod-affinityqzjkv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.171.188:80/ ; done'
Sep 11 17:58:31.179: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n"
Sep 11 17:58:31.179: INFO: stdout: "\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv\naffinity-clusterip-timeout-b6lnv"
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Received response from host: affinity-clusterip-timeout-b6lnv
Sep 11 17:58:31.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1341 execpod-affinityqzjkv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.171.188:80/'
Sep 11 17:58:31.415: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n"
Sep 11 17:58:31.415: INFO: stdout: "affinity-clusterip-timeout-b6lnv"
Sep 11 17:58:46.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1341 execpod-affinityqzjkv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.171.188:80/'
Sep 11 17:58:46.675: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.171.188:80/\n"
Sep 11 17:58:46.675: INFO: stdout: "affinity-clusterip-timeout-w248g"
Sep 11 17:58:46.675: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1341, will wait for the garbage collector to delete the pods
Sep 11 17:58:46.770: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.455903ms
Sep 11 17:58:47.870: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 1.10008463s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:59:03.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1341" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:54.202 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":69,"skipped":1304,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:59:03.361: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7954
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 11 17:59:03.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7954'
Sep 11 17:59:03.638: INFO: stderr: ""
Sep 11 17:59:03.638: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 11 17:59:08.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pod e2e-test-httpd-pod --namespace=kubectl-7954 -o json'
Sep 11 17:59:08.803: INFO: stderr: ""
Sep 11 17:59:08.803: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.123.24/32\",\n            \"kubernetes.io/psp\": \"admin\"\n        },\n        \"creationTimestamp\": \"2020-09-11T17:59:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-11T17:59:03Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-11T17:59:04Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"192.168.123.24\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-11T17:59:05Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7954\",\n        \"resourceVersion\": \"2757479\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7954/pods/e2e-test-httpd-pod\",\n        \"uid\": \"9b905dd1-31d3-4cca-88b9-619407aaf87f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-fztch\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-10-2-19.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-fztch\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-fztch\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-11T17:59:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-11T17:59:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-11T17:59:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-11T17:59:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://16f7f2bb664683ebefcbc9acd17e5360317d2f0bad6503e53d4e1e1f7b5a5299\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-09-11T17:59:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.2.19\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.123.24\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.123.24\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-09-11T17:59:03Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 11 17:59:08.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 replace -f - --namespace=kubectl-7954'
Sep 11 17:59:09.251: INFO: stderr: ""
Sep 11 17:59:09.251: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Sep 11 17:59:09.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete pods e2e-test-httpd-pod --namespace=kubectl-7954'
Sep 11 17:59:13.219: INFO: stderr: ""
Sep 11 17:59:13.219: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:59:13.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7954" for this suite.

â€¢ [SLOW TEST:9.892 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":70,"skipped":1311,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:59:13.254: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Sep 11 17:59:13.936: INFO: created pod pod-service-account-defaultsa
Sep 11 17:59:13.936: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 11 17:59:13.946: INFO: created pod pod-service-account-mountsa
Sep 11 17:59:13.946: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 11 17:59:13.955: INFO: created pod pod-service-account-nomountsa
Sep 11 17:59:13.955: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 11 17:59:13.963: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 11 17:59:13.963: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 11 17:59:13.978: INFO: created pod pod-service-account-mountsa-mountspec
Sep 11 17:59:13.978: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 11 17:59:13.991: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 11 17:59:13.991: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 11 17:59:13.999: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 11 17:59:13.999: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 11 17:59:14.020: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 11 17:59:14.020: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 11 17:59:14.027: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 11 17:59:14.027: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:59:14.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8272" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":71,"skipped":1322,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:59:14.085: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3870.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3870.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3870.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3870.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3870.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3870.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3870.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3870.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3870.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3870.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 124.89.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.89.124_udp@PTR;check="$$(dig +tcp +noall +answer +search 124.89.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.89.124_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3870.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3870.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3870.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3870.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3870.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3870.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3870.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3870.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3870.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3870.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3870.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 124.89.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.89.124_udp@PTR;check="$$(dig +tcp +noall +answer +search 124.89.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.89.124_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 11 17:59:18.373: INFO: Unable to read wheezy_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:18.378: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:18.383: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:18.394: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:18.437: INFO: Unable to read jessie_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:18.444: INFO: Unable to read jessie_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:18.452: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:18.458: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:18.492: INFO: Lookups using dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb failed for: [wheezy_udp@dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_udp@dns-test-service.dns-3870.svc.cluster.local jessie_tcp@dns-test-service.dns-3870.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local]

Sep 11 17:59:23.500: INFO: Unable to read wheezy_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:23.506: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:23.510: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:23.515: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:23.547: INFO: Unable to read jessie_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:23.553: INFO: Unable to read jessie_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:23.558: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:23.569: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:23.598: INFO: Lookups using dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb failed for: [wheezy_udp@dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_udp@dns-test-service.dns-3870.svc.cluster.local jessie_tcp@dns-test-service.dns-3870.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local]

Sep 11 17:59:28.498: INFO: Unable to read wheezy_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:28.503: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:28.508: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:28.512: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:28.543: INFO: Unable to read jessie_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:28.549: INFO: Unable to read jessie_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:28.556: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:28.560: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:28.589: INFO: Lookups using dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb failed for: [wheezy_udp@dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_udp@dns-test-service.dns-3870.svc.cluster.local jessie_tcp@dns-test-service.dns-3870.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local]

Sep 11 17:59:33.498: INFO: Unable to read wheezy_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:33.503: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:33.508: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:33.513: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:33.545: INFO: Unable to read jessie_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:33.550: INFO: Unable to read jessie_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:33.554: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:33.559: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:33.587: INFO: Lookups using dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb failed for: [wheezy_udp@dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_udp@dns-test-service.dns-3870.svc.cluster.local jessie_tcp@dns-test-service.dns-3870.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local]

Sep 11 17:59:38.498: INFO: Unable to read wheezy_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:38.508: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:38.513: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:38.518: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:38.555: INFO: Unable to read jessie_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:38.560: INFO: Unable to read jessie_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:38.565: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:38.570: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:38.599: INFO: Lookups using dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb failed for: [wheezy_udp@dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_udp@dns-test-service.dns-3870.svc.cluster.local jessie_tcp@dns-test-service.dns-3870.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local]

Sep 11 17:59:43.499: INFO: Unable to read wheezy_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:43.504: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:43.508: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:43.513: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:43.554: INFO: Unable to read jessie_udp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:43.559: INFO: Unable to read jessie_tcp@dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:43.563: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:43.568: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local from pod dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb: the server could not find the requested resource (get pods dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb)
Sep 11 17:59:43.604: INFO: Lookups using dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb failed for: [wheezy_udp@dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@dns-test-service.dns-3870.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_udp@dns-test-service.dns-3870.svc.cluster.local jessie_tcp@dns-test-service.dns-3870.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3870.svc.cluster.local]

Sep 11 17:59:48.637: INFO: DNS probes using dns-3870/dns-test-f621bbbd-2885-45c6-9397-d8def1e355fb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 17:59:48.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3870" for this suite.

â€¢ [SLOW TEST:34.799 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":72,"skipped":1326,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 17:59:48.884: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1748
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 17:59:49.042: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 11 17:59:54.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-1748 create -f -'
Sep 11 17:59:55.537: INFO: stderr: ""
Sep 11 17:59:55.537: INFO: stdout: "e2e-test-crd-publish-openapi-2364-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 11 17:59:55.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-1748 delete e2e-test-crd-publish-openapi-2364-crds test-cr'
Sep 11 17:59:55.648: INFO: stderr: ""
Sep 11 17:59:55.648: INFO: stdout: "e2e-test-crd-publish-openapi-2364-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 11 17:59:55.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-1748 apply -f -'
Sep 11 17:59:55.978: INFO: stderr: ""
Sep 11 17:59:55.978: INFO: stdout: "e2e-test-crd-publish-openapi-2364-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 11 17:59:55.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-1748 delete e2e-test-crd-publish-openapi-2364-crds test-cr'
Sep 11 17:59:56.087: INFO: stderr: ""
Sep 11 17:59:56.087: INFO: stdout: "e2e-test-crd-publish-openapi-2364-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 11 17:59:56.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 explain e2e-test-crd-publish-openapi-2364-crds'
Sep 11 17:59:56.483: INFO: stderr: ""
Sep 11 17:59:56.483: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2364-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:00:01.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1748" for this suite.

â€¢ [SLOW TEST:13.034 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":73,"skipped":1331,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:00:01.918: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1225
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-h945
STEP: Creating a pod to test atomic-volume-subpath
Sep 11 18:00:02.137: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-h945" in namespace "subpath-1225" to be "Succeeded or Failed"
Sep 11 18:00:02.146: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Pending", Reason="", readiness=false. Elapsed: 8.996681ms
Sep 11 18:00:04.151: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014432733s
Sep 11 18:00:06.156: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Running", Reason="", readiness=true. Elapsed: 4.019131469s
Sep 11 18:00:08.161: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Running", Reason="", readiness=true. Elapsed: 6.023882281s
Sep 11 18:00:10.166: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Running", Reason="", readiness=true. Elapsed: 8.029185568s
Sep 11 18:00:12.171: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Running", Reason="", readiness=true. Elapsed: 10.034249937s
Sep 11 18:00:14.176: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Running", Reason="", readiness=true. Elapsed: 12.038958196s
Sep 11 18:00:16.181: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Running", Reason="", readiness=true. Elapsed: 14.043877641s
Sep 11 18:00:18.185: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Running", Reason="", readiness=true. Elapsed: 16.04873186s
Sep 11 18:00:20.190: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Running", Reason="", readiness=true. Elapsed: 18.053391477s
Sep 11 18:00:22.196: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Running", Reason="", readiness=true. Elapsed: 20.058977798s
Sep 11 18:00:24.201: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Running", Reason="", readiness=true. Elapsed: 22.064219763s
Sep 11 18:00:26.206: INFO: Pod "pod-subpath-test-downwardapi-h945": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.069168684s
STEP: Saw pod success
Sep 11 18:00:26.206: INFO: Pod "pod-subpath-test-downwardapi-h945" satisfied condition "Succeeded or Failed"
Sep 11 18:00:26.210: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-subpath-test-downwardapi-h945 container test-container-subpath-downwardapi-h945: <nil>
STEP: delete the pod
Sep 11 18:00:26.253: INFO: Waiting for pod pod-subpath-test-downwardapi-h945 to disappear
Sep 11 18:00:26.257: INFO: Pod pod-subpath-test-downwardapi-h945 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-h945
Sep 11 18:00:26.257: INFO: Deleting pod "pod-subpath-test-downwardapi-h945" in namespace "subpath-1225"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:00:26.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1225" for this suite.

â€¢ [SLOW TEST:24.369 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":74,"skipped":1346,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:00:26.287: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6405
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-acaa1257-77fb-452d-961f-90a326394c7e
STEP: Creating secret with name s-test-opt-upd-e51d9874-aa84-4f65-a332-e1324a60c9ec
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-acaa1257-77fb-452d-961f-90a326394c7e
STEP: Updating secret s-test-opt-upd-e51d9874-aa84-4f65-a332-e1324a60c9ec
STEP: Creating secret with name s-test-opt-create-4fcaf75d-b18c-4c3f-ab34-024355788a27
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:00:30.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6405" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":75,"skipped":1349,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:00:30.645: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8008
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 11 18:00:30.821: INFO: Waiting up to 5m0s for pod "pod-a90c6190-db02-4938-9ea6-67739835cda1" in namespace "emptydir-8008" to be "Succeeded or Failed"
Sep 11 18:00:30.840: INFO: Pod "pod-a90c6190-db02-4938-9ea6-67739835cda1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.899623ms
Sep 11 18:00:32.845: INFO: Pod "pod-a90c6190-db02-4938-9ea6-67739835cda1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023819337s
Sep 11 18:00:34.849: INFO: Pod "pod-a90c6190-db02-4938-9ea6-67739835cda1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028170249s
STEP: Saw pod success
Sep 11 18:00:34.849: INFO: Pod "pod-a90c6190-db02-4938-9ea6-67739835cda1" satisfied condition "Succeeded or Failed"
Sep 11 18:00:34.853: INFO: Trying to get logs from node ip-10-10-1-141.ec2.internal pod pod-a90c6190-db02-4938-9ea6-67739835cda1 container test-container: <nil>
STEP: delete the pod
Sep 11 18:00:34.907: INFO: Waiting for pod pod-a90c6190-db02-4938-9ea6-67739835cda1 to disappear
Sep 11 18:00:34.911: INFO: Pod pod-a90c6190-db02-4938-9ea6-67739835cda1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:00:34.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8008" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":1383,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:00:34.938: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep 11 18:00:39.665: INFO: Successfully updated pod "labelsupdatef1f1b169-94f3-4782-a308-c906dd086a35"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:00:41.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7011" for this suite.

â€¢ [SLOW TEST:6.793 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":77,"skipped":1388,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:00:41.731: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4380
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-4m27
STEP: Creating a pod to test atomic-volume-subpath
Sep 11 18:00:41.974: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-4m27" in namespace "subpath-4380" to be "Succeeded or Failed"
Sep 11 18:00:41.984: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Pending", Reason="", readiness=false. Elapsed: 10.477031ms
Sep 11 18:00:43.989: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015301447s
Sep 11 18:00:45.994: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Running", Reason="", readiness=true. Elapsed: 4.020342452s
Sep 11 18:00:47.999: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Running", Reason="", readiness=true. Elapsed: 6.024964178s
Sep 11 18:00:50.003: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Running", Reason="", readiness=true. Elapsed: 8.029702481s
Sep 11 18:00:52.009: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Running", Reason="", readiness=true. Elapsed: 10.034843313s
Sep 11 18:00:54.013: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Running", Reason="", readiness=true. Elapsed: 12.039537126s
Sep 11 18:00:56.018: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Running", Reason="", readiness=true. Elapsed: 14.044012012s
Sep 11 18:00:58.023: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Running", Reason="", readiness=true. Elapsed: 16.04895671s
Sep 11 18:01:00.028: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Running", Reason="", readiness=true. Elapsed: 18.053975526s
Sep 11 18:01:02.033: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Running", Reason="", readiness=true. Elapsed: 20.058992825s
Sep 11 18:01:04.038: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Running", Reason="", readiness=true. Elapsed: 22.064627579s
Sep 11 18:01:06.043: INFO: Pod "pod-subpath-test-secret-4m27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.069623653s
STEP: Saw pod success
Sep 11 18:01:06.043: INFO: Pod "pod-subpath-test-secret-4m27" satisfied condition "Succeeded or Failed"
Sep 11 18:01:06.048: INFO: Trying to get logs from node ip-10-10-1-141.ec2.internal pod pod-subpath-test-secret-4m27 container test-container-subpath-secret-4m27: <nil>
STEP: delete the pod
Sep 11 18:01:06.111: INFO: Waiting for pod pod-subpath-test-secret-4m27 to disappear
Sep 11 18:01:06.117: INFO: Pod pod-subpath-test-secret-4m27 no longer exists
STEP: Deleting pod pod-subpath-test-secret-4m27
Sep 11 18:01:06.117: INFO: Deleting pod "pod-subpath-test-secret-4m27" in namespace "subpath-4380"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:01:06.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4380" for this suite.

â€¢ [SLOW TEST:24.431 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":78,"skipped":1390,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:01:06.162: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Sep 11 18:01:06.315: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-107083211 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:01:06.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9116" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":79,"skipped":1408,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:01:06.423: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:02:06.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3236" for this suite.

â€¢ [SLOW TEST:60.212 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1413,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:02:06.635: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:02:07.128: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 18:02:09.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735444127, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735444127, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735444127, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735444127, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:02:12.183: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:02:12.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7933" for this suite.
STEP: Destroying namespace "webhook-7933-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.009 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":81,"skipped":1427,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:02:12.645: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-270
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 11 18:02:16.879: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 11 18:02:16.885: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 11 18:02:18.885: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 11 18:02:18.890: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 11 18:02:20.885: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 11 18:02:20.891: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 11 18:02:22.885: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 11 18:02:22.891: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 11 18:02:24.885: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 11 18:02:24.891: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 11 18:02:26.885: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 11 18:02:26.890: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 11 18:02:28.885: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 11 18:02:28.890: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 11 18:02:30.885: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 11 18:02:30.892: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:02:30.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-270" for this suite.

â€¢ [SLOW TEST:18.286 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":82,"skipped":1463,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:02:30.932: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4612
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Sep 11 18:02:31.138: INFO: Found 0 stateful pods, waiting for 3
Sep 11 18:02:41.143: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:02:41.143: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:02:41.143: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 11 18:02:41.176: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 11 18:02:51.218: INFO: Updating stateful set ss2
Sep 11 18:02:51.227: INFO: Waiting for Pod statefulset-4612/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep 11 18:03:01.365: INFO: Found 2 stateful pods, waiting for 3
Sep 11 18:03:11.371: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:03:11.371: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:03:11.371: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 11 18:03:11.402: INFO: Updating stateful set ss2
Sep 11 18:03:11.411: INFO: Waiting for Pod statefulset-4612/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 11 18:03:21.445: INFO: Updating stateful set ss2
Sep 11 18:03:21.454: INFO: Waiting for StatefulSet statefulset-4612/ss2 to complete update
Sep 11 18:03:21.454: INFO: Waiting for Pod statefulset-4612/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 11 18:03:31.463: INFO: Waiting for StatefulSet statefulset-4612/ss2 to complete update
Sep 11 18:03:31.463: INFO: Waiting for Pod statefulset-4612/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 11 18:03:41.463: INFO: Deleting all statefulset in ns statefulset-4612
Sep 11 18:03:41.467: INFO: Scaling statefulset ss2 to 0
Sep 11 18:03:51.487: INFO: Waiting for statefulset status.replicas updated to 0
Sep 11 18:03:51.490: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:03:51.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4612" for this suite.

â€¢ [SLOW TEST:80.606 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":83,"skipped":1480,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:03:51.538: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9060
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9060
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9060
Sep 11 18:03:51.740: INFO: Found 0 stateful pods, waiting for 1
Sep 11 18:04:01.753: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 11 18:04:01.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-9060 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 11 18:04:02.057: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 11 18:04:02.057: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 11 18:04:02.057: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 11 18:04:02.062: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 11 18:04:12.066: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 11 18:04:12.067: INFO: Waiting for statefulset status.replicas updated to 0
Sep 11 18:04:12.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998495s
Sep 11 18:04:13.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994765313s
Sep 11 18:04:14.096: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990079141s
Sep 11 18:04:15.101: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985073954s
Sep 11 18:04:16.106: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980100874s
Sep 11 18:04:17.111: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974604013s
Sep 11 18:04:18.117: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969337494s
Sep 11 18:04:19.121: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964069621s
Sep 11 18:04:20.127: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.959236729s
Sep 11 18:04:21.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 953.861019ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9060
Sep 11 18:04:22.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-9060 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 11 18:04:22.383: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 11 18:04:22.383: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 11 18:04:22.383: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 11 18:04:22.389: INFO: Found 1 stateful pods, waiting for 3
Sep 11 18:04:32.395: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:04:32.395: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:04:32.395: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 11 18:04:32.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-9060 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 11 18:04:32.650: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 11 18:04:32.650: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 11 18:04:32.650: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 11 18:04:32.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-9060 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 11 18:04:32.889: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 11 18:04:32.889: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 11 18:04:32.889: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 11 18:04:32.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-9060 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 11 18:04:33.242: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 11 18:04:33.242: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 11 18:04:33.242: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 11 18:04:33.242: INFO: Waiting for statefulset status.replicas updated to 0
Sep 11 18:04:33.247: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 11 18:04:43.257: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 11 18:04:43.257: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 11 18:04:43.257: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 11 18:04:43.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998399s
Sep 11 18:04:44.277: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99490385s
Sep 11 18:04:45.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989073622s
Sep 11 18:04:46.288: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983515249s
Sep 11 18:04:47.294: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978184094s
Sep 11 18:04:48.299: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972582187s
Sep 11 18:04:49.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967885051s
Sep 11 18:04:50.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962536141s
Sep 11 18:04:51.315: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956846918s
Sep 11 18:04:52.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 951.246256ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9060
Sep 11 18:04:53.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-9060 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 11 18:04:53.654: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 11 18:04:53.654: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 11 18:04:53.654: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 11 18:04:53.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-9060 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 11 18:04:53.907: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 11 18:04:53.907: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 11 18:04:53.907: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 11 18:04:53.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-9060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 11 18:04:54.194: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 11 18:04:54.194: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 11 18:04:54.194: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 11 18:04:54.194: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 11 18:05:14.212: INFO: Deleting all statefulset in ns statefulset-9060
Sep 11 18:05:14.216: INFO: Scaling statefulset ss to 0
Sep 11 18:05:14.228: INFO: Waiting for statefulset status.replicas updated to 0
Sep 11 18:05:14.232: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:05:14.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9060" for this suite.

â€¢ [SLOW TEST:82.741 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":84,"skipped":1480,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:05:14.280: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8371
STEP: creating service affinity-clusterip-transition in namespace services-8371
STEP: creating replication controller affinity-clusterip-transition in namespace services-8371
I0911 18:05:14.482249      19 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-8371, replica count: 3
I0911 18:05:17.535865      19 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 18:05:17.544: INFO: Creating new exec pod
Sep 11 18:05:20.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-8371 execpod-affinityqs9pd -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Sep 11 18:05:20.860: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep 11 18:05:20.860: INFO: stdout: ""
Sep 11 18:05:20.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-8371 execpod-affinityqs9pd -- /bin/sh -x -c nc -zv -t -w 2 10.96.201.210 80'
Sep 11 18:05:21.172: INFO: stderr: "+ nc -zv -t -w 2 10.96.201.210 80\nConnection to 10.96.201.210 80 port [tcp/http] succeeded!\n"
Sep 11 18:05:21.172: INFO: stdout: ""
Sep 11 18:05:21.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-8371 execpod-affinityqs9pd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.201.210:80/ ; done'
Sep 11 18:05:21.514: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n"
Sep 11 18:05:21.514: INFO: stdout: "\naffinity-clusterip-transition-r5b8s\naffinity-clusterip-transition-tt89l\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-r5b8s\naffinity-clusterip-transition-tt89l\naffinity-clusterip-transition-tt89l\naffinity-clusterip-transition-r5b8s\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-tt89l\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-r5b8s\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg"
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-r5b8s
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-tt89l
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-r5b8s
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-tt89l
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-tt89l
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-r5b8s
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-tt89l
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-r5b8s
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.514: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-8371 execpod-affinityqs9pd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.201.210:80/ ; done'
Sep 11 18:05:21.867: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.210:80/\n"
Sep 11 18:05:21.867: INFO: stdout: "\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg\naffinity-clusterip-transition-d6qsg"
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Received response from host: affinity-clusterip-transition-d6qsg
Sep 11 18:05:21.867: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8371, will wait for the garbage collector to delete the pods
Sep 11 18:05:21.954: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.312364ms
Sep 11 18:05:22.054: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.270632ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:05:33.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8371" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:19.062 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":85,"skipped":1494,"failed":0}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:05:33.342: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep 11 18:05:33.495: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:05:36.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1988" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":86,"skipped":1501,"failed":0}

------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:05:36.587: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Sep 11 18:05:36.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-8981'
Sep 11 18:05:37.183: INFO: stderr: ""
Sep 11 18:05:37.183: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 11 18:05:37.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8981'
Sep 11 18:05:37.309: INFO: stderr: ""
Sep 11 18:05:37.309: INFO: stdout: "update-demo-nautilus-7brfw update-demo-nautilus-hzqpj "
Sep 11 18:05:37.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-7brfw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8981'
Sep 11 18:05:37.403: INFO: stderr: ""
Sep 11 18:05:37.403: INFO: stdout: ""
Sep 11 18:05:37.403: INFO: update-demo-nautilus-7brfw is created but not running
Sep 11 18:05:42.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8981'
Sep 11 18:05:42.514: INFO: stderr: ""
Sep 11 18:05:42.514: INFO: stdout: "update-demo-nautilus-7brfw update-demo-nautilus-hzqpj "
Sep 11 18:05:42.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-7brfw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8981'
Sep 11 18:05:42.626: INFO: stderr: ""
Sep 11 18:05:42.626: INFO: stdout: "true"
Sep 11 18:05:42.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-7brfw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8981'
Sep 11 18:05:42.747: INFO: stderr: ""
Sep 11 18:05:42.747: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 11 18:05:42.747: INFO: validating pod update-demo-nautilus-7brfw
Sep 11 18:05:42.754: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 11 18:05:42.754: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 11 18:05:42.754: INFO: update-demo-nautilus-7brfw is verified up and running
Sep 11 18:05:42.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-hzqpj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8981'
Sep 11 18:05:42.856: INFO: stderr: ""
Sep 11 18:05:42.856: INFO: stdout: "true"
Sep 11 18:05:42.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-hzqpj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8981'
Sep 11 18:05:42.961: INFO: stderr: ""
Sep 11 18:05:42.961: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 11 18:05:42.961: INFO: validating pod update-demo-nautilus-hzqpj
Sep 11 18:05:42.975: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 11 18:05:42.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 11 18:05:42.975: INFO: update-demo-nautilus-hzqpj is verified up and running
STEP: using delete to clean up resources
Sep 11 18:05:42.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete --grace-period=0 --force -f - --namespace=kubectl-8981'
Sep 11 18:05:43.079: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 11 18:05:43.079: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 11 18:05:43.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8981'
Sep 11 18:05:43.203: INFO: stderr: "No resources found in kubectl-8981 namespace.\n"
Sep 11 18:05:43.203: INFO: stdout: ""
Sep 11 18:05:43.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -l name=update-demo --namespace=kubectl-8981 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 11 18:05:43.303: INFO: stderr: ""
Sep 11 18:05:43.303: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:05:43.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8981" for this suite.

â€¢ [SLOW TEST:6.745 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":87,"skipped":1501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:05:43.332: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 11 18:05:43.552: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6854 /api/v1/namespaces/watch-6854/configmaps/e2e-watch-test-label-changed 345fff63-5d19-49fc-8524-171ca2400168 2760989 0 2020-09-11 18:05:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-11 18:05:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 18:05:43.552: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6854 /api/v1/namespaces/watch-6854/configmaps/e2e-watch-test-label-changed 345fff63-5d19-49fc-8524-171ca2400168 2760992 0 2020-09-11 18:05:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-11 18:05:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 18:05:43.552: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6854 /api/v1/namespaces/watch-6854/configmaps/e2e-watch-test-label-changed 345fff63-5d19-49fc-8524-171ca2400168 2760995 0 2020-09-11 18:05:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-11 18:05:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 11 18:05:53.597: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6854 /api/v1/namespaces/watch-6854/configmaps/e2e-watch-test-label-changed 345fff63-5d19-49fc-8524-171ca2400168 2761070 0 2020-09-11 18:05:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-11 18:05:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 18:05:53.598: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6854 /api/v1/namespaces/watch-6854/configmaps/e2e-watch-test-label-changed 345fff63-5d19-49fc-8524-171ca2400168 2761071 0 2020-09-11 18:05:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-11 18:05:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 18:05:53.598: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6854 /api/v1/namespaces/watch-6854/configmaps/e2e-watch-test-label-changed 345fff63-5d19-49fc-8524-171ca2400168 2761072 0 2020-09-11 18:05:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-11 18:05:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:05:53.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6854" for this suite.

â€¢ [SLOW TEST:10.303 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":88,"skipped":1524,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:05:53.636: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:07:53.825: INFO: Deleting pod "var-expansion-3e3ba9e8-bd31-46b3-b31a-a19d0df5b9c6" in namespace "var-expansion-3898"
Sep 11 18:07:53.836: INFO: Wait up to 5m0s for pod "var-expansion-3e3ba9e8-bd31-46b3-b31a-a19d0df5b9c6" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:07:55.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3898" for this suite.

â€¢ [SLOW TEST:122.238 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":89,"skipped":1532,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:07:55.874: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4989
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Sep 11 18:07:58.050: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4989 PodName:var-expansion-7a9db914-9aa1-49e3-8e8e-6c82cfcc1440 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:07:58.050: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: test for file in mounted path
Sep 11 18:07:58.169: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4989 PodName:var-expansion-7a9db914-9aa1-49e3-8e8e-6c82cfcc1440 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:07:58.169: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: updating the annotation value
Sep 11 18:07:58.848: INFO: Successfully updated pod "var-expansion-7a9db914-9aa1-49e3-8e8e-6c82cfcc1440"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Sep 11 18:07:58.852: INFO: Deleting pod "var-expansion-7a9db914-9aa1-49e3-8e8e-6c82cfcc1440" in namespace "var-expansion-4989"
Sep 11 18:07:58.863: INFO: Wait up to 5m0s for pod "var-expansion-7a9db914-9aa1-49e3-8e8e-6c82cfcc1440" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:08:34.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4989" for this suite.

â€¢ [SLOW TEST:39.023 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":90,"skipped":1550,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:08:34.897: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-12977d18-b0bb-4026-855a-91b2358e6f47
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:08:35.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2217" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":91,"skipped":1557,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:08:35.103: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:08:35.255: INFO: Creating deployment "webserver-deployment"
Sep 11 18:08:35.263: INFO: Waiting for observed generation 1
Sep 11 18:08:37.271: INFO: Waiting for all required pods to come up
Sep 11 18:08:37.277: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 11 18:08:39.299: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 11 18:08:39.306: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 11 18:08:39.316: INFO: Updating deployment webserver-deployment
Sep 11 18:08:39.316: INFO: Waiting for observed generation 2
Sep 11 18:08:41.326: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 11 18:08:41.330: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 11 18:08:41.335: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 11 18:08:41.346: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 11 18:08:41.346: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 11 18:08:41.349: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 11 18:08:41.357: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 11 18:08:41.357: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 11 18:08:41.368: INFO: Updating deployment webserver-deployment
Sep 11 18:08:41.368: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 11 18:08:41.377: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 11 18:08:41.382: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep 11 18:08:41.420: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1094 /apis/apps/v1/namespaces/deployment-1094/deployments/webserver-deployment 3694bfbe-a15c-4e4d-9af5-4526be0e69ce 2762251 3 2020-09-11 18:08:35 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0068a40b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-09-11 18:08:39 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-09-11 18:08:41 +0000 UTC,LastTransitionTime:2020-09-11 18:08:41 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 11 18:08:41.442: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1094 /apis/apps/v1/namespaces/deployment-1094/replicasets/webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 2762245 3 2020-09-11 18:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3694bfbe-a15c-4e4d-9af5-4526be0e69ce 0xc0068a4577 0xc0068a4578}] []  [{kube-controller-manager Update apps/v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3694bfbe-a15c-4e4d-9af5-4526be0e69ce\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0068a4618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 11 18:08:41.442: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 11 18:08:41.442: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-1094 /apis/apps/v1/namespaces/deployment-1094/replicasets/webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 2762244 3 2020-09-11 18:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3694bfbe-a15c-4e4d-9af5-4526be0e69ce 0xc0068a4677 0xc0068a4678}] []  [{kube-controller-manager Update apps/v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3694bfbe-a15c-4e4d-9af5-4526be0e69ce\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0068a46e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 11 18:08:41.494: INFO: Pod "webserver-deployment-795d758f88-2pm9d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2pm9d webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-2pm9d e0ea019a-edea-40ea-aa61-e56b490da41c 2762210 0 2020-09-11 18:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.249.255/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0014c2b87 0xc0014c2b88}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 18:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 18:08:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.249.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-3-166.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.3.166,PodIP:192.168.249.255,StartTime:2020-09-11 18:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.249.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.494: INFO: Pod "webserver-deployment-795d758f88-4kn4j" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4kn4j webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-4kn4j 4c5da47f-1a80-4604-bf61-1c8e3164430b 2762259 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0014c2e57 0xc0014c2e58}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.494: INFO: Pod "webserver-deployment-795d758f88-5x4r6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5x4r6 webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-5x4r6 21ad838d-bf19-4a6f-bc3e-fed1870953ee 2762267 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0014c30f0 0xc0014c30f1}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.498: INFO: Pod "webserver-deployment-795d758f88-6hlq2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6hlq2 webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-6hlq2 6b32ead1-f69d-4e43-b358-e1b4d90f0400 2762263 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0014c3240 0xc0014c3241}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-3-166.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.498: INFO: Pod "webserver-deployment-795d758f88-c5nrt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c5nrt webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-c5nrt 3cc3f36b-b6d9-4565-8e14-03310d2908ed 2762248 0 2020-09-11 18:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.123.52/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0014c3507 0xc0014c3508}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-09-11 18:08:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-2-19.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.2.19,PodIP:,StartTime:2020-09-11 18:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.498: INFO: Pod "webserver-deployment-795d758f88-hxpg9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hxpg9 webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-hxpg9 3366d478-4b34-47fa-9afb-fc22c772692f 2762269 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0014c36b7 0xc0014c36b8}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.498: INFO: Pod "webserver-deployment-795d758f88-kbnxt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kbnxt webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-kbnxt 23aba672-4eee-4ff8-a5cb-0668655fb331 2762270 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0014c37f0 0xc0014c37f1}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.498: INFO: Pod "webserver-deployment-795d758f88-kv2cb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kv2cb webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-kv2cb 0bf68395-20f6-49aa-8fb8-6a3eae73a50c 2762268 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0014c3910 0xc0014c3911}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.498: INFO: Pod "webserver-deployment-795d758f88-pwt25" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pwt25 webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-pwt25 3fc1427d-b89d-48f7-b19d-af7f70ab626d 2762211 0 2020-09-11 18:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.123.43/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0014c3a30 0xc0014c3a31}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-09-11 18:08:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-09-11 18:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-2-19.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.2.19,PodIP:,StartTime:2020-09-11 18:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.498: INFO: Pod "webserver-deployment-795d758f88-pxbtx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pxbtx webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-pxbtx 6ae3b6fa-8a8f-43f6-8a13-d0d921e99c96 2762230 0 2020-09-11 18:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.134.66/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0014c3e07 0xc0014c3e08}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-09-11 18:08:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-1-141.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.141,PodIP:,StartTime:2020-09-11 18:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.499: INFO: Pod "webserver-deployment-795d758f88-vvkqp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vvkqp webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-vvkqp 318ea753-800d-47cc-82b3-12b700283394 2762258 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0007b4117 0xc0007b4118}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.500: INFO: Pod "webserver-deployment-795d758f88-x8q7m" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-x8q7m webserver-deployment-795d758f88- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-795d758f88-x8q7m e31f6f54-75d3-42a9-b5e0-8233e5841d7d 2762219 0 2020-09-11 18:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.134.68/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d8f3741f-4c1b-4229-a1a7-bdff69135453 0xc0007b4610 0xc0007b4611}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8f3741f-4c1b-4229-a1a7-bdff69135453\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 18:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 18:08:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.134.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-1-141.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.141,PodIP:192.168.134.68,StartTime:2020-09-11 18:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.134.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.500: INFO: Pod "webserver-deployment-dd94f59b7-4mxj6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4mxj6 webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-4mxj6 d32b6e63-ffbe-4528-b378-418cabb9d2a6 2762261 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0007b4e67 0xc0007b4e68}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.500: INFO: Pod "webserver-deployment-dd94f59b7-4pmmk" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4pmmk webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-4pmmk 5d9e76ee-c40f-4a73-a8e1-d650b0cb091b 2762066 0 2020-09-11 18:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.249.253/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0007b5160 0xc0007b5161}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 18:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 18:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.249.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-3-166.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.3.166,PodIP:192.168.249.253,StartTime:2020-09-11 18:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 18:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://05ea42525129fde32971a0f7fcc4719015718641e7897d177e60c342510bb6e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.249.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.500: INFO: Pod "webserver-deployment-dd94f59b7-6mdbb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6mdbb webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-6mdbb 1939ef28-e27b-424b-92a3-34abcb1985a2 2762264 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0007b5467 0xc0007b5468}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-2-19.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.502: INFO: Pod "webserver-deployment-dd94f59b7-8br7w" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8br7w webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-8br7w d70eff80-d3b7-461d-b33c-739fb1d69490 2762260 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0005ea657 0xc0005ea658}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.502: INFO: Pod "webserver-deployment-dd94f59b7-8q27t" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8q27t webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-8q27t fbe0e2ed-2419-4123-a1ab-e5116584375b 2762249 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0005eaa00 0xc0005eaa01}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-2-19.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.502: INFO: Pod "webserver-deployment-dd94f59b7-94tds" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-94tds webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-94tds 6f9c6afa-f630-4fff-b5fc-f1b40ce96c4a 2762256 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0005ead97 0xc0005ead98}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.502: INFO: Pod "webserver-deployment-dd94f59b7-h4hdp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-h4hdp webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-h4hdp b76f7504-af65-42b0-8b5d-169db3b9b87f 2762062 0 2020-09-11 18:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.249.201/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0005eaf50 0xc0005eaf51}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 18:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 18:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.249.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-3-166.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.3.166,PodIP:192.168.249.201,StartTime:2020-09-11 18:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 18:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ec1fec0064dce9fb43477a43be263c4bc4808de82701bb74c85387c33518c355,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.249.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.502: INFO: Pod "webserver-deployment-dd94f59b7-j5j77" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-j5j77 webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-j5j77 7f13e579-312f-4cf3-b4aa-2f0617781fe9 2762070 0 2020-09-11 18:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.134.67/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0005eb2b7 0xc0005eb2b8}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 18:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 18:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.134.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-1-141.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.141,PodIP:192.168.134.67,StartTime:2020-09-11 18:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 18:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ef982c998314fcc01484e26795b27110fe30a30dfbbe0fc3c88639aeddd94fc7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.134.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.512: INFO: Pod "webserver-deployment-dd94f59b7-kpn8r" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kpn8r webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-kpn8r d8467458-9a99-4d3c-a555-06408ce3aaa1 2762266 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0005eb547 0xc0005eb548}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.512: INFO: Pod "webserver-deployment-dd94f59b7-mqb9h" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mqb9h webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-mqb9h 1a9c3341-6543-405f-8a17-1e2dbde0bb58 2762114 0 2020-09-11 18:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.123.44/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0005ebb90 0xc0005ebb91}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 18:08:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 18:08:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.123.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-2-19.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.2.19,PodIP:192.168.123.44,StartTime:2020-09-11 18:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 18:08:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1974b622e9308ffe84253cb526e21e4194943b507d8428b379e13417edb33415,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.123.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.512: INFO: Pod "webserver-deployment-dd94f59b7-rrg8h" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rrg8h webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-rrg8h a92a4988-69cc-4bdf-ac96-dae105a0ff29 2762117 0 2020-09-11 18:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.123.47/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc0003b9997 0xc0003b9998}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 18:08:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 18:08:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.123.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-2-19.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.2.19,PodIP:192.168.123.47,StartTime:2020-09-11 18:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 18:08:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://37232120acf0852f69215de54b6a68fc603847deeb8af96bd45810114ddd27e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.123.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.514: INFO: Pod "webserver-deployment-dd94f59b7-srlpb" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-srlpb webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-srlpb a5c4376a-7694-4d5d-96f8-3348c0409523 2762072 0 2020-09-11 18:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.134.127/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc000286117 0xc000286118}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 18:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 18:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.134.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-1-141.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.141,PodIP:192.168.134.127,StartTime:2020-09-11 18:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 18:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://452d4d78d33753a2bfb6425474292e36a789f070d90c09b121e4c2973f44f772,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.134.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.514: INFO: Pod "webserver-deployment-dd94f59b7-w5vkc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w5vkc webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-w5vkc 970bf2b8-8f20-4890-a9b6-ea0479165f41 2762262 0 2020-09-11 18:08:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc000a8ab57 0xc000a8ab58}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.514: INFO: Pod "webserver-deployment-dd94f59b7-wrfvr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wrfvr webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-wrfvr d342ad19-59eb-4366-8c79-64bd5d78f171 2762077 0 2020-09-11 18:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.134.126/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc000a8b290 0xc000a8b291}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 18:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 18:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.134.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-1-141.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.141,PodIP:192.168.134.126,StartTime:2020-09-11 18:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 18:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9787ac2a91f4bbf4986a2b5059cc24873b7ef59c5c7b3696e347e16f2b200bc9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.134.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 18:08:41.515: INFO: Pod "webserver-deployment-dd94f59b7-z8rh5" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z8rh5 webserver-deployment-dd94f59b7- deployment-1094 /api/v1/namespaces/deployment-1094/pods/webserver-deployment-dd94f59b7-z8rh5 93929883-6ed5-4e68-9cdf-1d73710938b8 2762059 0 2020-09-11 18:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.249.194/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 24fc927f-a4c9-4dfc-addf-a5f22a081788 0xc000a8b8c7 0xc000a8b8c8}] []  [{kube-controller-manager Update v1 2020-09-11 18:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24fc927f-a4c9-4dfc-addf-a5f22a081788\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-09-11 18:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-09-11 18:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.249.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cnjjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cnjjl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cnjjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-3-166.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.3.166,PodIP:192.168.249.194,StartTime:2020-09-11 18:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-11 18:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f84af6ab43d5969900dca2832551af2234947c62ac23b491c6386cc5ae10ee5c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.249.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:08:41.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1094" for this suite.

â€¢ [SLOW TEST:6.486 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":92,"skipped":1588,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:08:41.590: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:08:54.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1964" for this suite.

â€¢ [SLOW TEST:13.324 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":93,"skipped":1593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:08:54.914: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:08:55.632: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:08:58.679: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:08:58.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1390" for this suite.
STEP: Destroying namespace "webhook-1390-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":94,"skipped":1652,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:08:58.940: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-39ba608f-8dff-4ca8-8bb9-88f059b65de0
STEP: Creating a pod to test consume secrets
Sep 11 18:08:59.128: INFO: Waiting up to 5m0s for pod "pod-secrets-2013845a-e9f8-4121-ae22-9a4fc8691b4e" in namespace "secrets-5447" to be "Succeeded or Failed"
Sep 11 18:08:59.135: INFO: Pod "pod-secrets-2013845a-e9f8-4121-ae22-9a4fc8691b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.824897ms
Sep 11 18:09:01.140: INFO: Pod "pod-secrets-2013845a-e9f8-4121-ae22-9a4fc8691b4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012557782s
STEP: Saw pod success
Sep 11 18:09:01.140: INFO: Pod "pod-secrets-2013845a-e9f8-4121-ae22-9a4fc8691b4e" satisfied condition "Succeeded or Failed"
Sep 11 18:09:01.144: INFO: Trying to get logs from node ip-10-10-1-141.ec2.internal pod pod-secrets-2013845a-e9f8-4121-ae22-9a4fc8691b4e container secret-env-test: <nil>
STEP: delete the pod
Sep 11 18:09:01.207: INFO: Waiting for pod pod-secrets-2013845a-e9f8-4121-ae22-9a4fc8691b4e to disappear
Sep 11 18:09:01.213: INFO: Pod pod-secrets-2013845a-e9f8-4121-ae22-9a4fc8691b4e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:09:01.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5447" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1660,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:09:01.238: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-27
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-27
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-27
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-27
Sep 11 18:09:01.435: INFO: Found 0 stateful pods, waiting for 1
Sep 11 18:09:11.440: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 11 18:09:11.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-27 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 11 18:09:11.700: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 11 18:09:11.700: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 11 18:09:11.700: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 11 18:09:11.713: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 11 18:09:11.713: INFO: Waiting for statefulset status.replicas updated to 0
Sep 11 18:09:11.785: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Sep 11 18:09:11.785: INFO: ss-0  ip-10-10-2-19.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:11.785: INFO: 
Sep 11 18:09:11.785: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 11 18:09:12.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98294525s
Sep 11 18:09:13.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978376648s
Sep 11 18:09:14.800: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972720293s
Sep 11 18:09:15.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967835323s
Sep 11 18:09:16.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963061473s
Sep 11 18:09:17.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957712552s
Sep 11 18:09:18.823: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950224785s
Sep 11 18:09:19.828: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944931384s
Sep 11 18:09:20.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 939.818157ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-27
Sep 11 18:09:21.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-27 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 11 18:09:22.107: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 11 18:09:22.107: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 11 18:09:22.107: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 11 18:09:22.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-27 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 11 18:09:22.325: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 11 18:09:22.325: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 11 18:09:22.325: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 11 18:09:22.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-27 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 11 18:09:22.654: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 11 18:09:22.654: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 11 18:09:22.654: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 11 18:09:22.659: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep 11 18:09:32.665: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:09:32.665: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:09:32.665: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 11 18:09:32.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-27 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 11 18:09:32.977: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 11 18:09:32.977: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 11 18:09:32.977: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 11 18:09:32.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-27 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 11 18:09:33.208: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 11 18:09:33.208: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 11 18:09:33.208: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 11 18:09:33.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-27 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 11 18:09:33.460: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 11 18:09:33.460: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 11 18:09:33.461: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 11 18:09:33.461: INFO: Waiting for statefulset status.replicas updated to 0
Sep 11 18:09:33.464: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 11 18:09:43.473: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 11 18:09:43.474: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 11 18:09:43.474: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 11 18:09:43.490: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Sep 11 18:09:43.490: INFO: ss-0  ip-10-10-2-19.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:43.490: INFO: ss-1  ip-10-10-3-166.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  }]
Sep 11 18:09:43.490: INFO: ss-2  ip-10-10-1-141.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  }]
Sep 11 18:09:43.490: INFO: 
Sep 11 18:09:43.490: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 11 18:09:44.495: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Sep 11 18:09:44.495: INFO: ss-0  ip-10-10-2-19.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:44.495: INFO: ss-1  ip-10-10-3-166.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  }]
Sep 11 18:09:44.495: INFO: ss-2  ip-10-10-1-141.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  }]
Sep 11 18:09:44.495: INFO: 
Sep 11 18:09:44.495: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 11 18:09:45.500: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Sep 11 18:09:45.500: INFO: ss-0  ip-10-10-2-19.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:45.500: INFO: ss-2  ip-10-10-1-141.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  }]
Sep 11 18:09:45.500: INFO: 
Sep 11 18:09:45.500: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 11 18:09:46.506: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Sep 11 18:09:46.506: INFO: ss-0  ip-10-10-2-19.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:46.506: INFO: ss-2  ip-10-10-1-141.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  }]
Sep 11 18:09:46.506: INFO: 
Sep 11 18:09:46.506: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 11 18:09:47.511: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Sep 11 18:09:47.511: INFO: ss-0  ip-10-10-2-19.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:47.511: INFO: ss-2  ip-10-10-1-141.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  }]
Sep 11 18:09:47.511: INFO: 
Sep 11 18:09:47.511: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 11 18:09:48.517: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Sep 11 18:09:48.517: INFO: ss-0  ip-10-10-2-19.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:48.517: INFO: ss-2  ip-10-10-1-141.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  }]
Sep 11 18:09:48.517: INFO: 
Sep 11 18:09:48.517: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 11 18:09:49.522: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Sep 11 18:09:49.522: INFO: ss-0  ip-10-10-2-19.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:49.522: INFO: ss-2  ip-10-10-1-141.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  }]
Sep 11 18:09:49.522: INFO: 
Sep 11 18:09:49.522: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 11 18:09:50.528: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Sep 11 18:09:50.528: INFO: ss-0  ip-10-10-2-19.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:50.528: INFO: ss-2  ip-10-10-1-141.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:11 +0000 UTC  }]
Sep 11 18:09:50.528: INFO: 
Sep 11 18:09:50.528: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 11 18:09:51.533: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Sep 11 18:09:51.533: INFO: ss-0  ip-10-10-2-19.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:51.533: INFO: 
Sep 11 18:09:51.533: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 11 18:09:52.539: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Sep 11 18:09:52.539: INFO: ss-0  ip-10-10-2-19.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-11 18:09:01 +0000 UTC  }]
Sep 11 18:09:52.540: INFO: 
Sep 11 18:09:52.540: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-27
Sep 11 18:09:53.546: INFO: Scaling statefulset ss to 0
Sep 11 18:09:53.558: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 11 18:09:53.562: INFO: Deleting all statefulset in ns statefulset-27
Sep 11 18:09:53.566: INFO: Scaling statefulset ss to 0
Sep 11 18:09:53.577: INFO: Waiting for statefulset status.replicas updated to 0
Sep 11 18:09:53.581: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:09:53.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-27" for this suite.

â€¢ [SLOW TEST:52.396 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":96,"skipped":1666,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:09:53.635: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0911 18:10:03.837498      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 11 18:11:05.867: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:11:05.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5109" for this suite.

â€¢ [SLOW TEST:72.277 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":97,"skipped":1678,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:11:05.912: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:11:17.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6438" for this suite.

â€¢ [SLOW TEST:11.241 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":98,"skipped":1700,"failed":0}
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:11:17.153: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1744
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:11:17.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1744" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":99,"skipped":1700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:11:17.414: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5513
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-3ee17c9a-97f2-48d0-b826-0b8a275298e6
STEP: Creating a pod to test consume configMaps
Sep 11 18:11:17.590: INFO: Waiting up to 5m0s for pod "pod-configmaps-3273f6cd-de02-473b-ba10-0513b5e58e44" in namespace "configmap-5513" to be "Succeeded or Failed"
Sep 11 18:11:17.595: INFO: Pod "pod-configmaps-3273f6cd-de02-473b-ba10-0513b5e58e44": Phase="Pending", Reason="", readiness=false. Elapsed: 5.02989ms
Sep 11 18:11:19.600: INFO: Pod "pod-configmaps-3273f6cd-de02-473b-ba10-0513b5e58e44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009848579s
STEP: Saw pod success
Sep 11 18:11:19.600: INFO: Pod "pod-configmaps-3273f6cd-de02-473b-ba10-0513b5e58e44" satisfied condition "Succeeded or Failed"
Sep 11 18:11:19.604: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-configmaps-3273f6cd-de02-473b-ba10-0513b5e58e44 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 18:11:19.649: INFO: Waiting for pod pod-configmaps-3273f6cd-de02-473b-ba10-0513b5e58e44 to disappear
Sep 11 18:11:19.654: INFO: Pod pod-configmaps-3273f6cd-de02-473b-ba10-0513b5e58e44 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:11:19.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5513" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":100,"skipped":1739,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:11:19.685: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Sep 11 18:11:19.847: INFO: Waiting up to 5m0s for pod "var-expansion-3a34d7fd-622b-48de-abd8-160ee323ea0a" in namespace "var-expansion-1243" to be "Succeeded or Failed"
Sep 11 18:11:19.855: INFO: Pod "var-expansion-3a34d7fd-622b-48de-abd8-160ee323ea0a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.19698ms
Sep 11 18:11:21.860: INFO: Pod "var-expansion-3a34d7fd-622b-48de-abd8-160ee323ea0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013106685s
STEP: Saw pod success
Sep 11 18:11:21.860: INFO: Pod "var-expansion-3a34d7fd-622b-48de-abd8-160ee323ea0a" satisfied condition "Succeeded or Failed"
Sep 11 18:11:21.864: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod var-expansion-3a34d7fd-622b-48de-abd8-160ee323ea0a container dapi-container: <nil>
STEP: delete the pod
Sep 11 18:11:21.893: INFO: Waiting for pod var-expansion-3a34d7fd-622b-48de-abd8-160ee323ea0a to disappear
Sep 11 18:11:21.897: INFO: Pod var-expansion-3a34d7fd-622b-48de-abd8-160ee323ea0a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:11:21.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1243" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":101,"skipped":1751,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:11:21.930: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-f9c4d076-663b-47f4-b744-4c0aa3a00aec in namespace container-probe-2439
Sep 11 18:11:26.105: INFO: Started pod busybox-f9c4d076-663b-47f4-b744-4c0aa3a00aec in namespace container-probe-2439
STEP: checking the pod's current state and verifying that restartCount is present
Sep 11 18:11:26.112: INFO: Initial restart count of pod busybox-f9c4d076-663b-47f4-b744-4c0aa3a00aec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:15:27.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2439" for this suite.

â€¢ [SLOW TEST:245.274 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":102,"skipped":1752,"failed":0}
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:15:27.204: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1707
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1707
STEP: creating service affinity-nodeport-transition in namespace services-1707
STEP: creating replication controller affinity-nodeport-transition in namespace services-1707
I0911 18:15:27.404507      19 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-1707, replica count: 3
I0911 18:15:30.463001      19 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 18:15:30.476: INFO: Creating new exec pod
Sep 11 18:15:33.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1707 execpod-affinity86cf6 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Sep 11 18:15:33.944: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep 11 18:15:33.944: INFO: stdout: ""
Sep 11 18:15:33.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1707 execpod-affinity86cf6 -- /bin/sh -x -c nc -zv -t -w 2 10.107.22.250 80'
Sep 11 18:15:34.220: INFO: stderr: "+ nc -zv -t -w 2 10.107.22.250 80\nConnection to 10.107.22.250 80 port [tcp/http] succeeded!\n"
Sep 11 18:15:34.220: INFO: stdout: ""
Sep 11 18:15:34.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1707 execpod-affinity86cf6 -- /bin/sh -x -c nc -zv -t -w 2 10.10.3.166 31123'
Sep 11 18:15:34.496: INFO: stderr: "+ nc -zv -t -w 2 10.10.3.166 31123\nConnection to 10.10.3.166 31123 port [tcp/31123] succeeded!\n"
Sep 11 18:15:34.496: INFO: stdout: ""
Sep 11 18:15:34.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1707 execpod-affinity86cf6 -- /bin/sh -x -c nc -zv -t -w 2 10.10.2.19 31123'
Sep 11 18:15:34.721: INFO: stderr: "+ nc -zv -t -w 2 10.10.2.19 31123\nConnection to 10.10.2.19 31123 port [tcp/31123] succeeded!\n"
Sep 11 18:15:34.721: INFO: stdout: ""
Sep 11 18:15:34.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1707 execpod-affinity86cf6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.141:31123/ ; done'
Sep 11 18:15:35.127: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n"
Sep 11 18:15:35.128: INFO: stdout: "\naffinity-nodeport-transition-b4fz5\naffinity-nodeport-transition-b4fz5\naffinity-nodeport-transition-b4fz5\naffinity-nodeport-transition-68zcs\naffinity-nodeport-transition-68zcs\naffinity-nodeport-transition-68zcs\naffinity-nodeport-transition-b4fz5\naffinity-nodeport-transition-68zcs\naffinity-nodeport-transition-68zcs\naffinity-nodeport-transition-68zcs\naffinity-nodeport-transition-b4fz5\naffinity-nodeport-transition-b4fz5\naffinity-nodeport-transition-b4fz5\naffinity-nodeport-transition-68zcs\naffinity-nodeport-transition-68zcs\naffinity-nodeport-transition-b4fz5"
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-b4fz5
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-b4fz5
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-b4fz5
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-68zcs
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-68zcs
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-68zcs
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-b4fz5
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-68zcs
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-68zcs
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-68zcs
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-b4fz5
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-b4fz5
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-b4fz5
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-68zcs
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-68zcs
Sep 11 18:15:35.128: INFO: Received response from host: affinity-nodeport-transition-b4fz5
Sep 11 18:15:35.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1707 execpod-affinity86cf6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.141:31123/ ; done'
Sep 11 18:15:35.512: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:31123/\n"
Sep 11 18:15:35.512: INFO: stdout: "\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f\naffinity-nodeport-transition-fw79f"
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Received response from host: affinity-nodeport-transition-fw79f
Sep 11 18:15:35.512: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1707, will wait for the garbage collector to delete the pods
Sep 11 18:15:35.607: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.349899ms
Sep 11 18:15:36.707: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 1.100268166s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:15:51.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1707" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:23.901 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":103,"skipped":1752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:15:51.105: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 11 18:15:57.370: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 11 18:15:57.374: INFO: Pod pod-with-poststart-http-hook still exists
Sep 11 18:15:59.375: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 11 18:15:59.380: INFO: Pod pod-with-poststart-http-hook still exists
Sep 11 18:16:01.375: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 11 18:16:01.380: INFO: Pod pod-with-poststart-http-hook still exists
Sep 11 18:16:03.375: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 11 18:16:03.382: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:16:03.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2105" for this suite.

â€¢ [SLOW TEST:12.305 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":104,"skipped":1775,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:16:03.410: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-e221a6ca-967b-4a59-bdaf-3c1e25c1bb30
STEP: Creating a pod to test consume secrets
Sep 11 18:16:03.602: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-49b24483-c44e-4874-99fa-b1c333ea1fa0" in namespace "projected-741" to be "Succeeded or Failed"
Sep 11 18:16:03.605: INFO: Pod "pod-projected-secrets-49b24483-c44e-4874-99fa-b1c333ea1fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78837ms
Sep 11 18:16:05.611: INFO: Pod "pod-projected-secrets-49b24483-c44e-4874-99fa-b1c333ea1fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009655038s
Sep 11 18:16:07.621: INFO: Pod "pod-projected-secrets-49b24483-c44e-4874-99fa-b1c333ea1fa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018954466s
STEP: Saw pod success
Sep 11 18:16:07.621: INFO: Pod "pod-projected-secrets-49b24483-c44e-4874-99fa-b1c333ea1fa0" satisfied condition "Succeeded or Failed"
Sep 11 18:16:07.630: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-secrets-49b24483-c44e-4874-99fa-b1c333ea1fa0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 11 18:16:07.661: INFO: Waiting for pod pod-projected-secrets-49b24483-c44e-4874-99fa-b1c333ea1fa0 to disappear
Sep 11 18:16:07.665: INFO: Pod pod-projected-secrets-49b24483-c44e-4874-99fa-b1c333ea1fa0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:16:07.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-741" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":105,"skipped":1780,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:16:07.689: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:16:07.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4495" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
â€¢{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":106,"skipped":1791,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:16:07.886: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1281
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-402a3c9f-18f9-451e-9702-d6921fa2ec79
STEP: Creating a pod to test consume configMaps
Sep 11 18:16:08.071: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3512687-71dd-49ae-9a11-404e6ed6e8aa" in namespace "configmap-1281" to be "Succeeded or Failed"
Sep 11 18:16:08.075: INFO: Pod "pod-configmaps-f3512687-71dd-49ae-9a11-404e6ed6e8aa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.735606ms
Sep 11 18:16:10.079: INFO: Pod "pod-configmaps-f3512687-71dd-49ae-9a11-404e6ed6e8aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.007994353s
Sep 11 18:16:12.084: INFO: Pod "pod-configmaps-f3512687-71dd-49ae-9a11-404e6ed6e8aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012916492s
STEP: Saw pod success
Sep 11 18:16:12.084: INFO: Pod "pod-configmaps-f3512687-71dd-49ae-9a11-404e6ed6e8aa" satisfied condition "Succeeded or Failed"
Sep 11 18:16:12.088: INFO: Trying to get logs from node ip-10-10-3-166.ec2.internal pod pod-configmaps-f3512687-71dd-49ae-9a11-404e6ed6e8aa container configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 18:16:12.126: INFO: Waiting for pod pod-configmaps-f3512687-71dd-49ae-9a11-404e6ed6e8aa to disappear
Sep 11 18:16:12.131: INFO: Pod pod-configmaps-f3512687-71dd-49ae-9a11-404e6ed6e8aa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:16:12.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1281" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1828,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:16:12.159: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3398
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 11 18:16:12.309: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:16:18.188: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:16:39.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3398" for this suite.

â€¢ [SLOW TEST:27.400 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":108,"skipped":1853,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:16:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-5579
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:16:39.717: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Creating first CR 
Sep 11 18:16:40.301: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-11T18:16:40Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-11T18:16:40Z]] name:name1 resourceVersion:2765962 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:7351deae-1ac8-4922-acb6-6a4bf0353d54] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 11 18:16:50.309: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-11T18:16:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-11T18:16:50Z]] name:name2 resourceVersion:2766014 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ad67a639-ec8a-43b8-8eb6-cf4b3ca9c454] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 11 18:17:00.317: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-11T18:16:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-11T18:17:00Z]] name:name1 resourceVersion:2766062 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:7351deae-1ac8-4922-acb6-6a4bf0353d54] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 11 18:17:10.325: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-11T18:16:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-11T18:17:10Z]] name:name2 resourceVersion:2766111 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ad67a639-ec8a-43b8-8eb6-cf4b3ca9c454] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 11 18:17:20.340: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-11T18:16:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-11T18:17:00Z]] name:name1 resourceVersion:2766159 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:7351deae-1ac8-4922-acb6-6a4bf0353d54] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 11 18:17:30.353: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-11T18:16:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-11T18:17:10Z]] name:name2 resourceVersion:2766208 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ad67a639-ec8a-43b8-8eb6-cf4b3ca9c454] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:17:40.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5579" for this suite.

â€¢ [SLOW TEST:61.346 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":109,"skipped":1870,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:17:40.905: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:17:41.106: INFO: Creating deployment "test-recreate-deployment"
Sep 11 18:17:41.135: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 11 18:17:41.144: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 11 18:17:43.153: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 11 18:17:43.157: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445061, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445061, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445061, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445061, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 11 18:17:45.162: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 11 18:17:45.173: INFO: Updating deployment test-recreate-deployment
Sep 11 18:17:45.173: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep 11 18:17:45.277: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3219 /apis/apps/v1/namespaces/deployment-3219/deployments/test-recreate-deployment 9aca6dcb-a6e2-4c94-a154-e6023502d11f 2766330 2 2020-09-11 18:17:41 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-09-11 18:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-09-11 18:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005652af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-09-11 18:17:45 +0000 UTC,LastTransitionTime:2020-09-11 18:17:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-09-11 18:17:45 +0000 UTC,LastTransitionTime:2020-09-11 18:17:41 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 11 18:17:45.281: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-3219 /apis/apps/v1/namespaces/deployment-3219/replicasets/test-recreate-deployment-f79dd4667 f5410cb6-a0e9-4e3e-a952-58011fd0444d 2766329 1 2020-09-11 18:17:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9aca6dcb-a6e2-4c94-a154-e6023502d11f 0xc005653200 0xc005653201}] []  [{kube-controller-manager Update apps/v1 2020-09-11 18:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9aca6dcb-a6e2-4c94-a154-e6023502d11f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005653308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 11 18:17:45.281: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 11 18:17:45.281: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-3219 /apis/apps/v1/namespaces/deployment-3219/replicasets/test-recreate-deployment-c96cf48f dc699ec0-0774-48f1-93c1-8ab1ecac4171 2766317 2 2020-09-11 18:17:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9aca6dcb-a6e2-4c94-a154-e6023502d11f 0xc0056530af 0xc0056530c0}] []  [{kube-controller-manager Update apps/v1 2020-09-11 18:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9aca6dcb-a6e2-4c94-a154-e6023502d11f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005653148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 11 18:17:45.286: INFO: Pod "test-recreate-deployment-f79dd4667-74n8f" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-74n8f test-recreate-deployment-f79dd4667- deployment-3219 /api/v1/namespaces/deployment-3219/pods/test-recreate-deployment-f79dd4667-74n8f 01d949b2-b9a6-41a6-8a5b-326bc0d1709d 2766331 0 2020-09-11 18:17:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 f5410cb6-a0e9-4e3e-a952-58011fd0444d 0xc005674ce0 0xc005674ce1}] []  [{kube-controller-manager Update v1 2020-09-11 18:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5410cb6-a0e9-4e3e-a952-58011fd0444d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-09-11 18:17:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zf6j8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zf6j8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zf6j8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-10-2-19.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:17:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:17:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:17:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-11 18:17:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.2.19,PodIP:,StartTime:2020-09-11 18:17:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:17:45.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3219" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":110,"skipped":1881,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:17:45.318: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1870
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f4f30693-f3ba-4641-842f-f1228dc36440
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f4f30693-f3ba-4641-842f-f1228dc36440
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:19:18.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1870" for this suite.

â€¢ [SLOW TEST:92.915 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":111,"skipped":1899,"failed":0}
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:19:18.233: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-5436
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 11 18:19:19.184: INFO: starting watch
STEP: patching
STEP: updating
Sep 11 18:19:19.203: INFO: waiting for watch events with expected annotations
Sep 11 18:19:19.203: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:19:19.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5436" for this suite.
â€¢{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":112,"skipped":1900,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:19:19.319: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-5934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep 11 18:19:19.478: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 11 18:20:19.530: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:20:19.536: INFO: Starting informer...
STEP: Starting pods...
Sep 11 18:20:19.763: INFO: Pod1 is running on ip-10-10-2-19.ec2.internal. Tainting Node
Sep 11 18:20:21.987: INFO: Pod2 is running on ip-10-10-2-19.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep 11 18:20:31.810: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 11 18:20:49.229: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:20:49.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5934" for this suite.

â€¢ [SLOW TEST:90.004 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":113,"skipped":1903,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:20:49.323: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep 11 18:20:49.519: INFO: Waiting up to 5m0s for pod "downward-api-d481986c-9234-4025-8d47-8e5f84ccdfee" in namespace "downward-api-8002" to be "Succeeded or Failed"
Sep 11 18:20:49.525: INFO: Pod "downward-api-d481986c-9234-4025-8d47-8e5f84ccdfee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.718651ms
Sep 11 18:20:51.530: INFO: Pod "downward-api-d481986c-9234-4025-8d47-8e5f84ccdfee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01092601s
Sep 11 18:20:53.535: INFO: Pod "downward-api-d481986c-9234-4025-8d47-8e5f84ccdfee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015724343s
STEP: Saw pod success
Sep 11 18:20:53.535: INFO: Pod "downward-api-d481986c-9234-4025-8d47-8e5f84ccdfee" satisfied condition "Succeeded or Failed"
Sep 11 18:20:53.539: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downward-api-d481986c-9234-4025-8d47-8e5f84ccdfee container dapi-container: <nil>
STEP: delete the pod
Sep 11 18:20:53.575: INFO: Waiting for pod downward-api-d481986c-9234-4025-8d47-8e5f84ccdfee to disappear
Sep 11 18:20:53.579: INFO: Pod downward-api-d481986c-9234-4025-8d47-8e5f84ccdfee no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:20:53.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8002" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":1916,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:20:53.612: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-m6dd
STEP: Creating a pod to test atomic-volume-subpath
Sep 11 18:20:53.797: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-m6dd" in namespace "subpath-4357" to be "Succeeded or Failed"
Sep 11 18:20:53.801: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146735ms
Sep 11 18:20:55.806: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009020843s
Sep 11 18:20:57.811: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Running", Reason="", readiness=true. Elapsed: 4.014256201s
Sep 11 18:20:59.816: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Running", Reason="", readiness=true. Elapsed: 6.018973105s
Sep 11 18:21:01.821: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Running", Reason="", readiness=true. Elapsed: 8.024377473s
Sep 11 18:21:03.826: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Running", Reason="", readiness=true. Elapsed: 10.029221367s
Sep 11 18:21:05.888: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Running", Reason="", readiness=true. Elapsed: 12.090752018s
Sep 11 18:21:07.895: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Running", Reason="", readiness=true. Elapsed: 14.098072391s
Sep 11 18:21:09.900: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Running", Reason="", readiness=true. Elapsed: 16.103089852s
Sep 11 18:21:11.905: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Running", Reason="", readiness=true. Elapsed: 18.10865442s
Sep 11 18:21:13.910: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Running", Reason="", readiness=true. Elapsed: 20.113564418s
Sep 11 18:21:15.916: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Running", Reason="", readiness=true. Elapsed: 22.118759637s
Sep 11 18:21:17.920: INFO: Pod "pod-subpath-test-projected-m6dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.123494202s
STEP: Saw pod success
Sep 11 18:21:17.920: INFO: Pod "pod-subpath-test-projected-m6dd" satisfied condition "Succeeded or Failed"
Sep 11 18:21:17.924: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-subpath-test-projected-m6dd container test-container-subpath-projected-m6dd: <nil>
STEP: delete the pod
Sep 11 18:21:17.952: INFO: Waiting for pod pod-subpath-test-projected-m6dd to disappear
Sep 11 18:21:17.956: INFO: Pod pod-subpath-test-projected-m6dd no longer exists
STEP: Deleting pod pod-subpath-test-projected-m6dd
Sep 11 18:21:17.956: INFO: Deleting pod "pod-subpath-test-projected-m6dd" in namespace "subpath-4357"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:21:17.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4357" for this suite.

â€¢ [SLOW TEST:24.375 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":115,"skipped":1929,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:21:17.988: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:161
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:21:18.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9233" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":116,"skipped":1932,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:21:18.228: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3048
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 11 18:21:18.398: INFO: Waiting up to 5m0s for pod "pod-1b7d7649-5c4f-4c9d-874e-a8777b53eb3f" in namespace "emptydir-3048" to be "Succeeded or Failed"
Sep 11 18:21:18.404: INFO: Pod "pod-1b7d7649-5c4f-4c9d-874e-a8777b53eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.302871ms
Sep 11 18:21:20.409: INFO: Pod "pod-1b7d7649-5c4f-4c9d-874e-a8777b53eb3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010852657s
STEP: Saw pod success
Sep 11 18:21:20.409: INFO: Pod "pod-1b7d7649-5c4f-4c9d-874e-a8777b53eb3f" satisfied condition "Succeeded or Failed"
Sep 11 18:21:20.415: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-1b7d7649-5c4f-4c9d-874e-a8777b53eb3f container test-container: <nil>
STEP: delete the pod
Sep 11 18:21:20.444: INFO: Waiting for pod pod-1b7d7649-5c4f-4c9d-874e-a8777b53eb3f to disappear
Sep 11 18:21:20.451: INFO: Pod pod-1b7d7649-5c4f-4c9d-874e-a8777b53eb3f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:21:20.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3048" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":117,"skipped":1937,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:21:20.486: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 11 18:21:20.658: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 11 18:21:20.682: INFO: Waiting for terminating namespaces to be deleted...
Sep 11 18:21:20.687: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-1-141.ec2.internal before test
Sep 11 18:21:20.699: INFO: calico-node-jgr6s from kube-system started at 2020-08-31 21:04:41 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 18:21:20.699: INFO: kube-proxy-72jmn from kube-system started at 2020-08-31 21:04:41 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-cert-manager-7b65d9c9d6-jst49 from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-flux-memcached-d75b6d9d6-vpnqm from md-flux started at 2020-09-11 16:47:51 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container memcached ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-gangway-66c5d44b4c-f7psj from md-gangway started at 2020-09-11 16:47:59 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container gangway ready: true, restart count 0
Sep 11 18:21:20.699: INFO: harbor-pg-1 from md-harbor started at 2020-09-11 16:49:13 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container postgres ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-harbor-harbor-notary-server-ddbf4b74b-69jlj from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container notary-server ready: true, restart count 1
Sep 11 18:21:20.699: INFO: md-harbor-harbor-portal-7976969b-27bjm from md-harbor started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container portal ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-harbor-harbor-registry-9cdb888bc-97n4b from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container registry ready: true, restart count 0
Sep 11 18:21:20.699: INFO: 	Container registryctl ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-mlqtr from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-harbor-redis-redis-ha-server-2 from md-harbor started at 2020-09-11 16:49:22 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container redis ready: true, restart count 0
Sep 11 18:21:20.699: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-helm-operator-557b9b587c-vmkw8 from md-helm-operator started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container flux-helm-operator ready: false, restart count 1
Sep 11 18:21:20.699: INFO: md-kubedb-84d9fff66c-6922k from md-kubedb started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container operator ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-loki-stack-0 from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container loki ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-loki-stack-promtail-hm9cx from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container promtail ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-nginx-ingress-controller-dfxrw from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 18:21:20.699: INFO: alertmanager-md-prometheus-operator-alertmanager-0 from md-prometheus-operator started at 2020-09-11 16:49:13 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container alertmanager ready: true, restart count 0
Sep 11 18:21:20.699: INFO: 	Container config-reloader ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-prometheus-operator-kube-state-metrics-6f5d54b49-mtqmh from md-prometheus-operator started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-prometheus-operator-operator-7d5c6f8689-tfzwv from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 11 18:21:20.699: INFO: 	Container tls-proxy ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-prometheus-operator-prometheus-node-exporter-6sz5n from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 18:21:20.699: INFO: md-velero-6cc8bd66b5-rqkv9 from md-velero started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container velero ready: true, restart count 0
Sep 11 18:21:20.699: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-ps57h from sonobuoy started at 2020-09-11 17:37:25 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:20.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 18:21:20.699: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 11 18:21:20.699: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-2-19.ec2.internal before test
Sep 11 18:21:20.712: INFO: calico-node-w9tmq from kube-system started at 2020-08-31 21:04:43 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.712: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 18:21:20.712: INFO: kube-proxy-djv8h from kube-system started at 2020-08-31 21:04:43 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.712: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 18:21:20.712: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-5w2c8 from md-harbor started at 2020-09-11 18:20:54 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.712: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 18:21:20.712: INFO: md-harbor-redis-redis-ha-server-0 from md-harbor started at 2020-09-11 18:20:58 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:20.712: INFO: 	Container redis ready: true, restart count 0
Sep 11 18:21:20.712: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 18:21:20.712: INFO: md-loki-stack-promtail-gv7pm from md-loki-stack started at 2020-09-11 18:20:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.712: INFO: 	Container promtail ready: true, restart count 0
Sep 11 18:21:20.712: INFO: md-nginx-ingress-controller-28nhj from md-nginx-ingress started at 2020-09-11 18:20:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.712: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 18:21:20.712: INFO: md-prometheus-operator-prometheus-node-exporter-scwg9 from md-prometheus-operator started at 2020-09-11 18:20:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.712: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 18:21:20.712: INFO: prometheus-md-prometheus-operator-prometheus-0 from md-prometheus-operator started at 2020-09-11 18:20:51 +0000 UTC (3 container statuses recorded)
Sep 11 18:21:20.712: INFO: 	Container prometheus ready: true, restart count 0
Sep 11 18:21:20.712: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 11 18:21:20.712: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 11 18:21:20.712: INFO: pod-qos-class-df62ef4b-5f99-45c0-86dd-eaa2945ca444 from pods-9233 started at 2020-09-11 18:21:18 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.712: INFO: 	Container agnhost ready: true, restart count 0
Sep 11 18:21:20.712: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-jnktw from sonobuoy started at 2020-09-11 17:37:24 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:20.712: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 18:21:20.712: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 11 18:21:20.712: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-3-166.ec2.internal before test
Sep 11 18:21:20.724: INFO: calico-node-wfxpk from kube-system started at 2020-08-31 21:04:40 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 18:21:20.724: INFO: kube-proxy-9vh4x from kube-system started at 2020-08-31 21:04:40 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 18:21:20.724: INFO: sealed-secrets-controller-7dfd7dcc6b-g8r5f from kube-system started at 2020-09-11 16:30:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container sealed-secrets-controller ready: true, restart count 0
Sep 11 18:21:20.724: INFO: md-cert-manager-cainjector-76b464f9f7-wbmzz from md-cert-manager started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 18:21:20.724: INFO: md-cert-manager-webhook-656744686b-gch9j from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 18:21:20.724: INFO: md-dex-5b4f679bd4-dmmrq from md-dex started at 2020-09-11 16:47:54 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container main ready: true, restart count 1
Sep 11 18:21:20.724: INFO: md-flux-5465b59cbd-dd4fp from md-flux started at 2020-09-11 16:47:51 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container flux ready: true, restart count 0
Sep 11 18:21:20.724: INFO: harbor-pg-0 from md-harbor started at 2020-09-11 16:48:15 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container postgres ready: true, restart count 0
Sep 11 18:21:20.724: INFO: md-harbor-harbor-chartmuseum-55b4567ff8-cx2ch from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container chartmuseum ready: true, restart count 0
Sep 11 18:21:20.724: INFO: md-harbor-harbor-clair-6479d48569-vd6wm from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container adapter ready: true, restart count 0
Sep 11 18:21:20.724: INFO: 	Container clair ready: true, restart count 1
Sep 11 18:21:20.724: INFO: md-harbor-harbor-core-7999f49dc-vpk6g from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container core ready: true, restart count 0
Sep 11 18:21:20.724: INFO: md-harbor-harbor-jobservice-556554c4c7-gqzhp from md-harbor started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container jobservice ready: true, restart count 0
Sep 11 18:21:20.724: INFO: md-harbor-harbor-notary-signer-6789c9d4bc-db2kr from md-harbor started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.724: INFO: 	Container notary-signer ready: true, restart count 0
Sep 11 18:21:20.724: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-tdbqd from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.725: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 18:21:20.725: INFO: md-harbor-redis-redis-ha-server-1 from md-harbor started at 2020-09-11 16:48:53 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:20.725: INFO: 	Container redis ready: true, restart count 0
Sep 11 18:21:20.725: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 18:21:20.725: INFO: md-loki-stack-promtail-94946 from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.725: INFO: 	Container promtail ready: true, restart count 0
Sep 11 18:21:20.725: INFO: md-nginx-ingress-controller-lhjv9 from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.725: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 18:21:20.725: INFO: md-nginx-ingress-default-backend-74c98f4684-r7kpq from md-nginx-ingress started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.725: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep 11 18:21:20.725: INFO: md-oauth2-proxy-5c876db75d-jp4lz from md-oauth2-proxy started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.725: INFO: 	Container oauth2-proxy ready: true, restart count 0
Sep 11 18:21:20.725: INFO: md-prometheus-operator-prometheus-node-exporter-mjvrq from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.725: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 18:21:20.725: INFO: sonobuoy from sonobuoy started at 2020-09-11 17:37:23 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:20.725: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 11 18:21:20.725: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-lb975 from sonobuoy started at 2020-09-11 17:37:25 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:20.725: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 18:21:20.725: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1633cdb7866705d8], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1633cdb7874a6412], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:21:21.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-487" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":118,"skipped":1947,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:21:21.818: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-27
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep 11 18:21:21.985: INFO: Waiting up to 5m0s for pod "downward-api-00caa899-9711-447c-bbcd-5adbc5b6d58d" in namespace "downward-api-27" to be "Succeeded or Failed"
Sep 11 18:21:21.989: INFO: Pod "downward-api-00caa899-9711-447c-bbcd-5adbc5b6d58d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.140122ms
Sep 11 18:21:23.999: INFO: Pod "downward-api-00caa899-9711-447c-bbcd-5adbc5b6d58d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014522628s
STEP: Saw pod success
Sep 11 18:21:23.999: INFO: Pod "downward-api-00caa899-9711-447c-bbcd-5adbc5b6d58d" satisfied condition "Succeeded or Failed"
Sep 11 18:21:24.006: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downward-api-00caa899-9711-447c-bbcd-5adbc5b6d58d container dapi-container: <nil>
STEP: delete the pod
Sep 11 18:21:24.221: INFO: Waiting for pod downward-api-00caa899-9711-447c-bbcd-5adbc5b6d58d to disappear
Sep 11 18:21:24.227: INFO: Pod downward-api-00caa899-9711-447c-bbcd-5adbc5b6d58d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:21:24.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-27" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":119,"skipped":1962,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:21:24.263: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-v2mb
STEP: Creating a pod to test atomic-volume-subpath
Sep 11 18:21:24.470: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-v2mb" in namespace "subpath-7890" to be "Succeeded or Failed"
Sep 11 18:21:24.474: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.998011ms
Sep 11 18:21:26.479: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009574589s
Sep 11 18:21:28.484: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Running", Reason="", readiness=true. Elapsed: 4.014896855s
Sep 11 18:21:30.490: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Running", Reason="", readiness=true. Elapsed: 6.019968287s
Sep 11 18:21:32.495: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Running", Reason="", readiness=true. Elapsed: 8.025425715s
Sep 11 18:21:34.500: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Running", Reason="", readiness=true. Elapsed: 10.030197026s
Sep 11 18:21:36.505: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Running", Reason="", readiness=true. Elapsed: 12.035753857s
Sep 11 18:21:38.510: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Running", Reason="", readiness=true. Elapsed: 14.040413209s
Sep 11 18:21:40.515: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Running", Reason="", readiness=true. Elapsed: 16.045500616s
Sep 11 18:21:42.522: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Running", Reason="", readiness=true. Elapsed: 18.052117277s
Sep 11 18:21:44.529: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Running", Reason="", readiness=true. Elapsed: 20.059584207s
Sep 11 18:21:46.549: INFO: Pod "pod-subpath-test-configmap-v2mb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.079098508s
STEP: Saw pod success
Sep 11 18:21:46.549: INFO: Pod "pod-subpath-test-configmap-v2mb" satisfied condition "Succeeded or Failed"
Sep 11 18:21:46.553: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-subpath-test-configmap-v2mb container test-container-subpath-configmap-v2mb: <nil>
STEP: delete the pod
Sep 11 18:21:46.592: INFO: Waiting for pod pod-subpath-test-configmap-v2mb to disappear
Sep 11 18:21:46.602: INFO: Pod pod-subpath-test-configmap-v2mb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-v2mb
Sep 11 18:21:46.602: INFO: Deleting pod "pod-subpath-test-configmap-v2mb" in namespace "subpath-7890"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:21:46.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7890" for this suite.

â€¢ [SLOW TEST:22.388 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":120,"skipped":1965,"failed":0}
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:21:46.651: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8291
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep 11 18:21:49.376: INFO: Successfully updated pod "annotationupdate133c49d0-8539-4f4b-b381-3dec25a08e71"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:21:53.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8291" for this suite.

â€¢ [SLOW TEST:6.820 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":1965,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:21:53.472: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:21:55.718: INFO: Waiting up to 5m0s for pod "client-envvars-5896ec13-e1d9-4112-8188-041fbc9903cf" in namespace "pods-4916" to be "Succeeded or Failed"
Sep 11 18:21:55.723: INFO: Pod "client-envvars-5896ec13-e1d9-4112-8188-041fbc9903cf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.396129ms
Sep 11 18:21:57.728: INFO: Pod "client-envvars-5896ec13-e1d9-4112-8188-041fbc9903cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010092396s
STEP: Saw pod success
Sep 11 18:21:57.728: INFO: Pod "client-envvars-5896ec13-e1d9-4112-8188-041fbc9903cf" satisfied condition "Succeeded or Failed"
Sep 11 18:21:57.731: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod client-envvars-5896ec13-e1d9-4112-8188-041fbc9903cf container env3cont: <nil>
STEP: delete the pod
Sep 11 18:21:57.776: INFO: Waiting for pod client-envvars-5896ec13-e1d9-4112-8188-041fbc9903cf to disappear
Sep 11 18:21:57.782: INFO: Pod client-envvars-5896ec13-e1d9-4112-8188-041fbc9903cf no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:21:57.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4916" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":122,"skipped":1984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:21:57.814: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-7266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:21:57.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7266" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":123,"skipped":2020,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:21:58.012: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 11 18:21:58.186: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 11 18:21:58.200: INFO: Waiting for terminating namespaces to be deleted...
Sep 11 18:21:58.204: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-1-141.ec2.internal before test
Sep 11 18:21:58.216: INFO: calico-node-jgr6s from kube-system started at 2020-08-31 21:04:41 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 18:21:58.217: INFO: kube-proxy-72jmn from kube-system started at 2020-08-31 21:04:41 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-cert-manager-7b65d9c9d6-jst49 from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-flux-memcached-d75b6d9d6-vpnqm from md-flux started at 2020-09-11 16:47:51 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container memcached ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-gangway-66c5d44b4c-f7psj from md-gangway started at 2020-09-11 16:47:59 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container gangway ready: true, restart count 0
Sep 11 18:21:58.217: INFO: harbor-pg-1 from md-harbor started at 2020-09-11 16:49:13 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container postgres ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-harbor-harbor-notary-server-ddbf4b74b-69jlj from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container notary-server ready: true, restart count 1
Sep 11 18:21:58.217: INFO: md-harbor-harbor-portal-7976969b-27bjm from md-harbor started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container portal ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-harbor-harbor-registry-9cdb888bc-97n4b from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container registry ready: true, restart count 0
Sep 11 18:21:58.217: INFO: 	Container registryctl ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-mlqtr from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-harbor-redis-redis-ha-server-2 from md-harbor started at 2020-09-11 16:49:22 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container redis ready: true, restart count 0
Sep 11 18:21:58.217: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-helm-operator-557b9b587c-vmkw8 from md-helm-operator started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container flux-helm-operator ready: false, restart count 2
Sep 11 18:21:58.217: INFO: md-kubedb-84d9fff66c-6922k from md-kubedb started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container operator ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-loki-stack-0 from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container loki ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-loki-stack-promtail-hm9cx from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container promtail ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-nginx-ingress-controller-dfxrw from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 18:21:58.217: INFO: alertmanager-md-prometheus-operator-alertmanager-0 from md-prometheus-operator started at 2020-09-11 16:49:13 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container alertmanager ready: true, restart count 0
Sep 11 18:21:58.217: INFO: 	Container config-reloader ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-prometheus-operator-kube-state-metrics-6f5d54b49-mtqmh from md-prometheus-operator started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-prometheus-operator-operator-7d5c6f8689-tfzwv from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 11 18:21:58.217: INFO: 	Container tls-proxy ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-prometheus-operator-prometheus-node-exporter-6sz5n from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 18:21:58.217: INFO: md-velero-6cc8bd66b5-rqkv9 from md-velero started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container velero ready: true, restart count 0
Sep 11 18:21:58.217: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-ps57h from sonobuoy started at 2020-09-11 17:37:25 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:58.217: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 18:21:58.217: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 11 18:21:58.217: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-2-19.ec2.internal before test
Sep 11 18:21:58.228: INFO: calico-node-w9tmq from kube-system started at 2020-08-31 21:04:43 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 18:21:58.228: INFO: kube-proxy-djv8h from kube-system started at 2020-08-31 21:04:43 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 18:21:58.228: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-5w2c8 from md-harbor started at 2020-09-11 18:20:54 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 18:21:58.228: INFO: md-harbor-redis-redis-ha-server-0 from md-harbor started at 2020-09-11 18:20:58 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container redis ready: true, restart count 0
Sep 11 18:21:58.228: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 18:21:58.228: INFO: md-loki-stack-promtail-gv7pm from md-loki-stack started at 2020-09-11 18:20:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container promtail ready: true, restart count 0
Sep 11 18:21:58.228: INFO: md-nginx-ingress-controller-28nhj from md-nginx-ingress started at 2020-09-11 18:20:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 18:21:58.228: INFO: md-prometheus-operator-prometheus-node-exporter-scwg9 from md-prometheus-operator started at 2020-09-11 18:20:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 18:21:58.228: INFO: prometheus-md-prometheus-operator-prometheus-0 from md-prometheus-operator started at 2020-09-11 18:20:51 +0000 UTC (3 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container prometheus ready: true, restart count 0
Sep 11 18:21:58.228: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 11 18:21:58.228: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 11 18:21:58.228: INFO: server-envvars-0deb7358-8f06-4bbc-8735-be828c21751b from pods-4916 started at 2020-09-11 18:21:53 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container srv ready: true, restart count 0
Sep 11 18:21:58.228: INFO: annotationupdate133c49d0-8539-4f4b-b381-3dec25a08e71 from projected-8291 started at 2020-09-11 18:21:46 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container client-container ready: true, restart count 0
Sep 11 18:21:58.228: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-jnktw from sonobuoy started at 2020-09-11 17:37:24 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:58.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 18:21:58.228: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 11 18:21:58.228: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-3-166.ec2.internal before test
Sep 11 18:21:58.240: INFO: calico-node-wfxpk from kube-system started at 2020-08-31 21:04:40 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 18:21:58.240: INFO: kube-proxy-9vh4x from kube-system started at 2020-08-31 21:04:40 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 18:21:58.240: INFO: sealed-secrets-controller-7dfd7dcc6b-g8r5f from kube-system started at 2020-09-11 16:30:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container sealed-secrets-controller ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-cert-manager-cainjector-76b464f9f7-wbmzz from md-cert-manager started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-cert-manager-webhook-656744686b-gch9j from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-dex-5b4f679bd4-dmmrq from md-dex started at 2020-09-11 16:47:54 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container main ready: true, restart count 1
Sep 11 18:21:58.240: INFO: md-flux-5465b59cbd-dd4fp from md-flux started at 2020-09-11 16:47:51 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container flux ready: true, restart count 0
Sep 11 18:21:58.240: INFO: harbor-pg-0 from md-harbor started at 2020-09-11 16:48:15 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container postgres ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-harbor-harbor-chartmuseum-55b4567ff8-cx2ch from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container chartmuseum ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-harbor-harbor-clair-6479d48569-vd6wm from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container adapter ready: true, restart count 0
Sep 11 18:21:58.240: INFO: 	Container clair ready: true, restart count 1
Sep 11 18:21:58.240: INFO: md-harbor-harbor-core-7999f49dc-vpk6g from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container core ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-harbor-harbor-jobservice-556554c4c7-gqzhp from md-harbor started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container jobservice ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-harbor-harbor-notary-signer-6789c9d4bc-db2kr from md-harbor started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container notary-signer ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-tdbqd from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-harbor-redis-redis-ha-server-1 from md-harbor started at 2020-09-11 16:48:53 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container redis ready: true, restart count 0
Sep 11 18:21:58.240: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-loki-stack-promtail-94946 from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container promtail ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-nginx-ingress-controller-lhjv9 from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-nginx-ingress-default-backend-74c98f4684-r7kpq from md-nginx-ingress started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-oauth2-proxy-5c876db75d-jp4lz from md-oauth2-proxy started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container oauth2-proxy ready: true, restart count 0
Sep 11 18:21:58.240: INFO: md-prometheus-operator-prometheus-node-exporter-mjvrq from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 18:21:58.240: INFO: sonobuoy from sonobuoy started at 2020-09-11 17:37:23 +0000 UTC (1 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 11 18:21:58.240: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-lb975 from sonobuoy started at 2020-09-11 17:37:25 +0000 UTC (2 container statuses recorded)
Sep 11 18:21:58.240: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 18:21:58.240: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node ip-10-10-1-141.ec2.internal
STEP: verifying the node has the label node ip-10-10-2-19.ec2.internal
STEP: verifying the node has the label node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod calico-node-jgr6s requesting resource cpu=250m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod calico-node-w9tmq requesting resource cpu=250m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod calico-node-wfxpk requesting resource cpu=250m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod kube-proxy-72jmn requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod kube-proxy-9vh4x requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod kube-proxy-djv8h requesting resource cpu=0m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod sealed-secrets-controller-7dfd7dcc6b-g8r5f requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-cert-manager-7b65d9c9d6-jst49 requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-cert-manager-cainjector-76b464f9f7-wbmzz requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-cert-manager-webhook-656744686b-gch9j requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-dex-5b4f679bd4-dmmrq requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-flux-5465b59cbd-dd4fp requesting resource cpu=50m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-flux-memcached-d75b6d9d6-vpnqm requesting resource cpu=50m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-gangway-66c5d44b4c-f7psj requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod harbor-pg-0 requesting resource cpu=250m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod harbor-pg-1 requesting resource cpu=250m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-harbor-chartmuseum-55b4567ff8-cx2ch requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-harbor-clair-6479d48569-vd6wm requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-harbor-core-7999f49dc-vpk6g requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-harbor-jobservice-556554c4c7-gqzhp requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-harbor-notary-server-ddbf4b74b-69jlj requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-harbor-notary-signer-6789c9d4bc-db2kr requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-harbor-portal-7976969b-27bjm requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-harbor-registry-9cdb888bc-97n4b requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-5w2c8 requesting resource cpu=0m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-mlqtr requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-tdbqd requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-redis-redis-ha-server-0 requesting resource cpu=0m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-redis-redis-ha-server-1 requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-harbor-redis-redis-ha-server-2 requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-helm-operator-557b9b587c-vmkw8 requesting resource cpu=50m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-kubedb-84d9fff66c-6922k requesting resource cpu=100m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-loki-stack-0 requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-loki-stack-promtail-94946 requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-loki-stack-promtail-gv7pm requesting resource cpu=0m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-loki-stack-promtail-hm9cx requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-nginx-ingress-controller-28nhj requesting resource cpu=0m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-nginx-ingress-controller-dfxrw requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-nginx-ingress-controller-lhjv9 requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-nginx-ingress-default-backend-74c98f4684-r7kpq requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-oauth2-proxy-5c876db75d-jp4lz requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod alertmanager-md-prometheus-operator-alertmanager-0 requesting resource cpu=100m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-prometheus-operator-kube-state-metrics-6f5d54b49-mtqmh requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-prometheus-operator-operator-7d5c6f8689-tfzwv requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-prometheus-operator-prometheus-node-exporter-6sz5n requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-prometheus-operator-prometheus-node-exporter-mjvrq requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-prometheus-operator-prometheus-node-exporter-scwg9 requesting resource cpu=0m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod prometheus-md-prometheus-operator-prometheus-0 requesting resource cpu=200m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod md-velero-6cc8bd66b5-rqkv9 requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.353: INFO: Pod server-envvars-0deb7358-8f06-4bbc-8735-be828c21751b requesting resource cpu=0m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod annotationupdate133c49d0-8539-4f4b-b381-3dec25a08e71 requesting resource cpu=0m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-jnktw requesting resource cpu=0m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.353: INFO: Pod sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-lb975 requesting resource cpu=0m on Node ip-10-10-3-166.ec2.internal
Sep 11 18:21:58.353: INFO: Pod sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-ps57h requesting resource cpu=0m on Node ip-10-10-1-141.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
Sep 11 18:21:58.353: INFO: Creating a pod which consumes cpu=840m on Node ip-10-10-1-141.ec2.internal
Sep 11 18:21:58.364: INFO: Creating a pod which consumes cpu=1085m on Node ip-10-10-2-19.ec2.internal
Sep 11 18:21:58.375: INFO: Creating a pod which consumes cpu=1015m on Node ip-10-10-3-166.ec2.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d8c05d45-fad1-4cd5-9f9a-1afabeb7f94c.1633cdc082b9a3cd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b251aa26-1b31-4e75-9513-37e101da2eca.1633cdc095a739ac], Reason = [Started], Message = [Started container filler-pod-b251aa26-1b31-4e75-9513-37e101da2eca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d8c05d45-fad1-4cd5-9f9a-1afabeb7f94c.1633cdc047ebbda3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9085/filler-pod-d8c05d45-fad1-4cd5-9f9a-1afabeb7f94c to ip-10-10-1-141.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b251aa26-1b31-4e75-9513-37e101da2eca.1633cdc04911e2c4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9085/filler-pod-b251aa26-1b31-4e75-9513-37e101da2eca to ip-10-10-3-166.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b251aa26-1b31-4e75-9513-37e101da2eca.1633cdc089d959ea], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5251d25d-1c2c-4496-b988-871a4cf4c05f.1633cdc07ed9a8c2], Reason = [Created], Message = [Created container filler-pod-5251d25d-1c2c-4496-b988-871a4cf4c05f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5251d25d-1c2c-4496-b988-871a4cf4c05f.1633cdc085fdd7ce], Reason = [Started], Message = [Started container filler-pod-5251d25d-1c2c-4496-b988-871a4cf4c05f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5251d25d-1c2c-4496-b988-871a4cf4c05f.1633cdc048cab6ac], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9085/filler-pod-5251d25d-1c2c-4496-b988-871a4cf4c05f to ip-10-10-2-19.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d8c05d45-fad1-4cd5-9f9a-1afabeb7f94c.1633cdc08d24d11e], Reason = [Started], Message = [Started container filler-pod-d8c05d45-fad1-4cd5-9f9a-1afabeb7f94c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5251d25d-1c2c-4496-b988-871a4cf4c05f.1633cdc07cc6260b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d8c05d45-fad1-4cd5-9f9a-1afabeb7f94c.1633cdc0856bc89c], Reason = [Created], Message = [Created container filler-pod-d8c05d45-fad1-4cd5-9f9a-1afabeb7f94c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b251aa26-1b31-4e75-9513-37e101da2eca.1633cdc08da2683b], Reason = [Created], Message = [Created container filler-pod-b251aa26-1b31-4e75-9513-37e101da2eca]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1633cdc139cbc15c], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1633cdc13ac48170], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node ip-10-10-3-166.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-10-1-141.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-10-2-19.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:22:03.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9085" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:5.546 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":124,"skipped":2032,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:22:03.558: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep 11 18:22:06.335: INFO: Successfully updated pod "annotationupdatefb114306-3b8d-4545-b215-7968fb3bd1f6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:22:08.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-859" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":2050,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:22:08.393: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:22:08.561: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-576933b8-639c-47c3-9711-fc58c45473a7" in namespace "security-context-test-2622" to be "Succeeded or Failed"
Sep 11 18:22:08.570: INFO: Pod "busybox-readonly-false-576933b8-639c-47c3-9711-fc58c45473a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.087125ms
Sep 11 18:22:10.578: INFO: Pod "busybox-readonly-false-576933b8-639c-47c3-9711-fc58c45473a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017112093s
Sep 11 18:22:12.583: INFO: Pod "busybox-readonly-false-576933b8-639c-47c3-9711-fc58c45473a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021882008s
Sep 11 18:22:12.583: INFO: Pod "busybox-readonly-false-576933b8-639c-47c3-9711-fc58c45473a7" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:22:12.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2622" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":126,"skipped":2063,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:22:12.610: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-92ddfdb8-15d5-4cbe-b1d4-cfb7490a708e
STEP: Creating a pod to test consume secrets
Sep 11 18:22:12.788: INFO: Waiting up to 5m0s for pod "pod-secrets-15e13760-65f9-4867-9c7c-f21a6ff21a32" in namespace "secrets-3848" to be "Succeeded or Failed"
Sep 11 18:22:12.792: INFO: Pod "pod-secrets-15e13760-65f9-4867-9c7c-f21a6ff21a32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.880672ms
Sep 11 18:22:14.796: INFO: Pod "pod-secrets-15e13760-65f9-4867-9c7c-f21a6ff21a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008239588s
Sep 11 18:22:16.801: INFO: Pod "pod-secrets-15e13760-65f9-4867-9c7c-f21a6ff21a32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012980456s
STEP: Saw pod success
Sep 11 18:22:16.801: INFO: Pod "pod-secrets-15e13760-65f9-4867-9c7c-f21a6ff21a32" satisfied condition "Succeeded or Failed"
Sep 11 18:22:16.804: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-secrets-15e13760-65f9-4867-9c7c-f21a6ff21a32 container secret-volume-test: <nil>
STEP: delete the pod
Sep 11 18:22:16.832: INFO: Waiting for pod pod-secrets-15e13760-65f9-4867-9c7c-f21a6ff21a32 to disappear
Sep 11 18:22:16.836: INFO: Pod pod-secrets-15e13760-65f9-4867-9c7c-f21a6ff21a32 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:22:16.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3848" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":127,"skipped":2065,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:22:16.863: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-6122/secret-test-e05b4597-408a-40db-a620-4ee52b7bc662
STEP: Creating a pod to test consume secrets
Sep 11 18:22:17.047: INFO: Waiting up to 5m0s for pod "pod-configmaps-3da5ec6d-db07-44e5-becc-8fbde3990d6c" in namespace "secrets-6122" to be "Succeeded or Failed"
Sep 11 18:22:17.053: INFO: Pod "pod-configmaps-3da5ec6d-db07-44e5-becc-8fbde3990d6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.224059ms
Sep 11 18:22:19.058: INFO: Pod "pod-configmaps-3da5ec6d-db07-44e5-becc-8fbde3990d6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011432284s
Sep 11 18:22:21.064: INFO: Pod "pod-configmaps-3da5ec6d-db07-44e5-becc-8fbde3990d6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017186872s
STEP: Saw pod success
Sep 11 18:22:21.064: INFO: Pod "pod-configmaps-3da5ec6d-db07-44e5-becc-8fbde3990d6c" satisfied condition "Succeeded or Failed"
Sep 11 18:22:21.069: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-configmaps-3da5ec6d-db07-44e5-becc-8fbde3990d6c container env-test: <nil>
STEP: delete the pod
Sep 11 18:22:21.106: INFO: Waiting for pod pod-configmaps-3da5ec6d-db07-44e5-becc-8fbde3990d6c to disappear
Sep 11 18:22:21.112: INFO: Pod pod-configmaps-3da5ec6d-db07-44e5-becc-8fbde3990d6c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:22:21.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6122" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":128,"skipped":2079,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:22:21.146: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7880
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:22:21.296: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 11 18:22:23.336: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:22:24.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7880" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":129,"skipped":2096,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:22:24.377: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7580
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Sep 11 18:22:24.530: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:22:52.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7580" for this suite.

â€¢ [SLOW TEST:28.517 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":130,"skipped":2101,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:22:52.894: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1647
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Sep 11 18:22:53.050: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:23:23.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1647" for this suite.

â€¢ [SLOW TEST:30.320 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":131,"skipped":2110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:23:23.214: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:23:24.273: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 18:23:26.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445404, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445404, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445404, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445404, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:23:29.321: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:23:41.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1254" for this suite.
STEP: Destroying namespace "webhook-1254-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:18.456 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":132,"skipped":2132,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:23:41.671: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:23:42.459: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 18:23:44.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445422, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445422, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445422, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445422, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:23:47.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:23:47.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1554" for this suite.
STEP: Destroying namespace "webhook-1554-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.275 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":133,"skipped":2140,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:23:47.946: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9105
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 11 18:23:48.125: INFO: Waiting up to 5m0s for pod "pod-a33054a0-c459-49f5-87d4-5c27ccd1fcd5" in namespace "emptydir-9105" to be "Succeeded or Failed"
Sep 11 18:23:48.128: INFO: Pod "pod-a33054a0-c459-49f5-87d4-5c27ccd1fcd5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.631849ms
Sep 11 18:23:50.133: INFO: Pod "pod-a33054a0-c459-49f5-87d4-5c27ccd1fcd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008536869s
STEP: Saw pod success
Sep 11 18:23:50.133: INFO: Pod "pod-a33054a0-c459-49f5-87d4-5c27ccd1fcd5" satisfied condition "Succeeded or Failed"
Sep 11 18:23:50.137: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-a33054a0-c459-49f5-87d4-5c27ccd1fcd5 container test-container: <nil>
STEP: delete the pod
Sep 11 18:23:50.172: INFO: Waiting for pod pod-a33054a0-c459-49f5-87d4-5c27ccd1fcd5 to disappear
Sep 11 18:23:50.176: INFO: Pod pod-a33054a0-c459-49f5-87d4-5c27ccd1fcd5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:23:50.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9105" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":134,"skipped":2143,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:23:50.202: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1862
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-873a8160-ac00-4236-ad78-8cb61ad8eebf
STEP: Creating a pod to test consume configMaps
Sep 11 18:23:50.373: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-266c8aeb-96fa-4877-a858-a878dbcc25ef" in namespace "projected-1862" to be "Succeeded or Failed"
Sep 11 18:23:50.382: INFO: Pod "pod-projected-configmaps-266c8aeb-96fa-4877-a858-a878dbcc25ef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.750271ms
Sep 11 18:23:52.387: INFO: Pod "pod-projected-configmaps-266c8aeb-96fa-4877-a858-a878dbcc25ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01372687s
Sep 11 18:23:54.393: INFO: Pod "pod-projected-configmaps-266c8aeb-96fa-4877-a858-a878dbcc25ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020292526s
STEP: Saw pod success
Sep 11 18:23:54.393: INFO: Pod "pod-projected-configmaps-266c8aeb-96fa-4877-a858-a878dbcc25ef" satisfied condition "Succeeded or Failed"
Sep 11 18:23:54.397: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-configmaps-266c8aeb-96fa-4877-a858-a878dbcc25ef container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 18:23:54.432: INFO: Waiting for pod pod-projected-configmaps-266c8aeb-96fa-4877-a858-a878dbcc25ef to disappear
Sep 11 18:23:54.436: INFO: Pod pod-projected-configmaps-266c8aeb-96fa-4877-a858-a878dbcc25ef no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:23:54.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1862" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2182,"failed":0}

------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:23:54.461: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6816
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-6816
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6816 to expose endpoints map[]
Sep 11 18:23:54.640: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Sep 11 18:23:55.663: INFO: successfully validated that service multi-endpoint-test in namespace services-6816 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6816
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6816 to expose endpoints map[pod1:[100]]
Sep 11 18:23:57.705: INFO: successfully validated that service multi-endpoint-test in namespace services-6816 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6816
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6816 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 11 18:24:00.734: INFO: successfully validated that service multi-endpoint-test in namespace services-6816 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-6816
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6816 to expose endpoints map[pod2:[101]]
Sep 11 18:24:01.784: INFO: successfully validated that service multi-endpoint-test in namespace services-6816 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6816
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6816 to expose endpoints map[]
Sep 11 18:24:02.827: INFO: successfully validated that service multi-endpoint-test in namespace services-6816 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:24:02.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6816" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:8.448 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":136,"skipped":2182,"failed":0}
SSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:24:02.909: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Sep 11 18:24:03.070: INFO: created test-event-1
Sep 11 18:24:03.076: INFO: created test-event-2
Sep 11 18:24:03.087: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Sep 11 18:24:03.091: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Sep 11 18:24:03.138: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:24:03.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2456" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":137,"skipped":2185,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:24:03.176: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6697
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-9b99e9cc-8fb0-4f16-9857-5867f557c07e
STEP: Creating a pod to test consume secrets
Sep 11 18:24:03.351: INFO: Waiting up to 5m0s for pod "pod-secrets-850e7310-a1a6-45d7-a413-b2475d5ff96c" in namespace "secrets-6697" to be "Succeeded or Failed"
Sep 11 18:24:03.359: INFO: Pod "pod-secrets-850e7310-a1a6-45d7-a413-b2475d5ff96c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.433111ms
Sep 11 18:24:05.364: INFO: Pod "pod-secrets-850e7310-a1a6-45d7-a413-b2475d5ff96c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012267154s
STEP: Saw pod success
Sep 11 18:24:05.364: INFO: Pod "pod-secrets-850e7310-a1a6-45d7-a413-b2475d5ff96c" satisfied condition "Succeeded or Failed"
Sep 11 18:24:05.369: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-secrets-850e7310-a1a6-45d7-a413-b2475d5ff96c container secret-volume-test: <nil>
STEP: delete the pod
Sep 11 18:24:05.410: INFO: Waiting for pod pod-secrets-850e7310-a1a6-45d7-a413-b2475d5ff96c to disappear
Sep 11 18:24:05.415: INFO: Pod pod-secrets-850e7310-a1a6-45d7-a413-b2475d5ff96c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:24:05.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6697" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":138,"skipped":2203,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:24:05.451: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6649
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Sep 11 18:24:05.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-6649 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 11 18:24:05.710: INFO: stderr: ""
Sep 11 18:24:05.710: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Sep 11 18:24:05.710: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 11 18:24:05.710: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6649" to be "running and ready, or succeeded"
Sep 11 18:24:05.715: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.130954ms
Sep 11 18:24:07.720: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010084728s
Sep 11 18:24:09.726: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.015929019s
Sep 11 18:24:09.726: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 11 18:24:09.726: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 11 18:24:09.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 logs logs-generator logs-generator --namespace=kubectl-6649'
Sep 11 18:24:09.865: INFO: stderr: ""
Sep 11 18:24:09.865: INFO: stdout: "I0911 18:24:07.009341       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/krx 227\nI0911 18:24:07.209468       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/p4lm 289\nI0911 18:24:07.409518       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/ncn 307\nI0911 18:24:07.609436       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/nldx 377\nI0911 18:24:07.809387       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/hzpd 585\nI0911 18:24:08.009436       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/dtg 419\nI0911 18:24:08.209461       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/pp2 290\nI0911 18:24:08.409793       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/8fc6 589\nI0911 18:24:08.609434       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/bv7b 326\nI0911 18:24:08.822470       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/g5bd 264\nI0911 18:24:09.009444       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/rpqr 366\nI0911 18:24:09.211232       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/g8db 341\nI0911 18:24:09.409451       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/6pl 454\nI0911 18:24:09.609458       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/wmp 395\nI0911 18:24:09.809447       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/wpn 300\n"
STEP: limiting log lines
Sep 11 18:24:09.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 logs logs-generator logs-generator --namespace=kubectl-6649 --tail=1'
Sep 11 18:24:09.974: INFO: stderr: ""
Sep 11 18:24:09.974: INFO: stdout: "I0911 18:24:09.809447       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/wpn 300\n"
Sep 11 18:24:09.974: INFO: got output "I0911 18:24:09.809447       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/wpn 300\n"
STEP: limiting log bytes
Sep 11 18:24:09.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 logs logs-generator logs-generator --namespace=kubectl-6649 --limit-bytes=1'
Sep 11 18:24:10.089: INFO: stderr: ""
Sep 11 18:24:10.089: INFO: stdout: "I"
Sep 11 18:24:10.089: INFO: got output "I"
STEP: exposing timestamps
Sep 11 18:24:10.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 logs logs-generator logs-generator --namespace=kubectl-6649 --tail=1 --timestamps'
Sep 11 18:24:10.205: INFO: stderr: ""
Sep 11 18:24:10.205: INFO: stdout: "2020-09-11T18:24:10.009590141Z I0911 18:24:10.009472       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/rqdz 290\n"
Sep 11 18:24:10.205: INFO: got output "2020-09-11T18:24:10.009590141Z I0911 18:24:10.009472       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/rqdz 290\n"
STEP: restricting to a time range
Sep 11 18:24:12.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 logs logs-generator logs-generator --namespace=kubectl-6649 --since=1s'
Sep 11 18:24:12.823: INFO: stderr: ""
Sep 11 18:24:12.823: INFO: stdout: "I0911 18:24:12.009459       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/6r4r 473\nI0911 18:24:12.209443       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/hl64 336\nI0911 18:24:12.409574       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/krm 534\nI0911 18:24:12.609434       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/dkp 328\nI0911 18:24:12.809431       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/tz25 284\n"
Sep 11 18:24:12.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 logs logs-generator logs-generator --namespace=kubectl-6649 --since=24h'
Sep 11 18:24:12.939: INFO: stderr: ""
Sep 11 18:24:12.939: INFO: stdout: "I0911 18:24:07.009341       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/krx 227\nI0911 18:24:07.209468       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/p4lm 289\nI0911 18:24:07.409518       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/ncn 307\nI0911 18:24:07.609436       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/nldx 377\nI0911 18:24:07.809387       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/hzpd 585\nI0911 18:24:08.009436       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/dtg 419\nI0911 18:24:08.209461       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/pp2 290\nI0911 18:24:08.409793       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/8fc6 589\nI0911 18:24:08.609434       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/bv7b 326\nI0911 18:24:08.822470       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/g5bd 264\nI0911 18:24:09.009444       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/rpqr 366\nI0911 18:24:09.211232       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/g8db 341\nI0911 18:24:09.409451       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/6pl 454\nI0911 18:24:09.609458       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/wmp 395\nI0911 18:24:09.809447       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/wpn 300\nI0911 18:24:10.009472       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/rqdz 290\nI0911 18:24:10.209447       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/sg5 511\nI0911 18:24:10.409450       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/7dd 378\nI0911 18:24:10.609447       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/nmxl 314\nI0911 18:24:10.809851       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/d4g 245\nI0911 18:24:11.009452       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/j4hl 389\nI0911 18:24:11.209453       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/m6q 273\nI0911 18:24:11.409450       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/dhv 206\nI0911 18:24:11.609452       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/kpr 348\nI0911 18:24:11.809503       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/c8j 272\nI0911 18:24:12.009459       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/6r4r 473\nI0911 18:24:12.209443       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/hl64 336\nI0911 18:24:12.409574       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/krm 534\nI0911 18:24:12.609434       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/dkp 328\nI0911 18:24:12.809431       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/tz25 284\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Sep 11 18:24:12.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete pod logs-generator --namespace=kubectl-6649'
Sep 11 18:24:23.214: INFO: stderr: ""
Sep 11 18:24:23.214: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:24:23.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6649" for this suite.

â€¢ [SLOW TEST:17.791 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":139,"skipped":2215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:24:23.243: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep 11 18:24:25.947: INFO: Successfully updated pod "labelsupdatee927ab5a-0a04-4c1e-b3bd-dfdfc3b42f32"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:24:27.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8086" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":140,"skipped":2268,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:24:28.019: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1568
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:24:44.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1568" for this suite.

â€¢ [SLOW TEST:16.234 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":141,"skipped":2282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:24:44.253: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5949
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-5cc5a731-1510-4b89-8a3e-2138a2d2ef28
STEP: Creating a pod to test consume configMaps
Sep 11 18:24:44.428: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee192612-d1b8-4256-bba6-b5cad5648215" in namespace "configmap-5949" to be "Succeeded or Failed"
Sep 11 18:24:44.432: INFO: Pod "pod-configmaps-ee192612-d1b8-4256-bba6-b5cad5648215": Phase="Pending", Reason="", readiness=false. Elapsed: 3.83504ms
Sep 11 18:24:46.436: INFO: Pod "pod-configmaps-ee192612-d1b8-4256-bba6-b5cad5648215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008549103s
STEP: Saw pod success
Sep 11 18:24:46.436: INFO: Pod "pod-configmaps-ee192612-d1b8-4256-bba6-b5cad5648215" satisfied condition "Succeeded or Failed"
Sep 11 18:24:46.440: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-configmaps-ee192612-d1b8-4256-bba6-b5cad5648215 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 18:24:46.474: INFO: Waiting for pod pod-configmaps-ee192612-d1b8-4256-bba6-b5cad5648215 to disappear
Sep 11 18:24:46.478: INFO: Pod pod-configmaps-ee192612-d1b8-4256-bba6-b5cad5648215 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:24:46.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5949" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":142,"skipped":2315,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:24:46.505: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1774
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1774
I0911 18:24:46.732512      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1774, replica count: 2
Sep 11 18:24:49.783: INFO: Creating new exec pod
I0911 18:24:49.783311      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 18:24:52.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1774 execpod2wkp8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 11 18:24:53.059: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 11 18:24:53.059: INFO: stdout: ""
Sep 11 18:24:53.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1774 execpod2wkp8 -- /bin/sh -x -c nc -zv -t -w 2 10.98.223.76 80'
Sep 11 18:24:53.310: INFO: stderr: "+ nc -zv -t -w 2 10.98.223.76 80\nConnection to 10.98.223.76 80 port [tcp/http] succeeded!\n"
Sep 11 18:24:53.310: INFO: stdout: ""
Sep 11 18:24:53.310: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:24:53.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1774" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:6.920 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":143,"skipped":2323,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:24:53.425: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-284
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 11 18:24:53.592: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 11 18:24:53.647: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 11 18:24:55.651: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:24:57.652: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:24:59.653: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:25:01.651: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:25:03.652: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:25:05.652: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:25:07.651: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 11 18:25:07.659: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 11 18:25:07.666: INFO: The status of Pod netserver-2 is Running (Ready = false)
Sep 11 18:25:09.671: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 11 18:25:13.718: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.134.98:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-284 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:25:13.718: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:25:13.927: INFO: Found all expected endpoints: [netserver-0]
Sep 11 18:25:13.932: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.123.47:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-284 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:25:13.932: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:25:14.052: INFO: Found all expected endpoints: [netserver-1]
Sep 11 18:25:14.059: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.249.227:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-284 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:25:14.059: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:25:14.200: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:14.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-284" for this suite.

â€¢ [SLOW TEST:20.804 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":144,"skipped":2324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:14.229: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:16.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9362" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2347,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:16.462: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:25:17.376: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 18:25:19.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445517, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445517, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445517, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445517, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:25:22.421: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:25:22.436: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6479-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:23.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7986" for this suite.
STEP: Destroying namespace "webhook-7986-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:8.090 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":146,"skipped":2377,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:24.552: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-f699b4ee-05d5-433b-9c79-51afb0a4461b
STEP: Creating a pod to test consume secrets
Sep 11 18:25:25.356: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0ab78a4d-0e30-421e-a06d-474d1cf3a7fd" in namespace "projected-3272" to be "Succeeded or Failed"
Sep 11 18:25:25.388: INFO: Pod "pod-projected-secrets-0ab78a4d-0e30-421e-a06d-474d1cf3a7fd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.377885ms
Sep 11 18:25:27.932: INFO: Pod "pod-projected-secrets-0ab78a4d-0e30-421e-a06d-474d1cf3a7fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.576527681s
STEP: Saw pod success
Sep 11 18:25:27.933: INFO: Pod "pod-projected-secrets-0ab78a4d-0e30-421e-a06d-474d1cf3a7fd" satisfied condition "Succeeded or Failed"
Sep 11 18:25:27.952: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-secrets-0ab78a4d-0e30-421e-a06d-474d1cf3a7fd container secret-volume-test: <nil>
STEP: delete the pod
Sep 11 18:25:28.381: INFO: Waiting for pod pod-projected-secrets-0ab78a4d-0e30-421e-a06d-474d1cf3a7fd to disappear
Sep 11 18:25:28.389: INFO: Pod pod-projected-secrets-0ab78a4d-0e30-421e-a06d-474d1cf3a7fd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:28.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3272" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":147,"skipped":2398,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:28.429: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-46f708d2-b57e-4c0e-adcd-78c99716912f
STEP: Creating a pod to test consume configMaps
Sep 11 18:25:28.812: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e945bd3-9d10-40d3-963c-2c50c23e2ab2" in namespace "configmap-5999" to be "Succeeded or Failed"
Sep 11 18:25:28.824: INFO: Pod "pod-configmaps-8e945bd3-9d10-40d3-963c-2c50c23e2ab2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.736896ms
Sep 11 18:25:30.830: INFO: Pod "pod-configmaps-8e945bd3-9d10-40d3-963c-2c50c23e2ab2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017804227s
Sep 11 18:25:32.836: INFO: Pod "pod-configmaps-8e945bd3-9d10-40d3-963c-2c50c23e2ab2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0239559s
STEP: Saw pod success
Sep 11 18:25:32.836: INFO: Pod "pod-configmaps-8e945bd3-9d10-40d3-963c-2c50c23e2ab2" satisfied condition "Succeeded or Failed"
Sep 11 18:25:32.841: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-configmaps-8e945bd3-9d10-40d3-963c-2c50c23e2ab2 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 18:25:32.885: INFO: Waiting for pod pod-configmaps-8e945bd3-9d10-40d3-963c-2c50c23e2ab2 to disappear
Sep 11 18:25:32.892: INFO: Pod pod-configmaps-8e945bd3-9d10-40d3-963c-2c50c23e2ab2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:32.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5999" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2399,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:32.924: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3882
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:25:33.612: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 11 18:25:35.628: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445533, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445533, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445533, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445533, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:25:38.660: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:25:38.666: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:39.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3882" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:7.809 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":149,"skipped":2407,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:40.733: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-9854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 11 18:25:41.258: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 11 18:25:41.266: INFO: starting watch
STEP: patching
STEP: updating
Sep 11 18:25:41.304: INFO: waiting for watch events with expected annotations
Sep 11 18:25:41.304: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:41.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9854" for this suite.
â€¢{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":150,"skipped":2419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:41.989: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3091
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Sep 11 18:25:42.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 cluster-info'
Sep 11 18:25:42.419: INFO: stderr: ""
Sep 11 18:25:42.419: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:42.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3091" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":151,"skipped":2446,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:42.449: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 11 18:25:42.614: INFO: Waiting up to 5m0s for pod "pod-97d6d412-37d9-4cf7-b5a3-ae85c889bfa4" in namespace "emptydir-154" to be "Succeeded or Failed"
Sep 11 18:25:42.618: INFO: Pod "pod-97d6d412-37d9-4cf7-b5a3-ae85c889bfa4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.812499ms
Sep 11 18:25:44.898: INFO: Pod "pod-97d6d412-37d9-4cf7-b5a3-ae85c889bfa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.284049634s
STEP: Saw pod success
Sep 11 18:25:44.898: INFO: Pod "pod-97d6d412-37d9-4cf7-b5a3-ae85c889bfa4" satisfied condition "Succeeded or Failed"
Sep 11 18:25:44.908: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-97d6d412-37d9-4cf7-b5a3-ae85c889bfa4 container test-container: <nil>
STEP: delete the pod
Sep 11 18:25:44.971: INFO: Waiting for pod pod-97d6d412-37d9-4cf7-b5a3-ae85c889bfa4 to disappear
Sep 11 18:25:44.975: INFO: Pod pod-97d6d412-37d9-4cf7-b5a3-ae85c889bfa4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:44.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-154" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":152,"skipped":2484,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:45.003: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:25:45.934: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:25:48.984: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:49.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4819" for this suite.
STEP: Destroying namespace "webhook-4819-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":153,"skipped":2488,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:49.182: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1708
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Sep 11 18:25:49.365: INFO: Waiting up to 5m0s for pod "var-expansion-2906fc3c-ab13-4bab-a37b-4367068ca5b5" in namespace "var-expansion-1708" to be "Succeeded or Failed"
Sep 11 18:25:49.384: INFO: Pod "var-expansion-2906fc3c-ab13-4bab-a37b-4367068ca5b5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.092663ms
Sep 11 18:25:51.388: INFO: Pod "var-expansion-2906fc3c-ab13-4bab-a37b-4367068ca5b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023764785s
STEP: Saw pod success
Sep 11 18:25:51.388: INFO: Pod "var-expansion-2906fc3c-ab13-4bab-a37b-4367068ca5b5" satisfied condition "Succeeded or Failed"
Sep 11 18:25:51.393: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod var-expansion-2906fc3c-ab13-4bab-a37b-4367068ca5b5 container dapi-container: <nil>
STEP: delete the pod
Sep 11 18:25:51.426: INFO: Waiting for pod var-expansion-2906fc3c-ab13-4bab-a37b-4367068ca5b5 to disappear
Sep 11 18:25:51.430: INFO: Pod var-expansion-2906fc3c-ab13-4bab-a37b-4367068ca5b5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:51.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1708" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":154,"skipped":2496,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:51.457: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:25:51.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1121" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":155,"skipped":2501,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:25:51.641: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2940
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:25:51.801: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 11 18:25:57.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-2940 create -f -'
Sep 11 18:25:58.422: INFO: stderr: ""
Sep 11 18:25:58.422: INFO: stdout: "e2e-test-crd-publish-openapi-6195-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 11 18:25:58.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-2940 delete e2e-test-crd-publish-openapi-6195-crds test-foo'
Sep 11 18:25:58.539: INFO: stderr: ""
Sep 11 18:25:58.539: INFO: stdout: "e2e-test-crd-publish-openapi-6195-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 11 18:25:58.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-2940 apply -f -'
Sep 11 18:25:58.950: INFO: stderr: ""
Sep 11 18:25:58.950: INFO: stdout: "e2e-test-crd-publish-openapi-6195-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 11 18:25:58.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-2940 delete e2e-test-crd-publish-openapi-6195-crds test-foo'
Sep 11 18:25:59.075: INFO: stderr: ""
Sep 11 18:25:59.075: INFO: stdout: "e2e-test-crd-publish-openapi-6195-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 11 18:25:59.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-2940 create -f -'
Sep 11 18:25:59.428: INFO: rc: 1
Sep 11 18:25:59.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-2940 apply -f -'
Sep 11 18:25:59.723: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 11 18:25:59.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-2940 create -f -'
Sep 11 18:26:00.055: INFO: rc: 1
Sep 11 18:26:00.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-2940 apply -f -'
Sep 11 18:26:00.350: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 11 18:26:00.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 explain e2e-test-crd-publish-openapi-6195-crds'
Sep 11 18:26:00.721: INFO: stderr: ""
Sep 11 18:26:00.721: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6195-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 11 18:26:00.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 explain e2e-test-crd-publish-openapi-6195-crds.metadata'
Sep 11 18:26:01.058: INFO: stderr: ""
Sep 11 18:26:01.058: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6195-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 11 18:26:01.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 explain e2e-test-crd-publish-openapi-6195-crds.spec'
Sep 11 18:26:01.437: INFO: stderr: ""
Sep 11 18:26:01.438: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6195-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 11 18:26:01.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 explain e2e-test-crd-publish-openapi-6195-crds.spec.bars'
Sep 11 18:26:01.838: INFO: stderr: ""
Sep 11 18:26:01.838: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6195-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 11 18:26:01.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 explain e2e-test-crd-publish-openapi-6195-crds.spec.bars2'
Sep 11 18:26:02.179: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:07.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2940" for this suite.

â€¢ [SLOW TEST:16.347 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":156,"skipped":2522,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:07.989: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep 11 18:26:08.140: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:12.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9477" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":157,"skipped":2533,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:12.210: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-b2ff060d-02ad-4c71-8278-f503a99a0f45
STEP: Creating a pod to test consume secrets
Sep 11 18:26:12.382: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eef44c6c-9f2c-4a71-a91f-196a219ff723" in namespace "projected-9209" to be "Succeeded or Failed"
Sep 11 18:26:12.390: INFO: Pod "pod-projected-secrets-eef44c6c-9f2c-4a71-a91f-196a219ff723": Phase="Pending", Reason="", readiness=false. Elapsed: 8.347926ms
Sep 11 18:26:14.396: INFO: Pod "pod-projected-secrets-eef44c6c-9f2c-4a71-a91f-196a219ff723": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013610431s
STEP: Saw pod success
Sep 11 18:26:14.396: INFO: Pod "pod-projected-secrets-eef44c6c-9f2c-4a71-a91f-196a219ff723" satisfied condition "Succeeded or Failed"
Sep 11 18:26:14.399: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-secrets-eef44c6c-9f2c-4a71-a91f-196a219ff723 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 11 18:26:14.445: INFO: Waiting for pod pod-projected-secrets-eef44c6c-9f2c-4a71-a91f-196a219ff723 to disappear
Sep 11 18:26:14.452: INFO: Pod pod-projected-secrets-eef44c6c-9f2c-4a71-a91f-196a219ff723 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:14.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9209" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":158,"skipped":2548,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:14.479: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 18:26:14.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f0916f4-dcaf-460d-b567-e206fc24909e" in namespace "downward-api-9970" to be "Succeeded or Failed"
Sep 11 18:26:14.650: INFO: Pod "downwardapi-volume-1f0916f4-dcaf-460d-b567-e206fc24909e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.993064ms
Sep 11 18:26:16.655: INFO: Pod "downwardapi-volume-1f0916f4-dcaf-460d-b567-e206fc24909e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009242983s
STEP: Saw pod success
Sep 11 18:26:16.655: INFO: Pod "downwardapi-volume-1f0916f4-dcaf-460d-b567-e206fc24909e" satisfied condition "Succeeded or Failed"
Sep 11 18:26:16.659: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-1f0916f4-dcaf-460d-b567-e206fc24909e container client-container: <nil>
STEP: delete the pod
Sep 11 18:26:16.687: INFO: Waiting for pod downwardapi-volume-1f0916f4-dcaf-460d-b567-e206fc24909e to disappear
Sep 11 18:26:16.691: INFO: Pod downwardapi-volume-1f0916f4-dcaf-460d-b567-e206fc24909e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:16.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9970" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":159,"skipped":2567,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:16.720: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3857
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:26:16.877: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 11 18:26:22.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-3857 create -f -'
Sep 11 18:26:23.343: INFO: stderr: ""
Sep 11 18:26:23.343: INFO: stdout: "e2e-test-crd-publish-openapi-5203-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 11 18:26:23.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-3857 delete e2e-test-crd-publish-openapi-5203-crds test-cr'
Sep 11 18:26:23.507: INFO: stderr: ""
Sep 11 18:26:23.507: INFO: stdout: "e2e-test-crd-publish-openapi-5203-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 11 18:26:23.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-3857 apply -f -'
Sep 11 18:26:23.839: INFO: stderr: ""
Sep 11 18:26:23.839: INFO: stdout: "e2e-test-crd-publish-openapi-5203-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 11 18:26:23.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 --namespace=crd-publish-openapi-3857 delete e2e-test-crd-publish-openapi-5203-crds test-cr'
Sep 11 18:26:23.955: INFO: stderr: ""
Sep 11 18:26:23.955: INFO: stdout: "e2e-test-crd-publish-openapi-5203-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 11 18:26:23.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 explain e2e-test-crd-publish-openapi-5203-crds'
Sep 11 18:26:24.351: INFO: stderr: ""
Sep 11 18:26:24.351: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5203-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:30.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3857" for this suite.

â€¢ [SLOW TEST:13.406 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":160,"skipped":2570,"failed":0}
SSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:30.126: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-2237
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:30.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2237" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":161,"skipped":2577,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:30.368: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:34.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-541" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":162,"skipped":2594,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:34.576: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5177
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep 11 18:26:34.739: INFO: Waiting up to 5m0s for pod "downward-api-69cccd8c-d351-47b2-a106-8b0f874d7256" in namespace "downward-api-5177" to be "Succeeded or Failed"
Sep 11 18:26:34.743: INFO: Pod "downward-api-69cccd8c-d351-47b2-a106-8b0f874d7256": Phase="Pending", Reason="", readiness=false. Elapsed: 3.814299ms
Sep 11 18:26:36.748: INFO: Pod "downward-api-69cccd8c-d351-47b2-a106-8b0f874d7256": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008807804s
Sep 11 18:26:38.753: INFO: Pod "downward-api-69cccd8c-d351-47b2-a106-8b0f874d7256": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013763636s
STEP: Saw pod success
Sep 11 18:26:38.753: INFO: Pod "downward-api-69cccd8c-d351-47b2-a106-8b0f874d7256" satisfied condition "Succeeded or Failed"
Sep 11 18:26:38.757: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downward-api-69cccd8c-d351-47b2-a106-8b0f874d7256 container dapi-container: <nil>
STEP: delete the pod
Sep 11 18:26:38.796: INFO: Waiting for pod downward-api-69cccd8c-d351-47b2-a106-8b0f874d7256 to disappear
Sep 11 18:26:38.799: INFO: Pod downward-api-69cccd8c-d351-47b2-a106-8b0f874d7256 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:38.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5177" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":163,"skipped":2594,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:38.829: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 18:26:38.998: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb86360a-32c1-4b1e-95ee-ef8733f695e4" in namespace "downward-api-6702" to be "Succeeded or Failed"
Sep 11 18:26:39.015: INFO: Pod "downwardapi-volume-eb86360a-32c1-4b1e-95ee-ef8733f695e4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.853067ms
Sep 11 18:26:41.022: INFO: Pod "downwardapi-volume-eb86360a-32c1-4b1e-95ee-ef8733f695e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024059727s
STEP: Saw pod success
Sep 11 18:26:41.023: INFO: Pod "downwardapi-volume-eb86360a-32c1-4b1e-95ee-ef8733f695e4" satisfied condition "Succeeded or Failed"
Sep 11 18:26:41.026: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-eb86360a-32c1-4b1e-95ee-ef8733f695e4 container client-container: <nil>
STEP: delete the pod
Sep 11 18:26:41.078: INFO: Waiting for pod downwardapi-volume-eb86360a-32c1-4b1e-95ee-ef8733f695e4 to disappear
Sep 11 18:26:41.082: INFO: Pod downwardapi-volume-eb86360a-32c1-4b1e-95ee-ef8733f695e4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:41.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6702" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2596,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:41.115: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8107
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:41.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8107" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":165,"skipped":2605,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:41.336: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 11 18:26:41.503: INFO: Waiting up to 5m0s for pod "pod-9a6330f9-7fbc-4f2f-aa3b-5e348bdd4041" in namespace "emptydir-5539" to be "Succeeded or Failed"
Sep 11 18:26:41.508: INFO: Pod "pod-9a6330f9-7fbc-4f2f-aa3b-5e348bdd4041": Phase="Pending", Reason="", readiness=false. Elapsed: 4.530256ms
Sep 11 18:26:43.513: INFO: Pod "pod-9a6330f9-7fbc-4f2f-aa3b-5e348bdd4041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009160125s
STEP: Saw pod success
Sep 11 18:26:43.513: INFO: Pod "pod-9a6330f9-7fbc-4f2f-aa3b-5e348bdd4041" satisfied condition "Succeeded or Failed"
Sep 11 18:26:43.516: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-9a6330f9-7fbc-4f2f-aa3b-5e348bdd4041 container test-container: <nil>
STEP: delete the pod
Sep 11 18:26:43.545: INFO: Waiting for pod pod-9a6330f9-7fbc-4f2f-aa3b-5e348bdd4041 to disappear
Sep 11 18:26:43.550: INFO: Pod pod-9a6330f9-7fbc-4f2f-aa3b-5e348bdd4041 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:43.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5539" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":166,"skipped":2607,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:43.580: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 11 18:26:46.787: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:26:46.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1481" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":167,"skipped":2608,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:26:46.889: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6989
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6989
STEP: Creating statefulset with conflicting port in namespace statefulset-6989
STEP: Waiting until pod test-pod will start running in namespace statefulset-6989
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6989
Sep 11 18:26:51.142: INFO: Observed stateful pod in namespace: statefulset-6989, name: ss-0, uid: edb1618b-880f-46a4-a45d-cadabe51721a, status phase: Pending. Waiting for statefulset controller to delete.
Sep 11 18:26:51.738: INFO: Observed stateful pod in namespace: statefulset-6989, name: ss-0, uid: edb1618b-880f-46a4-a45d-cadabe51721a, status phase: Failed. Waiting for statefulset controller to delete.
Sep 11 18:26:52.055: INFO: Observed stateful pod in namespace: statefulset-6989, name: ss-0, uid: edb1618b-880f-46a4-a45d-cadabe51721a, status phase: Failed. Waiting for statefulset controller to delete.
Sep 11 18:26:52.055: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6989
STEP: Removing pod with conflicting port in namespace statefulset-6989
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6989 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 11 18:26:56.579: INFO: Deleting all statefulset in ns statefulset-6989
Sep 11 18:26:56.583: INFO: Scaling statefulset ss to 0
Sep 11 18:27:06.613: INFO: Waiting for statefulset status.replicas updated to 0
Sep 11 18:27:06.618: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:27:06.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6989" for this suite.

â€¢ [SLOW TEST:19.781 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":168,"skipped":2614,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:27:06.670: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0911 18:27:46.886839      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 11 18:28:48.908: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Sep 11 18:28:48.908: INFO: Deleting pod "simpletest.rc-4z4mc" in namespace "gc-1611"
Sep 11 18:28:48.930: INFO: Deleting pod "simpletest.rc-6rrzn" in namespace "gc-1611"
Sep 11 18:28:48.950: INFO: Deleting pod "simpletest.rc-8jf4r" in namespace "gc-1611"
Sep 11 18:28:48.972: INFO: Deleting pod "simpletest.rc-nchbn" in namespace "gc-1611"
Sep 11 18:28:48.992: INFO: Deleting pod "simpletest.rc-q2kwg" in namespace "gc-1611"
Sep 11 18:28:49.012: INFO: Deleting pod "simpletest.rc-s8tzd" in namespace "gc-1611"
Sep 11 18:28:49.028: INFO: Deleting pod "simpletest.rc-z8bnc" in namespace "gc-1611"
Sep 11 18:28:49.052: INFO: Deleting pod "simpletest.rc-zbchb" in namespace "gc-1611"
Sep 11 18:28:49.069: INFO: Deleting pod "simpletest.rc-zcthq" in namespace "gc-1611"
Sep 11 18:28:49.092: INFO: Deleting pod "simpletest.rc-zvchk" in namespace "gc-1611"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:28:49.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1611" for this suite.

â€¢ [SLOW TEST:102.494 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":169,"skipped":2628,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:28:49.164: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 11 18:28:53.868: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2195bc87-c58b-47ed-bb82-8d04e2fe9f91"
Sep 11 18:28:53.868: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2195bc87-c58b-47ed-bb82-8d04e2fe9f91" in namespace "pods-7031" to be "terminated due to deadline exceeded"
Sep 11 18:28:53.872: INFO: Pod "pod-update-activedeadlineseconds-2195bc87-c58b-47ed-bb82-8d04e2fe9f91": Phase="Running", Reason="", readiness=true. Elapsed: 3.730751ms
Sep 11 18:28:55.876: INFO: Pod "pod-update-activedeadlineseconds-2195bc87-c58b-47ed-bb82-8d04e2fe9f91": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.007883157s
Sep 11 18:28:55.876: INFO: Pod "pod-update-activedeadlineseconds-2195bc87-c58b-47ed-bb82-8d04e2fe9f91" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:28:55.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7031" for this suite.

â€¢ [SLOW TEST:6.743 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":170,"skipped":2648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:28:55.907: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7396
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-198d93ed-f461-4acf-917b-bcc7c7077dec
STEP: Creating secret with name s-test-opt-upd-ab986b66-01ac-4e2d-a408-83302509e61e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-198d93ed-f461-4acf-917b-bcc7c7077dec
STEP: Updating secret s-test-opt-upd-ab986b66-01ac-4e2d-a408-83302509e61e
STEP: Creating secret with name s-test-opt-create-04a5fc5f-5327-420e-bd66-7613664fbaa9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:29:00.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7396" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":171,"skipped":2713,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:29:00.269: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep 11 18:29:00.420: INFO: PodSpec: initContainers in spec.initContainers
Sep 11 18:29:45.182: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e9a302e5-ca91-48a2-9b96-fbd5e3c656c3", GenerateName:"", Namespace:"init-container-9721", SelfLink:"/api/v1/namespaces/init-container-9721/pods/pod-init-e9a302e5-ca91-48a2-9b96-fbd5e3c656c3", UID:"de47b1a8-53b9-41ec-beb8-18ac9ab130ad", ResourceVersion:"2773574", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63735445740, loc:(*time.Location)(0x7702840)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"420859746"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.123.14/32", "kubernetes.io/psp":"admin"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005114c20), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005114c40)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005114c60), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005114c80)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005114ca0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005114cc0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-dp7nn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc007db7380), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-dp7nn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-dp7nn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-dp7nn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005e7f7a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-10-2-19.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0035bc930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005e7f820)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005e7f840)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005e7f848), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005e7f84c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0054790b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445740, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445740, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445740, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445740, loc:(*time.Location)(0x7702840)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.2.19", PodIP:"192.168.123.14", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.123.14"}}, StartTime:(*v1.Time)(0xc005114ce0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035bca80)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035bcaf0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://23aab36a8961f28133e0b784bfc7f14b88688379213e0c3b8f8a207756081c4e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005114d20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005114d00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc005e7f8cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:29:45.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9721" for this suite.

â€¢ [SLOW TEST:44.941 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":172,"skipped":2727,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:29:45.211: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 11 18:29:45.368: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:29:51.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8006" for this suite.

â€¢ [SLOW TEST:6.154 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2735,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:29:51.364: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:29:52.668: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 18:29:54.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445792, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445792, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445792, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735445792, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:29:57.726: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:29:57.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1129" for this suite.
STEP: Destroying namespace "webhook-1129-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.583 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":174,"skipped":2735,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:29:57.948: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 18:29:58.117: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc1e2c91-98c5-4ee7-9406-0fac45a63921" in namespace "projected-2116" to be "Succeeded or Failed"
Sep 11 18:29:58.122: INFO: Pod "downwardapi-volume-fc1e2c91-98c5-4ee7-9406-0fac45a63921": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039608ms
Sep 11 18:30:00.127: INFO: Pod "downwardapi-volume-fc1e2c91-98c5-4ee7-9406-0fac45a63921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009297035s
STEP: Saw pod success
Sep 11 18:30:00.127: INFO: Pod "downwardapi-volume-fc1e2c91-98c5-4ee7-9406-0fac45a63921" satisfied condition "Succeeded or Failed"
Sep 11 18:30:00.131: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-fc1e2c91-98c5-4ee7-9406-0fac45a63921 container client-container: <nil>
STEP: delete the pod
Sep 11 18:30:00.169: INFO: Waiting for pod downwardapi-volume-fc1e2c91-98c5-4ee7-9406-0fac45a63921 to disappear
Sep 11 18:30:00.173: INFO: Pod downwardapi-volume-fc1e2c91-98c5-4ee7-9406-0fac45a63921 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:30:00.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2116" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":175,"skipped":2737,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:30:00.199: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 11 18:30:00.398: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:00.398: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:00.398: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:00.402: INFO: Number of nodes with available pods: 0
Sep 11 18:30:00.402: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:30:01.408: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:01.408: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:01.408: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:01.413: INFO: Number of nodes with available pods: 0
Sep 11 18:30:01.413: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:30:02.408: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:02.408: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:02.408: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:02.413: INFO: Number of nodes with available pods: 1
Sep 11 18:30:02.413: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:30:03.410: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:03.410: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:03.410: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:03.417: INFO: Number of nodes with available pods: 3
Sep 11 18:30:03.417: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 11 18:30:03.451: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:03.451: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:03.451: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:03.457: INFO: Number of nodes with available pods: 2
Sep 11 18:30:03.457: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:04.465: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:04.465: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:04.465: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:04.469: INFO: Number of nodes with available pods: 2
Sep 11 18:30:04.469: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:05.465: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:05.465: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:05.465: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:05.469: INFO: Number of nodes with available pods: 2
Sep 11 18:30:05.469: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:06.463: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:06.463: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:06.463: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:06.468: INFO: Number of nodes with available pods: 2
Sep 11 18:30:06.468: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:07.463: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:07.463: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:07.463: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:07.467: INFO: Number of nodes with available pods: 2
Sep 11 18:30:07.467: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:08.466: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:08.466: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:08.466: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:08.471: INFO: Number of nodes with available pods: 2
Sep 11 18:30:08.471: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:09.464: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:09.464: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:09.464: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:09.468: INFO: Number of nodes with available pods: 2
Sep 11 18:30:09.468: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:10.465: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:10.465: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:10.465: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:10.469: INFO: Number of nodes with available pods: 2
Sep 11 18:30:10.469: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:11.464: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:11.464: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:11.464: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:11.469: INFO: Number of nodes with available pods: 2
Sep 11 18:30:11.469: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:12.464: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:12.464: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:12.464: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:12.469: INFO: Number of nodes with available pods: 2
Sep 11 18:30:12.469: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:13.468: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:13.468: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:13.468: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:13.477: INFO: Number of nodes with available pods: 2
Sep 11 18:30:13.477: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:14.466: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:14.466: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:14.466: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:14.471: INFO: Number of nodes with available pods: 2
Sep 11 18:30:14.471: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:15.464: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:15.464: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:15.464: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:15.468: INFO: Number of nodes with available pods: 2
Sep 11 18:30:15.468: INFO: Node ip-10-10-2-19.ec2.internal is running more than one daemon pod
Sep 11 18:30:16.464: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:16.464: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:16.464: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:30:16.469: INFO: Number of nodes with available pods: 3
Sep 11 18:30:16.469: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9329, will wait for the garbage collector to delete the pods
Sep 11 18:30:16.539: INFO: Deleting DaemonSet.extensions daemon-set took: 12.830247ms
Sep 11 18:30:16.640: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.296046ms
Sep 11 18:30:19.644: INFO: Number of nodes with available pods: 0
Sep 11 18:30:19.644: INFO: Number of running nodes: 0, number of available pods: 0
Sep 11 18:30:19.648: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9329/daemonsets","resourceVersion":"2773993"},"items":null}

Sep 11 18:30:19.651: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9329/pods","resourceVersion":"2773993"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:30:19.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9329" for this suite.

â€¢ [SLOW TEST:19.499 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":176,"skipped":2751,"failed":0}
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:30:19.698: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:30:19.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1271" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":177,"skipped":2751,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:30:19.944: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6516
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 11 18:30:20.494: INFO: Pod name wrapped-volume-race-8670afbd-6f96-40e9-91c6-ec3f7887f2ed: Found 0 pods out of 5
Sep 11 18:30:25.503: INFO: Pod name wrapped-volume-race-8670afbd-6f96-40e9-91c6-ec3f7887f2ed: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8670afbd-6f96-40e9-91c6-ec3f7887f2ed in namespace emptydir-wrapper-6516, will wait for the garbage collector to delete the pods
Sep 11 18:30:35.604: INFO: Deleting ReplicationController wrapped-volume-race-8670afbd-6f96-40e9-91c6-ec3f7887f2ed took: 12.436566ms
Sep 11 18:30:36.705: INFO: Terminating ReplicationController wrapped-volume-race-8670afbd-6f96-40e9-91c6-ec3f7887f2ed pods took: 1.100263203s
STEP: Creating RC which spawns configmap-volume pods
Sep 11 18:30:43.440: INFO: Pod name wrapped-volume-race-b3f10fb2-60e5-43de-bb48-de3153c06cb8: Found 0 pods out of 5
Sep 11 18:30:48.447: INFO: Pod name wrapped-volume-race-b3f10fb2-60e5-43de-bb48-de3153c06cb8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b3f10fb2-60e5-43de-bb48-de3153c06cb8 in namespace emptydir-wrapper-6516, will wait for the garbage collector to delete the pods
Sep 11 18:30:58.550: INFO: Deleting ReplicationController wrapped-volume-race-b3f10fb2-60e5-43de-bb48-de3153c06cb8 took: 12.7787ms
Sep 11 18:30:59.850: INFO: Terminating ReplicationController wrapped-volume-race-b3f10fb2-60e5-43de-bb48-de3153c06cb8 pods took: 1.300174293s
STEP: Creating RC which spawns configmap-volume pods
Sep 11 18:31:13.679: INFO: Pod name wrapped-volume-race-34333ca1-92ee-41a3-98ae-5d53cdc2d1ec: Found 0 pods out of 5
Sep 11 18:31:18.686: INFO: Pod name wrapped-volume-race-34333ca1-92ee-41a3-98ae-5d53cdc2d1ec: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-34333ca1-92ee-41a3-98ae-5d53cdc2d1ec in namespace emptydir-wrapper-6516, will wait for the garbage collector to delete the pods
Sep 11 18:31:28.798: INFO: Deleting ReplicationController wrapped-volume-race-34333ca1-92ee-41a3-98ae-5d53cdc2d1ec took: 13.857422ms
Sep 11 18:31:29.898: INFO: Terminating ReplicationController wrapped-volume-race-34333ca1-92ee-41a3-98ae-5d53cdc2d1ec pods took: 1.100270613s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:31:41.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6516" for this suite.

â€¢ [SLOW TEST:81.695 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":178,"skipped":2801,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:31:41.640: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5574
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 11 18:31:41.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5574 /api/v1/namespaces/watch-5574/configmaps/e2e-watch-test-resource-version 70f2041b-1e16-4a9a-8338-52ab7874b724 2775199 0 2020-09-11 18:31:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-09-11 18:31:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 11 18:31:41.834: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5574 /api/v1/namespaces/watch-5574/configmaps/e2e-watch-test-resource-version 70f2041b-1e16-4a9a-8338-52ab7874b724 2775200 0 2020-09-11 18:31:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-09-11 18:31:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:31:41.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5574" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":179,"skipped":2815,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:31:41.860: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1535
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 11 18:31:42.010: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 11 18:31:42.021: INFO: Waiting for terminating namespaces to be deleted...
Sep 11 18:31:42.025: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-1-141.ec2.internal before test
Sep 11 18:31:42.039: INFO: calico-node-jgr6s from kube-system started at 2020-08-31 21:04:41 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 18:31:42.039: INFO: kube-proxy-72jmn from kube-system started at 2020-08-31 21:04:41 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-cert-manager-7b65d9c9d6-jst49 from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-flux-memcached-d75b6d9d6-vpnqm from md-flux started at 2020-09-11 16:47:51 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container memcached ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-gangway-66c5d44b4c-f7psj from md-gangway started at 2020-09-11 16:47:59 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container gangway ready: true, restart count 0
Sep 11 18:31:42.039: INFO: harbor-pg-1 from md-harbor started at 2020-09-11 16:49:13 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container postgres ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-harbor-harbor-notary-server-ddbf4b74b-69jlj from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container notary-server ready: true, restart count 1
Sep 11 18:31:42.039: INFO: md-harbor-harbor-portal-7976969b-27bjm from md-harbor started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container portal ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-harbor-harbor-registry-9cdb888bc-97n4b from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (2 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container registry ready: true, restart count 0
Sep 11 18:31:42.039: INFO: 	Container registryctl ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-mlqtr from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-harbor-redis-redis-ha-server-2 from md-harbor started at 2020-09-11 16:49:22 +0000 UTC (2 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container redis ready: true, restart count 0
Sep 11 18:31:42.039: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-helm-operator-557b9b587c-vmkw8 from md-helm-operator started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container flux-helm-operator ready: false, restart count 6
Sep 11 18:31:42.039: INFO: md-kubedb-84d9fff66c-6922k from md-kubedb started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container operator ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-loki-stack-0 from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container loki ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-loki-stack-promtail-hm9cx from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container promtail ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-nginx-ingress-controller-dfxrw from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 18:31:42.039: INFO: alertmanager-md-prometheus-operator-alertmanager-0 from md-prometheus-operator started at 2020-09-11 16:49:13 +0000 UTC (2 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container alertmanager ready: true, restart count 0
Sep 11 18:31:42.039: INFO: 	Container config-reloader ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-prometheus-operator-kube-state-metrics-6f5d54b49-mtqmh from md-prometheus-operator started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-prometheus-operator-operator-7d5c6f8689-tfzwv from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (2 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 11 18:31:42.039: INFO: 	Container tls-proxy ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-prometheus-operator-prometheus-node-exporter-6sz5n from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 18:31:42.039: INFO: md-velero-6cc8bd66b5-rqkv9 from md-velero started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container velero ready: true, restart count 0
Sep 11 18:31:42.039: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-ps57h from sonobuoy started at 2020-09-11 17:37:25 +0000 UTC (2 container statuses recorded)
Sep 11 18:31:42.039: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 18:31:42.039: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 11 18:31:42.039: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-2-19.ec2.internal before test
Sep 11 18:31:42.051: INFO: calico-node-w9tmq from kube-system started at 2020-08-31 21:04:43 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.051: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 18:31:42.051: INFO: kube-proxy-djv8h from kube-system started at 2020-08-31 21:04:43 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.051: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 18:31:42.051: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-5w2c8 from md-harbor started at 2020-09-11 18:20:54 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.051: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 18:31:42.051: INFO: md-harbor-redis-redis-ha-server-0 from md-harbor started at 2020-09-11 18:20:58 +0000 UTC (2 container statuses recorded)
Sep 11 18:31:42.051: INFO: 	Container redis ready: true, restart count 0
Sep 11 18:31:42.051: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 18:31:42.051: INFO: md-loki-stack-promtail-gv7pm from md-loki-stack started at 2020-09-11 18:20:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.051: INFO: 	Container promtail ready: true, restart count 0
Sep 11 18:31:42.051: INFO: md-nginx-ingress-controller-28nhj from md-nginx-ingress started at 2020-09-11 18:20:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.051: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 18:31:42.051: INFO: md-prometheus-operator-prometheus-node-exporter-scwg9 from md-prometheus-operator started at 2020-09-11 18:20:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.051: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 18:31:42.051: INFO: prometheus-md-prometheus-operator-prometheus-0 from md-prometheus-operator started at 2020-09-11 18:20:51 +0000 UTC (3 container statuses recorded)
Sep 11 18:31:42.051: INFO: 	Container prometheus ready: true, restart count 0
Sep 11 18:31:42.051: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 11 18:31:42.051: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 11 18:31:42.051: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-jnktw from sonobuoy started at 2020-09-11 17:37:24 +0000 UTC (2 container statuses recorded)
Sep 11 18:31:42.051: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 18:31:42.051: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 11 18:31:42.051: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-3-166.ec2.internal before test
Sep 11 18:31:42.064: INFO: calico-node-wfxpk from kube-system started at 2020-08-31 21:04:40 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 18:31:42.064: INFO: kube-proxy-9vh4x from kube-system started at 2020-08-31 21:04:40 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 18:31:42.064: INFO: sealed-secrets-controller-7dfd7dcc6b-g8r5f from kube-system started at 2020-09-11 16:30:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container sealed-secrets-controller ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-cert-manager-cainjector-76b464f9f7-wbmzz from md-cert-manager started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-cert-manager-webhook-656744686b-gch9j from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-dex-5b4f679bd4-dmmrq from md-dex started at 2020-09-11 16:47:54 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container main ready: true, restart count 1
Sep 11 18:31:42.064: INFO: md-flux-5465b59cbd-dd4fp from md-flux started at 2020-09-11 16:47:51 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container flux ready: true, restart count 0
Sep 11 18:31:42.064: INFO: harbor-pg-0 from md-harbor started at 2020-09-11 16:48:15 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container postgres ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-harbor-harbor-chartmuseum-55b4567ff8-cx2ch from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container chartmuseum ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-harbor-harbor-clair-6479d48569-vd6wm from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (2 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container adapter ready: true, restart count 0
Sep 11 18:31:42.064: INFO: 	Container clair ready: true, restart count 1
Sep 11 18:31:42.064: INFO: md-harbor-harbor-core-7999f49dc-vpk6g from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container core ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-harbor-harbor-jobservice-556554c4c7-gqzhp from md-harbor started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container jobservice ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-harbor-harbor-notary-signer-6789c9d4bc-db2kr from md-harbor started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container notary-signer ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-tdbqd from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-harbor-redis-redis-ha-server-1 from md-harbor started at 2020-09-11 16:48:53 +0000 UTC (2 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container redis ready: true, restart count 0
Sep 11 18:31:42.064: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-loki-stack-promtail-94946 from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container promtail ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-nginx-ingress-controller-lhjv9 from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-nginx-ingress-default-backend-74c98f4684-r7kpq from md-nginx-ingress started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-oauth2-proxy-5c876db75d-jp4lz from md-oauth2-proxy started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container oauth2-proxy ready: true, restart count 0
Sep 11 18:31:42.064: INFO: md-prometheus-operator-prometheus-node-exporter-mjvrq from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 18:31:42.064: INFO: sonobuoy from sonobuoy started at 2020-09-11 17:37:23 +0000 UTC (1 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 11 18:31:42.064: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-lb975 from sonobuoy started at 2020-09-11 17:37:25 +0000 UTC (2 container statuses recorded)
Sep 11 18:31:42.064: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 11 18:31:42.064: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-49fe2bfc-e553-45e6-8b2f-98c45fb0d05a 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-49fe2bfc-e553-45e6-8b2f-98c45fb0d05a off the node ip-10-10-2-19.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-49fe2bfc-e553-45e6-8b2f-98c45fb0d05a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:36:46.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1535" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:304.350 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":180,"skipped":2830,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:36:46.210: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Sep 11 18:36:46.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-7433'
Sep 11 18:36:46.960: INFO: stderr: ""
Sep 11 18:36:46.960: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 11 18:36:46.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7433'
Sep 11 18:36:47.089: INFO: stderr: ""
Sep 11 18:36:47.089: INFO: stdout: "update-demo-nautilus-647v2 update-demo-nautilus-rfp88 "
Sep 11 18:36:47.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-647v2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:36:47.182: INFO: stderr: ""
Sep 11 18:36:47.182: INFO: stdout: ""
Sep 11 18:36:47.182: INFO: update-demo-nautilus-647v2 is created but not running
Sep 11 18:36:52.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7433'
Sep 11 18:36:52.538: INFO: stderr: ""
Sep 11 18:36:52.538: INFO: stdout: "update-demo-nautilus-647v2 update-demo-nautilus-rfp88 "
Sep 11 18:36:52.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-647v2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:36:52.811: INFO: stderr: ""
Sep 11 18:36:52.811: INFO: stdout: "true"
Sep 11 18:36:52.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-647v2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:36:52.911: INFO: stderr: ""
Sep 11 18:36:52.911: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 11 18:36:52.911: INFO: validating pod update-demo-nautilus-647v2
Sep 11 18:36:52.925: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 11 18:36:52.925: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 11 18:36:52.925: INFO: update-demo-nautilus-647v2 is verified up and running
Sep 11 18:36:52.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-rfp88 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:36:53.047: INFO: stderr: ""
Sep 11 18:36:53.047: INFO: stdout: "true"
Sep 11 18:36:53.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-rfp88 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:36:53.165: INFO: stderr: ""
Sep 11 18:36:53.165: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 11 18:36:53.165: INFO: validating pod update-demo-nautilus-rfp88
Sep 11 18:36:53.171: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 11 18:36:53.171: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 11 18:36:53.171: INFO: update-demo-nautilus-rfp88 is verified up and running
STEP: scaling down the replication controller
Sep 11 18:36:53.174: INFO: scanned /root for discovery docs: <nil>
Sep 11 18:36:53.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7433'
Sep 11 18:36:54.343: INFO: stderr: ""
Sep 11 18:36:54.343: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 11 18:36:54.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7433'
Sep 11 18:36:54.465: INFO: stderr: ""
Sep 11 18:36:54.465: INFO: stdout: "update-demo-nautilus-647v2 update-demo-nautilus-rfp88 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 11 18:36:59.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7433'
Sep 11 18:36:59.583: INFO: stderr: ""
Sep 11 18:36:59.583: INFO: stdout: "update-demo-nautilus-647v2 update-demo-nautilus-rfp88 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 11 18:37:04.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7433'
Sep 11 18:37:04.680: INFO: stderr: ""
Sep 11 18:37:04.680: INFO: stdout: "update-demo-nautilus-647v2 "
Sep 11 18:37:04.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-647v2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:37:04.785: INFO: stderr: ""
Sep 11 18:37:04.785: INFO: stdout: "true"
Sep 11 18:37:04.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-647v2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:37:04.885: INFO: stderr: ""
Sep 11 18:37:04.885: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 11 18:37:04.885: INFO: validating pod update-demo-nautilus-647v2
Sep 11 18:37:04.892: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 11 18:37:04.892: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 11 18:37:04.892: INFO: update-demo-nautilus-647v2 is verified up and running
STEP: scaling up the replication controller
Sep 11 18:37:04.894: INFO: scanned /root for discovery docs: <nil>
Sep 11 18:37:04.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7433'
Sep 11 18:37:06.042: INFO: stderr: ""
Sep 11 18:37:06.042: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 11 18:37:06.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7433'
Sep 11 18:37:06.158: INFO: stderr: ""
Sep 11 18:37:06.158: INFO: stdout: "update-demo-nautilus-647v2 update-demo-nautilus-f7xbh "
Sep 11 18:37:06.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-647v2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:37:06.262: INFO: stderr: ""
Sep 11 18:37:06.262: INFO: stdout: "true"
Sep 11 18:37:06.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-647v2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:37:06.364: INFO: stderr: ""
Sep 11 18:37:06.365: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 11 18:37:06.365: INFO: validating pod update-demo-nautilus-647v2
Sep 11 18:37:06.370: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 11 18:37:06.370: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 11 18:37:06.370: INFO: update-demo-nautilus-647v2 is verified up and running
Sep 11 18:37:06.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-f7xbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:37:06.479: INFO: stderr: ""
Sep 11 18:37:06.479: INFO: stdout: "true"
Sep 11 18:37:06.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods update-demo-nautilus-f7xbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7433'
Sep 11 18:37:06.584: INFO: stderr: ""
Sep 11 18:37:06.584: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 11 18:37:06.584: INFO: validating pod update-demo-nautilus-f7xbh
Sep 11 18:37:06.590: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 11 18:37:06.590: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 11 18:37:06.590: INFO: update-demo-nautilus-f7xbh is verified up and running
STEP: using delete to clean up resources
Sep 11 18:37:06.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete --grace-period=0 --force -f - --namespace=kubectl-7433'
Sep 11 18:37:06.711: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 11 18:37:06.711: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 11 18:37:06.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7433'
Sep 11 18:37:06.841: INFO: stderr: "No resources found in kubectl-7433 namespace.\n"
Sep 11 18:37:06.841: INFO: stdout: ""
Sep 11 18:37:06.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -l name=update-demo --namespace=kubectl-7433 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 11 18:37:06.948: INFO: stderr: ""
Sep 11 18:37:06.948: INFO: stdout: "update-demo-nautilus-647v2\nupdate-demo-nautilus-f7xbh\n"
Sep 11 18:37:07.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7433'
Sep 11 18:37:07.558: INFO: stderr: "No resources found in kubectl-7433 namespace.\n"
Sep 11 18:37:07.558: INFO: stdout: ""
Sep 11 18:37:07.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -l name=update-demo --namespace=kubectl-7433 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 11 18:37:07.668: INFO: stderr: ""
Sep 11 18:37:07.668: INFO: stdout: "update-demo-nautilus-647v2\nupdate-demo-nautilus-f7xbh\n"
Sep 11 18:37:07.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7433'
Sep 11 18:37:08.058: INFO: stderr: "No resources found in kubectl-7433 namespace.\n"
Sep 11 18:37:08.058: INFO: stdout: ""
Sep 11 18:37:08.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -l name=update-demo --namespace=kubectl-7433 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 11 18:37:08.163: INFO: stderr: ""
Sep 11 18:37:08.163: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:08.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7433" for this suite.

â€¢ [SLOW TEST:21.983 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":181,"skipped":2832,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:08.193: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:37:09.161: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 11 18:37:11.175: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446229, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446229, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446229, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446229, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:37:14.215: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:37:14.220: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:15.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4281" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:8.022 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":182,"skipped":2837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:16.215: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3604
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-172
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5517
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:32.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3604" for this suite.
STEP: Destroying namespace "nsdeletetest-172" for this suite.
Sep 11 18:37:32.892: INFO: Namespace nsdeletetest-172 was already deleted
STEP: Destroying namespace "nsdeletetest-5517" for this suite.

â€¢ [SLOW TEST:16.701 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":183,"skipped":2861,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:32.917: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:37:33.126: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fb088002-72e0-47f0-935f-09d86ee094bf", Controller:(*bool)(0xc005ee3596), BlockOwnerDeletion:(*bool)(0xc005ee3597)}}
Sep 11 18:37:33.137: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"09692508-f3c5-4546-b70f-0ec09e114bb8", Controller:(*bool)(0xc005f958ce), BlockOwnerDeletion:(*bool)(0xc005f958cf)}}
Sep 11 18:37:33.148: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"811ed1f2-bcea-4f88-94bf-2f892acccc1f", Controller:(*bool)(0xc005ee378e), BlockOwnerDeletion:(*bool)(0xc005ee378f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:38.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6707" for this suite.

â€¢ [SLOW TEST:5.304 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":184,"skipped":2878,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:38.221: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 11 18:37:40.950: INFO: Successfully updated pod "pod-update-4ac3d007-cdb2-47e7-9833-45a9b9584e3b"
STEP: verifying the updated pod is in kubernetes
Sep 11 18:37:40.965: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:40.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4373" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":185,"skipped":2891,"failed":0}
S
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:41.000: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:41.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2653" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":186,"skipped":2892,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:41.237: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:43.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7127" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":187,"skipped":2902,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:43.564: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:44.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3743" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":188,"skipped":2905,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:44.130: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 18:37:44.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad33bc1a-a0bd-4278-b816-d5fb7f838bda" in namespace "downward-api-6255" to be "Succeeded or Failed"
Sep 11 18:37:44.312: INFO: Pod "downwardapi-volume-ad33bc1a-a0bd-4278-b816-d5fb7f838bda": Phase="Pending", Reason="", readiness=false. Elapsed: 6.988752ms
Sep 11 18:37:46.318: INFO: Pod "downwardapi-volume-ad33bc1a-a0bd-4278-b816-d5fb7f838bda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012789996s
Sep 11 18:37:48.323: INFO: Pod "downwardapi-volume-ad33bc1a-a0bd-4278-b816-d5fb7f838bda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018086145s
STEP: Saw pod success
Sep 11 18:37:48.323: INFO: Pod "downwardapi-volume-ad33bc1a-a0bd-4278-b816-d5fb7f838bda" satisfied condition "Succeeded or Failed"
Sep 11 18:37:48.327: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-ad33bc1a-a0bd-4278-b816-d5fb7f838bda container client-container: <nil>
STEP: delete the pod
Sep 11 18:37:48.370: INFO: Waiting for pod downwardapi-volume-ad33bc1a-a0bd-4278-b816-d5fb7f838bda to disappear
Sep 11 18:37:48.374: INFO: Pod downwardapi-volume-ad33bc1a-a0bd-4278-b816-d5fb7f838bda no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:48.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6255" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":2971,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:48.401: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7229
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep 11 18:37:48.552: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:52.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7229" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":190,"skipped":2981,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:52.564: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8748
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:37:52.723: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:54.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8748" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":191,"skipped":2985,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:54.889: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:37:55.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-7105'
Sep 11 18:37:55.489: INFO: stderr: ""
Sep 11 18:37:55.489: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep 11 18:37:55.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-7105'
Sep 11 18:37:55.830: INFO: stderr: ""
Sep 11 18:37:55.830: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 11 18:37:56.835: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 11 18:37:56.835: INFO: Found 1 / 1
Sep 11 18:37:56.835: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 11 18:37:56.839: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 11 18:37:56.839: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 11 18:37:56.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 describe pod agnhost-primary-2hpbh --namespace=kubectl-7105'
Sep 11 18:37:56.961: INFO: stderr: ""
Sep 11 18:37:56.961: INFO: stdout: "Name:         agnhost-primary-2hpbh\nNamespace:    kubectl-7105\nPriority:     0\nNode:         ip-10-10-2-19.ec2.internal/10.10.2.19\nStart Time:   Fri, 11 Sep 2020 18:37:55 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 192.168.123.54/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           192.168.123.54\nIPs:\n  IP:           192.168.123.54\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://45afb8525756d6fd7939cd533da2801cb9bed2d3d8322c63ad0492ff1dbc4522\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 11 Sep 2020 18:37:56 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9mfd8 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-9mfd8:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-9mfd8\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From                                 Message\n  ----    ------     ----  ----                                 -------\n  Normal  Scheduled  1s                                         Successfully assigned kubectl-7105/agnhost-primary-2hpbh to ip-10-10-2-19.ec2.internal\n  Normal  Pulled     0s    kubelet, ip-10-10-2-19.ec2.internal  Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    0s    kubelet, ip-10-10-2-19.ec2.internal  Created container agnhost-primary\n  Normal  Started    0s    kubelet, ip-10-10-2-19.ec2.internal  Started container agnhost-primary\n"
Sep 11 18:37:56.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 describe rc agnhost-primary --namespace=kubectl-7105'
Sep 11 18:37:57.096: INFO: stderr: ""
Sep 11 18:37:57.096: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7105\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-2hpbh\n"
Sep 11 18:37:57.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 describe service agnhost-primary --namespace=kubectl-7105'
Sep 11 18:37:57.210: INFO: stderr: ""
Sep 11 18:37:57.210: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7105\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.111.187.67\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.123.54:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 11 18:37:57.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 describe node ip-10-10-1-141.ec2.internal'
Sep 11 18:37:57.379: INFO: stderr: ""
Sep 11 18:37:57.379: INFO: stdout: "Name:               ip-10-10-1-141.ec2.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-10-1-141.ec2.internal\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=t3.large\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1a\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.10.1.141/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.134.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 31 Aug 2020 21:04:41 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-10-1-141.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 11 Sep 2020 18:37:49 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 31 Aug 2020 21:04:59 +0000   Mon, 31 Aug 2020 21:04:59 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 11 Sep 2020 18:36:18 +0000   Mon, 31 Aug 2020 21:04:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 11 Sep 2020 18:36:18 +0000   Mon, 31 Aug 2020 21:04:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 11 Sep 2020 18:36:18 +0000   Mon, 31 Aug 2020 21:04:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 11 Sep 2020 18:36:18 +0000   Mon, 31 Aug 2020 21:04:51 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.10.1.141\n  Hostname:     ip-10-10-1-141.ec2.internal\n  InternalDNS:  ip-10-10-1-141.ec2.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           52417516Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      7904680Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       3\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           48307982666\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      7802280Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       3\nSystem Info:\n  Machine ID:                 3d5c05376530a2eb49e3e90576f83c5b\n  System UUID:                EC2DA72D-9F24-C3E4-1798-862938DFBD3D\n  Boot ID:                    9927f441-0ffe-4df0-be91-ed8e06bb1999\n  Kernel Version:             3.10.0-1062.12.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.12\n  Kubelet Version:            v1.19.0\n  Kube-Proxy Version:         v1.19.0\nPodCIDR:                      192.168.4.0/24\nPodCIDRs:                     192.168.4.0/24\nProviderID:                   aws:///us-east-1a/i-057a6686411fbec84\nNon-terminated Pods:          (22 in total)\n  Namespace                   Name                                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                         ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-jgr6s                                            250m (12%)    0 (0%)      0 (0%)           0 (0%)         10d\n  kube-system                 kube-proxy-72jmn                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         10d\n  md-cert-manager             md-cert-manager-7b65d9c9d6-jst49                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         110m\n  md-flux                     md-flux-memcached-d75b6d9d6-vpnqm                            50m (2%)      0 (0%)      64Mi (0%)        0 (0%)         110m\n  md-gangway                  md-gangway-66c5d44b4c-f7psj                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  md-harbor                   harbor-pg-1                                                  250m (12%)    1 (50%)     256M (3%)        512M (6%)      109m\n  md-harbor                   md-harbor-harbor-notary-server-ddbf4b74b-69jlj               0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  md-harbor                   md-harbor-harbor-portal-7976969b-27bjm                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  md-harbor                   md-harbor-harbor-registry-9cdb888bc-97n4b                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  md-harbor                   md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-mlqtr            0 (0%)        0 (0%)      0 (0%)           0 (0%)         110m\n  md-harbor                   md-harbor-redis-redis-ha-server-2                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         108m\n  md-helm-operator            md-helm-operator-557b9b587c-vmkw8                            50m (2%)      0 (0%)      64Mi (0%)        1Gi (13%)      17m\n  md-kubedb                   md-kubedb-84d9fff66c-6922k                                   100m (5%)     0 (0%)      0 (0%)           0 (0%)         17m\n  md-loki-stack               md-loki-stack-0                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  md-loki-stack               md-loki-stack-promtail-hm9cx                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  md-nginx-ingress            md-nginx-ingress-controller-dfxrw                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  md-prometheus-operator      alertmanager-md-prometheus-operator-alertmanager-0           100m (5%)     100m (5%)   225Mi (2%)       25Mi (0%)      108m\n  md-prometheus-operator      md-prometheus-operator-kube-state-metrics-6f5d54b49-mtqmh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  md-prometheus-operator      md-prometheus-operator-operator-7d5c6f8689-tfzwv             0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  md-prometheus-operator      md-prometheus-operator-prometheus-node-exporter-6sz5n        0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  md-velero                   md-velero-6cc8bd66b5-rqkv9                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-ps57h      0 (0%)        0 (0%)      0 (0%)           0 (0%)         60m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests       Limits\n  --------                    --------       ------\n  cpu                         800m (40%)     1100m (55%)\n  memory                      611472Ki (7%)  1611956224 (20%)\n  ephemeral-storage           0 (0%)         0 (0%)\n  hugepages-1Gi               0 (0%)         0 (0%)\n  hugepages-2Mi               0 (0%)         0 (0%)\n  attachable-volumes-aws-ebs  0              0\n  scheduling.k8s.io/foo       0              0\nEvents:                       <none>\n"
Sep 11 18:37:57.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 describe namespace kubectl-7105'
Sep 11 18:37:57.495: INFO: stderr: ""
Sep 11 18:37:57.495: INFO: stdout: "Name:         kubectl-7105\nLabels:       e2e-framework=kubectl\n              e2e-run=3f89ce03-4277-47b6-941c-d01a38cf0285\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:37:57.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7105" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":192,"skipped":2998,"failed":0}

------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:37:57.530: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-5587
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5587 to expose endpoints map[]
Sep 11 18:37:57.761: INFO: successfully validated that service endpoint-test2 in namespace services-5587 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5587
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5587 to expose endpoints map[pod1:[80]]
Sep 11 18:37:59.817: INFO: successfully validated that service endpoint-test2 in namespace services-5587 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-5587
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5587 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 11 18:38:01.854: INFO: successfully validated that service endpoint-test2 in namespace services-5587 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-5587
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5587 to expose endpoints map[pod2:[80]]
Sep 11 18:38:01.887: INFO: successfully validated that service endpoint-test2 in namespace services-5587 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-5587
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5587 to expose endpoints map[]
Sep 11 18:38:02.919: INFO: successfully validated that service endpoint-test2 in namespace services-5587 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:38:02.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5587" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:5.487 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":193,"skipped":2998,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:38:03.017: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 11 18:38:03.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2645'
Sep 11 18:38:03.299: INFO: stderr: ""
Sep 11 18:38:03.299: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Sep 11 18:38:03.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pod e2e-test-httpd-pod -o json --namespace=kubectl-2645'
Sep 11 18:38:03.436: INFO: stderr: ""
Sep 11 18:38:03.436: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"admin\"\n        },\n        \"creationTimestamp\": \"2020-09-11T18:38:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-11T18:38:03Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-11T18:38:03Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2645\",\n        \"resourceVersion\": \"2777934\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2645/pods/e2e-test-httpd-pod\",\n        \"uid\": \"726f4be7-61ab-422f-b49c-1cad0d411294\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-pf78l\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-10-2-19.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-pf78l\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-pf78l\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-11T18:38:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-11T18:38:03Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-11T18:38:03Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-11T18:38:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.2.19\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-09-11T18:38:03Z\"\n    }\n}\n"
Sep 11 18:38:03.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 replace -f - --dry-run server --namespace=kubectl-2645'
Sep 11 18:38:03.908: INFO: stderr: "W0911 18:38:03.512878     758 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Sep 11 18:38:03.908: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Sep 11 18:38:03.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete pods e2e-test-httpd-pod --namespace=kubectl-2645'
Sep 11 18:38:13.215: INFO: stderr: ""
Sep 11 18:38:13.215: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:38:13.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2645" for this suite.

â€¢ [SLOW TEST:10.239 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:919
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":194,"skipped":2999,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:38:13.256: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2050
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8376
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2520
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:38:20.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2050" for this suite.
STEP: Destroying namespace "nsdeletetest-8376" for this suite.
Sep 11 18:38:20.821: INFO: Namespace nsdeletetest-8376 was already deleted
STEP: Destroying namespace "nsdeletetest-2520" for this suite.

â€¢ [SLOW TEST:7.588 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":195,"skipped":3005,"failed":0}
SSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:38:20.844: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 11 18:38:25.076: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1947 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:38:25.076: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:38:25.289: INFO: Exec stderr: ""
Sep 11 18:38:25.289: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1947 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:38:25.289: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:38:25.440: INFO: Exec stderr: ""
Sep 11 18:38:25.440: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1947 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:38:25.440: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:38:25.611: INFO: Exec stderr: ""
Sep 11 18:38:25.611: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1947 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:38:25.611: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:38:25.735: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 11 18:38:25.735: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1947 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:38:25.735: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:38:25.873: INFO: Exec stderr: ""
Sep 11 18:38:25.875: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1947 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:38:25.875: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:38:25.985: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 11 18:38:25.985: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1947 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:38:25.985: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:38:26.411: INFO: Exec stderr: ""
Sep 11 18:38:26.412: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1947 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:38:26.412: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:38:26.519: INFO: Exec stderr: ""
Sep 11 18:38:26.519: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1947 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:38:26.519: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:38:26.634: INFO: Exec stderr: ""
Sep 11 18:38:26.634: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1947 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:38:26.634: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:38:26.765: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:38:26.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1947" for this suite.

â€¢ [SLOW TEST:5.949 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":196,"skipped":3009,"failed":0}
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:38:26.794: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 18:38:26.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6cf75b6a-4116-4434-87b0-f2ff888e8737" in namespace "downward-api-655" to be "Succeeded or Failed"
Sep 11 18:38:26.993: INFO: Pod "downwardapi-volume-6cf75b6a-4116-4434-87b0-f2ff888e8737": Phase="Pending", Reason="", readiness=false. Elapsed: 12.253077ms
Sep 11 18:38:28.998: INFO: Pod "downwardapi-volume-6cf75b6a-4116-4434-87b0-f2ff888e8737": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017032978s
STEP: Saw pod success
Sep 11 18:38:28.998: INFO: Pod "downwardapi-volume-6cf75b6a-4116-4434-87b0-f2ff888e8737" satisfied condition "Succeeded or Failed"
Sep 11 18:38:29.002: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-6cf75b6a-4116-4434-87b0-f2ff888e8737 container client-container: <nil>
STEP: delete the pod
Sep 11 18:38:29.033: INFO: Waiting for pod downwardapi-volume-6cf75b6a-4116-4434-87b0-f2ff888e8737 to disappear
Sep 11 18:38:29.038: INFO: Pod downwardapi-volume-6cf75b6a-4116-4434-87b0-f2ff888e8737 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:38:29.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-655" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":197,"skipped":3009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:38:29.064: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 18:38:29.229: INFO: Waiting up to 5m0s for pod "downwardapi-volume-847d490f-d43d-4999-a564-69a400d1acef" in namespace "downward-api-8963" to be "Succeeded or Failed"
Sep 11 18:38:29.247: INFO: Pod "downwardapi-volume-847d490f-d43d-4999-a564-69a400d1acef": Phase="Pending", Reason="", readiness=false. Elapsed: 16.747903ms
Sep 11 18:38:31.252: INFO: Pod "downwardapi-volume-847d490f-d43d-4999-a564-69a400d1acef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021842057s
STEP: Saw pod success
Sep 11 18:38:31.252: INFO: Pod "downwardapi-volume-847d490f-d43d-4999-a564-69a400d1acef" satisfied condition "Succeeded or Failed"
Sep 11 18:38:31.255: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-847d490f-d43d-4999-a564-69a400d1acef container client-container: <nil>
STEP: delete the pod
Sep 11 18:38:31.295: INFO: Waiting for pod downwardapi-volume-847d490f-d43d-4999-a564-69a400d1acef to disappear
Sep 11 18:38:31.299: INFO: Pod downwardapi-volume-847d490f-d43d-4999-a564-69a400d1acef no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:38:31.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8963" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":198,"skipped":3042,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:38:31.332: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-5712/configmap-test-89a47fef-a2d6-489f-ab74-ee63501c02a5
STEP: Creating a pod to test consume configMaps
Sep 11 18:38:31.515: INFO: Waiting up to 5m0s for pod "pod-configmaps-637beeae-d2b2-4e4b-89c5-52e735599c7a" in namespace "configmap-5712" to be "Succeeded or Failed"
Sep 11 18:38:31.519: INFO: Pod "pod-configmaps-637beeae-d2b2-4e4b-89c5-52e735599c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.577373ms
Sep 11 18:38:33.524: INFO: Pod "pod-configmaps-637beeae-d2b2-4e4b-89c5-52e735599c7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008457012s
STEP: Saw pod success
Sep 11 18:38:33.524: INFO: Pod "pod-configmaps-637beeae-d2b2-4e4b-89c5-52e735599c7a" satisfied condition "Succeeded or Failed"
Sep 11 18:38:33.528: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-configmaps-637beeae-d2b2-4e4b-89c5-52e735599c7a container env-test: <nil>
STEP: delete the pod
Sep 11 18:38:33.557: INFO: Waiting for pod pod-configmaps-637beeae-d2b2-4e4b-89c5-52e735599c7a to disappear
Sep 11 18:38:33.560: INFO: Pod pod-configmaps-637beeae-d2b2-4e4b-89c5-52e735599c7a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:38:33.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5712" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":199,"skipped":3047,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:38:33.587: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 18:38:33.754: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f308c469-6b03-440d-b10e-b190c1b0b38d" in namespace "projected-8626" to be "Succeeded or Failed"
Sep 11 18:38:33.763: INFO: Pod "downwardapi-volume-f308c469-6b03-440d-b10e-b190c1b0b38d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.269244ms
Sep 11 18:38:35.772: INFO: Pod "downwardapi-volume-f308c469-6b03-440d-b10e-b190c1b0b38d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017716479s
STEP: Saw pod success
Sep 11 18:38:35.772: INFO: Pod "downwardapi-volume-f308c469-6b03-440d-b10e-b190c1b0b38d" satisfied condition "Succeeded or Failed"
Sep 11 18:38:35.778: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-f308c469-6b03-440d-b10e-b190c1b0b38d container client-container: <nil>
STEP: delete the pod
Sep 11 18:38:35.824: INFO: Waiting for pod downwardapi-volume-f308c469-6b03-440d-b10e-b190c1b0b38d to disappear
Sep 11 18:38:35.827: INFO: Pod downwardapi-volume-f308c469-6b03-440d-b10e-b190c1b0b38d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:38:35.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8626" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":200,"skipped":3049,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:38:35.856: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6667, will wait for the garbage collector to delete the pods
Sep 11 18:38:40.095: INFO: Deleting Job.batch foo took: 16.811187ms
Sep 11 18:38:41.195: INFO: Terminating Job.batch foo pods took: 1.100263857s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:39:23.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6667" for this suite.

â€¢ [SLOW TEST:47.471 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":201,"skipped":3074,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:39:23.327: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3989
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-903e7279-ac6a-41f1-ae6a-0a16ab3b134a
STEP: Creating a pod to test consume secrets
Sep 11 18:39:23.509: INFO: Waiting up to 5m0s for pod "pod-secrets-be11519c-ff46-4cd0-b58f-424612cb1980" in namespace "secrets-3989" to be "Succeeded or Failed"
Sep 11 18:39:23.513: INFO: Pod "pod-secrets-be11519c-ff46-4cd0-b58f-424612cb1980": Phase="Pending", Reason="", readiness=false. Elapsed: 3.989632ms
Sep 11 18:39:25.518: INFO: Pod "pod-secrets-be11519c-ff46-4cd0-b58f-424612cb1980": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008939434s
Sep 11 18:39:27.523: INFO: Pod "pod-secrets-be11519c-ff46-4cd0-b58f-424612cb1980": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013974715s
STEP: Saw pod success
Sep 11 18:39:27.523: INFO: Pod "pod-secrets-be11519c-ff46-4cd0-b58f-424612cb1980" satisfied condition "Succeeded or Failed"
Sep 11 18:39:27.527: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-secrets-be11519c-ff46-4cd0-b58f-424612cb1980 container secret-volume-test: <nil>
STEP: delete the pod
Sep 11 18:39:27.559: INFO: Waiting for pod pod-secrets-be11519c-ff46-4cd0-b58f-424612cb1980 to disappear
Sep 11 18:39:27.563: INFO: Pod pod-secrets-be11519c-ff46-4cd0-b58f-424612cb1980 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:39:27.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3989" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":202,"skipped":3106,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:39:27.603: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1409
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:39:28.323: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:39:31.357: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:39:41.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1409" for this suite.
STEP: Destroying namespace "webhook-1409-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:14.069 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":203,"skipped":3111,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:39:41.672: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2837
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0911 18:39:42.896829      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 11 18:40:44.920: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:40:44.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2837" for this suite.

â€¢ [SLOW TEST:63.277 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":204,"skipped":3115,"failed":0}
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:40:44.950: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5623.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5623.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 11 18:40:47.186: INFO: DNS probes using dns-5623/dns-test-d71d9eae-71e3-4ddd-a8f4-fd47d1a0d0b4 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:40:47.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5623" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":205,"skipped":3115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:40:47.235: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3866
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 11 18:40:47.384: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 11 18:41:07.341: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:41:13.113: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:41:34.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3866" for this suite.

â€¢ [SLOW TEST:47.084 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":206,"skipped":3138,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:41:34.320: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 18:41:34.489: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18e92789-6269-4433-9951-b572635eb706" in namespace "downward-api-4544" to be "Succeeded or Failed"
Sep 11 18:41:34.493: INFO: Pod "downwardapi-volume-18e92789-6269-4433-9951-b572635eb706": Phase="Pending", Reason="", readiness=false. Elapsed: 3.860209ms
Sep 11 18:41:36.498: INFO: Pod "downwardapi-volume-18e92789-6269-4433-9951-b572635eb706": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008473812s
STEP: Saw pod success
Sep 11 18:41:36.498: INFO: Pod "downwardapi-volume-18e92789-6269-4433-9951-b572635eb706" satisfied condition "Succeeded or Failed"
Sep 11 18:41:36.502: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-18e92789-6269-4433-9951-b572635eb706 container client-container: <nil>
STEP: delete the pod
Sep 11 18:41:36.544: INFO: Waiting for pod downwardapi-volume-18e92789-6269-4433-9951-b572635eb706 to disappear
Sep 11 18:41:36.548: INFO: Pod downwardapi-volume-18e92789-6269-4433-9951-b572635eb706 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:41:36.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4544" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":207,"skipped":3151,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:41:36.574: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5322
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5322
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Sep 11 18:41:36.756: INFO: Found 0 stateful pods, waiting for 3
Sep 11 18:41:46.761: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:41:46.761: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:41:46.761: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 11 18:41:46.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-5322 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 11 18:41:47.011: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 11 18:41:47.011: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 11 18:41:47.011: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 11 18:41:57.050: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 11 18:42:07.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-5322 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 11 18:42:07.307: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 11 18:42:07.307: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 11 18:42:07.307: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 11 18:42:17.332: INFO: Waiting for StatefulSet statefulset-5322/ss2 to complete update
Sep 11 18:42:17.333: INFO: Waiting for Pod statefulset-5322/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 11 18:42:17.333: INFO: Waiting for Pod statefulset-5322/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 11 18:42:17.333: INFO: Waiting for Pod statefulset-5322/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 11 18:42:27.341: INFO: Waiting for StatefulSet statefulset-5322/ss2 to complete update
Sep 11 18:42:27.341: INFO: Waiting for Pod statefulset-5322/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Sep 11 18:42:37.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-5322 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 11 18:42:37.581: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 11 18:42:37.581: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 11 18:42:37.581: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 11 18:42:47.623: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 11 18:42:57.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=statefulset-5322 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 11 18:42:57.884: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 11 18:42:57.884: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 11 18:42:57.884: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 11 18:43:07.918: INFO: Waiting for StatefulSet statefulset-5322/ss2 to complete update
Sep 11 18:43:07.918: INFO: Waiting for Pod statefulset-5322/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 11 18:43:07.918: INFO: Waiting for Pod statefulset-5322/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 11 18:43:07.918: INFO: Waiting for Pod statefulset-5322/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 11 18:43:17.929: INFO: Waiting for StatefulSet statefulset-5322/ss2 to complete update
Sep 11 18:43:17.929: INFO: Waiting for Pod statefulset-5322/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 11 18:43:27.927: INFO: Deleting all statefulset in ns statefulset-5322
Sep 11 18:43:27.931: INFO: Scaling statefulset ss2 to 0
Sep 11 18:43:47.968: INFO: Waiting for statefulset status.replicas updated to 0
Sep 11 18:43:47.972: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:43:47.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5322" for this suite.

â€¢ [SLOW TEST:131.452 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":208,"skipped":3159,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:43:48.026: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:43:48.212: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 11 18:43:48.224: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:48.224: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:48.224: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:48.228: INFO: Number of nodes with available pods: 0
Sep 11 18:43:48.228: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:43:49.234: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:49.234: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:49.234: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:49.239: INFO: Number of nodes with available pods: 0
Sep 11 18:43:49.239: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:43:50.234: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:50.234: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:50.234: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:50.238: INFO: Number of nodes with available pods: 1
Sep 11 18:43:50.238: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:43:51.234: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:51.234: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:51.234: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:51.238: INFO: Number of nodes with available pods: 3
Sep 11 18:43:51.238: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 11 18:43:51.278: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:51.278: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:51.278: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:51.283: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:51.284: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:51.284: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:52.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:52.289: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:52.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:52.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:52.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:52.294: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:53.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:53.289: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:53.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:53.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:53.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:53.294: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:54.290: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:54.290: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:54.290: INFO: Pod daemon-set-c48xf is not available
Sep 11 18:43:54.290: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:54.296: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:54.296: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:54.296: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:55.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:55.289: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:55.289: INFO: Pod daemon-set-c48xf is not available
Sep 11 18:43:55.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:55.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:55.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:55.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:56.290: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:56.290: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:56.290: INFO: Pod daemon-set-c48xf is not available
Sep 11 18:43:56.290: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:56.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:56.296: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:56.296: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:57.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:57.289: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:57.289: INFO: Pod daemon-set-c48xf is not available
Sep 11 18:43:57.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:57.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:57.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:57.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:58.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:58.290: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:58.290: INFO: Pod daemon-set-c48xf is not available
Sep 11 18:43:58.290: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:58.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:58.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:58.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:59.288: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:59.288: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:59.288: INFO: Pod daemon-set-c48xf is not available
Sep 11 18:43:59.288: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:43:59.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:59.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:43:59.294: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:00.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:00.289: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:00.289: INFO: Pod daemon-set-c48xf is not available
Sep 11 18:44:00.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:00.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:00.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:00.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:01.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:01.289: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:01.289: INFO: Pod daemon-set-c48xf is not available
Sep 11 18:44:01.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:01.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:01.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:01.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:02.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:02.289: INFO: Wrong image for pod: daemon-set-c48xf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:02.289: INFO: Pod daemon-set-c48xf is not available
Sep 11 18:44:02.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:02.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:02.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:02.294: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:03.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:03.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:03.289: INFO: Pod daemon-set-rr7sd is not available
Sep 11 18:44:03.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:03.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:03.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:04.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:04.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:04.289: INFO: Pod daemon-set-rr7sd is not available
Sep 11 18:44:04.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:04.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:04.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:05.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:05.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:05.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:05.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:05.294: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:06.290: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:06.290: INFO: Pod daemon-set-9nhkq is not available
Sep 11 18:44:06.290: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:06.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:06.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:06.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:07.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:07.289: INFO: Pod daemon-set-9nhkq is not available
Sep 11 18:44:07.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:07.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:07.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:07.294: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:08.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:08.289: INFO: Pod daemon-set-9nhkq is not available
Sep 11 18:44:08.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:08.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:08.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:08.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:09.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:09.289: INFO: Pod daemon-set-9nhkq is not available
Sep 11 18:44:09.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:09.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:09.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:09.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:10.289: INFO: Wrong image for pod: daemon-set-9nhkq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:10.289: INFO: Pod daemon-set-9nhkq is not available
Sep 11 18:44:10.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:10.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:10.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:10.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:11.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:11.289: INFO: Pod daemon-set-wn5gd is not available
Sep 11 18:44:11.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:11.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:11.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:12.291: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:12.291: INFO: Pod daemon-set-wn5gd is not available
Sep 11 18:44:12.296: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:12.296: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:12.296: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:13.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:13.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:13.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:13.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:14.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:14.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:14.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:14.294: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:15.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:15.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:15.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:15.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:16.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:16.289: INFO: Pod daemon-set-cwvrq is not available
Sep 11 18:44:16.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:16.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:16.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:17.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:17.289: INFO: Pod daemon-set-cwvrq is not available
Sep 11 18:44:17.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:17.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:17.294: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:18.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:18.289: INFO: Pod daemon-set-cwvrq is not available
Sep 11 18:44:18.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:18.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:18.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:19.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:19.289: INFO: Pod daemon-set-cwvrq is not available
Sep 11 18:44:19.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:19.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:19.294: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:20.289: INFO: Wrong image for pod: daemon-set-cwvrq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep 11 18:44:20.289: INFO: Pod daemon-set-cwvrq is not available
Sep 11 18:44:20.295: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:20.295: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:20.295: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:21.289: INFO: Pod daemon-set-zjz5p is not available
Sep 11 18:44:21.294: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:21.294: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:21.294: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 11 18:44:21.299: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:21.299: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:21.299: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:21.303: INFO: Number of nodes with available pods: 2
Sep 11 18:44:21.303: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:44:22.309: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:22.309: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:22.309: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:22.314: INFO: Number of nodes with available pods: 2
Sep 11 18:44:22.314: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:44:23.309: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:23.309: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:23.309: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:44:23.314: INFO: Number of nodes with available pods: 3
Sep 11 18:44:23.314: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2933, will wait for the garbage collector to delete the pods
Sep 11 18:44:23.400: INFO: Deleting DaemonSet.extensions daemon-set took: 13.204432ms
Sep 11 18:44:24.501: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100321829s
Sep 11 18:44:33.306: INFO: Number of nodes with available pods: 0
Sep 11 18:44:33.306: INFO: Number of running nodes: 0, number of available pods: 0
Sep 11 18:44:33.310: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2933/daemonsets","resourceVersion":"2781148"},"items":null}

Sep 11 18:44:33.313: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2933/pods","resourceVersion":"2781148"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:44:33.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2933" for this suite.

â€¢ [SLOW TEST:45.333 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":209,"skipped":3184,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:44:33.359: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Sep 11 18:44:33.512: INFO: namespace kubectl-8621
Sep 11 18:44:33.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-8621'
Sep 11 18:44:33.910: INFO: stderr: ""
Sep 11 18:44:33.910: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 11 18:44:34.915: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 11 18:44:34.915: INFO: Found 0 / 1
Sep 11 18:44:35.914: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 11 18:44:35.914: INFO: Found 0 / 1
Sep 11 18:44:36.915: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 11 18:44:36.915: INFO: Found 1 / 1
Sep 11 18:44:36.915: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 11 18:44:36.919: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 11 18:44:36.919: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 11 18:44:36.919: INFO: wait on agnhost-primary startup in kubectl-8621 
Sep 11 18:44:36.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 logs agnhost-primary-96h87 agnhost-primary --namespace=kubectl-8621'
Sep 11 18:44:37.052: INFO: stderr: ""
Sep 11 18:44:37.052: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep 11 18:44:37.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8621'
Sep 11 18:44:37.194: INFO: stderr: ""
Sep 11 18:44:37.194: INFO: stdout: "service/rm2 exposed\n"
Sep 11 18:44:37.200: INFO: Service rm2 in namespace kubectl-8621 found.
STEP: exposing service
Sep 11 18:44:39.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8621'
Sep 11 18:44:39.357: INFO: stderr: ""
Sep 11 18:44:39.357: INFO: stdout: "service/rm3 exposed\n"
Sep 11 18:44:39.368: INFO: Service rm3 in namespace kubectl-8621 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:44:41.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8621" for this suite.

â€¢ [SLOW TEST:8.047 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":210,"skipped":3193,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:44:41.407: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4074
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Sep 11 18:44:41.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 api-versions'
Sep 11 18:44:41.652: INFO: stderr: ""
Sep 11 18:44:41.652: INFO: stdout: "acme.cert-manager.io/v1alpha2\nadmissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\nappcatalog.appscode.com/v1alpha1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbitnami.com/v1alpha1\ncatalog.kubedb.com/v1alpha1\ncert-manager.io/v1alpha2\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndex.coreos.com/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhelm.fluxcd.io/v1\nkubedb.com/v1alpha1\nmonitoring.coreos.com/v1\nmutators.kubedb.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvalidators.kubedb.com/v1alpha1\nvelero.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:44:41.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4074" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":211,"skipped":3205,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:44:41.680: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-133218b9-e12a-4ff0-95a3-cde4876b4b92
STEP: Creating a pod to test consume configMaps
Sep 11 18:44:41.873: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e2e1ec36-4ed1-4f10-934e-a755d7ff7d61" in namespace "projected-3467" to be "Succeeded or Failed"
Sep 11 18:44:41.891: INFO: Pod "pod-projected-configmaps-e2e1ec36-4ed1-4f10-934e-a755d7ff7d61": Phase="Pending", Reason="", readiness=false. Elapsed: 17.428492ms
Sep 11 18:44:43.895: INFO: Pod "pod-projected-configmaps-e2e1ec36-4ed1-4f10-934e-a755d7ff7d61": Phase="Running", Reason="", readiness=true. Elapsed: 2.021917902s
Sep 11 18:44:45.900: INFO: Pod "pod-projected-configmaps-e2e1ec36-4ed1-4f10-934e-a755d7ff7d61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026482808s
STEP: Saw pod success
Sep 11 18:44:45.900: INFO: Pod "pod-projected-configmaps-e2e1ec36-4ed1-4f10-934e-a755d7ff7d61" satisfied condition "Succeeded or Failed"
Sep 11 18:44:45.904: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-configmaps-e2e1ec36-4ed1-4f10-934e-a755d7ff7d61 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 18:44:45.947: INFO: Waiting for pod pod-projected-configmaps-e2e1ec36-4ed1-4f10-934e-a755d7ff7d61 to disappear
Sep 11 18:44:45.951: INFO: Pod pod-projected-configmaps-e2e1ec36-4ed1-4f10-934e-a755d7ff7d61 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:44:45.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3467" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":212,"skipped":3218,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:44:45.982: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 11 18:44:54.201: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 11 18:44:54.209: INFO: Pod pod-with-prestop-http-hook still exists
Sep 11 18:44:56.209: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 11 18:44:56.215: INFO: Pod pod-with-prestop-http-hook still exists
Sep 11 18:44:58.209: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 11 18:44:58.215: INFO: Pod pod-with-prestop-http-hook still exists
Sep 11 18:45:00.209: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 11 18:45:00.215: INFO: Pod pod-with-prestop-http-hook still exists
Sep 11 18:45:02.209: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 11 18:45:02.217: INFO: Pod pod-with-prestop-http-hook still exists
Sep 11 18:45:04.209: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 11 18:45:04.215: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:45:04.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2723" for this suite.

â€¢ [SLOW TEST:18.284 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":213,"skipped":3233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:45:04.267: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 11 18:45:04.435: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 11 18:45:09.440: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:45:10.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5275" for this suite.

â€¢ [SLOW TEST:6.225 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":214,"skipped":3262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:45:10.492: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 11 18:45:16.717: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 11 18:45:16.727: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 11 18:45:18.727: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 11 18:45:18.732: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 11 18:45:20.727: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 11 18:45:20.733: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 11 18:45:22.728: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 11 18:45:22.732: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 11 18:45:24.728: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 11 18:45:24.732: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:45:24.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-314" for this suite.

â€¢ [SLOW TEST:14.268 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":215,"skipped":3309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:45:24.760: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:45:29.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8741" for this suite.

â€¢ [SLOW TEST:5.125 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":216,"skipped":3344,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:45:29.886: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1076
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 11 18:45:30.089: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:30.090: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:30.090: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:30.094: INFO: Number of nodes with available pods: 0
Sep 11 18:45:30.094: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:45:31.102: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:31.102: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:31.102: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:31.107: INFO: Number of nodes with available pods: 0
Sep 11 18:45:31.107: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:45:32.101: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:32.101: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:32.101: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:32.105: INFO: Number of nodes with available pods: 1
Sep 11 18:45:32.106: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:45:33.100: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:33.100: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:33.101: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:33.106: INFO: Number of nodes with available pods: 3
Sep 11 18:45:33.106: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 11 18:45:33.135: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:33.135: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:33.135: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:33.143: INFO: Number of nodes with available pods: 2
Sep 11 18:45:33.143: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:45:34.149: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:34.149: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:34.149: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:34.153: INFO: Number of nodes with available pods: 2
Sep 11 18:45:34.153: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:45:35.150: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:35.150: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:35.150: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:45:35.154: INFO: Number of nodes with available pods: 3
Sep 11 18:45:35.154: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1076, will wait for the garbage collector to delete the pods
Sep 11 18:45:35.233: INFO: Deleting DaemonSet.extensions daemon-set took: 13.177802ms
Sep 11 18:45:36.333: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100236558s
Sep 11 18:45:43.338: INFO: Number of nodes with available pods: 0
Sep 11 18:45:43.338: INFO: Number of running nodes: 0, number of available pods: 0
Sep 11 18:45:43.342: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1076/daemonsets","resourceVersion":"2782034"},"items":null}

Sep 11 18:45:43.345: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1076/pods","resourceVersion":"2782034"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:45:43.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1076" for this suite.

â€¢ [SLOW TEST:13.501 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":217,"skipped":3350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:45:43.387: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-p4x68 in namespace proxy-2620
I0911 18:45:43.571670      19 runners.go:190] Created replication controller with name: proxy-service-p4x68, namespace: proxy-2620, replica count: 1
I0911 18:45:44.625988      19 runners.go:190] proxy-service-p4x68 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0911 18:45:45.626224      19 runners.go:190] proxy-service-p4x68 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0911 18:45:46.626438      19 runners.go:190] proxy-service-p4x68 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 18:45:46.631: INFO: setup took 3.094523012s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 11 18:45:46.645: INFO: (0) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 14.338834ms)
Sep 11 18:45:46.648: INFO: (0) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 16.931126ms)
Sep 11 18:45:46.648: INFO: (0) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 16.910365ms)
Sep 11 18:45:46.653: INFO: (0) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 21.690987ms)
Sep 11 18:45:46.656: INFO: (0) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 24.684844ms)
Sep 11 18:45:46.656: INFO: (0) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 24.439313ms)
Sep 11 18:45:46.656: INFO: (0) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 24.527442ms)
Sep 11 18:45:46.656: INFO: (0) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 24.683218ms)
Sep 11 18:45:46.656: INFO: (0) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 24.458938ms)
Sep 11 18:45:46.656: INFO: (0) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 24.315007ms)
Sep 11 18:45:46.656: INFO: (0) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 24.443288ms)
Sep 11 18:45:46.656: INFO: (0) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 24.249124ms)
Sep 11 18:45:46.657: INFO: (0) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 24.752265ms)
Sep 11 18:45:46.658: INFO: (0) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 26.578402ms)
Sep 11 18:45:46.662: INFO: (0) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 26.357131ms)
Sep 11 18:45:46.662: INFO: (0) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 26.54574ms)
Sep 11 18:45:46.671: INFO: (1) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 8.813583ms)
Sep 11 18:45:46.677: INFO: (1) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 15.170563ms)
Sep 11 18:45:46.678: INFO: (1) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 16.317722ms)
Sep 11 18:45:46.678: INFO: (1) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 15.880444ms)
Sep 11 18:45:46.678: INFO: (1) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 15.86203ms)
Sep 11 18:45:46.679: INFO: (1) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 16.461561ms)
Sep 11 18:45:46.679: INFO: (1) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 15.9829ms)
Sep 11 18:45:46.679: INFO: (1) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 16.386753ms)
Sep 11 18:45:46.679: INFO: (1) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 16.216566ms)
Sep 11 18:45:46.682: INFO: (1) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 19.818765ms)
Sep 11 18:45:46.682: INFO: (1) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 19.936146ms)
Sep 11 18:45:46.677: INFO: (1) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 14.565621ms)
Sep 11 18:45:46.687: INFO: (1) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 24.636132ms)
Sep 11 18:45:46.687: INFO: (1) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 24.664867ms)
Sep 11 18:45:46.688: INFO: (1) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 25.614829ms)
Sep 11 18:45:46.688: INFO: (1) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 25.381497ms)
Sep 11 18:45:46.695: INFO: (2) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 7.158111ms)
Sep 11 18:45:46.702: INFO: (2) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 12.843145ms)
Sep 11 18:45:46.702: INFO: (2) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 13.439554ms)
Sep 11 18:45:46.702: INFO: (2) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 13.182394ms)
Sep 11 18:45:46.702: INFO: (2) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 13.317012ms)
Sep 11 18:45:46.702: INFO: (2) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 12.911772ms)
Sep 11 18:45:46.702: INFO: (2) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 13.77819ms)
Sep 11 18:45:46.702: INFO: (2) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 13.774509ms)
Sep 11 18:45:46.702: INFO: (2) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 13.312234ms)
Sep 11 18:45:46.702: INFO: (2) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 13.185203ms)
Sep 11 18:45:46.705: INFO: (2) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 16.841013ms)
Sep 11 18:45:46.707: INFO: (2) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 18.562019ms)
Sep 11 18:45:46.710: INFO: (2) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 21.910004ms)
Sep 11 18:45:46.710: INFO: (2) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 21.181015ms)
Sep 11 18:45:46.710: INFO: (2) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 22.171341ms)
Sep 11 18:45:46.710: INFO: (2) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 21.521486ms)
Sep 11 18:45:46.718: INFO: (3) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 7.51711ms)
Sep 11 18:45:46.723: INFO: (3) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 12.43149ms)
Sep 11 18:45:46.723: INFO: (3) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 12.438736ms)
Sep 11 18:45:46.723: INFO: (3) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 12.939288ms)
Sep 11 18:45:46.724: INFO: (3) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 12.815922ms)
Sep 11 18:45:46.724: INFO: (3) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 13.162399ms)
Sep 11 18:45:46.724: INFO: (3) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 13.791582ms)
Sep 11 18:45:46.724: INFO: (3) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 12.924813ms)
Sep 11 18:45:46.724: INFO: (3) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 13.329955ms)
Sep 11 18:45:46.724: INFO: (3) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 13.201461ms)
Sep 11 18:45:46.726: INFO: (3) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 15.402944ms)
Sep 11 18:45:46.729: INFO: (3) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 17.940419ms)
Sep 11 18:45:46.729: INFO: (3) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 18.242163ms)
Sep 11 18:45:46.729: INFO: (3) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 17.787915ms)
Sep 11 18:45:46.729: INFO: (3) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 17.882002ms)
Sep 11 18:45:46.729: INFO: (3) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 17.996204ms)
Sep 11 18:45:46.743: INFO: (4) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 13.289037ms)
Sep 11 18:45:46.743: INFO: (4) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 13.76905ms)
Sep 11 18:45:46.743: INFO: (4) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 13.299405ms)
Sep 11 18:45:46.743: INFO: (4) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 13.3988ms)
Sep 11 18:45:46.743: INFO: (4) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 13.29222ms)
Sep 11 18:45:46.743: INFO: (4) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 13.438457ms)
Sep 11 18:45:46.744: INFO: (4) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 14.415202ms)
Sep 11 18:45:46.744: INFO: (4) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 14.586676ms)
Sep 11 18:45:46.744: INFO: (4) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 13.989372ms)
Sep 11 18:45:46.744: INFO: (4) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 14.747046ms)
Sep 11 18:45:46.745: INFO: (4) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 15.54075ms)
Sep 11 18:45:46.746: INFO: (4) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 16.386858ms)
Sep 11 18:45:46.748: INFO: (4) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 18.845272ms)
Sep 11 18:45:46.749: INFO: (4) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 19.00922ms)
Sep 11 18:45:46.749: INFO: (4) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 19.15301ms)
Sep 11 18:45:46.749: INFO: (4) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 19.145057ms)
Sep 11 18:45:46.761: INFO: (5) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 11.819199ms)
Sep 11 18:45:46.761: INFO: (5) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 11.747809ms)
Sep 11 18:45:46.761: INFO: (5) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 11.79696ms)
Sep 11 18:45:46.761: INFO: (5) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 11.752307ms)
Sep 11 18:45:46.762: INFO: (5) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 13.623186ms)
Sep 11 18:45:46.762: INFO: (5) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 13.398269ms)
Sep 11 18:45:46.762: INFO: (5) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 13.395004ms)
Sep 11 18:45:46.762: INFO: (5) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 13.682573ms)
Sep 11 18:45:46.762: INFO: (5) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 13.355051ms)
Sep 11 18:45:46.763: INFO: (5) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 14.252548ms)
Sep 11 18:45:46.763: INFO: (5) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 14.446985ms)
Sep 11 18:45:46.766: INFO: (5) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 17.026374ms)
Sep 11 18:45:46.766: INFO: (5) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 17.161154ms)
Sep 11 18:45:46.766: INFO: (5) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 17.496486ms)
Sep 11 18:45:46.766: INFO: (5) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 17.842695ms)
Sep 11 18:45:46.767: INFO: (5) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 17.801674ms)
Sep 11 18:45:46.774: INFO: (6) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 6.839396ms)
Sep 11 18:45:46.784: INFO: (6) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 16.621672ms)
Sep 11 18:45:46.784: INFO: (6) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 16.614541ms)
Sep 11 18:45:46.784: INFO: (6) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 16.527843ms)
Sep 11 18:45:46.784: INFO: (6) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 16.079686ms)
Sep 11 18:45:46.784: INFO: (6) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 16.908506ms)
Sep 11 18:45:46.784: INFO: (6) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 16.993496ms)
Sep 11 18:45:46.784: INFO: (6) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 16.182822ms)
Sep 11 18:45:46.784: INFO: (6) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 16.655425ms)
Sep 11 18:45:46.784: INFO: (6) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 16.903376ms)
Sep 11 18:45:46.786: INFO: (6) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 19.26063ms)
Sep 11 18:45:46.789: INFO: (6) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 21.923093ms)
Sep 11 18:45:46.790: INFO: (6) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 21.891535ms)
Sep 11 18:45:46.790: INFO: (6) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 22.209183ms)
Sep 11 18:45:46.792: INFO: (6) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 24.729637ms)
Sep 11 18:45:46.792: INFO: (6) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 24.18634ms)
Sep 11 18:45:46.803: INFO: (7) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 11.029444ms)
Sep 11 18:45:46.803: INFO: (7) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 10.795878ms)
Sep 11 18:45:46.803: INFO: (7) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 11.253022ms)
Sep 11 18:45:46.803: INFO: (7) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 10.920531ms)
Sep 11 18:45:46.803: INFO: (7) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 11.124967ms)
Sep 11 18:45:46.803: INFO: (7) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 10.953923ms)
Sep 11 18:45:46.804: INFO: (7) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 11.484131ms)
Sep 11 18:45:46.806: INFO: (7) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 13.583026ms)
Sep 11 18:45:46.806: INFO: (7) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 13.888665ms)
Sep 11 18:45:46.806: INFO: (7) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 13.775353ms)
Sep 11 18:45:46.806: INFO: (7) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 14.166232ms)
Sep 11 18:45:46.808: INFO: (7) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 15.763465ms)
Sep 11 18:45:46.810: INFO: (7) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 18.362953ms)
Sep 11 18:45:46.810: INFO: (7) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 18.16804ms)
Sep 11 18:45:46.810: INFO: (7) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 18.301396ms)
Sep 11 18:45:46.810: INFO: (7) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 18.214551ms)
Sep 11 18:45:46.818: INFO: (8) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 7.870659ms)
Sep 11 18:45:46.824: INFO: (8) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 12.973231ms)
Sep 11 18:45:46.824: INFO: (8) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 12.930342ms)
Sep 11 18:45:46.824: INFO: (8) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 12.918108ms)
Sep 11 18:45:46.824: INFO: (8) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 13.269628ms)
Sep 11 18:45:46.824: INFO: (8) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 13.124371ms)
Sep 11 18:45:46.824: INFO: (8) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 13.114749ms)
Sep 11 18:45:46.824: INFO: (8) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 13.192091ms)
Sep 11 18:45:46.824: INFO: (8) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 12.913169ms)
Sep 11 18:45:46.824: INFO: (8) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 12.960477ms)
Sep 11 18:45:46.827: INFO: (8) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 15.46977ms)
Sep 11 18:45:46.829: INFO: (8) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 18.084029ms)
Sep 11 18:45:46.830: INFO: (8) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 18.559654ms)
Sep 11 18:45:46.830: INFO: (8) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 18.822631ms)
Sep 11 18:45:46.830: INFO: (8) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 18.463673ms)
Sep 11 18:45:46.830: INFO: (8) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 18.64962ms)
Sep 11 18:45:46.837: INFO: (9) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 6.726604ms)
Sep 11 18:45:46.839: INFO: (9) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 9.298169ms)
Sep 11 18:45:46.840: INFO: (9) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 9.684443ms)
Sep 11 18:45:46.840: INFO: (9) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 9.340458ms)
Sep 11 18:45:46.840: INFO: (9) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 9.723406ms)
Sep 11 18:45:46.840: INFO: (9) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 9.596823ms)
Sep 11 18:45:46.844: INFO: (9) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 12.952913ms)
Sep 11 18:45:46.844: INFO: (9) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 13.134923ms)
Sep 11 18:45:46.844: INFO: (9) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 13.634037ms)
Sep 11 18:45:46.844: INFO: (9) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 13.743725ms)
Sep 11 18:45:46.846: INFO: (9) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 15.540696ms)
Sep 11 18:45:46.848: INFO: (9) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 17.406279ms)
Sep 11 18:45:46.848: INFO: (9) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 17.479319ms)
Sep 11 18:45:46.849: INFO: (9) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 18.211264ms)
Sep 11 18:45:46.849: INFO: (9) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 18.305962ms)
Sep 11 18:45:46.849: INFO: (9) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 18.053245ms)
Sep 11 18:45:46.857: INFO: (10) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 7.570509ms)
Sep 11 18:45:46.862: INFO: (10) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 12.524658ms)
Sep 11 18:45:46.862: INFO: (10) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 13.437911ms)
Sep 11 18:45:46.862: INFO: (10) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 12.694712ms)
Sep 11 18:45:46.862: INFO: (10) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 12.819197ms)
Sep 11 18:45:46.862: INFO: (10) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 12.944774ms)
Sep 11 18:45:46.863: INFO: (10) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 12.804938ms)
Sep 11 18:45:46.863: INFO: (10) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 13.32684ms)
Sep 11 18:45:46.863: INFO: (10) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 12.909463ms)
Sep 11 18:45:46.863: INFO: (10) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 12.544567ms)
Sep 11 18:45:46.863: INFO: (10) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 12.496397ms)
Sep 11 18:45:46.864: INFO: (10) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 15.153404ms)
Sep 11 18:45:46.867: INFO: (10) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 16.744861ms)
Sep 11 18:45:46.867: INFO: (10) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 17.354906ms)
Sep 11 18:45:46.867: INFO: (10) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 17.195904ms)
Sep 11 18:45:46.867: INFO: (10) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 17.928597ms)
Sep 11 18:45:46.873: INFO: (11) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 5.89587ms)
Sep 11 18:45:46.877: INFO: (11) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 9.509905ms)
Sep 11 18:45:46.877: INFO: (11) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 9.201662ms)
Sep 11 18:45:46.877: INFO: (11) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 9.350882ms)
Sep 11 18:45:46.877: INFO: (11) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 9.677431ms)
Sep 11 18:45:46.880: INFO: (11) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 11.741169ms)
Sep 11 18:45:46.880: INFO: (11) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 11.662008ms)
Sep 11 18:45:46.881: INFO: (11) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 12.842234ms)
Sep 11 18:45:46.881: INFO: (11) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 12.986105ms)
Sep 11 18:45:46.882: INFO: (11) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 14.043869ms)
Sep 11 18:45:46.882: INFO: (11) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 13.192536ms)
Sep 11 18:45:46.884: INFO: (11) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 16.146533ms)
Sep 11 18:45:46.885: INFO: (11) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 16.811562ms)
Sep 11 18:45:46.885: INFO: (11) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 17.172854ms)
Sep 11 18:45:46.885: INFO: (11) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 17.191222ms)
Sep 11 18:45:46.885: INFO: (11) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 17.940699ms)
Sep 11 18:45:46.891: INFO: (12) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 6.032915ms)
Sep 11 18:45:46.904: INFO: (12) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 19.203587ms)
Sep 11 18:45:46.909: INFO: (12) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 23.369483ms)
Sep 11 18:45:46.909: INFO: (12) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 23.624553ms)
Sep 11 18:45:46.909: INFO: (12) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 23.275104ms)
Sep 11 18:45:46.909: INFO: (12) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 23.448225ms)
Sep 11 18:45:46.909: INFO: (12) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 23.263716ms)
Sep 11 18:45:46.909: INFO: (12) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 23.429326ms)
Sep 11 18:45:46.909: INFO: (12) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 23.22392ms)
Sep 11 18:45:46.909: INFO: (12) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 23.537213ms)
Sep 11 18:45:46.909: INFO: (12) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 23.758068ms)
Sep 11 18:45:46.911: INFO: (12) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 24.573305ms)
Sep 11 18:45:46.915: INFO: (12) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 28.452416ms)
Sep 11 18:45:46.915: INFO: (12) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 29.58726ms)
Sep 11 18:45:46.915: INFO: (12) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 28.485867ms)
Sep 11 18:45:46.918: INFO: (12) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 32.609382ms)
Sep 11 18:45:46.929: INFO: (13) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 10.99875ms)
Sep 11 18:45:46.937: INFO: (13) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 17.651776ms)
Sep 11 18:45:46.937: INFO: (13) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 17.605001ms)
Sep 11 18:45:46.937: INFO: (13) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 17.382628ms)
Sep 11 18:45:46.937: INFO: (13) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 16.299508ms)
Sep 11 18:45:46.937: INFO: (13) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 17.479334ms)
Sep 11 18:45:46.937: INFO: (13) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 17.641861ms)
Sep 11 18:45:46.937: INFO: (13) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 17.73839ms)
Sep 11 18:45:46.937: INFO: (13) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 17.986667ms)
Sep 11 18:45:46.937: INFO: (13) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 17.875023ms)
Sep 11 18:45:46.939: INFO: (13) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 20.656972ms)
Sep 11 18:45:46.942: INFO: (13) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 22.461878ms)
Sep 11 18:45:46.942: INFO: (13) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 22.84433ms)
Sep 11 18:45:46.943: INFO: (13) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 24.157976ms)
Sep 11 18:45:46.944: INFO: (13) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 23.638068ms)
Sep 11 18:45:46.944: INFO: (13) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 25.304841ms)
Sep 11 18:45:46.960: INFO: (14) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 15.508985ms)
Sep 11 18:45:46.960: INFO: (14) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 15.268231ms)
Sep 11 18:45:46.960: INFO: (14) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 15.620642ms)
Sep 11 18:45:46.960: INFO: (14) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 15.4521ms)
Sep 11 18:45:46.960: INFO: (14) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 15.501031ms)
Sep 11 18:45:46.960: INFO: (14) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 15.945926ms)
Sep 11 18:45:46.960: INFO: (14) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 15.821083ms)
Sep 11 18:45:46.961: INFO: (14) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 15.415873ms)
Sep 11 18:45:46.961: INFO: (14) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 15.904106ms)
Sep 11 18:45:46.961: INFO: (14) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 15.607044ms)
Sep 11 18:45:46.961: INFO: (14) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 16.13076ms)
Sep 11 18:45:46.961: INFO: (14) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 16.06436ms)
Sep 11 18:45:46.961: INFO: (14) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 15.772856ms)
Sep 11 18:45:46.961: INFO: (14) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 15.82904ms)
Sep 11 18:45:46.961: INFO: (14) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 15.526054ms)
Sep 11 18:45:46.961: INFO: (14) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 16.31459ms)
Sep 11 18:45:46.971: INFO: (15) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 9.760317ms)
Sep 11 18:45:46.971: INFO: (15) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 10.227233ms)
Sep 11 18:45:46.971: INFO: (15) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 9.922386ms)
Sep 11 18:45:46.971: INFO: (15) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 8.215147ms)
Sep 11 18:45:46.971: INFO: (15) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 10.326045ms)
Sep 11 18:45:46.971: INFO: (15) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 10.028527ms)
Sep 11 18:45:46.971: INFO: (15) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 9.794836ms)
Sep 11 18:45:46.974: INFO: (15) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 13.102632ms)
Sep 11 18:45:46.974: INFO: (15) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 12.961353ms)
Sep 11 18:45:46.974: INFO: (15) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 11.482317ms)
Sep 11 18:45:46.974: INFO: (15) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 11.299268ms)
Sep 11 18:45:46.976: INFO: (15) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 15.054619ms)
Sep 11 18:45:46.976: INFO: (15) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 15.258404ms)
Sep 11 18:45:46.978: INFO: (15) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 17.388897ms)
Sep 11 18:45:46.978: INFO: (15) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 15.623762ms)
Sep 11 18:45:46.979: INFO: (15) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 15.978839ms)
Sep 11 18:45:46.990: INFO: (16) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 11.015619ms)
Sep 11 18:45:46.990: INFO: (16) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 10.651127ms)
Sep 11 18:45:46.994: INFO: (16) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 15.116678ms)
Sep 11 18:45:46.994: INFO: (16) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 14.676301ms)
Sep 11 18:45:46.994: INFO: (16) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 15.005733ms)
Sep 11 18:45:46.994: INFO: (16) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 15.448972ms)
Sep 11 18:45:46.994: INFO: (16) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 14.988873ms)
Sep 11 18:45:46.995: INFO: (16) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 15.13509ms)
Sep 11 18:45:46.995: INFO: (16) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 15.897651ms)
Sep 11 18:45:46.995: INFO: (16) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 15.80424ms)
Sep 11 18:45:46.995: INFO: (16) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 15.743859ms)
Sep 11 18:45:46.996: INFO: (16) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 16.44039ms)
Sep 11 18:45:46.997: INFO: (16) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 18.192993ms)
Sep 11 18:45:46.997: INFO: (16) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 18.435875ms)
Sep 11 18:45:46.997: INFO: (16) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 17.866579ms)
Sep 11 18:45:46.998: INFO: (16) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 18.399739ms)
Sep 11 18:45:47.025: INFO: (17) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 27.107567ms)
Sep 11 18:45:47.025: INFO: (17) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 26.93559ms)
Sep 11 18:45:47.025: INFO: (17) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 27.044279ms)
Sep 11 18:45:47.025: INFO: (17) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 27.643457ms)
Sep 11 18:45:47.025: INFO: (17) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 25.35476ms)
Sep 11 18:45:47.025: INFO: (17) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 27.568643ms)
Sep 11 18:45:47.025: INFO: (17) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 27.392079ms)
Sep 11 18:45:47.025: INFO: (17) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 27.12419ms)
Sep 11 18:45:47.025: INFO: (17) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 27.264664ms)
Sep 11 18:45:47.026: INFO: (17) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 28.085563ms)
Sep 11 18:45:47.027: INFO: (17) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 29.315533ms)
Sep 11 18:45:47.030: INFO: (17) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 31.73504ms)
Sep 11 18:45:47.030: INFO: (17) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 32.376745ms)
Sep 11 18:45:47.030: INFO: (17) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 30.464792ms)
Sep 11 18:45:47.031: INFO: (17) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 30.75754ms)
Sep 11 18:45:47.031: INFO: (17) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 30.578189ms)
Sep 11 18:45:47.038: INFO: (18) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 7.307525ms)
Sep 11 18:45:47.043: INFO: (18) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 11.616101ms)
Sep 11 18:45:47.043: INFO: (18) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 12.524352ms)
Sep 11 18:45:47.043: INFO: (18) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 11.785377ms)
Sep 11 18:45:47.043: INFO: (18) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 12.302978ms)
Sep 11 18:45:47.043: INFO: (18) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 12.016145ms)
Sep 11 18:45:47.043: INFO: (18) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 12.088043ms)
Sep 11 18:45:47.043: INFO: (18) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 11.935942ms)
Sep 11 18:45:47.043: INFO: (18) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 12.19225ms)
Sep 11 18:45:47.044: INFO: (18) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 12.846692ms)
Sep 11 18:45:47.046: INFO: (18) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 15.106528ms)
Sep 11 18:45:47.048: INFO: (18) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 17.496625ms)
Sep 11 18:45:47.049: INFO: (18) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 18.453643ms)
Sep 11 18:45:47.049: INFO: (18) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 18.497066ms)
Sep 11 18:45:47.049: INFO: (18) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 18.355908ms)
Sep 11 18:45:47.049: INFO: (18) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 17.716936ms)
Sep 11 18:45:47.063: INFO: (19) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 13.458367ms)
Sep 11 18:45:47.063: INFO: (19) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 13.2004ms)
Sep 11 18:45:47.063: INFO: (19) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:443/proxy/tlsrewritem... (200; 12.977084ms)
Sep 11 18:45:47.063: INFO: (19) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:462/proxy/: tls qux (200; 12.921667ms)
Sep 11 18:45:47.063: INFO: (19) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:160/proxy/: foo (200; 13.124037ms)
Sep 11 18:45:47.063: INFO: (19) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:162/proxy/: bar (200; 13.101627ms)
Sep 11 18:45:47.064: INFO: (19) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname1/proxy/: tls baz (200; 15.197159ms)
Sep 11 18:45:47.064: INFO: (19) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname2/proxy/: bar (200; 15.112676ms)
Sep 11 18:45:47.065: INFO: (19) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn/proxy/rewriteme">test</a> (200; 14.935618ms)
Sep 11 18:45:47.065: INFO: (19) /api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">test<... (200; 14.895865ms)
Sep 11 18:45:47.065: INFO: (19) /api/v1/namespaces/proxy-2620/pods/https:proxy-service-p4x68-2v8gn:460/proxy/: tls baz (200; 14.738637ms)
Sep 11 18:45:47.065: INFO: (19) /api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/: <a href="/api/v1/namespaces/proxy-2620/pods/http:proxy-service-p4x68-2v8gn:1080/proxy/rewriteme">... (200; 14.880491ms)
Sep 11 18:45:47.066: INFO: (19) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname2/proxy/: bar (200; 16.51737ms)
Sep 11 18:45:47.066: INFO: (19) /api/v1/namespaces/proxy-2620/services/http:proxy-service-p4x68:portname1/proxy/: foo (200; 16.890777ms)
Sep 11 18:45:47.066: INFO: (19) /api/v1/namespaces/proxy-2620/services/proxy-service-p4x68:portname1/proxy/: foo (200; 16.364512ms)
Sep 11 18:45:47.067: INFO: (19) /api/v1/namespaces/proxy-2620/services/https:proxy-service-p4x68:tlsportname2/proxy/: tls qux (200; 16.708588ms)
STEP: deleting ReplicationController proxy-service-p4x68 in namespace proxy-2620, will wait for the garbage collector to delete the pods
Sep 11 18:45:47.132: INFO: Deleting ReplicationController proxy-service-p4x68 took: 11.71313ms
Sep 11 18:45:47.233: INFO: Terminating ReplicationController proxy-service-p4x68 pods took: 100.236617ms
[AfterEach] version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:45:53.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2620" for this suite.

â€¢ [SLOW TEST:9.871 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":218,"skipped":3374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:45:53.258: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9540
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 11 18:45:53.420: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:45:59.315: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:46:20.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9540" for this suite.

â€¢ [SLOW TEST:27.202 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":219,"skipped":3403,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:46:20.461: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6397
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:46:21.171: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 11 18:46:23.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446781, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446781, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446781, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446781, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:46:26.238: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:46:26.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6397" for this suite.
STEP: Destroying namespace "webhook-6397-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.019 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":220,"skipped":3424,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:46:26.480: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3255
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 11 18:46:26.660: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 11 18:46:26.707: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 11 18:46:28.712: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:46:30.712: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:46:32.712: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:46:34.712: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:46:36.712: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 11 18:46:38.713: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 11 18:46:38.725: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 11 18:46:40.730: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 11 18:46:42.730: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 11 18:46:44.732: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 11 18:46:44.740: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 11 18:46:46.771: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.123.14:8080/dial?request=hostname&protocol=http&host=192.168.134.107&port=8080&tries=1'] Namespace:pod-network-test-3255 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:46:46.771: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:46:46.890: INFO: Waiting for responses: map[]
Sep 11 18:46:46.895: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.123.14:8080/dial?request=hostname&protocol=http&host=192.168.123.16&port=8080&tries=1'] Namespace:pod-network-test-3255 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:46:46.895: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:46:47.005: INFO: Waiting for responses: map[]
Sep 11 18:46:47.010: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.123.14:8080/dial?request=hostname&protocol=http&host=192.168.249.243&port=8080&tries=1'] Namespace:pod-network-test-3255 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 18:46:47.010: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 18:46:47.142: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:46:47.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3255" for this suite.

â€¢ [SLOW TEST:20.690 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":221,"skipped":3465,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:46:47.170: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Sep 11 18:46:47.328: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep 11 18:46:47.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-1936'
Sep 11 18:46:47.930: INFO: stderr: ""
Sep 11 18:46:47.933: INFO: stdout: "service/agnhost-replica created\n"
Sep 11 18:46:47.933: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep 11 18:46:47.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-1936'
Sep 11 18:46:48.359: INFO: stderr: ""
Sep 11 18:46:48.359: INFO: stdout: "service/agnhost-primary created\n"
Sep 11 18:46:48.359: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 11 18:46:48.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-1936'
Sep 11 18:46:48.771: INFO: stderr: ""
Sep 11 18:46:48.771: INFO: stdout: "service/frontend created\n"
Sep 11 18:46:48.772: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep 11 18:46:48.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-1936'
Sep 11 18:46:49.217: INFO: stderr: ""
Sep 11 18:46:49.217: INFO: stdout: "deployment.apps/frontend created\n"
Sep 11 18:46:49.217: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 11 18:46:49.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-1936'
Sep 11 18:46:49.599: INFO: stderr: ""
Sep 11 18:46:49.599: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep 11 18:46:49.599: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 11 18:46:49.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-1936'
Sep 11 18:46:50.046: INFO: stderr: ""
Sep 11 18:46:50.046: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Sep 11 18:46:50.046: INFO: Waiting for all frontend pods to be Running.
Sep 11 18:46:55.096: INFO: Waiting for frontend to serve content.
Sep 11 18:46:55.113: INFO: Trying to add a new entry to the guestbook.
Sep 11 18:46:55.126: INFO: Verifying that added entry can be retrieved.
Sep 11 18:46:55.137: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Sep 11 18:47:00.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete --grace-period=0 --force -f - --namespace=kubectl-1936'
Sep 11 18:47:00.290: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 11 18:47:00.290: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Sep 11 18:47:00.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete --grace-period=0 --force -f - --namespace=kubectl-1936'
Sep 11 18:47:00.440: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 11 18:47:00.440: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 11 18:47:00.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete --grace-period=0 --force -f - --namespace=kubectl-1936'
Sep 11 18:47:00.587: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 11 18:47:00.587: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 11 18:47:00.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete --grace-period=0 --force -f - --namespace=kubectl-1936'
Sep 11 18:47:00.699: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 11 18:47:00.699: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 11 18:47:00.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete --grace-period=0 --force -f - --namespace=kubectl-1936'
Sep 11 18:47:00.823: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 11 18:47:00.823: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 11 18:47:00.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete --grace-period=0 --force -f - --namespace=kubectl-1936'
Sep 11 18:47:00.932: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 11 18:47:00.932: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:00.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1936" for this suite.

â€¢ [SLOW TEST:13.795 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":222,"skipped":3473,"failed":0}
SSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:00.965: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-2981
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Sep 11 18:47:01.211: INFO: created test-podtemplate-1
Sep 11 18:47:01.304: INFO: created test-podtemplate-2
Sep 11 18:47:01.312: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Sep 11 18:47:01.325: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Sep 11 18:47:01.474: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:01.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2981" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":223,"skipped":3476,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:01.506: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-3502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Sep 11 18:47:01.669: INFO: Major version: 1
STEP: Confirm minor version
Sep 11 18:47:01.669: INFO: cleanMinorVersion: 19
Sep 11 18:47:01.669: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:01.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3502" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":224,"skipped":3492,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:01.709: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-7b6993ed-11b1-4eb7-b704-414002b50a3e
STEP: Creating a pod to test consume secrets
Sep 11 18:47:01.927: INFO: Waiting up to 5m0s for pod "pod-secrets-6d7beb06-ab12-461e-a66e-8955191357ad" in namespace "secrets-9451" to be "Succeeded or Failed"
Sep 11 18:47:01.932: INFO: Pod "pod-secrets-6d7beb06-ab12-461e-a66e-8955191357ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.913589ms
Sep 11 18:47:03.937: INFO: Pod "pod-secrets-6d7beb06-ab12-461e-a66e-8955191357ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010000128s
Sep 11 18:47:05.944: INFO: Pod "pod-secrets-6d7beb06-ab12-461e-a66e-8955191357ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016912847s
STEP: Saw pod success
Sep 11 18:47:05.944: INFO: Pod "pod-secrets-6d7beb06-ab12-461e-a66e-8955191357ad" satisfied condition "Succeeded or Failed"
Sep 11 18:47:05.948: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-secrets-6d7beb06-ab12-461e-a66e-8955191357ad container secret-volume-test: <nil>
STEP: delete the pod
Sep 11 18:47:05.992: INFO: Waiting for pod pod-secrets-6d7beb06-ab12-461e-a66e-8955191357ad to disappear
Sep 11 18:47:05.996: INFO: Pod pod-secrets-6d7beb06-ab12-461e-a66e-8955191357ad no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:05.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9451" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":225,"skipped":3529,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:06.031: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 11 18:47:06.202: INFO: Waiting up to 5m0s for pod "pod-bd4b342f-6869-44fe-9b28-c47c69a41611" in namespace "emptydir-3551" to be "Succeeded or Failed"
Sep 11 18:47:06.216: INFO: Pod "pod-bd4b342f-6869-44fe-9b28-c47c69a41611": Phase="Pending", Reason="", readiness=false. Elapsed: 13.889736ms
Sep 11 18:47:08.221: INFO: Pod "pod-bd4b342f-6869-44fe-9b28-c47c69a41611": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019007233s
STEP: Saw pod success
Sep 11 18:47:08.221: INFO: Pod "pod-bd4b342f-6869-44fe-9b28-c47c69a41611" satisfied condition "Succeeded or Failed"
Sep 11 18:47:08.225: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-bd4b342f-6869-44fe-9b28-c47c69a41611 container test-container: <nil>
STEP: delete the pod
Sep 11 18:47:08.254: INFO: Waiting for pod pod-bd4b342f-6869-44fe-9b28-c47c69a41611 to disappear
Sep 11 18:47:08.258: INFO: Pod pod-bd4b342f-6869-44fe-9b28-c47c69a41611 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:08.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3551" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":226,"skipped":3540,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:08.284: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3152
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-be85311f-e60a-4686-b04c-e55f51453449-1538
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:08.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3152" for this suite.
STEP: Destroying namespace "nspatchtest-be85311f-e60a-4686-b04c-e55f51453449-1538" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":227,"skipped":3575,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:08.726: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-9d198385-5e3e-4f35-8975-cab78913a938
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:08.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4748" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":228,"skipped":3585,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:09.162: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-2236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 11 18:47:09.355: INFO: starting watch
STEP: patching
STEP: updating
Sep 11 18:47:09.368: INFO: waiting for watch events with expected annotations
Sep 11 18:47:09.368: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:09.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2236" for this suite.
â€¢{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":229,"skipped":3649,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:09.692: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4597
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:47:09.848: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:11.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4597" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":230,"skipped":3653,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:11.558: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:47:13.314: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 18:47:15.551: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446833, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446833, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446833, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446833, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:47:18.590: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:47:18.595: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:20.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1742" for this suite.
STEP: Destroying namespace "webhook-1742-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:9.688 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":231,"skipped":3658,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:21.249: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4126
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 11 18:47:21.547: INFO: Waiting up to 5m0s for pod "pod-9ce3755f-7810-4230-a09f-3545570fbe56" in namespace "emptydir-4126" to be "Succeeded or Failed"
Sep 11 18:47:21.551: INFO: Pod "pod-9ce3755f-7810-4230-a09f-3545570fbe56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.244183ms
Sep 11 18:47:23.731: INFO: Pod "pod-9ce3755f-7810-4230-a09f-3545570fbe56": Phase="Running", Reason="", readiness=true. Elapsed: 2.183362449s
Sep 11 18:47:25.735: INFO: Pod "pod-9ce3755f-7810-4230-a09f-3545570fbe56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.188190388s
STEP: Saw pod success
Sep 11 18:47:25.735: INFO: Pod "pod-9ce3755f-7810-4230-a09f-3545570fbe56" satisfied condition "Succeeded or Failed"
Sep 11 18:47:25.739: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-9ce3755f-7810-4230-a09f-3545570fbe56 container test-container: <nil>
STEP: delete the pod
Sep 11 18:47:25.792: INFO: Waiting for pod pod-9ce3755f-7810-4230-a09f-3545570fbe56 to disappear
Sep 11 18:47:25.795: INFO: Pod pod-9ce3755f-7810-4230-a09f-3545570fbe56 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:25.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4126" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":232,"skipped":3667,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:25.832: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:47:26.602: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 18:47:28.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446846, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446846, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446846, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735446846, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:47:31.646: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:47:31.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9183" for this suite.
STEP: Destroying namespace "webhook-9183-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.128 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":233,"skipped":3690,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:47:31.960: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0911 18:47:42.435021      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 11 18:48:44.456: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Sep 11 18:48:44.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-22nqq" in namespace "gc-2975"
Sep 11 18:48:44.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pjbh" in namespace "gc-2975"
Sep 11 18:48:44.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-79rx9" in namespace "gc-2975"
Sep 11 18:48:44.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dpfn" in namespace "gc-2975"
Sep 11 18:48:44.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dwgg" in namespace "gc-2975"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:48:44.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2975" for this suite.

â€¢ [SLOW TEST:72.694 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":234,"skipped":3693,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:48:44.655: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-9874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-9874
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9874
STEP: Deleting pre-stop pod
Sep 11 18:48:53.949: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:48:53.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9874" for this suite.

â€¢ [SLOW TEST:9.337 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":235,"skipped":3712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:48:53.992: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6840
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 11 18:48:54.163: INFO: Waiting up to 5m0s for pod "pod-bab8d1a8-4297-4189-86f5-00e67a4af6d9" in namespace "emptydir-6840" to be "Succeeded or Failed"
Sep 11 18:48:54.166: INFO: Pod "pod-bab8d1a8-4297-4189-86f5-00e67a4af6d9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.897219ms
Sep 11 18:48:56.171: INFO: Pod "pod-bab8d1a8-4297-4189-86f5-00e67a4af6d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008893141s
STEP: Saw pod success
Sep 11 18:48:56.171: INFO: Pod "pod-bab8d1a8-4297-4189-86f5-00e67a4af6d9" satisfied condition "Succeeded or Failed"
Sep 11 18:48:56.175: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-bab8d1a8-4297-4189-86f5-00e67a4af6d9 container test-container: <nil>
STEP: delete the pod
Sep 11 18:48:56.211: INFO: Waiting for pod pod-bab8d1a8-4297-4189-86f5-00e67a4af6d9 to disappear
Sep 11 18:48:56.216: INFO: Pod pod-bab8d1a8-4297-4189-86f5-00e67a4af6d9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:48:56.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6840" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":236,"skipped":3739,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:48:56.244: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5536
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-3924e1d2-24d0-425d-8396-6da630bda90a in namespace container-probe-5536
Sep 11 18:48:58.419: INFO: Started pod liveness-3924e1d2-24d0-425d-8396-6da630bda90a in namespace container-probe-5536
STEP: checking the pod's current state and verifying that restartCount is present
Sep 11 18:48:58.423: INFO: Initial restart count of pod liveness-3924e1d2-24d0-425d-8396-6da630bda90a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:52:59.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5536" for this suite.

â€¢ [SLOW TEST:242.911 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":237,"skipped":3740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:52:59.156: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Sep 11 18:53:01.348: INFO: Pod pod-hostip-dea9caf1-8398-423a-a87c-3f81b66b1e47 has hostIP: 10.10.2.19
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:53:01.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6906" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":238,"skipped":3788,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:53:01.376: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:53:01.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 version'
Sep 11 18:53:01.617: INFO: stderr: ""
Sep 11 18:53:01.617: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:30:33Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:23:04Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:53:01.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7774" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":239,"skipped":3820,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:53:01.648: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-861df9f5-ed70-4aba-ab3e-b6987aaa5f12
STEP: Creating a pod to test consume configMaps
Sep 11 18:53:01.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-0886e7df-9315-4914-9d85-c9ef69e53360" in namespace "configmap-3480" to be "Succeeded or Failed"
Sep 11 18:53:01.851: INFO: Pod "pod-configmaps-0886e7df-9315-4914-9d85-c9ef69e53360": Phase="Pending", Reason="", readiness=false. Elapsed: 5.564884ms
Sep 11 18:53:03.856: INFO: Pod "pod-configmaps-0886e7df-9315-4914-9d85-c9ef69e53360": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010342459s
Sep 11 18:53:05.861: INFO: Pod "pod-configmaps-0886e7df-9315-4914-9d85-c9ef69e53360": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015001672s
STEP: Saw pod success
Sep 11 18:53:05.861: INFO: Pod "pod-configmaps-0886e7df-9315-4914-9d85-c9ef69e53360" satisfied condition "Succeeded or Failed"
Sep 11 18:53:05.864: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-configmaps-0886e7df-9315-4914-9d85-c9ef69e53360 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 18:53:05.909: INFO: Waiting for pod pod-configmaps-0886e7df-9315-4914-9d85-c9ef69e53360 to disappear
Sep 11 18:53:05.914: INFO: Pod pod-configmaps-0886e7df-9315-4914-9d85-c9ef69e53360 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:53:05.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3480" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":240,"skipped":3822,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:53:05.945: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:53:06.137: INFO: Create a RollingUpdate DaemonSet
Sep 11 18:53:06.143: INFO: Check that daemon pods launch on every node of the cluster
Sep 11 18:53:06.148: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:06.149: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:06.149: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:06.153: INFO: Number of nodes with available pods: 0
Sep 11 18:53:06.153: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:53:07.160: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:07.160: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:07.160: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:07.167: INFO: Number of nodes with available pods: 0
Sep 11 18:53:07.167: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:53:08.166: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:08.166: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:08.166: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:08.170: INFO: Number of nodes with available pods: 0
Sep 11 18:53:08.170: INFO: Node ip-10-10-1-141.ec2.internal is running more than one daemon pod
Sep 11 18:53:09.160: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:09.160: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:09.160: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:09.165: INFO: Number of nodes with available pods: 3
Sep 11 18:53:09.165: INFO: Number of running nodes: 3, number of available pods: 3
Sep 11 18:53:09.165: INFO: Update the DaemonSet to trigger a rollout
Sep 11 18:53:09.175: INFO: Updating DaemonSet daemon-set
Sep 11 18:53:24.195: INFO: Roll back the DaemonSet before rollout is complete
Sep 11 18:53:24.206: INFO: Updating DaemonSet daemon-set
Sep 11 18:53:24.206: INFO: Make sure DaemonSet rollback is complete
Sep 11 18:53:24.210: INFO: Wrong image for pod: daemon-set-rkp75. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 11 18:53:24.210: INFO: Pod daemon-set-rkp75 is not available
Sep 11 18:53:24.218: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:24.218: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:24.218: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:25.226: INFO: Wrong image for pod: daemon-set-rkp75. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 11 18:53:25.226: INFO: Pod daemon-set-rkp75 is not available
Sep 11 18:53:25.233: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:25.233: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:25.233: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:26.223: INFO: Pod daemon-set-djgvs is not available
Sep 11 18:53:26.229: INFO: DaemonSet pods can't tolerate node ip-10-10-1-237.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:26.229: INFO: DaemonSet pods can't tolerate node ip-10-10-2-205.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 11 18:53:26.229: INFO: DaemonSet pods can't tolerate node ip-10-10-3-109.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6357, will wait for the garbage collector to delete the pods
Sep 11 18:53:26.303: INFO: Deleting DaemonSet.extensions daemon-set took: 11.905499ms
Sep 11 18:53:27.403: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100258916s
Sep 11 18:54:48.108: INFO: Number of nodes with available pods: 0
Sep 11 18:54:48.108: INFO: Number of running nodes: 0, number of available pods: 0
Sep 11 18:54:48.112: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6357/daemonsets","resourceVersion":"2786206"},"items":null}

Sep 11 18:54:48.115: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6357/pods","resourceVersion":"2786206"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:54:48.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6357" for this suite.

â€¢ [SLOW TEST:102.215 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":241,"skipped":3834,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:54:48.161: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 18:54:49.555: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 11 18:54:51.568: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735447289, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735447289, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735447289, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735447289, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 18:54:54.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:54:54.603: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4115-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:54:56.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8011" for this suite.
STEP: Destroying namespace "webhook-8011-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:8.570 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":242,"skipped":3839,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:54:56.731: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-b62d14e2-4757-4e1f-a36a-d0857d8bd3be in namespace container-probe-9598
Sep 11 18:55:00.934: INFO: Started pod liveness-b62d14e2-4757-4e1f-a36a-d0857d8bd3be in namespace container-probe-9598
STEP: checking the pod's current state and verifying that restartCount is present
Sep 11 18:55:00.938: INFO: Initial restart count of pod liveness-b62d14e2-4757-4e1f-a36a-d0857d8bd3be is 0
Sep 11 18:55:20.994: INFO: Restart count of pod container-probe-9598/liveness-b62d14e2-4757-4e1f-a36a-d0857d8bd3be is now 1 (20.055385955s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:55:21.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9598" for this suite.

â€¢ [SLOW TEST:24.320 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":243,"skipped":3840,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:55:21.051: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5263
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 11 18:55:21.230: INFO: Waiting up to 5m0s for pod "pod-7807ddd4-563d-48a0-ac12-29f7870c9cd9" in namespace "emptydir-5263" to be "Succeeded or Failed"
Sep 11 18:55:21.234: INFO: Pod "pod-7807ddd4-563d-48a0-ac12-29f7870c9cd9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.797709ms
Sep 11 18:55:23.239: INFO: Pod "pod-7807ddd4-563d-48a0-ac12-29f7870c9cd9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008658533s
Sep 11 18:55:25.244: INFO: Pod "pod-7807ddd4-563d-48a0-ac12-29f7870c9cd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013897409s
STEP: Saw pod success
Sep 11 18:55:25.244: INFO: Pod "pod-7807ddd4-563d-48a0-ac12-29f7870c9cd9" satisfied condition "Succeeded or Failed"
Sep 11 18:55:25.248: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-7807ddd4-563d-48a0-ac12-29f7870c9cd9 container test-container: <nil>
STEP: delete the pod
Sep 11 18:55:25.288: INFO: Waiting for pod pod-7807ddd4-563d-48a0-ac12-29f7870c9cd9 to disappear
Sep 11 18:55:25.292: INFO: Pod pod-7807ddd4-563d-48a0-ac12-29f7870c9cd9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:55:25.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5263" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":244,"skipped":3846,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:55:25.319: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9246
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:55:42.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9246" for this suite.

â€¢ [SLOW TEST:17.257 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":245,"skipped":3862,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:55:42.576: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-475
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-1ffa98e3-2792-42d3-ad8e-37e93439f907
STEP: Creating secret with name secret-projected-all-test-volume-d05582d6-023f-4312-a4fa-d970c9ca4480
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 11 18:55:42.759: INFO: Waiting up to 5m0s for pod "projected-volume-c40d262d-802b-4028-b49e-03dfad31b190" in namespace "projected-475" to be "Succeeded or Failed"
Sep 11 18:55:42.764: INFO: Pod "projected-volume-c40d262d-802b-4028-b49e-03dfad31b190": Phase="Pending", Reason="", readiness=false. Elapsed: 4.690741ms
Sep 11 18:55:44.769: INFO: Pod "projected-volume-c40d262d-802b-4028-b49e-03dfad31b190": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009978749s
Sep 11 18:55:46.775: INFO: Pod "projected-volume-c40d262d-802b-4028-b49e-03dfad31b190": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015517065s
STEP: Saw pod success
Sep 11 18:55:46.775: INFO: Pod "projected-volume-c40d262d-802b-4028-b49e-03dfad31b190" satisfied condition "Succeeded or Failed"
Sep 11 18:55:46.782: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod projected-volume-c40d262d-802b-4028-b49e-03dfad31b190 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 11 18:55:46.811: INFO: Waiting for pod projected-volume-c40d262d-802b-4028-b49e-03dfad31b190 to disappear
Sep 11 18:55:46.816: INFO: Pod projected-volume-c40d262d-802b-4028-b49e-03dfad31b190 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:55:46.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-475" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":246,"skipped":3867,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:55:46.841: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2683
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:55:46.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2683" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":247,"skipped":3881,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:55:47.028: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-221
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 11 18:55:49.728: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-221 pod-service-account-6f979652-3b60-4c5d-b235-d8b897cc75d3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 11 18:55:49.963: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-221 pod-service-account-6f979652-3b60-4c5d-b235-d8b897cc75d3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 11 18:55:50.190: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-221 pod-service-account-6f979652-3b60-4c5d-b235-d8b897cc75d3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:55:50.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-221" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":248,"skipped":3900,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:55:50.437: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 11 18:55:52.636: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:55:52.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8412" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":249,"skipped":3943,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:55:52.705: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6109
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-cfe4aa6e-aeb0-4b6b-8feb-5326e95807b8
STEP: Creating configMap with name cm-test-opt-upd-172cd0a5-a211-4b8d-9ece-5ff37eae8b08
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-cfe4aa6e-aeb0-4b6b-8feb-5326e95807b8
STEP: Updating configmap cm-test-opt-upd-172cd0a5-a211-4b8d-9ece-5ff37eae8b08
STEP: Creating configMap with name cm-test-opt-create-58ccbb77-697d-4529-88d7-56314cdd6b03
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:55:57.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6109" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":250,"skipped":3945,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:55:57.067: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:55:57.216: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:55:59.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4498" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":251,"skipped":3959,"failed":0}

------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:55:59.310: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 11 18:56:01.509: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:56:01.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-841" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":252,"skipped":3959,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:56:01.565: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-764
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:56:17.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-764" for this suite.

â€¢ [SLOW TEST:16.341 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":253,"skipped":3983,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:56:17.910: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6126
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 18:56:18.076: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1bdd3c6d-8532-4d43-9800-910a3a40cc02" in namespace "projected-6126" to be "Succeeded or Failed"
Sep 11 18:56:18.080: INFO: Pod "downwardapi-volume-1bdd3c6d-8532-4d43-9800-910a3a40cc02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.47215ms
Sep 11 18:56:20.085: INFO: Pod "downwardapi-volume-1bdd3c6d-8532-4d43-9800-910a3a40cc02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009625081s
STEP: Saw pod success
Sep 11 18:56:20.085: INFO: Pod "downwardapi-volume-1bdd3c6d-8532-4d43-9800-910a3a40cc02" satisfied condition "Succeeded or Failed"
Sep 11 18:56:20.089: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-1bdd3c6d-8532-4d43-9800-910a3a40cc02 container client-container: <nil>
STEP: delete the pod
Sep 11 18:56:20.118: INFO: Waiting for pod downwardapi-volume-1bdd3c6d-8532-4d43-9800-910a3a40cc02 to disappear
Sep 11 18:56:20.122: INFO: Pod downwardapi-volume-1bdd3c6d-8532-4d43-9800-910a3a40cc02 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:56:20.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6126" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":254,"skipped":4004,"failed":0}

------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:56:20.149: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:56:28.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2158" for this suite.

â€¢ [SLOW TEST:8.197 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":255,"skipped":4004,"failed":0}
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:56:28.346: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:56:28.505: INFO: Creating ReplicaSet my-hostname-basic-6cb1a5a8-f311-4fdf-b58c-ac501b39442b
Sep 11 18:56:28.522: INFO: Pod name my-hostname-basic-6cb1a5a8-f311-4fdf-b58c-ac501b39442b: Found 0 pods out of 1
Sep 11 18:56:33.527: INFO: Pod name my-hostname-basic-6cb1a5a8-f311-4fdf-b58c-ac501b39442b: Found 1 pods out of 1
Sep 11 18:56:33.527: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6cb1a5a8-f311-4fdf-b58c-ac501b39442b" is running
Sep 11 18:56:33.533: INFO: Pod "my-hostname-basic-6cb1a5a8-f311-4fdf-b58c-ac501b39442b-4qzp5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-11 18:56:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-11 18:56:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-11 18:56:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-11 18:56:28 +0000 UTC Reason: Message:}])
Sep 11 18:56:33.533: INFO: Trying to dial the pod
Sep 11 18:56:38.547: INFO: Controller my-hostname-basic-6cb1a5a8-f311-4fdf-b58c-ac501b39442b: Got expected result from replica 1 [my-hostname-basic-6cb1a5a8-f311-4fdf-b58c-ac501b39442b-4qzp5]: "my-hostname-basic-6cb1a5a8-f311-4fdf-b58c-ac501b39442b-4qzp5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:56:38.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5967" for this suite.

â€¢ [SLOW TEST:10.226 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":256,"skipped":4004,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:56:38.572: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6318
STEP: creating service affinity-nodeport in namespace services-6318
STEP: creating replication controller affinity-nodeport in namespace services-6318
I0911 18:56:38.827315      19 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-6318, replica count: 3
I0911 18:56:41.877674      19 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 18:56:41.891: INFO: Creating new exec pod
Sep 11 18:56:44.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-6318 execpod-affinitywbx5b -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Sep 11 18:56:45.151: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep 11 18:56:45.151: INFO: stdout: ""
Sep 11 18:56:45.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-6318 execpod-affinitywbx5b -- /bin/sh -x -c nc -zv -t -w 2 10.105.242.103 80'
Sep 11 18:56:45.389: INFO: stderr: "+ nc -zv -t -w 2 10.105.242.103 80\nConnection to 10.105.242.103 80 port [tcp/http] succeeded!\n"
Sep 11 18:56:45.389: INFO: stdout: ""
Sep 11 18:56:45.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-6318 execpod-affinitywbx5b -- /bin/sh -x -c nc -zv -t -w 2 10.10.3.166 30852'
Sep 11 18:56:45.659: INFO: stderr: "+ nc -zv -t -w 2 10.10.3.166 30852\nConnection to 10.10.3.166 30852 port [tcp/30852] succeeded!\n"
Sep 11 18:56:45.659: INFO: stdout: ""
Sep 11 18:56:45.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-6318 execpod-affinitywbx5b -- /bin/sh -x -c nc -zv -t -w 2 10.10.2.19 30852'
Sep 11 18:56:45.927: INFO: stderr: "+ nc -zv -t -w 2 10.10.2.19 30852\nConnection to 10.10.2.19 30852 port [tcp/30852] succeeded!\n"
Sep 11 18:56:45.927: INFO: stdout: ""
Sep 11 18:56:45.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-6318 execpod-affinitywbx5b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.141:30852/ ; done'
Sep 11 18:56:46.438: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.141:30852/\n"
Sep 11 18:56:46.439: INFO: stdout: "\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf\naffinity-nodeport-bgwnf"
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Received response from host: affinity-nodeport-bgwnf
Sep 11 18:56:46.439: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6318, will wait for the garbage collector to delete the pods
Sep 11 18:56:46.529: INFO: Deleting ReplicationController affinity-nodeport took: 12.79136ms
Sep 11 18:56:47.630: INFO: Terminating ReplicationController affinity-nodeport pods took: 1.100269641s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:57:03.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6318" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:24.774 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":257,"skipped":4007,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:57:03.347: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Sep 11 18:57:03.531: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:57:03.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4266" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":258,"skipped":4018,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:57:03.602: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Sep 11 18:57:03.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f -'
Sep 11 18:57:04.339: INFO: stderr: ""
Sep 11 18:57:04.339: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Sep 11 18:57:04.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 diff -f -'
Sep 11 18:57:04.966: INFO: rc: 1
Sep 11 18:57:04.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete -f -'
Sep 11 18:57:05.077: INFO: stderr: ""
Sep 11 18:57:05.077: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:57:05.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3888" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":259,"skipped":4027,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:57:05.127: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-9784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep 11 18:57:05.276: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 11 18:58:05.327: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 18:58:05.332: INFO: Starting informer...
STEP: Starting pod...
Sep 11 18:58:05.550: INFO: Pod is running on ip-10-10-2-19.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep 11 18:58:05.571: INFO: Pod wasn't evicted. Proceeding
Sep 11 18:58:05.571: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep 11 18:59:20.694: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:59:20.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9784" for this suite.

â€¢ [SLOW TEST:135.605 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":260,"skipped":4029,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:59:20.731: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3452
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep 11 18:59:20.907: INFO: Waiting up to 5m0s for pod "downward-api-eff235f8-0d68-4b44-a2da-37fc50559ca4" in namespace "downward-api-3452" to be "Succeeded or Failed"
Sep 11 18:59:20.912: INFO: Pod "downward-api-eff235f8-0d68-4b44-a2da-37fc50559ca4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05974ms
Sep 11 18:59:22.916: INFO: Pod "downward-api-eff235f8-0d68-4b44-a2da-37fc50559ca4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008909178s
STEP: Saw pod success
Sep 11 18:59:22.916: INFO: Pod "downward-api-eff235f8-0d68-4b44-a2da-37fc50559ca4" satisfied condition "Succeeded or Failed"
Sep 11 18:59:22.920: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downward-api-eff235f8-0d68-4b44-a2da-37fc50559ca4 container dapi-container: <nil>
STEP: delete the pod
Sep 11 18:59:22.961: INFO: Waiting for pod downward-api-eff235f8-0d68-4b44-a2da-37fc50559ca4 to disappear
Sep 11 18:59:22.966: INFO: Pod downward-api-eff235f8-0d68-4b44-a2da-37fc50559ca4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:59:22.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3452" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":261,"skipped":4036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:59:23.005: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 11 18:59:25.200: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 18:59:25.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5581" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":262,"skipped":4118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 18:59:25.281: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8221
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-9c827264-3571-4aa2-8428-4d24faeb78b6 in namespace container-probe-8221
Sep 11 18:59:27.463: INFO: Started pod liveness-9c827264-3571-4aa2-8428-4d24faeb78b6 in namespace container-probe-8221
STEP: checking the pod's current state and verifying that restartCount is present
Sep 11 18:59:27.467: INFO: Initial restart count of pod liveness-9c827264-3571-4aa2-8428-4d24faeb78b6 is 0
Sep 11 18:59:39.498: INFO: Restart count of pod container-probe-8221/liveness-9c827264-3571-4aa2-8428-4d24faeb78b6 is now 1 (12.031271971s elapsed)
Sep 11 18:59:59.548: INFO: Restart count of pod container-probe-8221/liveness-9c827264-3571-4aa2-8428-4d24faeb78b6 is now 2 (32.081012992s elapsed)
Sep 11 19:00:19.605: INFO: Restart count of pod container-probe-8221/liveness-9c827264-3571-4aa2-8428-4d24faeb78b6 is now 3 (52.137815522s elapsed)
Sep 11 19:00:39.653: INFO: Restart count of pod container-probe-8221/liveness-9c827264-3571-4aa2-8428-4d24faeb78b6 is now 4 (1m12.186388284s elapsed)
Sep 11 19:01:45.878: INFO: Restart count of pod container-probe-8221/liveness-9c827264-3571-4aa2-8428-4d24faeb78b6 is now 5 (2m18.41170652s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:01:45.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8221" for this suite.

â€¢ [SLOW TEST:140.646 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":263,"skipped":4144,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:01:45.928: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Sep 11 19:01:46.082: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-107083211 proxy --unix-socket=/tmp/kubectl-proxy-unix962776834/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:01:46.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9537" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":264,"skipped":4149,"failed":0}

------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:01:46.185: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4017
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Sep 11 19:01:46.356: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 11 19:02:46.415: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:02:46.419: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-1485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Sep 11 19:02:48.635: INFO: found a healthy node: ip-10-10-2-19.ec2.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 19:02:56.739: INFO: pods created so far: [1 1 1]
Sep 11 19:02:56.739: INFO: length of pods created so far: 3
Sep 11 19:03:04.755: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:03:11.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1485" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:03:11.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4017" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

â€¢ [SLOW TEST:85.795 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":265,"skipped":4149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:03:11.979: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3962
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-ee9c8360-5dc1-4aae-b456-ea6250b72d18
STEP: Creating a pod to test consume configMaps
Sep 11 19:03:12.152: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b00a1f12-52b7-4752-aa5b-1f6376dfc541" in namespace "projected-3962" to be "Succeeded or Failed"
Sep 11 19:03:12.158: INFO: Pod "pod-projected-configmaps-b00a1f12-52b7-4752-aa5b-1f6376dfc541": Phase="Pending", Reason="", readiness=false. Elapsed: 5.938269ms
Sep 11 19:03:14.163: INFO: Pod "pod-projected-configmaps-b00a1f12-52b7-4752-aa5b-1f6376dfc541": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011092474s
STEP: Saw pod success
Sep 11 19:03:14.163: INFO: Pod "pod-projected-configmaps-b00a1f12-52b7-4752-aa5b-1f6376dfc541" satisfied condition "Succeeded or Failed"
Sep 11 19:03:14.167: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-configmaps-b00a1f12-52b7-4752-aa5b-1f6376dfc541 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 19:03:14.216: INFO: Waiting for pod pod-projected-configmaps-b00a1f12-52b7-4752-aa5b-1f6376dfc541 to disappear
Sep 11 19:03:14.222: INFO: Pod pod-projected-configmaps-b00a1f12-52b7-4752-aa5b-1f6376dfc541 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:03:14.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3962" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":266,"skipped":4173,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:03:14.248: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 19:03:15.101: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 19:03:17.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735447795, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735447795, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735447795, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735447795, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 19:03:20.147: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 19:03:20.153: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4773-crds.webhook.example.com via the AdmissionRegistration API
Sep 11 19:03:20.979: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:03:22.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2238" for this suite.
STEP: Destroying namespace "webhook-2238-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:8.960 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":267,"skipped":4192,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:03:23.208: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Sep 11 19:03:23.693: INFO: created test-pod-1
Sep 11 19:03:23.703: INFO: created test-pod-2
Sep 11 19:03:23.713: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:03:23.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2125" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":268,"skipped":4203,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:03:23.881: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8253
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:03:50.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8253" for this suite.

â€¢ [SLOW TEST:26.721 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":269,"skipped":4221,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:03:50.603: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8574
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 19:05:50.798: INFO: Deleting pod "var-expansion-a6bb0680-a5d9-4308-b2f8-9500d1f4c821" in namespace "var-expansion-8574"
Sep 11 19:05:50.807: INFO: Wait up to 5m0s for pod "var-expansion-a6bb0680-a5d9-4308-b2f8-9500d1f4c821" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:05:52.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8574" for this suite.

â€¢ [SLOW TEST:122.247 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":270,"skipped":4278,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:05:52.850: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Sep 11 19:05:59.043: INFO: 10 pods remaining
Sep 11 19:05:59.043: INFO: 10 pods has nil DeletionTimestamp
Sep 11 19:05:59.043: INFO: 
STEP: Gathering metrics
W0911 19:06:00.052085      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 11 19:07:02.074: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:02.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4468" for this suite.

â€¢ [SLOW TEST:69.254 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":271,"skipped":4293,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:02.104: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep 11 19:07:02.272: INFO: Created pod &Pod{ObjectMeta:{dns-7056  dns-7056 /api/v1/namespaces/dns-7056/pods/dns-7056 ff1f4b9a-e747-4b1e-a8e6-1c12ac992699 2791882 0 2020-09-11 19:07:02 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:admin] [] []  [{e2e.test Update v1 2020-09-11 19:07:02 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-r7p2q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-r7p2q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-r7p2q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 11 19:07:02.280: INFO: The status of Pod dns-7056 is Pending, waiting for it to be Running (with Ready = true)
Sep 11 19:07:04.285: INFO: The status of Pod dns-7056 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep 11 19:07:04.285: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7056 PodName:dns-7056 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 19:07:04.285: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Verifying customized DNS server is configured on pod...
Sep 11 19:07:04.514: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7056 PodName:dns-7056 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 11 19:07:04.514: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
Sep 11 19:07:04.766: INFO: Deleting pod dns-7056...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:04.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7056" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":272,"skipped":4297,"failed":0}

------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:04.839: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 19:07:05.004: INFO: Waiting up to 5m0s for pod "busybox-user-65534-966ada00-0885-437b-b694-006664511988" in namespace "security-context-test-1417" to be "Succeeded or Failed"
Sep 11 19:07:05.028: INFO: Pod "busybox-user-65534-966ada00-0885-437b-b694-006664511988": Phase="Pending", Reason="", readiness=false. Elapsed: 24.222129ms
Sep 11 19:07:07.040: INFO: Pod "busybox-user-65534-966ada00-0885-437b-b694-006664511988": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035931405s
Sep 11 19:07:09.045: INFO: Pod "busybox-user-65534-966ada00-0885-437b-b694-006664511988": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041235214s
Sep 11 19:07:09.045: INFO: Pod "busybox-user-65534-966ada00-0885-437b-b694-006664511988" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:09.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1417" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":273,"skipped":4297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:09.489: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-9408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep 11 19:07:09.651: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Sep 11 19:07:10.449: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 11 19:07:12.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448030, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448030, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448030, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448030, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 11 19:07:16.087: INFO: Waited 1.527273909s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:17.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9408" for this suite.

â€¢ [SLOW TEST:8.167 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":274,"skipped":4321,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:17.656: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-7104c550-eaa4-46aa-8c85-636cdc304c17
STEP: Creating a pod to test consume secrets
Sep 11 19:07:17.944: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d88398bf-bb92-41fd-a3fe-4651027234d4" in namespace "projected-2306" to be "Succeeded or Failed"
Sep 11 19:07:17.954: INFO: Pod "pod-projected-secrets-d88398bf-bb92-41fd-a3fe-4651027234d4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.398976ms
Sep 11 19:07:19.959: INFO: Pod "pod-projected-secrets-d88398bf-bb92-41fd-a3fe-4651027234d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015166575s
STEP: Saw pod success
Sep 11 19:07:19.959: INFO: Pod "pod-projected-secrets-d88398bf-bb92-41fd-a3fe-4651027234d4" satisfied condition "Succeeded or Failed"
Sep 11 19:07:19.965: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-secrets-d88398bf-bb92-41fd-a3fe-4651027234d4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 11 19:07:20.006: INFO: Waiting for pod pod-projected-secrets-d88398bf-bb92-41fd-a3fe-4651027234d4 to disappear
Sep 11 19:07:20.010: INFO: Pod pod-projected-secrets-d88398bf-bb92-41fd-a3fe-4651027234d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:20.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2306" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":275,"skipped":4327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:20.052: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:23.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7690" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":276,"skipped":4379,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:23.279: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 11 19:07:25.487: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:25.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4628" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":277,"skipped":4383,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:25.561: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7113
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:25.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7113" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":278,"skipped":4398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:25.818: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-f6c5ee30-6813-4f50-bf11-797b857457b6
STEP: Creating a pod to test consume configMaps
Sep 11 19:07:25.997: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-79ea0d68-f24e-4a48-9330-5fdeeecd79ba" in namespace "projected-1632" to be "Succeeded or Failed"
Sep 11 19:07:26.007: INFO: Pod "pod-projected-configmaps-79ea0d68-f24e-4a48-9330-5fdeeecd79ba": Phase="Pending", Reason="", readiness=false. Elapsed: 10.092407ms
Sep 11 19:07:28.012: INFO: Pod "pod-projected-configmaps-79ea0d68-f24e-4a48-9330-5fdeeecd79ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015010424s
STEP: Saw pod success
Sep 11 19:07:28.012: INFO: Pod "pod-projected-configmaps-79ea0d68-f24e-4a48-9330-5fdeeecd79ba" satisfied condition "Succeeded or Failed"
Sep 11 19:07:28.016: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-configmaps-79ea0d68-f24e-4a48-9330-5fdeeecd79ba container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 11 19:07:28.046: INFO: Waiting for pod pod-projected-configmaps-79ea0d68-f24e-4a48-9330-5fdeeecd79ba to disappear
Sep 11 19:07:28.050: INFO: Pod pod-projected-configmaps-79ea0d68-f24e-4a48-9330-5fdeeecd79ba no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:28.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1632" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":279,"skipped":4428,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:28.075: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7185
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:32.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7185" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":280,"skipped":4450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:32.297: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7424
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 11 19:07:32.448: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 11 19:07:32.463: INFO: Waiting for terminating namespaces to be deleted...
Sep 11 19:07:32.467: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-1-141.ec2.internal before test
Sep 11 19:07:32.479: INFO: calico-node-jgr6s from kube-system started at 2020-08-31 21:04:41 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 19:07:32.479: INFO: kube-proxy-72jmn from kube-system started at 2020-08-31 21:04:41 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-cert-manager-7b65d9c9d6-jst49 from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-flux-memcached-d75b6d9d6-vpnqm from md-flux started at 2020-09-11 16:47:51 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container memcached ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-gangway-66c5d44b4c-f7psj from md-gangway started at 2020-09-11 16:47:59 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container gangway ready: true, restart count 0
Sep 11 19:07:32.479: INFO: harbor-pg-1 from md-harbor started at 2020-09-11 16:49:13 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container postgres ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-harbor-harbor-notary-server-ddbf4b74b-69jlj from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container notary-server ready: true, restart count 1
Sep 11 19:07:32.479: INFO: md-harbor-harbor-portal-7976969b-27bjm from md-harbor started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container portal ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-harbor-harbor-registry-9cdb888bc-97n4b from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (2 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container registry ready: true, restart count 0
Sep 11 19:07:32.479: INFO: 	Container registryctl ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-mlqtr from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-harbor-redis-redis-ha-server-2 from md-harbor started at 2020-09-11 16:49:22 +0000 UTC (2 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container redis ready: true, restart count 0
Sep 11 19:07:32.479: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-helm-operator-557b9b587c-vmkw8 from md-helm-operator started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container flux-helm-operator ready: true, restart count 11
Sep 11 19:07:32.479: INFO: md-kubedb-84d9fff66c-6922k from md-kubedb started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container operator ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-loki-stack-0 from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container loki ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-loki-stack-promtail-hm9cx from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container promtail ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-nginx-ingress-controller-dfxrw from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 19:07:32.479: INFO: alertmanager-md-prometheus-operator-alertmanager-0 from md-prometheus-operator started at 2020-09-11 16:49:13 +0000 UTC (2 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container alertmanager ready: true, restart count 0
Sep 11 19:07:32.479: INFO: 	Container config-reloader ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-prometheus-operator-kube-state-metrics-6f5d54b49-mtqmh from md-prometheus-operator started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-prometheus-operator-operator-7d5c6f8689-tfzwv from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (2 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 11 19:07:32.479: INFO: 	Container tls-proxy ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-prometheus-operator-prometheus-node-exporter-6sz5n from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 19:07:32.479: INFO: md-velero-6cc8bd66b5-rqkv9 from md-velero started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container velero ready: true, restart count 0
Sep 11 19:07:32.479: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-ps57h from sonobuoy started at 2020-09-11 17:37:25 +0000 UTC (2 container statuses recorded)
Sep 11 19:07:32.479: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 11 19:07:32.479: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 11 19:07:32.479: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-2-19.ec2.internal before test
Sep 11 19:07:32.492: INFO: calico-node-w9tmq from kube-system started at 2020-08-31 21:04:43 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.492: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 19:07:32.492: INFO: kube-proxy-djv8h from kube-system started at 2020-08-31 21:04:43 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.492: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 19:07:32.492: INFO: busybox-readonly-fs7b8d7aab-2b9a-4a0e-98c4-1637f13268c7 from kubelet-test-7185 started at 2020-09-11 19:07:28 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.492: INFO: 	Container busybox-readonly-fs7b8d7aab-2b9a-4a0e-98c4-1637f13268c7 ready: true, restart count 0
Sep 11 19:07:32.492: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-mqxbg from md-harbor started at 2020-09-11 18:58:43 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.492: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 19:07:32.492: INFO: md-harbor-redis-redis-ha-server-0 from md-harbor started at 2020-09-11 18:58:09 +0000 UTC (2 container statuses recorded)
Sep 11 19:07:32.492: INFO: 	Container redis ready: true, restart count 0
Sep 11 19:07:32.492: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 19:07:32.492: INFO: md-loki-stack-promtail-w67pc from md-loki-stack started at 2020-09-11 18:58:13 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.492: INFO: 	Container promtail ready: true, restart count 0
Sep 11 19:07:32.492: INFO: md-nginx-ingress-controller-nf4pc from md-nginx-ingress started at 2020-09-11 18:58:23 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.492: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 19:07:32.492: INFO: md-prometheus-operator-prometheus-node-exporter-9zht2 from md-prometheus-operator started at 2020-09-11 18:58:13 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.492: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 19:07:32.492: INFO: prometheus-md-prometheus-operator-prometheus-0 from md-prometheus-operator started at 2020-09-11 18:58:13 +0000 UTC (3 container statuses recorded)
Sep 11 19:07:32.492: INFO: 	Container prometheus ready: true, restart count 0
Sep 11 19:07:32.492: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 11 19:07:32.492: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 11 19:07:32.492: INFO: pod-adoption from replication-controller-7690 started at 2020-09-11 19:07:20 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.493: INFO: 	Container pod-adoption ready: false, restart count 0
Sep 11 19:07:32.493: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-jnktw from sonobuoy started at 2020-09-11 17:37:24 +0000 UTC (2 container statuses recorded)
Sep 11 19:07:32.493: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 11 19:07:32.493: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 11 19:07:32.493: INFO: 
Logging pods the apiserver thinks is on node ip-10-10-3-166.ec2.internal before test
Sep 11 19:07:32.505: INFO: calico-node-wfxpk from kube-system started at 2020-08-31 21:04:40 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container calico-node ready: true, restart count 0
Sep 11 19:07:32.505: INFO: kube-proxy-9vh4x from kube-system started at 2020-08-31 21:04:40 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 11 19:07:32.505: INFO: sealed-secrets-controller-7dfd7dcc6b-g8r5f from kube-system started at 2020-09-11 16:30:49 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container sealed-secrets-controller ready: true, restart count 0
Sep 11 19:07:32.505: INFO: md-cert-manager-cainjector-76b464f9f7-wbmzz from md-cert-manager started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 19:07:32.505: INFO: md-cert-manager-webhook-656744686b-gch9j from md-cert-manager started at 2020-09-11 16:47:49 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container cert-manager ready: true, restart count 0
Sep 11 19:07:32.505: INFO: md-dex-5b4f679bd4-dmmrq from md-dex started at 2020-09-11 16:47:54 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container main ready: true, restart count 1
Sep 11 19:07:32.505: INFO: md-flux-5465b59cbd-dd4fp from md-flux started at 2020-09-11 16:47:51 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container flux ready: true, restart count 0
Sep 11 19:07:32.505: INFO: harbor-pg-0 from md-harbor started at 2020-09-11 16:48:15 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container postgres ready: true, restart count 0
Sep 11 19:07:32.505: INFO: md-harbor-harbor-chartmuseum-55b4567ff8-cx2ch from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container chartmuseum ready: true, restart count 0
Sep 11 19:07:32.505: INFO: md-harbor-harbor-clair-6479d48569-vd6wm from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (2 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container adapter ready: true, restart count 0
Sep 11 19:07:32.505: INFO: 	Container clair ready: true, restart count 1
Sep 11 19:07:32.505: INFO: md-harbor-harbor-core-7999f49dc-vpk6g from md-harbor started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.505: INFO: 	Container core ready: true, restart count 0
Sep 11 19:07:32.506: INFO: md-harbor-harbor-jobservice-556554c4c7-gqzhp from md-harbor started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container jobservice ready: true, restart count 0
Sep 11 19:07:32.506: INFO: md-harbor-harbor-notary-signer-6789c9d4bc-db2kr from md-harbor started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container notary-signer ready: true, restart count 0
Sep 11 19:07:32.506: INFO: md-harbor-redis-redis-ha-haproxy-7d5bf5cb59-tdbqd from md-harbor started at 2020-09-11 16:47:57 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container haproxy ready: true, restart count 0
Sep 11 19:07:32.506: INFO: md-harbor-redis-redis-ha-server-1 from md-harbor started at 2020-09-11 16:48:53 +0000 UTC (2 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container redis ready: true, restart count 0
Sep 11 19:07:32.506: INFO: 	Container sentinel ready: true, restart count 0
Sep 11 19:07:32.506: INFO: md-loki-stack-promtail-94946 from md-loki-stack started at 2020-09-11 16:48:08 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container promtail ready: true, restart count 0
Sep 11 19:07:32.506: INFO: md-nginx-ingress-controller-lhjv9 from md-nginx-ingress started at 2020-09-11 16:48:28 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 11 19:07:32.506: INFO: md-nginx-ingress-default-backend-74c98f4684-r7kpq from md-nginx-ingress started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep 11 19:07:32.506: INFO: md-oauth2-proxy-5c876db75d-jp4lz from md-oauth2-proxy started at 2020-09-11 18:20:22 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container oauth2-proxy ready: true, restart count 0
Sep 11 19:07:32.506: INFO: md-prometheus-operator-prometheus-node-exporter-mjvrq from md-prometheus-operator started at 2020-09-11 16:48:57 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container node-exporter ready: true, restart count 0
Sep 11 19:07:32.506: INFO: sonobuoy from sonobuoy started at 2020-09-11 17:37:23 +0000 UTC (1 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 11 19:07:32.506: INFO: sonobuoy-systemd-logs-daemon-set-5ec8bb9ed65c4f8b-lb975 from sonobuoy started at 2020-09-11 17:37:25 +0000 UTC (2 container statuses recorded)
Sep 11 19:07:32.506: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 11 19:07:32.506: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c75ddfe2-e3de-45ea-8e3e-5392a14c9dba 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-c75ddfe2-e3de-45ea-8e3e-5392a14c9dba off the node ip-10-10-2-19.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c75ddfe2-e3de-45ea-8e3e-5392a14c9dba
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:44.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7424" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:12.411 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":281,"skipped":4472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:44.708: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7307
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:51.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7307" for this suite.

â€¢ [SLOW TEST:7.191 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":282,"skipped":4543,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:51.899: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Sep 11 19:07:52.070: INFO: Waiting up to 5m0s for pod "var-expansion-bf4f3050-294e-44ff-a228-f65d3674f0be" in namespace "var-expansion-4476" to be "Succeeded or Failed"
Sep 11 19:07:52.080: INFO: Pod "var-expansion-bf4f3050-294e-44ff-a228-f65d3674f0be": Phase="Pending", Reason="", readiness=false. Elapsed: 10.282787ms
Sep 11 19:07:54.085: INFO: Pod "var-expansion-bf4f3050-294e-44ff-a228-f65d3674f0be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015502916s
STEP: Saw pod success
Sep 11 19:07:54.085: INFO: Pod "var-expansion-bf4f3050-294e-44ff-a228-f65d3674f0be" satisfied condition "Succeeded or Failed"
Sep 11 19:07:54.089: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod var-expansion-bf4f3050-294e-44ff-a228-f65d3674f0be container dapi-container: <nil>
STEP: delete the pod
Sep 11 19:07:54.117: INFO: Waiting for pod var-expansion-bf4f3050-294e-44ff-a228-f65d3674f0be to disappear
Sep 11 19:07:54.121: INFO: Pod var-expansion-bf4f3050-294e-44ff-a228-f65d3674f0be no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:07:54.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4476" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":283,"skipped":4563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:07:54.146: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7620
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-7620
Sep 11 19:07:54.346: INFO: Found 0 stateful pods, waiting for 1
Sep 11 19:08:04.352: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 11 19:08:04.378: INFO: Deleting all statefulset in ns statefulset-7620
Sep 11 19:08:04.385: INFO: Scaling statefulset ss to 0
Sep 11 19:08:24.422: INFO: Waiting for statefulset status.replicas updated to 0
Sep 11 19:08:24.426: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:08:24.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7620" for this suite.

â€¢ [SLOW TEST:30.330 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":284,"skipped":4599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:08:24.477: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1851
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1851.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1851.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 11 19:08:28.674: INFO: DNS probes using dns-test-23ab4e53-8e6a-4478-b3b0-4ce59ef1a75a succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1851.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1851.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 11 19:08:32.756: INFO: File wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:32.761: INFO: File jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:32.761: INFO: Lookups using dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d failed for: [wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local]

Sep 11 19:08:37.767: INFO: File wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:37.772: INFO: File jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:37.772: INFO: Lookups using dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d failed for: [wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local]

Sep 11 19:08:42.766: INFO: File wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:42.771: INFO: File jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:42.771: INFO: Lookups using dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d failed for: [wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local]

Sep 11 19:08:47.773: INFO: File wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:47.782: INFO: File jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:47.782: INFO: Lookups using dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d failed for: [wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local]

Sep 11 19:08:52.767: INFO: File wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:52.772: INFO: File jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:52.772: INFO: Lookups using dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d failed for: [wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local]

Sep 11 19:08:57.767: INFO: File wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local from pod  dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 11 19:08:57.772: INFO: Lookups using dns-1851/dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d failed for: [wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local]

Sep 11 19:09:02.773: INFO: DNS probes using dns-test-574e98cd-fe47-4255-b5e7-ba66f051731d succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1851.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1851.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1851.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1851.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 11 19:09:04.904: INFO: DNS probes using dns-test-4a892282-1ebc-4401-8b67-87983791f6c6 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:04.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1851" for this suite.

â€¢ [SLOW TEST:40.559 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":285,"skipped":4622,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:05.036: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 19:09:05.645: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 19:09:07.658: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448145, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448145, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448145, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448145, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 19:09:10.689: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 11 19:09:12.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 attach --namespace=webhook-4769 to-be-attached-pod -i -c=container1'
Sep 11 19:09:13.034: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:13.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4769" for this suite.
STEP: Destroying namespace "webhook-4769-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:8.163 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":286,"skipped":4628,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:13.200: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4098
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 19:09:13.362: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ecf06392-0297-492f-93f6-2971b790e3db" in namespace "projected-4098" to be "Succeeded or Failed"
Sep 11 19:09:13.374: INFO: Pod "downwardapi-volume-ecf06392-0297-492f-93f6-2971b790e3db": Phase="Pending", Reason="", readiness=false. Elapsed: 11.625908ms
Sep 11 19:09:15.378: INFO: Pod "downwardapi-volume-ecf06392-0297-492f-93f6-2971b790e3db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016656211s
STEP: Saw pod success
Sep 11 19:09:15.379: INFO: Pod "downwardapi-volume-ecf06392-0297-492f-93f6-2971b790e3db" satisfied condition "Succeeded or Failed"
Sep 11 19:09:15.382: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-ecf06392-0297-492f-93f6-2971b790e3db container client-container: <nil>
STEP: delete the pod
Sep 11 19:09:15.413: INFO: Waiting for pod downwardapi-volume-ecf06392-0297-492f-93f6-2971b790e3db to disappear
Sep 11 19:09:15.418: INFO: Pod downwardapi-volume-ecf06392-0297-492f-93f6-2971b790e3db no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:15.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4098" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":287,"skipped":4631,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:15.445: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5687.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5687.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5687.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5687.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5687.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5687.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 11 19:09:17.719: INFO: DNS probes using dns-5687/dns-test-c75d9de9-790d-45e8-a74b-d2c2b2c6ef6e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:17.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5687" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":288,"skipped":4661,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:17.850: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4311
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-aa507742-a358-4edc-8d92-3e4801fe146a
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:20.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4311" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":289,"skipped":4672,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:20.106: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-9456/configmap-test-74d92f20-8fb4-426b-82ea-b663ec7643f2
STEP: Creating a pod to test consume configMaps
Sep 11 19:09:20.282: INFO: Waiting up to 5m0s for pod "pod-configmaps-20e96ef4-b7c9-456b-bf23-7eb5f7c8c738" in namespace "configmap-9456" to be "Succeeded or Failed"
Sep 11 19:09:20.287: INFO: Pod "pod-configmaps-20e96ef4-b7c9-456b-bf23-7eb5f7c8c738": Phase="Pending", Reason="", readiness=false. Elapsed: 4.62372ms
Sep 11 19:09:22.292: INFO: Pod "pod-configmaps-20e96ef4-b7c9-456b-bf23-7eb5f7c8c738": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009547309s
STEP: Saw pod success
Sep 11 19:09:22.292: INFO: Pod "pod-configmaps-20e96ef4-b7c9-456b-bf23-7eb5f7c8c738" satisfied condition "Succeeded or Failed"
Sep 11 19:09:22.295: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-configmaps-20e96ef4-b7c9-456b-bf23-7eb5f7c8c738 container env-test: <nil>
STEP: delete the pod
Sep 11 19:09:22.323: INFO: Waiting for pod pod-configmaps-20e96ef4-b7c9-456b-bf23-7eb5f7c8c738 to disappear
Sep 11 19:09:22.327: INFO: Pod pod-configmaps-20e96ef4-b7c9-456b-bf23-7eb5f7c8c738 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:22.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9456" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":290,"skipped":4679,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:22.352: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Sep 11 19:09:22.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 create -f - --namespace=kubectl-3624'
Sep 11 19:09:22.948: INFO: stderr: ""
Sep 11 19:09:22.948: INFO: stdout: "pod/pause created\n"
Sep 11 19:09:22.948: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 11 19:09:22.949: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3624" to be "running and ready"
Sep 11 19:09:22.952: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.928083ms
Sep 11 19:09:24.956: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007893969s
Sep 11 19:09:26.961: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.012614521s
Sep 11 19:09:26.961: INFO: Pod "pause" satisfied condition "running and ready"
Sep 11 19:09:26.961: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 11 19:09:26.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 label pods pause testing-label=testing-label-value --namespace=kubectl-3624'
Sep 11 19:09:27.089: INFO: stderr: ""
Sep 11 19:09:27.089: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 11 19:09:27.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pod pause -L testing-label --namespace=kubectl-3624'
Sep 11 19:09:27.182: INFO: stderr: ""
Sep 11 19:09:27.182: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 11 19:09:27.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 label pods pause testing-label- --namespace=kubectl-3624'
Sep 11 19:09:27.292: INFO: stderr: ""
Sep 11 19:09:27.292: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 11 19:09:27.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pod pause -L testing-label --namespace=kubectl-3624'
Sep 11 19:09:27.395: INFO: stderr: ""
Sep 11 19:09:27.395: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Sep 11 19:09:27.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 delete --grace-period=0 --force -f - --namespace=kubectl-3624'
Sep 11 19:09:27.518: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 11 19:09:27.518: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 11 19:09:27.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get rc,svc -l name=pause --no-headers --namespace=kubectl-3624'
Sep 11 19:09:27.638: INFO: stderr: "No resources found in kubectl-3624 namespace.\n"
Sep 11 19:09:27.638: INFO: stdout: ""
Sep 11 19:09:27.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 get pods -l name=pause --namespace=kubectl-3624 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 11 19:09:27.764: INFO: stderr: ""
Sep 11 19:09:27.764: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:27.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3624" for this suite.

â€¢ [SLOW TEST:5.451 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1330
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":291,"skipped":4698,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:27.803: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2360
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:28.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2360" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
â€¢{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":292,"skipped":4705,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:28.122: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7671
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Sep 11 19:09:28.294: INFO: Waiting up to 5m0s for pod "client-containers-5eb42127-27e8-4ce8-bd6d-aa2679d987b7" in namespace "containers-7671" to be "Succeeded or Failed"
Sep 11 19:09:28.315: INFO: Pod "client-containers-5eb42127-27e8-4ce8-bd6d-aa2679d987b7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.322122ms
Sep 11 19:09:30.320: INFO: Pod "client-containers-5eb42127-27e8-4ce8-bd6d-aa2679d987b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025565224s
STEP: Saw pod success
Sep 11 19:09:30.320: INFO: Pod "client-containers-5eb42127-27e8-4ce8-bd6d-aa2679d987b7" satisfied condition "Succeeded or Failed"
Sep 11 19:09:30.324: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod client-containers-5eb42127-27e8-4ce8-bd6d-aa2679d987b7 container test-container: <nil>
STEP: delete the pod
Sep 11 19:09:30.361: INFO: Waiting for pod client-containers-5eb42127-27e8-4ce8-bd6d-aa2679d987b7 to disappear
Sep 11 19:09:30.366: INFO: Pod client-containers-5eb42127-27e8-4ce8-bd6d-aa2679d987b7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:30.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7671" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":293,"skipped":4764,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:30.393: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 11 19:09:31.231: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 11 19:09:33.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448171, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448171, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448171, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735448171, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 11 19:09:36.286: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:36.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8246" for this suite.
STEP: Destroying namespace "webhook-8246-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.345 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":294,"skipped":4767,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:36.738: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 11 19:09:36.912: INFO: Waiting up to 5m0s for pod "pod-11af63a0-46bf-4db9-b815-5f77fa9d3f71" in namespace "emptydir-7347" to be "Succeeded or Failed"
Sep 11 19:09:36.937: INFO: Pod "pod-11af63a0-46bf-4db9-b815-5f77fa9d3f71": Phase="Pending", Reason="", readiness=false. Elapsed: 25.343432ms
Sep 11 19:09:38.942: INFO: Pod "pod-11af63a0-46bf-4db9-b815-5f77fa9d3f71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029974597s
STEP: Saw pod success
Sep 11 19:09:38.942: INFO: Pod "pod-11af63a0-46bf-4db9-b815-5f77fa9d3f71" satisfied condition "Succeeded or Failed"
Sep 11 19:09:38.946: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-11af63a0-46bf-4db9-b815-5f77fa9d3f71 container test-container: <nil>
STEP: delete the pod
Sep 11 19:09:38.973: INFO: Waiting for pod pod-11af63a0-46bf-4db9-b815-5f77fa9d3f71 to disappear
Sep 11 19:09:38.978: INFO: Pod pod-11af63a0-46bf-4db9-b815-5f77fa9d3f71 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:09:38.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7347" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":295,"skipped":4767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:09:39.005: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-5f74
STEP: Creating a pod to test atomic-volume-subpath
Sep 11 19:09:39.194: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5f74" in namespace "subpath-6141" to be "Succeeded or Failed"
Sep 11 19:09:39.198: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Pending", Reason="", readiness=false. Elapsed: 3.611319ms
Sep 11 19:09:41.203: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 2.008787996s
Sep 11 19:09:43.208: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 4.013606857s
Sep 11 19:09:45.212: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 6.017934728s
Sep 11 19:09:47.216: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 8.021914899s
Sep 11 19:09:49.220: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 10.025921014s
Sep 11 19:09:51.225: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 12.030832621s
Sep 11 19:09:53.232: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 14.037659631s
Sep 11 19:09:55.249: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 16.055003763s
Sep 11 19:09:57.256: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 18.06214235s
Sep 11 19:09:59.260: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 20.066155208s
Sep 11 19:10:01.266: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Running", Reason="", readiness=true. Elapsed: 22.071631838s
Sep 11 19:10:03.271: INFO: Pod "pod-subpath-test-configmap-5f74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.077048741s
STEP: Saw pod success
Sep 11 19:10:03.271: INFO: Pod "pod-subpath-test-configmap-5f74" satisfied condition "Succeeded or Failed"
Sep 11 19:10:03.276: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-subpath-test-configmap-5f74 container test-container-subpath-configmap-5f74: <nil>
STEP: delete the pod
Sep 11 19:10:03.312: INFO: Waiting for pod pod-subpath-test-configmap-5f74 to disappear
Sep 11 19:10:03.317: INFO: Pod pod-subpath-test-configmap-5f74 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5f74
Sep 11 19:10:03.317: INFO: Deleting pod "pod-subpath-test-configmap-5f74" in namespace "subpath-6141"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:10:03.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6141" for this suite.

â€¢ [SLOW TEST:24.350 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":296,"skipped":4815,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:10:03.355: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 19:10:03.519: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b8b6b51-f68d-4759-aa6e-557125225776" in namespace "projected-7857" to be "Succeeded or Failed"
Sep 11 19:10:03.523: INFO: Pod "downwardapi-volume-0b8b6b51-f68d-4759-aa6e-557125225776": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108918ms
Sep 11 19:10:05.528: INFO: Pod "downwardapi-volume-0b8b6b51-f68d-4759-aa6e-557125225776": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009203473s
STEP: Saw pod success
Sep 11 19:10:05.528: INFO: Pod "downwardapi-volume-0b8b6b51-f68d-4759-aa6e-557125225776" satisfied condition "Succeeded or Failed"
Sep 11 19:10:05.532: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-0b8b6b51-f68d-4759-aa6e-557125225776 container client-container: <nil>
STEP: delete the pod
Sep 11 19:10:05.566: INFO: Waiting for pod downwardapi-volume-0b8b6b51-f68d-4759-aa6e-557125225776 to disappear
Sep 11 19:10:05.570: INFO: Pod downwardapi-volume-0b8b6b51-f68d-4759-aa6e-557125225776 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:10:05.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7857" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":297,"skipped":4822,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:10:05.601: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-3d277150-b325-46a7-83cb-3a4836ac3f1a
STEP: Creating a pod to test consume secrets
Sep 11 19:10:05.782: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1c903fe0-9b5f-423d-9b6d-e8fc77fd9919" in namespace "projected-7623" to be "Succeeded or Failed"
Sep 11 19:10:05.790: INFO: Pod "pod-projected-secrets-1c903fe0-9b5f-423d-9b6d-e8fc77fd9919": Phase="Pending", Reason="", readiness=false. Elapsed: 7.430219ms
Sep 11 19:10:07.796: INFO: Pod "pod-projected-secrets-1c903fe0-9b5f-423d-9b6d-e8fc77fd9919": Phase="Running", Reason="", readiness=true. Elapsed: 2.013529561s
Sep 11 19:10:09.800: INFO: Pod "pod-projected-secrets-1c903fe0-9b5f-423d-9b6d-e8fc77fd9919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018042099s
STEP: Saw pod success
Sep 11 19:10:09.800: INFO: Pod "pod-projected-secrets-1c903fe0-9b5f-423d-9b6d-e8fc77fd9919" satisfied condition "Succeeded or Failed"
Sep 11 19:10:09.804: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-projected-secrets-1c903fe0-9b5f-423d-9b6d-e8fc77fd9919 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 11 19:10:09.837: INFO: Waiting for pod pod-projected-secrets-1c903fe0-9b5f-423d-9b6d-e8fc77fd9919 to disappear
Sep 11 19:10:09.841: INFO: Pod pod-projected-secrets-1c903fe0-9b5f-423d-9b6d-e8fc77fd9919 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:10:09.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7623" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":298,"skipped":4829,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:10:09.870: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8978
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:10:21.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8978" for this suite.

â€¢ [SLOW TEST:11.273 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":299,"skipped":4838,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:10:21.143: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2590
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 11 19:10:21.437: INFO: Waiting up to 5m0s for pod "pod-c274d564-2fa4-42ab-b5da-79a3f9f104bc" in namespace "emptydir-2590" to be "Succeeded or Failed"
Sep 11 19:10:21.458: INFO: Pod "pod-c274d564-2fa4-42ab-b5da-79a3f9f104bc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.569736ms
Sep 11 19:10:23.464: INFO: Pod "pod-c274d564-2fa4-42ab-b5da-79a3f9f104bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026966316s
STEP: Saw pod success
Sep 11 19:10:23.464: INFO: Pod "pod-c274d564-2fa4-42ab-b5da-79a3f9f104bc" satisfied condition "Succeeded or Failed"
Sep 11 19:10:23.469: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod pod-c274d564-2fa4-42ab-b5da-79a3f9f104bc container test-container: <nil>
STEP: delete the pod
Sep 11 19:10:23.506: INFO: Waiting for pod pod-c274d564-2fa4-42ab-b5da-79a3f9f104bc to disappear
Sep 11 19:10:23.512: INFO: Pod pod-c274d564-2fa4-42ab-b5da-79a3f9f104bc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:10:23.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2590" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4844,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:10:23.538: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:10:23.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1972" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":301,"skipped":4849,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:10:23.788: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 19:10:23.958: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Pending, waiting for it to be Running (with Ready = true)
Sep 11 19:10:25.963: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = false)
Sep 11 19:10:27.963: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = false)
Sep 11 19:10:29.963: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = false)
Sep 11 19:10:31.963: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = false)
Sep 11 19:10:33.963: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = false)
Sep 11 19:10:35.969: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = false)
Sep 11 19:10:37.964: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = false)
Sep 11 19:10:39.963: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = false)
Sep 11 19:10:41.963: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = false)
Sep 11 19:10:43.963: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = false)
Sep 11 19:10:45.963: INFO: The status of Pod test-webserver-b389b1ca-79e5-41c6-8b88-bfefe5bb21f9 is Running (Ready = true)
Sep 11 19:10:45.967: INFO: Container started at 2020-09-11 19:10:25 +0000 UTC, pod became ready at 2020-09-11 19:10:44 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:10:45.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9612" for this suite.

â€¢ [SLOW TEST:22.227 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":302,"skipped":4865,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:10:46.015: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1710
STEP: creating service affinity-clusterip in namespace services-1710
STEP: creating replication controller affinity-clusterip in namespace services-1710
I0911 19:10:46.255090      19 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-1710, replica count: 3
I0911 19:10:49.305563      19 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 11 19:10:49.314: INFO: Creating new exec pod
Sep 11 19:10:52.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1710 execpod-affinity8cbmb -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Sep 11 19:10:52.563: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep 11 19:10:52.563: INFO: stdout: ""
Sep 11 19:10:52.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1710 execpod-affinity8cbmb -- /bin/sh -x -c nc -zv -t -w 2 10.99.174.244 80'
Sep 11 19:10:52.791: INFO: stderr: "+ nc -zv -t -w 2 10.99.174.244 80\nConnection to 10.99.174.244 80 port [tcp/http] succeeded!\n"
Sep 11 19:10:52.791: INFO: stdout: ""
Sep 11 19:10:52.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-107083211 exec --namespace=services-1710 execpod-affinity8cbmb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.174.244:80/ ; done'
Sep 11 19:10:53.108: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.174.244:80/\n"
Sep 11 19:10:53.108: INFO: stdout: "\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2\naffinity-clusterip-vlmr2"
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Received response from host: affinity-clusterip-vlmr2
Sep 11 19:10:53.108: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1710, will wait for the garbage collector to delete the pods
Sep 11 19:10:53.193: INFO: Deleting ReplicationController affinity-clusterip took: 11.56599ms
Sep 11 19:10:54.294: INFO: Terminating ReplicationController affinity-clusterip pods took: 1.100228661s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:11:03.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1710" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:17.367 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":303,"skipped":4874,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:11:03.382: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3641
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep 11 19:11:03.533: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:11:04.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3641" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":304,"skipped":4891,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 11 19:11:04.147: INFO: >>> kubeConfig: /tmp/kubeconfig-107083211
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep 11 19:11:04.695: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc344f2e-7054-4cd7-95fd-e0fd48033b85" in namespace "downward-api-3231" to be "Succeeded or Failed"
Sep 11 19:11:04.719: INFO: Pod "downwardapi-volume-fc344f2e-7054-4cd7-95fd-e0fd48033b85": Phase="Pending", Reason="", readiness=false. Elapsed: 23.905919ms
Sep 11 19:11:06.724: INFO: Pod "downwardapi-volume-fc344f2e-7054-4cd7-95fd-e0fd48033b85": Phase="Running", Reason="", readiness=true. Elapsed: 2.028675982s
Sep 11 19:11:08.735: INFO: Pod "downwardapi-volume-fc344f2e-7054-4cd7-95fd-e0fd48033b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039950621s
STEP: Saw pod success
Sep 11 19:11:08.736: INFO: Pod "downwardapi-volume-fc344f2e-7054-4cd7-95fd-e0fd48033b85" satisfied condition "Succeeded or Failed"
Sep 11 19:11:08.750: INFO: Trying to get logs from node ip-10-10-2-19.ec2.internal pod downwardapi-volume-fc344f2e-7054-4cd7-95fd-e0fd48033b85 container client-container: <nil>
STEP: delete the pod
Sep 11 19:11:08.792: INFO: Waiting for pod downwardapi-volume-fc344f2e-7054-4cd7-95fd-e0fd48033b85 to disappear
Sep 11 19:11:08.798: INFO: Pod downwardapi-volume-fc344f2e-7054-4cd7-95fd-e0fd48033b85 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 11 19:11:08.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3231" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":305,"skipped":4906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSep 11 19:11:08.833: INFO: Running AfterSuite actions on all nodes
Sep 11 19:11:08.833: INFO: Running AfterSuite actions on node 1
Sep 11 19:11:08.833: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4927,"failed":0}

Ran 305 of 5232 Specs in 5615.837 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4927 Skipped
PASS

Ginkgo ran 1 suite in 1h33m37.582871774s
Test Suite Passed
