I0521 18:56:28.957223      20 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-321270312
I0521 18:56:28.957296      20 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0521 18:56:28.957660      20 e2e.go:129] Starting e2e run "b9feb580-a75b-4ebc-bcf2-fe1f1cbe618d" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1621623383 - Will randomize all specs
Will run 305 of 5238 specs

May 21 18:56:28.991: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 18:56:28.999: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 21 18:56:29.038: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 21 18:56:29.143: INFO: 7 / 7 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 21 18:56:29.143: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
May 21 18:56:29.143: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 21 18:56:29.166: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May 21 18:56:29.167: INFO: e2e test version: v1.19.6
May 21 18:56:29.170: INFO: kube-apiserver version: v1.19.6
May 21 18:56:29.170: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 18:56:29.190: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 18:56:29.193: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename podtemplate
May 21 18:56:29.305: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
May 21 18:56:29.323: INFO: created test-podtemplate-1
May 21 18:56:29.335: INFO: created test-podtemplate-2
May 21 18:56:29.341: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 21 18:56:29.348: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 21 18:56:29.381: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 18:56:29.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9743" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":1,"skipped":49,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 18:56:29.408: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 21 18:56:29.495: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 18:56:37.972: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 18:57:12.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5929" for this suite.

• [SLOW TEST:42.992 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":2,"skipped":50,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 18:57:12.400: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 18:57:12.476: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 21 18:57:21.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 --namespace=crd-publish-openapi-856 create -f -'
May 21 18:57:22.770: INFO: stderr: ""
May 21 18:57:22.770: INFO: stdout: "e2e-test-crd-publish-openapi-4657-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 21 18:57:22.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 --namespace=crd-publish-openapi-856 delete e2e-test-crd-publish-openapi-4657-crds test-foo'
May 21 18:57:23.116: INFO: stderr: ""
May 21 18:57:23.116: INFO: stdout: "e2e-test-crd-publish-openapi-4657-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 21 18:57:23.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 --namespace=crd-publish-openapi-856 apply -f -'
May 21 18:57:23.825: INFO: stderr: ""
May 21 18:57:23.825: INFO: stdout: "e2e-test-crd-publish-openapi-4657-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 21 18:57:23.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 --namespace=crd-publish-openapi-856 delete e2e-test-crd-publish-openapi-4657-crds test-foo'
May 21 18:57:24.149: INFO: stderr: ""
May 21 18:57:24.149: INFO: stdout: "e2e-test-crd-publish-openapi-4657-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 21 18:57:24.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 --namespace=crd-publish-openapi-856 create -f -'
May 21 18:57:24.964: INFO: rc: 1
May 21 18:57:24.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 --namespace=crd-publish-openapi-856 apply -f -'
May 21 18:57:25.639: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 21 18:57:25.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 --namespace=crd-publish-openapi-856 create -f -'
May 21 18:57:26.530: INFO: rc: 1
May 21 18:57:26.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 --namespace=crd-publish-openapi-856 apply -f -'
May 21 18:57:27.300: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 21 18:57:27.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 explain e2e-test-crd-publish-openapi-4657-crds'
May 21 18:57:28.023: INFO: stderr: ""
May 21 18:57:28.024: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4657-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 21 18:57:28.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 explain e2e-test-crd-publish-openapi-4657-crds.metadata'
May 21 18:57:28.682: INFO: stderr: ""
May 21 18:57:28.682: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4657-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 21 18:57:28.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 explain e2e-test-crd-publish-openapi-4657-crds.spec'
May 21 18:57:29.343: INFO: stderr: ""
May 21 18:57:29.343: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4657-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 21 18:57:29.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 explain e2e-test-crd-publish-openapi-4657-crds.spec.bars'
May 21 18:57:29.977: INFO: stderr: ""
May 21 18:57:29.977: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4657-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 21 18:57:29.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-856 explain e2e-test-crd-publish-openapi-4657-crds.spec.bars2'
May 21 18:57:30.622: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 18:57:39.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-856" for this suite.

• [SLOW TEST:27.589 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":3,"skipped":65,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 18:57:39.989: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
May 21 18:57:40.118: INFO: Waiting up to 5m0s for pod "pod-3148a37c-164a-461d-b70c-6ec051627716" in namespace "emptydir-2785" to be "Succeeded or Failed"
May 21 18:57:40.126: INFO: Pod "pod-3148a37c-164a-461d-b70c-6ec051627716": Phase="Pending", Reason="", readiness=false. Elapsed: 8.495231ms
May 21 18:57:42.134: INFO: Pod "pod-3148a37c-164a-461d-b70c-6ec051627716": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015901687s
May 21 18:57:44.140: INFO: Pod "pod-3148a37c-164a-461d-b70c-6ec051627716": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021942846s
May 21 18:57:46.168: INFO: Pod "pod-3148a37c-164a-461d-b70c-6ec051627716": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050650449s
May 21 18:57:48.176: INFO: Pod "pod-3148a37c-164a-461d-b70c-6ec051627716": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.05870005s
STEP: Saw pod success
May 21 18:57:48.177: INFO: Pod "pod-3148a37c-164a-461d-b70c-6ec051627716" satisfied condition "Succeeded or Failed"
May 21 18:57:48.199: INFO: Trying to get logs from node dc-hg-1 pod pod-3148a37c-164a-461d-b70c-6ec051627716 container test-container: <nil>
STEP: delete the pod
May 21 18:57:48.316: INFO: Waiting for pod pod-3148a37c-164a-461d-b70c-6ec051627716 to disappear
May 21 18:57:48.326: INFO: Pod pod-3148a37c-164a-461d-b70c-6ec051627716 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 18:57:48.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2785" for this suite.

• [SLOW TEST:8.356 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":4,"skipped":70,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 18:57:48.345: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 21 18:57:48.440: INFO: PodSpec: initContainers in spec.initContainers
May 21 18:58:42.734: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e6c91a4c-0806-480f-b0d1-32f0401a48bf", GenerateName:"", Namespace:"init-container-5822", SelfLink:"/api/v1/namespaces/init-container-5822/pods/pod-init-e6c91a4c-0806-480f-b0d1-32f0401a48bf", UID:"99977e4b-0163-4c69-82b3-5eda95794f1c", ResourceVersion:"1266164", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63757220268, loc:(*time.Location)(0x76f4840)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"440382946"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.161.158/32", "cni.projectcalico.org/podIPs":"192.168.161.158/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0063a1f60), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0063a1f80)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0063a1fa0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0063a1fc0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0063a1fe0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0063d8000)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-8ngvk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001c2bc40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8ngvk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8ngvk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8ngvk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0063cb498), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"dc-hg-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00125f340), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0063cb520)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0063cb540)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0063cb548), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0063cb54c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc006337ef0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220268, loc:(*time.Location)(0x76f4840)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220268, loc:(*time.Location)(0x76f4840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220268, loc:(*time.Location)(0x76f4840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220268, loc:(*time.Location)(0x76f4840)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.1.125", PodIP:"192.168.161.158", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.161.158"}}, StartTime:(*v1.Time)(0xc0063d8020), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00125f490)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00125f500)}, Ready:false, RestartCount:3, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"docker-pullable://mirror.gcr.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://5dd758f2325abd88bcec6f3df55306fe8c8c9fe1e84734a7fce0e85b030aa08f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0063d8060), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0063d8040), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0063cb5cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 18:58:42.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5822" for this suite.

• [SLOW TEST:54.413 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":5,"skipped":79,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 18:58:42.758: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7020 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7020;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7020 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7020;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7020.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7020.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7020.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7020.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7020.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7020.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7020.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7020.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7020.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7020.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7020.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 177.57.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.57.177_udp@PTR;check="$$(dig +tcp +noall +answer +search 177.57.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.57.177_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7020 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7020;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7020 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7020;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7020.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7020.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7020.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7020.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7020.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7020.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7020.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7020.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7020.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7020.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7020.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7020.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 177.57.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.57.177_udp@PTR;check="$$(dig +tcp +noall +answer +search 177.57.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.57.177_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 18:58:47.296: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.309: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.319: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.330: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.340: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.347: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.360: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.372: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.468: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.474: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.480: INFO: Unable to read jessie_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.492: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.500: INFO: Unable to read jessie_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.507: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.513: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.523: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:47.563: INFO: Lookups using dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7020 wheezy_tcp@dns-test-service.dns-7020 wheezy_udp@dns-test-service.dns-7020.svc wheezy_tcp@dns-test-service.dns-7020.svc wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7020 jessie_tcp@dns-test-service.dns-7020 jessie_udp@dns-test-service.dns-7020.svc jessie_tcp@dns-test-service.dns-7020.svc jessie_udp@_http._tcp.dns-test-service.dns-7020.svc jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc]

May 21 18:58:52.575: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.589: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.595: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.601: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.611: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.619: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.627: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.650: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.706: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.712: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.716: INFO: Unable to read jessie_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.722: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.728: INFO: Unable to read jessie_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.735: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.744: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.753: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:52.793: INFO: Lookups using dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7020 wheezy_tcp@dns-test-service.dns-7020 wheezy_udp@dns-test-service.dns-7020.svc wheezy_tcp@dns-test-service.dns-7020.svc wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7020 jessie_tcp@dns-test-service.dns-7020 jessie_udp@dns-test-service.dns-7020.svc jessie_tcp@dns-test-service.dns-7020.svc jessie_udp@_http._tcp.dns-test-service.dns-7020.svc jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc]

May 21 18:58:57.578: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.597: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.606: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.613: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.622: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.628: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.634: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.647: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.822: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.829: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.839: INFO: Unable to read jessie_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.848: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.857: INFO: Unable to read jessie_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.877: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.890: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.902: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:58:57.998: INFO: Lookups using dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7020 wheezy_tcp@dns-test-service.dns-7020 wheezy_udp@dns-test-service.dns-7020.svc wheezy_tcp@dns-test-service.dns-7020.svc wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7020 jessie_tcp@dns-test-service.dns-7020 jessie_udp@dns-test-service.dns-7020.svc jessie_tcp@dns-test-service.dns-7020.svc jessie_udp@_http._tcp.dns-test-service.dns-7020.svc jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc]

May 21 18:59:02.571: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.578: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.586: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.593: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.600: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.608: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.617: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.629: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.684: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.692: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.698: INFO: Unable to read jessie_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.704: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.711: INFO: Unable to read jessie_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.719: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.726: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.733: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:02.788: INFO: Lookups using dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7020 wheezy_tcp@dns-test-service.dns-7020 wheezy_udp@dns-test-service.dns-7020.svc wheezy_tcp@dns-test-service.dns-7020.svc wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7020 jessie_tcp@dns-test-service.dns-7020 jessie_udp@dns-test-service.dns-7020.svc jessie_tcp@dns-test-service.dns-7020.svc jessie_udp@_http._tcp.dns-test-service.dns-7020.svc jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc]

May 21 18:59:07.580: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.597: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.615: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.630: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.653: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.695: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.717: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.734: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.827: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.834: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.839: INFO: Unable to read jessie_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.847: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.860: INFO: Unable to read jessie_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.867: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.874: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.881: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:07.952: INFO: Lookups using dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7020 wheezy_tcp@dns-test-service.dns-7020 wheezy_udp@dns-test-service.dns-7020.svc wheezy_tcp@dns-test-service.dns-7020.svc wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7020 jessie_tcp@dns-test-service.dns-7020 jessie_udp@dns-test-service.dns-7020.svc jessie_tcp@dns-test-service.dns-7020.svc jessie_udp@_http._tcp.dns-test-service.dns-7020.svc jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc]

May 21 18:59:12.571: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.578: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.586: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.604: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.614: INFO: Unable to read wheezy_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.620: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.629: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.643: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.717: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.724: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.732: INFO: Unable to read jessie_udp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.739: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020 from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.747: INFO: Unable to read jessie_udp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.762: INFO: Unable to read jessie_tcp@dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.774: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.790: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc from pod dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8: the server could not find the requested resource (get pods dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8)
May 21 18:59:12.836: INFO: Lookups using dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7020 wheezy_tcp@dns-test-service.dns-7020 wheezy_udp@dns-test-service.dns-7020.svc wheezy_tcp@dns-test-service.dns-7020.svc wheezy_udp@_http._tcp.dns-test-service.dns-7020.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7020.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7020 jessie_tcp@dns-test-service.dns-7020 jessie_udp@dns-test-service.dns-7020.svc jessie_tcp@dns-test-service.dns-7020.svc jessie_udp@_http._tcp.dns-test-service.dns-7020.svc jessie_tcp@_http._tcp.dns-test-service.dns-7020.svc]

May 21 18:59:17.888: INFO: DNS probes using dns-7020/dns-test-fab1daa9-b817-4f6e-a97e-7bccee82ced8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 18:59:18.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7020" for this suite.

• [SLOW TEST:35.455 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":6,"skipped":84,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 18:59:18.215: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8173
May 21 18:59:22.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8173 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 21 18:59:22.927: INFO: rc: 7
May 21 18:59:22.986: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 18:59:23.000: INFO: Pod kube-proxy-mode-detector still exists
May 21 18:59:25.000: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 18:59:25.006: INFO: Pod kube-proxy-mode-detector still exists
May 21 18:59:27.000: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 18:59:27.008: INFO: Pod kube-proxy-mode-detector no longer exists
May 21 18:59:27.008: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8173 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-8173
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8173
I0521 18:59:27.079675      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8173, replica count: 3
I0521 18:59:30.130242      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 18:59:33.130710      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 18:59:33.149: INFO: Creating new exec pod
May 21 18:59:38.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8173 exec execpod-affinitylfckk -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
May 21 18:59:38.716: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 21 18:59:38.716: INFO: stdout: ""
May 21 18:59:38.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8173 exec execpod-affinitylfckk -- /bin/sh -x -c nc -zv -t -w 2 10.10.172.129 80'
May 21 18:59:39.253: INFO: stderr: "+ nc -zv -t -w 2 10.10.172.129 80\nConnection to 10.10.172.129 80 port [tcp/http] succeeded!\n"
May 21 18:59:39.253: INFO: stdout: ""
May 21 18:59:39.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8173 exec execpod-affinitylfckk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.172.129:80/ ; done'
May 21 18:59:40.016: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n"
May 21 18:59:40.016: INFO: stdout: "\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f\naffinity-clusterip-timeout-7f22f"
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.016: INFO: Received response from host: affinity-clusterip-timeout-7f22f
May 21 18:59:40.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8173 exec execpod-affinitylfckk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.172.129:80/'
May 21 18:59:40.633: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n"
May 21 18:59:40.633: INFO: stdout: "affinity-clusterip-timeout-7f22f"
May 21 18:59:55.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8173 exec execpod-affinitylfckk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.172.129:80/'
May 21 18:59:56.334: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n"
May 21 18:59:56.335: INFO: stdout: "affinity-clusterip-timeout-7f22f"
May 21 19:00:11.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8173 exec execpod-affinitylfckk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.172.129:80/'
May 21 19:00:11.932: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.172.129:80/\n"
May 21 19:00:11.932: INFO: stdout: "affinity-clusterip-timeout-6mqhl"
May 21 19:00:11.932: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8173, will wait for the garbage collector to delete the pods
May 21 19:00:12.041: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 12.287626ms
May 21 19:00:12.742: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 700.74591ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:00:18.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8173" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:59.843 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":7,"skipped":91,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:00:18.061: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-3177d5bf-58e8-4b83-a117-6031811f95c8
STEP: Creating a pod to test consume secrets
May 21 19:00:18.214: INFO: Waiting up to 5m0s for pod "pod-secrets-0a30cbed-04fd-4654-837a-273bbd3e4aff" in namespace "secrets-303" to be "Succeeded or Failed"
May 21 19:00:18.259: INFO: Pod "pod-secrets-0a30cbed-04fd-4654-837a-273bbd3e4aff": Phase="Pending", Reason="", readiness=false. Elapsed: 45.303022ms
May 21 19:00:20.265: INFO: Pod "pod-secrets-0a30cbed-04fd-4654-837a-273bbd3e4aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05186891s
May 21 19:00:22.274: INFO: Pod "pod-secrets-0a30cbed-04fd-4654-837a-273bbd3e4aff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060338578s
STEP: Saw pod success
May 21 19:00:22.274: INFO: Pod "pod-secrets-0a30cbed-04fd-4654-837a-273bbd3e4aff" satisfied condition "Succeeded or Failed"
May 21 19:00:22.283: INFO: Trying to get logs from node dc-hg-1 pod pod-secrets-0a30cbed-04fd-4654-837a-273bbd3e4aff container secret-env-test: <nil>
STEP: delete the pod
May 21 19:00:22.373: INFO: Waiting for pod pod-secrets-0a30cbed-04fd-4654-837a-273bbd3e4aff to disappear
May 21 19:00:22.379: INFO: Pod pod-secrets-0a30cbed-04fd-4654-837a-273bbd3e4aff no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:00:22.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-303" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":118,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:00:22.397: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-d2f04f79-6e59-4682-a6dc-b8d7ec70b61c
May 21 19:00:22.493: INFO: Pod name my-hostname-basic-d2f04f79-6e59-4682-a6dc-b8d7ec70b61c: Found 0 pods out of 1
May 21 19:00:27.502: INFO: Pod name my-hostname-basic-d2f04f79-6e59-4682-a6dc-b8d7ec70b61c: Found 1 pods out of 1
May 21 19:00:27.502: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d2f04f79-6e59-4682-a6dc-b8d7ec70b61c" are running
May 21 19:00:27.511: INFO: Pod "my-hostname-basic-d2f04f79-6e59-4682-a6dc-b8d7ec70b61c-4ppmq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 19:00:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 19:00:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 19:00:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 19:00:22 +0000 UTC Reason: Message:}])
May 21 19:00:27.512: INFO: Trying to dial the pod
May 21 19:00:32.536: INFO: Controller my-hostname-basic-d2f04f79-6e59-4682-a6dc-b8d7ec70b61c: Got expected result from replica 1 [my-hostname-basic-d2f04f79-6e59-4682-a6dc-b8d7ec70b61c-4ppmq]: "my-hostname-basic-d2f04f79-6e59-4682-a6dc-b8d7ec70b61c-4ppmq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:00:32.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6005" for this suite.

• [SLOW TEST:10.160 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":9,"skipped":124,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:00:32.558: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:00:32.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9235" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":10,"skipped":129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:00:32.714: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:00:32.921: INFO: Create a RollingUpdate DaemonSet
May 21 19:00:32.938: INFO: Check that daemon pods launch on every node of the cluster
May 21 19:00:33.005: INFO: Number of nodes with available pods: 0
May 21 19:00:33.005: INFO: Node conformance1 is running more than one daemon pod
May 21 19:00:34.046: INFO: Number of nodes with available pods: 0
May 21 19:00:34.046: INFO: Node conformance1 is running more than one daemon pod
May 21 19:00:35.029: INFO: Number of nodes with available pods: 0
May 21 19:00:35.029: INFO: Node conformance1 is running more than one daemon pod
May 21 19:00:36.026: INFO: Number of nodes with available pods: 1
May 21 19:00:36.026: INFO: Node conformance1 is running more than one daemon pod
May 21 19:00:37.023: INFO: Number of nodes with available pods: 1
May 21 19:00:37.024: INFO: Node conformance1 is running more than one daemon pod
May 21 19:00:38.026: INFO: Number of nodes with available pods: 1
May 21 19:00:38.026: INFO: Node conformance1 is running more than one daemon pod
May 21 19:00:39.020: INFO: Number of nodes with available pods: 1
May 21 19:00:39.020: INFO: Node conformance1 is running more than one daemon pod
May 21 19:00:40.022: INFO: Number of nodes with available pods: 2
May 21 19:00:40.022: INFO: Node conformance1 is running more than one daemon pod
May 21 19:00:41.020: INFO: Number of nodes with available pods: 3
May 21 19:00:41.020: INFO: Number of running nodes: 3, number of available pods: 3
May 21 19:00:41.020: INFO: Update the DaemonSet to trigger a rollout
May 21 19:00:41.055: INFO: Updating DaemonSet daemon-set
May 21 19:00:55.088: INFO: Roll back the DaemonSet before rollout is complete
May 21 19:00:55.105: INFO: Updating DaemonSet daemon-set
May 21 19:00:55.105: INFO: Make sure DaemonSet rollback is complete
May 21 19:00:55.121: INFO: Wrong image for pod: daemon-set-xrsp5. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 19:00:55.121: INFO: Pod daemon-set-xrsp5 is not available
May 21 19:00:56.147: INFO: Wrong image for pod: daemon-set-xrsp5. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 19:00:56.147: INFO: Pod daemon-set-xrsp5 is not available
May 21 19:00:57.148: INFO: Wrong image for pod: daemon-set-xrsp5. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 19:00:57.148: INFO: Pod daemon-set-xrsp5 is not available
May 21 19:00:58.145: INFO: Wrong image for pod: daemon-set-xrsp5. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 19:00:58.145: INFO: Pod daemon-set-xrsp5 is not available
May 21 19:00:59.145: INFO: Wrong image for pod: daemon-set-xrsp5. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 21 19:00:59.145: INFO: Pod daemon-set-xrsp5 is not available
May 21 19:01:00.146: INFO: Pod daemon-set-hm949 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8508, will wait for the garbage collector to delete the pods
May 21 19:01:00.239: INFO: Deleting DaemonSet.extensions daemon-set took: 18.413426ms
May 21 19:01:00.440: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.916358ms
May 21 19:01:08.771: INFO: Number of nodes with available pods: 0
May 21 19:01:08.771: INFO: Number of running nodes: 0, number of available pods: 0
May 21 19:01:08.788: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8508/daemonsets","resourceVersion":"1266715"},"items":null}

May 21 19:01:08.795: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8508/pods","resourceVersion":"1266715"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:08.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8508" for this suite.

• [SLOW TEST:36.130 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":11,"skipped":156,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:08.844: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:09.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2895" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":12,"skipped":157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:09.066: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:09.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9391" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":13,"skipped":228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:09.244: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 21 19:01:09.340: INFO: Waiting up to 5m0s for pod "downward-api-c1f4ae1d-3e5c-407f-a1cc-371c208296e4" in namespace "downward-api-1390" to be "Succeeded or Failed"
May 21 19:01:09.346: INFO: Pod "downward-api-c1f4ae1d-3e5c-407f-a1cc-371c208296e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.247122ms
May 21 19:01:11.354: INFO: Pod "downward-api-c1f4ae1d-3e5c-407f-a1cc-371c208296e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013788363s
May 21 19:01:13.361: INFO: Pod "downward-api-c1f4ae1d-3e5c-407f-a1cc-371c208296e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02103257s
STEP: Saw pod success
May 21 19:01:13.361: INFO: Pod "downward-api-c1f4ae1d-3e5c-407f-a1cc-371c208296e4" satisfied condition "Succeeded or Failed"
May 21 19:01:13.367: INFO: Trying to get logs from node dc-hg-1 pod downward-api-c1f4ae1d-3e5c-407f-a1cc-371c208296e4 container dapi-container: <nil>
STEP: delete the pod
May 21 19:01:13.424: INFO: Waiting for pod downward-api-c1f4ae1d-3e5c-407f-a1cc-371c208296e4 to disappear
May 21 19:01:13.433: INFO: Pod downward-api-c1f4ae1d-3e5c-407f-a1cc-371c208296e4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:13.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1390" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":14,"skipped":270,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:13.461: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-be48784d-6245-4c64-9555-5506801ce19d
STEP: Creating a pod to test consume secrets
May 21 19:01:13.605: INFO: Waiting up to 5m0s for pod "pod-secrets-786559f9-da61-47ab-a9c9-ac35f9a7b945" in namespace "secrets-7072" to be "Succeeded or Failed"
May 21 19:01:13.617: INFO: Pod "pod-secrets-786559f9-da61-47ab-a9c9-ac35f9a7b945": Phase="Pending", Reason="", readiness=false. Elapsed: 11.382396ms
May 21 19:01:15.623: INFO: Pod "pod-secrets-786559f9-da61-47ab-a9c9-ac35f9a7b945": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017491796s
May 21 19:01:17.629: INFO: Pod "pod-secrets-786559f9-da61-47ab-a9c9-ac35f9a7b945": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023734196s
STEP: Saw pod success
May 21 19:01:17.629: INFO: Pod "pod-secrets-786559f9-da61-47ab-a9c9-ac35f9a7b945" satisfied condition "Succeeded or Failed"
May 21 19:01:17.635: INFO: Trying to get logs from node dc-hg-1 pod pod-secrets-786559f9-da61-47ab-a9c9-ac35f9a7b945 container secret-volume-test: <nil>
STEP: delete the pod
May 21 19:01:17.729: INFO: Waiting for pod pod-secrets-786559f9-da61-47ab-a9c9-ac35f9a7b945 to disappear
May 21 19:01:17.745: INFO: Pod pod-secrets-786559f9-da61-47ab-a9c9-ac35f9a7b945 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:17.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7072" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":15,"skipped":275,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:17.769: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 21 19:01:17.921: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6457 /api/v1/namespaces/watch-6457/configmaps/e2e-watch-test-resource-version c12aa9a8-be3f-442f-a759-198cb5abea24 1266838 0 2021-05-21 19:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-21 19:01:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:01:17.922: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6457 /api/v1/namespaces/watch-6457/configmaps/e2e-watch-test-resource-version c12aa9a8-be3f-442f-a759-198cb5abea24 1266839 0 2021-05-21 19:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-21 19:01:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:17.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6457" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":16,"skipped":282,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:17.984: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:18.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9649" for this suite.
STEP: Destroying namespace "nspatchtest-b0f30483-8891-4b7a-a936-67806b234b0b-5595" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":17,"skipped":285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:18.257: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:25.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3332" for this suite.
STEP: Destroying namespace "nsdeletetest-8987" for this suite.
May 21 19:01:25.808: INFO: Namespace nsdeletetest-8987 was already deleted
STEP: Destroying namespace "nsdeletetest-2561" for this suite.

• [SLOW TEST:7.565 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":18,"skipped":365,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:25.823: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:37.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8459" for this suite.

• [SLOW TEST:11.353 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":19,"skipped":369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:37.177: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
May 21 19:01:37.275: INFO: Waiting up to 5m0s for pod "pod-7504d949-bf3b-42ed-87d9-cf3ac0f76eee" in namespace "emptydir-4725" to be "Succeeded or Failed"
May 21 19:01:37.289: INFO: Pod "pod-7504d949-bf3b-42ed-87d9-cf3ac0f76eee": Phase="Pending", Reason="", readiness=false. Elapsed: 13.720138ms
May 21 19:01:39.299: INFO: Pod "pod-7504d949-bf3b-42ed-87d9-cf3ac0f76eee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024216025s
May 21 19:01:41.311: INFO: Pod "pod-7504d949-bf3b-42ed-87d9-cf3ac0f76eee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036091795s
STEP: Saw pod success
May 21 19:01:41.311: INFO: Pod "pod-7504d949-bf3b-42ed-87d9-cf3ac0f76eee" satisfied condition "Succeeded or Failed"
May 21 19:01:41.319: INFO: Trying to get logs from node dc-hg-1 pod pod-7504d949-bf3b-42ed-87d9-cf3ac0f76eee container test-container: <nil>
STEP: delete the pod
May 21 19:01:41.405: INFO: Waiting for pod pod-7504d949-bf3b-42ed-87d9-cf3ac0f76eee to disappear
May 21 19:01:41.415: INFO: Pod pod-7504d949-bf3b-42ed-87d9-cf3ac0f76eee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:41.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4725" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":20,"skipped":397,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:41.455: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0521 19:01:43.289531      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
May 21 19:01:43.290: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:43.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0521 19:01:43.289735      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 19:01:43.289852      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9408" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":21,"skipped":400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:43.327: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:01:44.243: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 19:01:46.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220504, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220504, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220504, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220504, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:01:49.334: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:49.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9185" for this suite.
STEP: Destroying namespace "webhook-9185-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.545 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":22,"skipped":425,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:49.873: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
May 21 19:01:58.096: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-846 PodName:pod-sharedvolume-d62aede6-8378-4822-8f8d-3a0e32c28cf2 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:01:58.096: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:01:58.298: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:01:58.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-846" for this suite.

• [SLOW TEST:8.448 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":23,"skipped":429,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:01:58.322: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
May 21 19:01:58.418: INFO: Waiting up to 5m0s for pod "pod-5e3b7808-56db-4c5f-a074-4c605fb06e27" in namespace "emptydir-4099" to be "Succeeded or Failed"
May 21 19:01:58.435: INFO: Pod "pod-5e3b7808-56db-4c5f-a074-4c605fb06e27": Phase="Pending", Reason="", readiness=false. Elapsed: 16.576092ms
May 21 19:02:00.448: INFO: Pod "pod-5e3b7808-56db-4c5f-a074-4c605fb06e27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029588737s
May 21 19:02:02.461: INFO: Pod "pod-5e3b7808-56db-4c5f-a074-4c605fb06e27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043213216s
STEP: Saw pod success
May 21 19:02:02.462: INFO: Pod "pod-5e3b7808-56db-4c5f-a074-4c605fb06e27" satisfied condition "Succeeded or Failed"
May 21 19:02:02.470: INFO: Trying to get logs from node dc-hg-2 pod pod-5e3b7808-56db-4c5f-a074-4c605fb06e27 container test-container: <nil>
STEP: delete the pod
May 21 19:02:02.580: INFO: Waiting for pod pod-5e3b7808-56db-4c5f-a074-4c605fb06e27 to disappear
May 21 19:02:02.585: INFO: Pod pod-5e3b7808-56db-4c5f-a074-4c605fb06e27 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:02:02.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4099" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":24,"skipped":434,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:02:02.605: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:02:07.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9767" for this suite.

• [SLOW TEST:5.292 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":25,"skipped":449,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:02:07.897: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-ea301bcd-de32-4855-901a-9aaf1b6374b5
STEP: Creating a pod to test consume secrets
May 21 19:02:08.094: INFO: Waiting up to 5m0s for pod "pod-secrets-64e5ef77-0b23-4710-a340-842e04a70000" in namespace "secrets-6042" to be "Succeeded or Failed"
May 21 19:02:08.111: INFO: Pod "pod-secrets-64e5ef77-0b23-4710-a340-842e04a70000": Phase="Pending", Reason="", readiness=false. Elapsed: 16.91752ms
May 21 19:02:10.118: INFO: Pod "pod-secrets-64e5ef77-0b23-4710-a340-842e04a70000": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024687492s
May 21 19:02:12.126: INFO: Pod "pod-secrets-64e5ef77-0b23-4710-a340-842e04a70000": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032696047s
May 21 19:02:14.143: INFO: Pod "pod-secrets-64e5ef77-0b23-4710-a340-842e04a70000": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049862619s
STEP: Saw pod success
May 21 19:02:14.144: INFO: Pod "pod-secrets-64e5ef77-0b23-4710-a340-842e04a70000" satisfied condition "Succeeded or Failed"
May 21 19:02:14.198: INFO: Trying to get logs from node dc-hg-1 pod pod-secrets-64e5ef77-0b23-4710-a340-842e04a70000 container secret-volume-test: <nil>
STEP: delete the pod
May 21 19:02:14.254: INFO: Waiting for pod pod-secrets-64e5ef77-0b23-4710-a340-842e04a70000 to disappear
May 21 19:02:14.268: INFO: Pod pod-secrets-64e5ef77-0b23-4710-a340-842e04a70000 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:02:14.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6042" for this suite.

• [SLOW TEST:6.411 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":26,"skipped":453,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:02:14.308: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
May 21 19:02:25.242: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 21 19:02:25.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vq75" in namespace "gc-1775"
W0521 19:02:25.242278      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 19:02:25.242336      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 19:02:25.242354      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 19:02:25.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-bt2nx" in namespace "gc-1775"
May 21 19:02:25.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5q4d" in namespace "gc-1775"
May 21 19:02:25.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsjrp" in namespace "gc-1775"
May 21 19:02:25.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-knd6q" in namespace "gc-1775"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:02:25.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1775" for this suite.

• [SLOW TEST:11.538 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":27,"skipped":456,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:02:25.846: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:02:26.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-110" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":28,"skipped":460,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:02:26.280: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 21 19:02:26.690: INFO: Number of nodes with available pods: 0
May 21 19:02:26.690: INFO: Node conformance1 is running more than one daemon pod
May 21 19:02:27.778: INFO: Number of nodes with available pods: 0
May 21 19:02:27.778: INFO: Node conformance1 is running more than one daemon pod
May 21 19:02:28.719: INFO: Number of nodes with available pods: 0
May 21 19:02:28.719: INFO: Node conformance1 is running more than one daemon pod
May 21 19:02:29.711: INFO: Number of nodes with available pods: 0
May 21 19:02:29.711: INFO: Node conformance1 is running more than one daemon pod
May 21 19:02:30.734: INFO: Number of nodes with available pods: 2
May 21 19:02:30.734: INFO: Node conformance1 is running more than one daemon pod
May 21 19:02:31.769: INFO: Number of nodes with available pods: 3
May 21 19:02:31.769: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 21 19:02:32.236: INFO: Number of nodes with available pods: 2
May 21 19:02:32.236: INFO: Node conformance1 is running more than one daemon pod
May 21 19:02:33.294: INFO: Number of nodes with available pods: 2
May 21 19:02:33.294: INFO: Node conformance1 is running more than one daemon pod
May 21 19:02:34.259: INFO: Number of nodes with available pods: 2
May 21 19:02:34.259: INFO: Node conformance1 is running more than one daemon pod
May 21 19:02:35.255: INFO: Number of nodes with available pods: 2
May 21 19:02:35.255: INFO: Node conformance1 is running more than one daemon pod
May 21 19:02:36.258: INFO: Number of nodes with available pods: 2
May 21 19:02:36.258: INFO: Node conformance1 is running more than one daemon pod
May 21 19:02:37.250: INFO: Number of nodes with available pods: 3
May 21 19:02:37.250: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1571, will wait for the garbage collector to delete the pods
May 21 19:02:37.334: INFO: Deleting DaemonSet.extensions daemon-set took: 15.859677ms
May 21 19:02:37.957: INFO: Terminating DaemonSet.extensions daemon-set pods took: 604.871652ms
May 21 19:02:41.885: INFO: Number of nodes with available pods: 0
May 21 19:02:41.885: INFO: Number of running nodes: 0, number of available pods: 0
May 21 19:02:41.916: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1571/daemonsets","resourceVersion":"1267704"},"items":null}

May 21 19:02:41.931: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1571/pods","resourceVersion":"1267704"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:02:41.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1571" for this suite.

• [SLOW TEST:15.728 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":29,"skipped":465,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:02:42.009: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:02:44.298: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 19:02:46.318: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:02:48.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:02:50.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220564, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:02:53.351: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:02:53.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9922" for this suite.
STEP: Destroying namespace "webhook-9922-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.030 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":30,"skipped":467,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:02:54.039: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:02:54.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-068ebd0f-a64b-42be-ae59-d2f26ba626c7" in namespace "downward-api-4500" to be "Succeeded or Failed"
May 21 19:02:54.210: INFO: Pod "downwardapi-volume-068ebd0f-a64b-42be-ae59-d2f26ba626c7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.39024ms
May 21 19:02:56.219: INFO: Pod "downwardapi-volume-068ebd0f-a64b-42be-ae59-d2f26ba626c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014073628s
May 21 19:02:58.229: INFO: Pod "downwardapi-volume-068ebd0f-a64b-42be-ae59-d2f26ba626c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023562123s
STEP: Saw pod success
May 21 19:02:58.229: INFO: Pod "downwardapi-volume-068ebd0f-a64b-42be-ae59-d2f26ba626c7" satisfied condition "Succeeded or Failed"
May 21 19:02:58.241: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-068ebd0f-a64b-42be-ae59-d2f26ba626c7 container client-container: <nil>
STEP: delete the pod
May 21 19:02:58.297: INFO: Waiting for pod downwardapi-volume-068ebd0f-a64b-42be-ae59-d2f26ba626c7 to disappear
May 21 19:02:58.303: INFO: Pod downwardapi-volume-068ebd0f-a64b-42be-ae59-d2f26ba626c7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:02:58.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4500" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":31,"skipped":468,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:02:58.327: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 21 19:02:58.500: INFO: Waiting up to 5m0s for pod "pod-ae6c06b9-0640-4cc0-87ab-8f3831da7df7" in namespace "emptydir-4127" to be "Succeeded or Failed"
May 21 19:02:58.508: INFO: Pod "pod-ae6c06b9-0640-4cc0-87ab-8f3831da7df7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.738002ms
May 21 19:03:00.514: INFO: Pod "pod-ae6c06b9-0640-4cc0-87ab-8f3831da7df7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013737733s
May 21 19:03:02.521: INFO: Pod "pod-ae6c06b9-0640-4cc0-87ab-8f3831da7df7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020132948s
STEP: Saw pod success
May 21 19:03:02.521: INFO: Pod "pod-ae6c06b9-0640-4cc0-87ab-8f3831da7df7" satisfied condition "Succeeded or Failed"
May 21 19:03:02.526: INFO: Trying to get logs from node dc-hg-1 pod pod-ae6c06b9-0640-4cc0-87ab-8f3831da7df7 container test-container: <nil>
STEP: delete the pod
May 21 19:03:02.583: INFO: Waiting for pod pod-ae6c06b9-0640-4cc0-87ab-8f3831da7df7 to disappear
May 21 19:03:02.593: INFO: Pod pod-ae6c06b9-0640-4cc0-87ab-8f3831da7df7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:03:02.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4127" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":32,"skipped":483,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:03:02.617: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 21 19:03:02.784: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8540 /api/v1/namespaces/watch-8540/configmaps/e2e-watch-test-label-changed 8ee37499-161e-49e5-b739-0eb9c97ffdfb 1267881 0 2021-05-21 19:03:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 19:03:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:03:02.784: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8540 /api/v1/namespaces/watch-8540/configmaps/e2e-watch-test-label-changed 8ee37499-161e-49e5-b739-0eb9c97ffdfb 1267882 0 2021-05-21 19:03:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 19:03:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:03:02.785: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8540 /api/v1/namespaces/watch-8540/configmaps/e2e-watch-test-label-changed 8ee37499-161e-49e5-b739-0eb9c97ffdfb 1267883 0 2021-05-21 19:03:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 19:03:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 21 19:03:12.868: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8540 /api/v1/namespaces/watch-8540/configmaps/e2e-watch-test-label-changed 8ee37499-161e-49e5-b739-0eb9c97ffdfb 1267908 0 2021-05-21 19:03:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 19:03:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:03:12.869: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8540 /api/v1/namespaces/watch-8540/configmaps/e2e-watch-test-label-changed 8ee37499-161e-49e5-b739-0eb9c97ffdfb 1267909 0 2021-05-21 19:03:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 19:03:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:03:12.870: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8540 /api/v1/namespaces/watch-8540/configmaps/e2e-watch-test-label-changed 8ee37499-161e-49e5-b739-0eb9c97ffdfb 1267910 0 2021-05-21 19:03:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-21 19:03:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:03:12.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8540" for this suite.

• [SLOW TEST:10.280 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":33,"skipped":500,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:03:12.897: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:03:13.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61738c44-148a-4b17-94a0-8470b89f23b9" in namespace "downward-api-5273" to be "Succeeded or Failed"
May 21 19:03:13.062: INFO: Pod "downwardapi-volume-61738c44-148a-4b17-94a0-8470b89f23b9": Phase="Pending", Reason="", readiness=false. Elapsed: 27.2769ms
May 21 19:03:15.068: INFO: Pod "downwardapi-volume-61738c44-148a-4b17-94a0-8470b89f23b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033624848s
May 21 19:03:17.088: INFO: Pod "downwardapi-volume-61738c44-148a-4b17-94a0-8470b89f23b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053686779s
STEP: Saw pod success
May 21 19:03:17.088: INFO: Pod "downwardapi-volume-61738c44-148a-4b17-94a0-8470b89f23b9" satisfied condition "Succeeded or Failed"
May 21 19:03:17.098: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-61738c44-148a-4b17-94a0-8470b89f23b9 container client-container: <nil>
STEP: delete the pod
May 21 19:03:17.163: INFO: Waiting for pod downwardapi-volume-61738c44-148a-4b17-94a0-8470b89f23b9 to disappear
May 21 19:03:17.169: INFO: Pod downwardapi-volume-61738c44-148a-4b17-94a0-8470b89f23b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:03:17.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5273" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":34,"skipped":519,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:03:17.248: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
May 21 19:03:17.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 create -f -'
May 21 19:03:18.407: INFO: stderr: ""
May 21 19:03:18.407: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 19:03:18.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 19:03:18.760: INFO: stderr: ""
May 21 19:03:18.760: INFO: stdout: "update-demo-nautilus-gn7kq update-demo-nautilus-wbr7f "
May 21 19:03:18.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods update-demo-nautilus-gn7kq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:03:19.053: INFO: stderr: ""
May 21 19:03:19.053: INFO: stdout: ""
May 21 19:03:19.053: INFO: update-demo-nautilus-gn7kq is created but not running
May 21 19:03:24.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 19:03:24.343: INFO: stderr: ""
May 21 19:03:24.343: INFO: stdout: "update-demo-nautilus-gn7kq update-demo-nautilus-wbr7f "
May 21 19:03:24.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods update-demo-nautilus-gn7kq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:03:24.625: INFO: stderr: ""
May 21 19:03:24.625: INFO: stdout: "true"
May 21 19:03:24.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods update-demo-nautilus-gn7kq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 19:03:24.967: INFO: stderr: ""
May 21 19:03:24.967: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 19:03:24.967: INFO: validating pod update-demo-nautilus-gn7kq
May 21 19:03:24.989: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 19:03:24.989: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 19:03:24.990: INFO: update-demo-nautilus-gn7kq is verified up and running
May 21 19:03:24.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods update-demo-nautilus-wbr7f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:03:25.258: INFO: stderr: ""
May 21 19:03:25.258: INFO: stdout: ""
May 21 19:03:25.258: INFO: update-demo-nautilus-wbr7f is created but not running
May 21 19:03:30.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 19:03:30.547: INFO: stderr: ""
May 21 19:03:30.547: INFO: stdout: "update-demo-nautilus-gn7kq update-demo-nautilus-wbr7f "
May 21 19:03:30.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods update-demo-nautilus-gn7kq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:03:30.861: INFO: stderr: ""
May 21 19:03:30.861: INFO: stdout: "true"
May 21 19:03:30.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods update-demo-nautilus-gn7kq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 19:03:31.106: INFO: stderr: ""
May 21 19:03:31.106: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 19:03:31.106: INFO: validating pod update-demo-nautilus-gn7kq
May 21 19:03:31.115: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 19:03:31.115: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 19:03:31.115: INFO: update-demo-nautilus-gn7kq is verified up and running
May 21 19:03:31.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods update-demo-nautilus-wbr7f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:03:31.387: INFO: stderr: ""
May 21 19:03:31.387: INFO: stdout: "true"
May 21 19:03:31.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods update-demo-nautilus-wbr7f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 19:03:31.656: INFO: stderr: ""
May 21 19:03:31.656: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 19:03:31.656: INFO: validating pod update-demo-nautilus-wbr7f
May 21 19:03:31.674: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 19:03:31.674: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 19:03:31.674: INFO: update-demo-nautilus-wbr7f is verified up and running
STEP: using delete to clean up resources
May 21 19:03:31.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 delete --grace-period=0 --force -f -'
May 21 19:03:32.001: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 19:03:32.001: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 21 19:03:32.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get rc,svc -l name=update-demo --no-headers'
May 21 19:03:32.328: INFO: stderr: "No resources found in kubectl-9200 namespace.\n"
May 21 19:03:32.328: INFO: stdout: ""
May 21 19:03:32.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9200 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 19:03:32.710: INFO: stderr: ""
May 21 19:03:32.710: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:03:32.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9200" for this suite.

• [SLOW TEST:15.483 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":35,"skipped":539,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:03:32.733: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:03:32.875: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 19:03:41.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-5593 --namespace=crd-publish-openapi-5593 create -f -'
May 21 19:03:42.719: INFO: stderr: ""
May 21 19:03:42.720: INFO: stdout: "e2e-test-crd-publish-openapi-3143-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 21 19:03:42.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-5593 --namespace=crd-publish-openapi-5593 delete e2e-test-crd-publish-openapi-3143-crds test-cr'
May 21 19:03:43.001: INFO: stderr: ""
May 21 19:03:43.001: INFO: stdout: "e2e-test-crd-publish-openapi-3143-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 21 19:03:43.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-5593 --namespace=crd-publish-openapi-5593 apply -f -'
May 21 19:03:43.702: INFO: stderr: ""
May 21 19:03:43.702: INFO: stdout: "e2e-test-crd-publish-openapi-3143-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 21 19:03:43.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-5593 --namespace=crd-publish-openapi-5593 delete e2e-test-crd-publish-openapi-3143-crds test-cr'
May 21 19:03:43.972: INFO: stderr: ""
May 21 19:03:43.972: INFO: stdout: "e2e-test-crd-publish-openapi-3143-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 21 19:03:43.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-5593 explain e2e-test-crd-publish-openapi-3143-crds'
May 21 19:03:44.652: INFO: stderr: ""
May 21 19:03:44.652: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3143-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:03:53.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5593" for this suite.

• [SLOW TEST:20.474 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":36,"skipped":547,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:03:53.207: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-b49ff2a1-20ca-46cf-b985-963b71a77f59
STEP: Creating a pod to test consume secrets
May 21 19:03:53.349: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f70af3be-5d6f-4f4e-9c64-82d013b25b6a" in namespace "projected-1451" to be "Succeeded or Failed"
May 21 19:03:53.359: INFO: Pod "pod-projected-secrets-f70af3be-5d6f-4f4e-9c64-82d013b25b6a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.344738ms
May 21 19:03:55.368: INFO: Pod "pod-projected-secrets-f70af3be-5d6f-4f4e-9c64-82d013b25b6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018698325s
May 21 19:03:57.375: INFO: Pod "pod-projected-secrets-f70af3be-5d6f-4f4e-9c64-82d013b25b6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026278649s
STEP: Saw pod success
May 21 19:03:57.375: INFO: Pod "pod-projected-secrets-f70af3be-5d6f-4f4e-9c64-82d013b25b6a" satisfied condition "Succeeded or Failed"
May 21 19:03:57.385: INFO: Trying to get logs from node dc-hg-1 pod pod-projected-secrets-f70af3be-5d6f-4f4e-9c64-82d013b25b6a container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 19:03:57.448: INFO: Waiting for pod pod-projected-secrets-f70af3be-5d6f-4f4e-9c64-82d013b25b6a to disappear
May 21 19:03:57.470: INFO: Pod pod-projected-secrets-f70af3be-5d6f-4f4e-9c64-82d013b25b6a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:03:57.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1451" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:03:57.495: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:03:57.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d97b97e5-3e07-4c60-b30d-12a6bca432bd" in namespace "projected-2766" to be "Succeeded or Failed"
May 21 19:03:57.728: INFO: Pod "downwardapi-volume-d97b97e5-3e07-4c60-b30d-12a6bca432bd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.1352ms
May 21 19:03:59.734: INFO: Pod "downwardapi-volume-d97b97e5-3e07-4c60-b30d-12a6bca432bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025152188s
May 21 19:04:01.740: INFO: Pod "downwardapi-volume-d97b97e5-3e07-4c60-b30d-12a6bca432bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031158613s
STEP: Saw pod success
May 21 19:04:01.740: INFO: Pod "downwardapi-volume-d97b97e5-3e07-4c60-b30d-12a6bca432bd" satisfied condition "Succeeded or Failed"
May 21 19:04:01.746: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-d97b97e5-3e07-4c60-b30d-12a6bca432bd container client-container: <nil>
STEP: delete the pod
May 21 19:04:01.785: INFO: Waiting for pod downwardapi-volume-d97b97e5-3e07-4c60-b30d-12a6bca432bd to disappear
May 21 19:04:01.794: INFO: Pod downwardapi-volume-d97b97e5-3e07-4c60-b30d-12a6bca432bd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:04:01.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2766" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":38,"skipped":574,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:04:01.821: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5158
STEP: creating service affinity-nodeport in namespace services-5158
STEP: creating replication controller affinity-nodeport in namespace services-5158
I0521 19:04:01.947253      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-5158, replica count: 3
I0521 19:04:04.998021      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 19:04:07.998421      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 19:04:10.998913      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 19:04:11.024: INFO: Creating new exec pod
May 21 19:04:16.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-5158 exec execpod-affinitycbxrz -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
May 21 19:04:16.772: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 21 19:04:16.772: INFO: stdout: ""
May 21 19:04:16.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-5158 exec execpod-affinitycbxrz -- /bin/sh -x -c nc -zv -t -w 2 10.10.240.88 80'
May 21 19:04:17.376: INFO: stderr: "+ nc -zv -t -w 2 10.10.240.88 80\nConnection to 10.10.240.88 80 port [tcp/http] succeeded!\n"
May 21 19:04:17.377: INFO: stdout: ""
May 21 19:04:17.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-5158 exec execpod-affinitycbxrz -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.136 32686'
May 21 19:04:18.009: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.136 32686\nConnection to 10.10.1.136 32686 port [tcp/32686] succeeded!\n"
May 21 19:04:18.010: INFO: stdout: ""
May 21 19:04:18.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-5158 exec execpod-affinitycbxrz -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.125 32686'
May 21 19:04:18.515: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.125 32686\nConnection to 10.10.1.125 32686 port [tcp/32686] succeeded!\n"
May 21 19:04:18.515: INFO: stdout: ""
May 21 19:04:18.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-5158 exec execpod-affinitycbxrz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.86:32686/ ; done'
May 21 19:04:19.295: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:32686/\n"
May 21 19:04:19.295: INFO: stdout: "\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9\naffinity-nodeport-d5kb9"
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Received response from host: affinity-nodeport-d5kb9
May 21 19:04:19.296: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5158, will wait for the garbage collector to delete the pods
May 21 19:04:19.426: INFO: Deleting ReplicationController affinity-nodeport took: 14.95429ms
May 21 19:04:20.031: INFO: Terminating ReplicationController affinity-nodeport pods took: 604.856969ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:04:27.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5158" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:26.116 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":39,"skipped":581,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:04:27.938: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-6290
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6290 to expose endpoints map[]
May 21 19:04:28.135: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 21 19:04:29.154: INFO: successfully validated that service multi-endpoint-test in namespace services-6290 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6290
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6290 to expose endpoints map[pod1:[100]]
May 21 19:04:32.245: INFO: successfully validated that service multi-endpoint-test in namespace services-6290 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6290
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6290 to expose endpoints map[pod1:[100] pod2:[101]]
May 21 19:04:36.319: INFO: Unexpected endpoints: found map[bcd8abdb-2ebc-4361-b6a4-9df621175c3e:[100]], expected map[pod1:[100] pod2:[101]], will retry
May 21 19:04:38.324: INFO: successfully validated that service multi-endpoint-test in namespace services-6290 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-6290
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6290 to expose endpoints map[pod2:[101]]
May 21 19:04:38.437: INFO: successfully validated that service multi-endpoint-test in namespace services-6290 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6290
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6290 to expose endpoints map[]
May 21 19:04:38.741: INFO: successfully validated that service multi-endpoint-test in namespace services-6290 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:04:38.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6290" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:11.018 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":40,"skipped":602,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:04:38.956: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 21 19:04:39.355: INFO: Number of nodes with available pods: 0
May 21 19:04:39.355: INFO: Node conformance1 is running more than one daemon pod
May 21 19:04:40.389: INFO: Number of nodes with available pods: 0
May 21 19:04:40.389: INFO: Node conformance1 is running more than one daemon pod
May 21 19:04:41.369: INFO: Number of nodes with available pods: 0
May 21 19:04:41.369: INFO: Node conformance1 is running more than one daemon pod
May 21 19:04:42.398: INFO: Number of nodes with available pods: 0
May 21 19:04:42.398: INFO: Node conformance1 is running more than one daemon pod
May 21 19:04:43.390: INFO: Number of nodes with available pods: 0
May 21 19:04:43.390: INFO: Node conformance1 is running more than one daemon pod
May 21 19:04:44.438: INFO: Number of nodes with available pods: 1
May 21 19:04:44.438: INFO: Node conformance1 is running more than one daemon pod
May 21 19:04:45.367: INFO: Number of nodes with available pods: 2
May 21 19:04:45.367: INFO: Node conformance1 is running more than one daemon pod
May 21 19:04:46.372: INFO: Number of nodes with available pods: 3
May 21 19:04:46.372: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 21 19:04:46.422: INFO: Number of nodes with available pods: 2
May 21 19:04:46.422: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:47.440: INFO: Number of nodes with available pods: 2
May 21 19:04:47.440: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:48.438: INFO: Number of nodes with available pods: 2
May 21 19:04:48.438: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:49.437: INFO: Number of nodes with available pods: 2
May 21 19:04:49.437: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:50.442: INFO: Number of nodes with available pods: 2
May 21 19:04:50.442: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:51.440: INFO: Number of nodes with available pods: 2
May 21 19:04:51.440: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:52.438: INFO: Number of nodes with available pods: 2
May 21 19:04:52.438: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:53.435: INFO: Number of nodes with available pods: 2
May 21 19:04:53.435: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:54.438: INFO: Number of nodes with available pods: 2
May 21 19:04:54.438: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:55.438: INFO: Number of nodes with available pods: 2
May 21 19:04:55.438: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:56.439: INFO: Number of nodes with available pods: 2
May 21 19:04:56.439: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:57.439: INFO: Number of nodes with available pods: 2
May 21 19:04:57.439: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:58.447: INFO: Number of nodes with available pods: 2
May 21 19:04:58.447: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 19:04:59.461: INFO: Number of nodes with available pods: 3
May 21 19:04:59.461: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8480, will wait for the garbage collector to delete the pods
May 21 19:04:59.582: INFO: Deleting DaemonSet.extensions daemon-set took: 43.659134ms
May 21 19:04:59.782: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.363965ms
May 21 19:05:14.788: INFO: Number of nodes with available pods: 0
May 21 19:05:14.788: INFO: Number of running nodes: 0, number of available pods: 0
May 21 19:05:14.794: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8480/daemonsets","resourceVersion":"1268502"},"items":null}

May 21 19:05:14.799: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8480/pods","resourceVersion":"1268502"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:05:14.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8480" for this suite.

• [SLOW TEST:35.892 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":41,"skipped":609,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:05:14.849: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-ctl44 in namespace proxy-5219
I0521 19:05:15.014309      20 runners.go:190] Created replication controller with name: proxy-service-ctl44, namespace: proxy-5219, replica count: 1
I0521 19:05:16.064837      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 19:05:17.066017      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 19:05:18.066353      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 19:05:19.066687      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 19:05:20.067041      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 19:05:21.067455      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 19:05:22.067836      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 19:05:23.068273      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 19:05:24.068909      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 19:05:25.070200      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 19:05:26.070795      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0521 19:05:27.071104      20 runners.go:190] proxy-service-ctl44 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 19:05:27.076: INFO: setup took 12.102188497s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 21 19:05:27.110: INFO: (0) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 33.502733ms)
May 21 19:05:27.110: INFO: (0) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 33.245015ms)
May 21 19:05:27.110: INFO: (0) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 32.169991ms)
May 21 19:05:27.110: INFO: (0) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 32.051714ms)
May 21 19:05:27.119: INFO: (0) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 40.242145ms)
May 21 19:05:27.120: INFO: (0) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 42.059779ms)
May 21 19:05:27.121: INFO: (0) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 43.332664ms)
May 21 19:05:27.121: INFO: (0) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 43.22666ms)
May 21 19:05:27.121: INFO: (0) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 44.612222ms)
May 21 19:05:27.129: INFO: (0) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 51.911405ms)
May 21 19:05:27.130: INFO: (0) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 53.215281ms)
May 21 19:05:27.134: INFO: (0) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 56.912682ms)
May 21 19:05:27.136: INFO: (0) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 58.82327ms)
May 21 19:05:27.136: INFO: (0) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 58.523979ms)
May 21 19:05:27.139: INFO: (0) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 61.004643ms)
May 21 19:05:27.140: INFO: (0) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 62.070221ms)
May 21 19:05:27.160: INFO: (1) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 18.881145ms)
May 21 19:05:27.162: INFO: (1) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 21.008061ms)
May 21 19:05:27.162: INFO: (1) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 21.616649ms)
May 21 19:05:27.181: INFO: (1) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 40.17935ms)
May 21 19:05:27.182: INFO: (1) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 39.730712ms)
May 21 19:05:27.185: INFO: (1) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 43.294322ms)
May 21 19:05:27.185: INFO: (1) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 41.726373ms)
May 21 19:05:27.185: INFO: (1) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 43.211624ms)
May 21 19:05:27.198: INFO: (1) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 55.413344ms)
May 21 19:05:27.199: INFO: (1) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 56.436541ms)
May 21 19:05:27.199: INFO: (1) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 56.320476ms)
May 21 19:05:27.198: INFO: (1) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 56.133665ms)
May 21 19:05:27.199: INFO: (1) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 56.16925ms)
May 21 19:05:27.200: INFO: (1) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 58.222198ms)
May 21 19:05:27.200: INFO: (1) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 56.988705ms)
May 21 19:05:27.201: INFO: (1) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 58.342517ms)
May 21 19:05:27.210: INFO: (2) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 8.727374ms)
May 21 19:05:27.236: INFO: (2) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 35.388739ms)
May 21 19:05:27.243: INFO: (2) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 40.829375ms)
May 21 19:05:27.243: INFO: (2) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 41.408595ms)
May 21 19:05:27.243: INFO: (2) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 41.161936ms)
May 21 19:05:27.258: INFO: (2) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 54.357112ms)
May 21 19:05:27.258: INFO: (2) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 54.340779ms)
May 21 19:05:27.258: INFO: (2) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 54.89681ms)
May 21 19:05:27.259: INFO: (2) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 55.977782ms)
May 21 19:05:27.259: INFO: (2) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 55.617618ms)
May 21 19:05:27.259: INFO: (2) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 55.774851ms)
May 21 19:05:27.259: INFO: (2) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 56.457622ms)
May 21 19:05:27.259: INFO: (2) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 56.641198ms)
May 21 19:05:27.264: INFO: (2) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 59.695702ms)
May 21 19:05:27.265: INFO: (2) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 62.216181ms)
May 21 19:05:27.265: INFO: (2) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 61.030859ms)
May 21 19:05:27.293: INFO: (3) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 26.496415ms)
May 21 19:05:27.293: INFO: (3) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 27.073028ms)
May 21 19:05:27.293: INFO: (3) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 26.859529ms)
May 21 19:05:27.294: INFO: (3) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 27.552013ms)
May 21 19:05:27.302: INFO: (3) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 36.798432ms)
May 21 19:05:27.304: INFO: (3) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 37.958921ms)
May 21 19:05:27.309: INFO: (3) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 42.526929ms)
May 21 19:05:27.309: INFO: (3) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 44.239341ms)
May 21 19:05:27.312: INFO: (3) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 45.517623ms)
May 21 19:05:27.312: INFO: (3) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 45.745753ms)
May 21 19:05:27.312: INFO: (3) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 45.733186ms)
May 21 19:05:27.312: INFO: (3) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 45.9769ms)
May 21 19:05:27.312: INFO: (3) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 45.83759ms)
May 21 19:05:27.322: INFO: (3) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 56.121312ms)
May 21 19:05:27.323: INFO: (3) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 56.583239ms)
May 21 19:05:27.322: INFO: (3) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 56.60289ms)
May 21 19:05:27.339: INFO: (4) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 15.858728ms)
May 21 19:05:27.345: INFO: (4) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 21.881091ms)
May 21 19:05:27.346: INFO: (4) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 22.626471ms)
May 21 19:05:27.347: INFO: (4) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 23.905314ms)
May 21 19:05:27.351: INFO: (4) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 28.19163ms)
May 21 19:05:27.351: INFO: (4) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 27.282807ms)
May 21 19:05:27.351: INFO: (4) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 27.212866ms)
May 21 19:05:27.367: INFO: (4) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 43.963754ms)
May 21 19:05:27.367: INFO: (4) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 43.379082ms)
May 21 19:05:27.368: INFO: (4) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 44.529864ms)
May 21 19:05:27.368: INFO: (4) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 44.918927ms)
May 21 19:05:27.369: INFO: (4) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 43.73398ms)
May 21 19:05:27.373: INFO: (4) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 49.547956ms)
May 21 19:05:27.373: INFO: (4) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 49.432278ms)
May 21 19:05:27.373: INFO: (4) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 49.562129ms)
May 21 19:05:27.373: INFO: (4) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 50.004961ms)
May 21 19:05:27.384: INFO: (5) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 9.768473ms)
May 21 19:05:27.385: INFO: (5) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 11.34949ms)
May 21 19:05:27.388: INFO: (5) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 14.160506ms)
May 21 19:05:27.390: INFO: (5) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 16.273754ms)
May 21 19:05:27.413: INFO: (5) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 37.733056ms)
May 21 19:05:27.413: INFO: (5) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 38.736268ms)
May 21 19:05:27.414: INFO: (5) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 39.286307ms)
May 21 19:05:27.416: INFO: (5) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 40.742628ms)
May 21 19:05:27.419: INFO: (5) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 44.849736ms)
May 21 19:05:27.422: INFO: (5) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 47.689075ms)
May 21 19:05:27.423: INFO: (5) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 47.731187ms)
May 21 19:05:27.424: INFO: (5) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 49.240248ms)
May 21 19:05:27.426: INFO: (5) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 51.415586ms)
May 21 19:05:27.427: INFO: (5) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 51.792424ms)
May 21 19:05:27.427: INFO: (5) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 51.953813ms)
May 21 19:05:27.427: INFO: (5) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 52.312792ms)
May 21 19:05:27.443: INFO: (6) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 15.750692ms)
May 21 19:05:27.443: INFO: (6) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 16.09351ms)
May 21 19:05:27.444: INFO: (6) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 16.875181ms)
May 21 19:05:27.448: INFO: (6) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 20.201263ms)
May 21 19:05:27.459: INFO: (6) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 30.719303ms)
May 21 19:05:27.494: INFO: (6) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 65.8919ms)
May 21 19:05:27.495: INFO: (6) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 67.269549ms)
May 21 19:05:27.497: INFO: (6) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 68.243992ms)
May 21 19:05:27.510: INFO: (6) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 81.517921ms)
May 21 19:05:27.510: INFO: (6) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 81.410041ms)
May 21 19:05:27.520: INFO: (6) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 91.645247ms)
May 21 19:05:27.521: INFO: (6) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 91.71582ms)
May 21 19:05:27.528: INFO: (6) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 100.167517ms)
May 21 19:05:27.528: INFO: (6) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 99.489534ms)
May 21 19:05:27.528: INFO: (6) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 99.754389ms)
May 21 19:05:27.528: INFO: (6) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 99.723492ms)
May 21 19:05:27.543: INFO: (7) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 15.136717ms)
May 21 19:05:27.551: INFO: (7) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 21.903527ms)
May 21 19:05:27.551: INFO: (7) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 22.460717ms)
May 21 19:05:27.551: INFO: (7) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 22.218048ms)
May 21 19:05:27.553: INFO: (7) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 24.535207ms)
May 21 19:05:27.557: INFO: (7) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 28.64337ms)
May 21 19:05:27.557: INFO: (7) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 28.614447ms)
May 21 19:05:27.560: INFO: (7) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 30.512465ms)
May 21 19:05:27.560: INFO: (7) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 30.316886ms)
May 21 19:05:27.561: INFO: (7) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 31.332249ms)
May 21 19:05:27.561: INFO: (7) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 31.123349ms)
May 21 19:05:27.561: INFO: (7) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 31.636886ms)
May 21 19:05:27.563: INFO: (7) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 32.819352ms)
May 21 19:05:27.563: INFO: (7) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 34.264195ms)
May 21 19:05:27.564: INFO: (7) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 34.113585ms)
May 21 19:05:27.564: INFO: (7) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 34.368499ms)
May 21 19:05:27.592: INFO: (8) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 28.176173ms)
May 21 19:05:27.618: INFO: (8) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 53.64989ms)
May 21 19:05:27.621: INFO: (8) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 56.650628ms)
May 21 19:05:27.622: INFO: (8) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 58.143129ms)
May 21 19:05:27.638: INFO: (8) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 72.336134ms)
May 21 19:05:27.638: INFO: (8) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 73.054728ms)
May 21 19:05:27.638: INFO: (8) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 72.429535ms)
May 21 19:05:27.638: INFO: (8) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 72.932003ms)
May 21 19:05:27.643: INFO: (8) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 77.899803ms)
May 21 19:05:27.644: INFO: (8) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 78.70511ms)
May 21 19:05:27.650: INFO: (8) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 85.117979ms)
May 21 19:05:27.650: INFO: (8) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 85.017368ms)
May 21 19:05:27.651: INFO: (8) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 86.572122ms)
May 21 19:05:27.653: INFO: (8) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 88.306293ms)
May 21 19:05:27.665: INFO: (8) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 100.475004ms)
May 21 19:05:27.665: INFO: (8) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 100.746578ms)
May 21 19:05:27.733: INFO: (9) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 66.619445ms)
May 21 19:05:27.733: INFO: (9) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 67.222632ms)
May 21 19:05:27.735: INFO: (9) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 69.287997ms)
May 21 19:05:27.735: INFO: (9) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 68.444978ms)
May 21 19:05:27.735: INFO: (9) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 69.120806ms)
May 21 19:05:27.735: INFO: (9) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 69.59524ms)
May 21 19:05:27.735: INFO: (9) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 69.362124ms)
May 21 19:05:27.735: INFO: (9) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 68.885943ms)
May 21 19:05:27.735: INFO: (9) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 69.273258ms)
May 21 19:05:27.741: INFO: (9) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 75.319427ms)
May 21 19:05:27.743: INFO: (9) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 76.543007ms)
May 21 19:05:27.743: INFO: (9) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 76.807809ms)
May 21 19:05:27.743: INFO: (9) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 76.777138ms)
May 21 19:05:27.744: INFO: (9) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 77.066194ms)
May 21 19:05:27.744: INFO: (9) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 77.223398ms)
May 21 19:05:27.744: INFO: (9) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 77.814385ms)
May 21 19:05:27.761: INFO: (10) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 16.976714ms)
May 21 19:05:27.765: INFO: (10) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 20.723257ms)
May 21 19:05:27.767: INFO: (10) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 22.497056ms)
May 21 19:05:27.770: INFO: (10) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 25.885717ms)
May 21 19:05:27.770: INFO: (10) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 25.780759ms)
May 21 19:05:27.770: INFO: (10) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 25.138807ms)
May 21 19:05:27.770: INFO: (10) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 25.098026ms)
May 21 19:05:27.773: INFO: (10) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 28.666722ms)
May 21 19:05:27.777: INFO: (10) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 31.753877ms)
May 21 19:05:27.778: INFO: (10) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 32.388061ms)
May 21 19:05:27.778: INFO: (10) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 32.558181ms)
May 21 19:05:27.778: INFO: (10) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 33.338145ms)
May 21 19:05:27.778: INFO: (10) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 33.697032ms)
May 21 19:05:27.778: INFO: (10) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 33.638438ms)
May 21 19:05:27.779: INFO: (10) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 33.644891ms)
May 21 19:05:27.779: INFO: (10) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 35.097621ms)
May 21 19:05:27.792: INFO: (11) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 12.161695ms)
May 21 19:05:27.793: INFO: (11) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 12.736703ms)
May 21 19:05:27.798: INFO: (11) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 17.132275ms)
May 21 19:05:27.798: INFO: (11) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 17.355924ms)
May 21 19:05:27.798: INFO: (11) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 17.24954ms)
May 21 19:05:27.799: INFO: (11) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 18.959168ms)
May 21 19:05:27.803: INFO: (11) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 22.481706ms)
May 21 19:05:27.804: INFO: (11) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 23.361968ms)
May 21 19:05:27.804: INFO: (11) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 23.769337ms)
May 21 19:05:27.807: INFO: (11) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 25.695308ms)
May 21 19:05:27.807: INFO: (11) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 25.829393ms)
May 21 19:05:27.809: INFO: (11) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 27.501365ms)
May 21 19:05:27.811: INFO: (11) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 30.054156ms)
May 21 19:05:27.812: INFO: (11) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 30.851968ms)
May 21 19:05:27.812: INFO: (11) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 30.973603ms)
May 21 19:05:27.812: INFO: (11) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 31.608849ms)
May 21 19:05:27.828: INFO: (12) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 14.992017ms)
May 21 19:05:27.828: INFO: (12) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 15.594395ms)
May 21 19:05:27.839: INFO: (12) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 26.941666ms)
May 21 19:05:27.843: INFO: (12) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 30.107206ms)
May 21 19:05:27.848: INFO: (12) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 34.718552ms)
May 21 19:05:27.848: INFO: (12) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 34.514891ms)
May 21 19:05:27.850: INFO: (12) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 35.936167ms)
May 21 19:05:27.850: INFO: (12) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 36.821056ms)
May 21 19:05:27.850: INFO: (12) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 36.179892ms)
May 21 19:05:27.851: INFO: (12) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 36.714547ms)
May 21 19:05:27.851: INFO: (12) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 37.758774ms)
May 21 19:05:27.851: INFO: (12) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 37.47212ms)
May 21 19:05:27.852: INFO: (12) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 37.793788ms)
May 21 19:05:27.853: INFO: (12) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 38.983561ms)
May 21 19:05:27.854: INFO: (12) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 39.66762ms)
May 21 19:05:27.855: INFO: (12) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 40.697864ms)
May 21 19:05:27.865: INFO: (13) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 9.362333ms)
May 21 19:05:27.865: INFO: (13) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 9.861998ms)
May 21 19:05:27.877: INFO: (13) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 20.41422ms)
May 21 19:05:27.877: INFO: (13) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 21.003162ms)
May 21 19:05:27.880: INFO: (13) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 21.450728ms)
May 21 19:05:27.880: INFO: (13) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 21.877218ms)
May 21 19:05:27.890: INFO: (13) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 32.243341ms)
May 21 19:05:27.890: INFO: (13) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 32.269103ms)
May 21 19:05:27.890: INFO: (13) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 32.65756ms)
May 21 19:05:27.890: INFO: (13) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 33.04535ms)
May 21 19:05:27.890: INFO: (13) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 33.591583ms)
May 21 19:05:27.890: INFO: (13) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 33.92706ms)
May 21 19:05:27.890: INFO: (13) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 32.959107ms)
May 21 19:05:27.890: INFO: (13) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 33.113346ms)
May 21 19:05:27.891: INFO: (13) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 33.584098ms)
May 21 19:05:27.891: INFO: (13) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 33.470994ms)
May 21 19:05:27.900: INFO: (14) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 9.788625ms)
May 21 19:05:27.909: INFO: (14) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 16.654773ms)
May 21 19:05:27.909: INFO: (14) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 17.314903ms)
May 21 19:05:27.909: INFO: (14) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 17.594219ms)
May 21 19:05:27.909: INFO: (14) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 17.074537ms)
May 21 19:05:27.910: INFO: (14) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 19.015614ms)
May 21 19:05:27.917: INFO: (14) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 23.937441ms)
May 21 19:05:27.919: INFO: (14) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 26.642559ms)
May 21 19:05:27.921: INFO: (14) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 29.164446ms)
May 21 19:05:27.922: INFO: (14) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 27.977687ms)
May 21 19:05:27.923: INFO: (14) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 30.025467ms)
May 21 19:05:27.923: INFO: (14) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 30.421957ms)
May 21 19:05:27.926: INFO: (14) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 32.231495ms)
May 21 19:05:27.931: INFO: (14) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 37.333137ms)
May 21 19:05:27.932: INFO: (14) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 38.513163ms)
May 21 19:05:27.932: INFO: (14) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 38.867048ms)
May 21 19:05:27.946: INFO: (15) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 14.298474ms)
May 21 19:05:27.946: INFO: (15) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 13.966364ms)
May 21 19:05:27.951: INFO: (15) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 17.870174ms)
May 21 19:05:27.956: INFO: (15) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 22.260998ms)
May 21 19:05:27.956: INFO: (15) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 23.350634ms)
May 21 19:05:27.957: INFO: (15) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 24.267402ms)
May 21 19:05:27.968: INFO: (15) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 33.617095ms)
May 21 19:05:27.972: INFO: (15) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 37.139466ms)
May 21 19:05:27.972: INFO: (15) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 38.289009ms)
May 21 19:05:27.972: INFO: (15) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 39.175725ms)
May 21 19:05:27.972: INFO: (15) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 38.38394ms)
May 21 19:05:27.972: INFO: (15) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 37.903896ms)
May 21 19:05:27.972: INFO: (15) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 38.186624ms)
May 21 19:05:27.974: INFO: (15) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 41.029409ms)
May 21 19:05:27.974: INFO: (15) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 39.75198ms)
May 21 19:05:27.975: INFO: (15) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 41.33675ms)
May 21 19:05:27.997: INFO: (16) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 21.98149ms)
May 21 19:05:28.001: INFO: (16) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 25.206095ms)
May 21 19:05:28.002: INFO: (16) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 25.774053ms)
May 21 19:05:28.012: INFO: (16) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 35.543953ms)
May 21 19:05:28.012: INFO: (16) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 33.804436ms)
May 21 19:05:28.012: INFO: (16) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 35.831653ms)
May 21 19:05:28.014: INFO: (16) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 37.175993ms)
May 21 19:05:28.015: INFO: (16) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 37.410482ms)
May 21 19:05:28.019: INFO: (16) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 41.735801ms)
May 21 19:05:28.020: INFO: (16) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 42.989992ms)
May 21 19:05:28.021: INFO: (16) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 43.561472ms)
May 21 19:05:28.025: INFO: (16) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 47.076767ms)
May 21 19:05:28.027: INFO: (16) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 49.176064ms)
May 21 19:05:28.028: INFO: (16) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 50.94331ms)
May 21 19:05:28.030: INFO: (16) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 52.069304ms)
May 21 19:05:28.034: INFO: (16) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 56.479591ms)
May 21 19:05:28.058: INFO: (17) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 23.137977ms)
May 21 19:05:28.058: INFO: (17) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 23.118791ms)
May 21 19:05:28.058: INFO: (17) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 23.18405ms)
May 21 19:05:28.061: INFO: (17) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 26.66361ms)
May 21 19:05:28.061: INFO: (17) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 26.162074ms)
May 21 19:05:28.067: INFO: (17) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 32.810681ms)
May 21 19:05:28.068: INFO: (17) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 33.133999ms)
May 21 19:05:28.068: INFO: (17) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 34.02241ms)
May 21 19:05:28.070: INFO: (17) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 35.020706ms)
May 21 19:05:28.071: INFO: (17) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 37.142676ms)
May 21 19:05:28.071: INFO: (17) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 36.141597ms)
May 21 19:05:28.071: INFO: (17) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 36.544803ms)
May 21 19:05:28.072: INFO: (17) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 37.189522ms)
May 21 19:05:28.072: INFO: (17) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 36.743677ms)
May 21 19:05:28.072: INFO: (17) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 36.734147ms)
May 21 19:05:28.072: INFO: (17) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 36.815573ms)
May 21 19:05:28.091: INFO: (18) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 18.848938ms)
May 21 19:05:28.093: INFO: (18) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 20.55426ms)
May 21 19:05:28.096: INFO: (18) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 22.432977ms)
May 21 19:05:28.098: INFO: (18) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 24.981942ms)
May 21 19:05:28.098: INFO: (18) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 24.046381ms)
May 21 19:05:28.100: INFO: (18) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 26.407242ms)
May 21 19:05:28.104: INFO: (18) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 28.721797ms)
May 21 19:05:28.105: INFO: (18) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 30.021698ms)
May 21 19:05:28.105: INFO: (18) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 30.142297ms)
May 21 19:05:28.106: INFO: (18) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 30.707966ms)
May 21 19:05:28.106: INFO: (18) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 32.37494ms)
May 21 19:05:28.109: INFO: (18) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 34.817659ms)
May 21 19:05:28.109: INFO: (18) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 33.843904ms)
May 21 19:05:28.109: INFO: (18) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 34.046307ms)
May 21 19:05:28.109: INFO: (18) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 34.384661ms)
May 21 19:05:28.110: INFO: (18) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 35.538057ms)
May 21 19:05:28.120: INFO: (19) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 9.759161ms)
May 21 19:05:28.139: INFO: (19) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname1/proxy/: foo (200; 28.617368ms)
May 21 19:05:28.139: INFO: (19) /api/v1/namespaces/proxy-5219/services/proxy-service-ctl44:portname2/proxy/: bar (200; 28.399482ms)
May 21 19:05:28.139: INFO: (19) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">... (200; 26.677947ms)
May 21 19:05:28.140: INFO: (19) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:162/proxy/: bar (200; 28.822609ms)
May 21 19:05:28.143: INFO: (19) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:460/proxy/: tls baz (200; 31.844207ms)
May 21 19:05:28.146: INFO: (19) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname1/proxy/: tls baz (200; 35.181608ms)
May 21 19:05:28.147: INFO: (19) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname2/proxy/: bar (200; 36.094608ms)
May 21 19:05:28.147: INFO: (19) /api/v1/namespaces/proxy-5219/services/http:proxy-service-ctl44:portname1/proxy/: foo (200; 35.382043ms)
May 21 19:05:28.147: INFO: (19) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 34.96644ms)
May 21 19:05:28.147: INFO: (19) /api/v1/namespaces/proxy-5219/pods/http:proxy-service-ctl44-rnnpt:160/proxy/: foo (200; 35.784774ms)
May 21 19:05:28.147: INFO: (19) /api/v1/namespaces/proxy-5219/services/https:proxy-service-ctl44:tlsportname2/proxy/: tls qux (200; 35.510039ms)
May 21 19:05:28.147: INFO: (19) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt:1080/proxy/rewriteme">test<... (200; 35.438818ms)
May 21 19:05:28.147: INFO: (19) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:443/proxy/tlsrewritem... (200; 35.68129ms)
May 21 19:05:28.147: INFO: (19) /api/v1/namespaces/proxy-5219/pods/https:proxy-service-ctl44-rnnpt:462/proxy/: tls qux (200; 36.301658ms)
May 21 19:05:28.148: INFO: (19) /api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/: <a href="/api/v1/namespaces/proxy-5219/pods/proxy-service-ctl44-rnnpt/proxy/rewriteme">test</a> (200; 36.073222ms)
STEP: deleting ReplicationController proxy-service-ctl44 in namespace proxy-5219, will wait for the garbage collector to delete the pods
May 21 19:05:28.224: INFO: Deleting ReplicationController proxy-service-ctl44 took: 20.148913ms
May 21 19:05:28.824: INFO: Terminating ReplicationController proxy-service-ctl44 pods took: 600.550862ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:05:34.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5219" for this suite.

• [SLOW TEST:19.994 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":42,"skipped":611,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:05:34.844: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-83c143eb-23ba-4453-8225-ba917bf12aec
STEP: Creating configMap with name cm-test-opt-upd-49974f0a-890e-4b36-bde7-c723f582eab2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-83c143eb-23ba-4453-8225-ba917bf12aec
STEP: Updating configmap cm-test-opt-upd-49974f0a-890e-4b36-bde7-c723f582eab2
STEP: Creating configMap with name cm-test-opt-create-dadade58-02d2-4538-9884-4c2763afe163
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:05:43.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9587" for this suite.

• [SLOW TEST:8.432 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":43,"skipped":616,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:05:43.277: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:05:43.374: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04760f4f-83d8-451d-b23f-dec06a86e246" in namespace "projected-4133" to be "Succeeded or Failed"
May 21 19:05:43.384: INFO: Pod "downwardapi-volume-04760f4f-83d8-451d-b23f-dec06a86e246": Phase="Pending", Reason="", readiness=false. Elapsed: 9.772185ms
May 21 19:05:45.399: INFO: Pod "downwardapi-volume-04760f4f-83d8-451d-b23f-dec06a86e246": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024257987s
May 21 19:05:47.407: INFO: Pod "downwardapi-volume-04760f4f-83d8-451d-b23f-dec06a86e246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032274717s
STEP: Saw pod success
May 21 19:05:47.407: INFO: Pod "downwardapi-volume-04760f4f-83d8-451d-b23f-dec06a86e246" satisfied condition "Succeeded or Failed"
May 21 19:05:47.412: INFO: Trying to get logs from node dc-hg-2 pod downwardapi-volume-04760f4f-83d8-451d-b23f-dec06a86e246 container client-container: <nil>
STEP: delete the pod
May 21 19:05:47.479: INFO: Waiting for pod downwardapi-volume-04760f4f-83d8-451d-b23f-dec06a86e246 to disappear
May 21 19:05:47.490: INFO: Pod downwardapi-volume-04760f4f-83d8-451d-b23f-dec06a86e246 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:05:47.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4133" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":44,"skipped":633,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:05:47.506: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:05:47.624: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f074ca9b-b922-4ebd-ac85-6b7c36b0b679" in namespace "downward-api-4754" to be "Succeeded or Failed"
May 21 19:05:47.641: INFO: Pod "downwardapi-volume-f074ca9b-b922-4ebd-ac85-6b7c36b0b679": Phase="Pending", Reason="", readiness=false. Elapsed: 16.792855ms
May 21 19:05:49.648: INFO: Pod "downwardapi-volume-f074ca9b-b922-4ebd-ac85-6b7c36b0b679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023983794s
May 21 19:05:51.657: INFO: Pod "downwardapi-volume-f074ca9b-b922-4ebd-ac85-6b7c36b0b679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03285837s
STEP: Saw pod success
May 21 19:05:51.657: INFO: Pod "downwardapi-volume-f074ca9b-b922-4ebd-ac85-6b7c36b0b679" satisfied condition "Succeeded or Failed"
May 21 19:05:51.666: INFO: Trying to get logs from node dc-hg-2 pod downwardapi-volume-f074ca9b-b922-4ebd-ac85-6b7c36b0b679 container client-container: <nil>
STEP: delete the pod
May 21 19:05:51.745: INFO: Waiting for pod downwardapi-volume-f074ca9b-b922-4ebd-ac85-6b7c36b0b679 to disappear
May 21 19:05:51.756: INFO: Pod downwardapi-volume-f074ca9b-b922-4ebd-ac85-6b7c36b0b679 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:05:51.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4754" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":45,"skipped":637,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:05:51.783: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:05:51.905: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:05:56.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1485" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":46,"skipped":642,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:05:56.210: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-b48fed0b-fafd-4ed0-a3ce-4ae26278d5ed
STEP: Creating a pod to test consume configMaps
May 21 19:05:56.307: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e59988f6-21fb-4d9a-a9a3-c5652fbb9e11" in namespace "projected-1975" to be "Succeeded or Failed"
May 21 19:05:56.346: INFO: Pod "pod-projected-configmaps-e59988f6-21fb-4d9a-a9a3-c5652fbb9e11": Phase="Pending", Reason="", readiness=false. Elapsed: 38.905663ms
May 21 19:05:58.354: INFO: Pod "pod-projected-configmaps-e59988f6-21fb-4d9a-a9a3-c5652fbb9e11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04709074s
May 21 19:06:00.362: INFO: Pod "pod-projected-configmaps-e59988f6-21fb-4d9a-a9a3-c5652fbb9e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054900058s
STEP: Saw pod success
May 21 19:06:00.362: INFO: Pod "pod-projected-configmaps-e59988f6-21fb-4d9a-a9a3-c5652fbb9e11" satisfied condition "Succeeded or Failed"
May 21 19:06:00.372: INFO: Trying to get logs from node dc-hg-2 pod pod-projected-configmaps-e59988f6-21fb-4d9a-a9a3-c5652fbb9e11 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 19:06:00.458: INFO: Waiting for pod pod-projected-configmaps-e59988f6-21fb-4d9a-a9a3-c5652fbb9e11 to disappear
May 21 19:06:00.466: INFO: Pod pod-projected-configmaps-e59988f6-21fb-4d9a-a9a3-c5652fbb9e11 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:06:00.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1975" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":47,"skipped":682,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:06:00.526: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:06:00.645: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 21 19:06:02.760: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:06:03.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7683" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":48,"skipped":702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:06:03.816: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:06:04.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-251" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":49,"skipped":764,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:06:04.051: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 21 19:06:06.030: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 21 19:06:08.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220766, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220766, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220766, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757220766, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:06:11.093: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:06:11.103: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:06:12.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3261" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.331 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":50,"skipped":771,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:06:13.382: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 21 19:06:14.065: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 19:07:14.179: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
May 21 19:07:14.241: INFO: Created pod: pod0-sched-preemption-low-priority
May 21 19:07:14.270: INFO: Created pod: pod1-sched-preemption-medium-priority
May 21 19:07:14.353: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:07:30.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6267" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:77.483 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":51,"skipped":781,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:07:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:07:30.989: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0ee1b10-4dae-4423-a478-1ef2b086a23e" in namespace "downward-api-6615" to be "Succeeded or Failed"
May 21 19:07:31.016: INFO: Pod "downwardapi-volume-f0ee1b10-4dae-4423-a478-1ef2b086a23e": Phase="Pending", Reason="", readiness=false. Elapsed: 27.431385ms
May 21 19:07:33.035: INFO: Pod "downwardapi-volume-f0ee1b10-4dae-4423-a478-1ef2b086a23e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045887513s
May 21 19:07:35.041: INFO: Pod "downwardapi-volume-f0ee1b10-4dae-4423-a478-1ef2b086a23e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052193579s
STEP: Saw pod success
May 21 19:07:35.041: INFO: Pod "downwardapi-volume-f0ee1b10-4dae-4423-a478-1ef2b086a23e" satisfied condition "Succeeded or Failed"
May 21 19:07:35.046: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-f0ee1b10-4dae-4423-a478-1ef2b086a23e container client-container: <nil>
STEP: delete the pod
May 21 19:07:35.124: INFO: Waiting for pod downwardapi-volume-f0ee1b10-4dae-4423-a478-1ef2b086a23e to disappear
May 21 19:07:35.129: INFO: Pod downwardapi-volume-f0ee1b10-4dae-4423-a478-1ef2b086a23e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:07:35.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6615" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":52,"skipped":790,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:07:35.158: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 21 19:07:35.261: INFO: Waiting up to 5m0s for pod "pod-65798ab2-39cc-4bf1-817f-e600b7e290bc" in namespace "emptydir-3274" to be "Succeeded or Failed"
May 21 19:07:35.280: INFO: Pod "pod-65798ab2-39cc-4bf1-817f-e600b7e290bc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.917456ms
May 21 19:07:37.295: INFO: Pod "pod-65798ab2-39cc-4bf1-817f-e600b7e290bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034136503s
May 21 19:07:39.312: INFO: Pod "pod-65798ab2-39cc-4bf1-817f-e600b7e290bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051073954s
May 21 19:07:41.319: INFO: Pod "pod-65798ab2-39cc-4bf1-817f-e600b7e290bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058563049s
STEP: Saw pod success
May 21 19:07:41.319: INFO: Pod "pod-65798ab2-39cc-4bf1-817f-e600b7e290bc" satisfied condition "Succeeded or Failed"
May 21 19:07:41.326: INFO: Trying to get logs from node dc-hg-1 pod pod-65798ab2-39cc-4bf1-817f-e600b7e290bc container test-container: <nil>
STEP: delete the pod
May 21 19:07:41.370: INFO: Waiting for pod pod-65798ab2-39cc-4bf1-817f-e600b7e290bc to disappear
May 21 19:07:41.379: INFO: Pod pod-65798ab2-39cc-4bf1-817f-e600b7e290bc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:07:41.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3274" for this suite.

• [SLOW TEST:6.242 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":53,"skipped":795,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:07:41.401: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
May 21 19:07:41.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-1722 create -f -'
May 21 19:07:42.347: INFO: stderr: ""
May 21 19:07:42.347: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 21 19:07:42.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-1722 diff -f -'
May 21 19:07:43.404: INFO: rc: 1
May 21 19:07:43.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-1722 delete -f -'
May 21 19:07:43.771: INFO: stderr: ""
May 21 19:07:43.771: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:07:43.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1722" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":54,"skipped":810,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:07:43.794: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-c1e3e96b-2b6c-49c7-9ec4-df09d697691e in namespace container-probe-2475
May 21 19:07:47.953: INFO: Started pod liveness-c1e3e96b-2b6c-49c7-9ec4-df09d697691e in namespace container-probe-2475
STEP: checking the pod's current state and verifying that restartCount is present
May 21 19:07:47.959: INFO: Initial restart count of pod liveness-c1e3e96b-2b6c-49c7-9ec4-df09d697691e is 0
May 21 19:08:10.043: INFO: Restart count of pod container-probe-2475/liveness-c1e3e96b-2b6c-49c7-9ec4-df09d697691e is now 1 (22.083369892s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:08:10.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2475" for this suite.

• [SLOW TEST:26.343 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":55,"skipped":817,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:08:10.138: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-2581/secret-test-644edd62-9c09-4971-b173-79c186705ce4
STEP: Creating a pod to test consume secrets
May 21 19:08:10.259: INFO: Waiting up to 5m0s for pod "pod-configmaps-0cba868f-a7d2-4fcd-95e0-369d0c735a65" in namespace "secrets-2581" to be "Succeeded or Failed"
May 21 19:08:10.265: INFO: Pod "pod-configmaps-0cba868f-a7d2-4fcd-95e0-369d0c735a65": Phase="Pending", Reason="", readiness=false. Elapsed: 5.032656ms
May 21 19:08:12.283: INFO: Pod "pod-configmaps-0cba868f-a7d2-4fcd-95e0-369d0c735a65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02377079s
May 21 19:08:14.290: INFO: Pod "pod-configmaps-0cba868f-a7d2-4fcd-95e0-369d0c735a65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030246175s
STEP: Saw pod success
May 21 19:08:14.290: INFO: Pod "pod-configmaps-0cba868f-a7d2-4fcd-95e0-369d0c735a65" satisfied condition "Succeeded or Failed"
May 21 19:08:14.295: INFO: Trying to get logs from node dc-hg-1 pod pod-configmaps-0cba868f-a7d2-4fcd-95e0-369d0c735a65 container env-test: <nil>
STEP: delete the pod
May 21 19:08:14.346: INFO: Waiting for pod pod-configmaps-0cba868f-a7d2-4fcd-95e0-369d0c735a65 to disappear
May 21 19:08:14.351: INFO: Pod pod-configmaps-0cba868f-a7d2-4fcd-95e0-369d0c735a65 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:08:14.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2581" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":56,"skipped":821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:08:14.374: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8879.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8879.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8879.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8879.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8879.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8879.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 19:08:18.541: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:18.548: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:18.554: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:18.562: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:18.580: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:18.586: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:18.594: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:18.600: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:18.620: INFO: Lookups using dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local]

May 21 19:08:23.650: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:23.666: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:23.679: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:23.691: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:23.745: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:23.758: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:23.766: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:23.775: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:23.790: INFO: Lookups using dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local]

May 21 19:08:28.631: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:28.641: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:28.650: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:28.661: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:28.693: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:28.700: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:28.713: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:28.724: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:28.745: INFO: Lookups using dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local]

May 21 19:08:33.633: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:33.644: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:33.654: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:33.677: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:33.728: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:33.737: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:33.744: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:33.761: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:33.773: INFO: Lookups using dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local]

May 21 19:08:38.628: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:38.636: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:38.643: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:38.650: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:38.675: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:38.681: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:38.688: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:38.697: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:38.710: INFO: Lookups using dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local]

May 21 19:08:43.634: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:43.646: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:43.665: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:43.674: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:43.736: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:43.747: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:43.767: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:43.778: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local from pod dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21: the server could not find the requested resource (get pods dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21)
May 21 19:08:43.797: INFO: Lookups using dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8879.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8879.svc.cluster.local jessie_udp@dns-test-service-2.dns-8879.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8879.svc.cluster.local]

May 21 19:08:48.751: INFO: DNS probes using dns-8879/dns-test-58c8eb5c-bb53-4b20-bbb2-37b6b5bb2f21 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:08:49.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8879" for this suite.

• [SLOW TEST:34.792 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":57,"skipped":847,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:08:49.177: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-1e03684b-9358-4c4e-933b-1fec1eb056ba
STEP: Creating a pod to test consume configMaps
May 21 19:08:49.292: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8b49142f-b99f-4e4f-ad88-b953f0cc8e13" in namespace "projected-9245" to be "Succeeded or Failed"
May 21 19:08:49.312: INFO: Pod "pod-projected-configmaps-8b49142f-b99f-4e4f-ad88-b953f0cc8e13": Phase="Pending", Reason="", readiness=false. Elapsed: 19.306795ms
May 21 19:08:51.319: INFO: Pod "pod-projected-configmaps-8b49142f-b99f-4e4f-ad88-b953f0cc8e13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026819091s
May 21 19:08:53.326: INFO: Pod "pod-projected-configmaps-8b49142f-b99f-4e4f-ad88-b953f0cc8e13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033109173s
May 21 19:08:55.334: INFO: Pod "pod-projected-configmaps-8b49142f-b99f-4e4f-ad88-b953f0cc8e13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041559118s
STEP: Saw pod success
May 21 19:08:55.334: INFO: Pod "pod-projected-configmaps-8b49142f-b99f-4e4f-ad88-b953f0cc8e13" satisfied condition "Succeeded or Failed"
May 21 19:08:55.339: INFO: Trying to get logs from node dc-hg-1 pod pod-projected-configmaps-8b49142f-b99f-4e4f-ad88-b953f0cc8e13 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 19:08:55.377: INFO: Waiting for pod pod-projected-configmaps-8b49142f-b99f-4e4f-ad88-b953f0cc8e13 to disappear
May 21 19:08:55.390: INFO: Pod pod-projected-configmaps-8b49142f-b99f-4e4f-ad88-b953f0cc8e13 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:08:55.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9245" for this suite.

• [SLOW TEST:6.246 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":58,"skipped":905,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:08:55.424: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:08:55.509: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:08:59.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-206" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":59,"skipped":922,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:08:59.777: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-5045
STEP: creating replication controller nodeport-test in namespace services-5045
I0521 19:08:59.946704      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-5045, replica count: 2
I0521 19:09:02.997369      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 19:09:05.997: INFO: Creating new exec pod
I0521 19:09:05.997844      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 19:09:11.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-5045 exec execpodtb6d6 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 21 19:09:11.600: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 21 19:09:11.601: INFO: stdout: ""
May 21 19:09:11.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-5045 exec execpodtb6d6 -- /bin/sh -x -c nc -zv -t -w 2 10.10.174.58 80'
May 21 19:09:12.132: INFO: stderr: "+ nc -zv -t -w 2 10.10.174.58 80\nConnection to 10.10.174.58 80 port [tcp/http] succeeded!\n"
May 21 19:09:12.132: INFO: stdout: ""
May 21 19:09:12.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-5045 exec execpodtb6d6 -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.136 31641'
May 21 19:09:12.694: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.136 31641\nConnection to 10.10.1.136 31641 port [tcp/31641] succeeded!\n"
May 21 19:09:12.695: INFO: stdout: ""
May 21 19:09:12.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-5045 exec execpodtb6d6 -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.125 31641'
May 21 19:09:13.265: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.125 31641\nConnection to 10.10.1.125 31641 port [tcp/31641] succeeded!\n"
May 21 19:09:13.265: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:09:13.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5045" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:13.508 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":60,"skipped":929,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:09:13.285: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 21 19:09:13.436: INFO: Waiting up to 5m0s for pod "pod-19e9a225-7e40-4459-96cd-3263e064f930" in namespace "emptydir-6764" to be "Succeeded or Failed"
May 21 19:09:13.460: INFO: Pod "pod-19e9a225-7e40-4459-96cd-3263e064f930": Phase="Pending", Reason="", readiness=false. Elapsed: 24.545104ms
May 21 19:09:15.467: INFO: Pod "pod-19e9a225-7e40-4459-96cd-3263e064f930": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031264098s
May 21 19:09:17.473: INFO: Pod "pod-19e9a225-7e40-4459-96cd-3263e064f930": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037656934s
STEP: Saw pod success
May 21 19:09:17.474: INFO: Pod "pod-19e9a225-7e40-4459-96cd-3263e064f930" satisfied condition "Succeeded or Failed"
May 21 19:09:17.482: INFO: Trying to get logs from node dc-hg-1 pod pod-19e9a225-7e40-4459-96cd-3263e064f930 container test-container: <nil>
STEP: delete the pod
May 21 19:09:17.533: INFO: Waiting for pod pod-19e9a225-7e40-4459-96cd-3263e064f930 to disappear
May 21 19:09:17.580: INFO: Pod pod-19e9a225-7e40-4459-96cd-3263e064f930 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:09:17.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6764" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":61,"skipped":929,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:09:17.610: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 1 pods
STEP: Gathering metrics
May 21 19:09:19.495: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:09:19.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0521 19:09:19.495098      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 19:09:19.495149      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 19:09:19.495166      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-6570" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":62,"skipped":929,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:09:19.540: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 21 19:09:19.757: INFO: Waiting up to 5m0s for pod "downward-api-088a527f-fb95-41ae-a945-bfc010779c8e" in namespace "downward-api-6362" to be "Succeeded or Failed"
May 21 19:09:19.782: INFO: Pod "downward-api-088a527f-fb95-41ae-a945-bfc010779c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.35167ms
May 21 19:09:21.791: INFO: Pod "downward-api-088a527f-fb95-41ae-a945-bfc010779c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033279243s
May 21 19:09:23.798: INFO: Pod "downward-api-088a527f-fb95-41ae-a945-bfc010779c8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040528953s
STEP: Saw pod success
May 21 19:09:23.798: INFO: Pod "downward-api-088a527f-fb95-41ae-a945-bfc010779c8e" satisfied condition "Succeeded or Failed"
May 21 19:09:23.804: INFO: Trying to get logs from node dc-hg-1 pod downward-api-088a527f-fb95-41ae-a945-bfc010779c8e container dapi-container: <nil>
STEP: delete the pod
May 21 19:09:23.854: INFO: Waiting for pod downward-api-088a527f-fb95-41ae-a945-bfc010779c8e to disappear
May 21 19:09:23.860: INFO: Pod downward-api-088a527f-fb95-41ae-a945-bfc010779c8e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:09:23.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6362" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":63,"skipped":971,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:09:23.873: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:09:23.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8789" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":64,"skipped":988,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:09:23.988: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
May 21 19:09:24.070: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:10:10.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9803" for this suite.

• [SLOW TEST:46.391 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":65,"skipped":994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:10:10.380: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-df7bae5c-8a48-477e-86c7-eda9bb2d7ee0
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-df7bae5c-8a48-477e-86c7-eda9bb2d7ee0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:11:27.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8185" for this suite.

• [SLOW TEST:77.177 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":1022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:11:27.558: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:11:44.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6158" for this suite.

• [SLOW TEST:17.267 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":67,"skipped":1058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:11:44.827: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
May 21 19:11:44.952: INFO: Waiting up to 5m0s for pod "var-expansion-7545204d-08b8-415b-84cb-693261d237a5" in namespace "var-expansion-7844" to be "Succeeded or Failed"
May 21 19:11:44.968: INFO: Pod "var-expansion-7545204d-08b8-415b-84cb-693261d237a5": Phase="Pending", Reason="", readiness=false. Elapsed: 15.196581ms
May 21 19:11:46.988: INFO: Pod "var-expansion-7545204d-08b8-415b-84cb-693261d237a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03562067s
May 21 19:11:49.052: INFO: Pod "var-expansion-7545204d-08b8-415b-84cb-693261d237a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.099275642s
STEP: Saw pod success
May 21 19:11:49.052: INFO: Pod "var-expansion-7545204d-08b8-415b-84cb-693261d237a5" satisfied condition "Succeeded or Failed"
May 21 19:11:49.077: INFO: Trying to get logs from node dc-hg-1 pod var-expansion-7545204d-08b8-415b-84cb-693261d237a5 container dapi-container: <nil>
STEP: delete the pod
May 21 19:11:49.187: INFO: Waiting for pod var-expansion-7545204d-08b8-415b-84cb-693261d237a5 to disappear
May 21 19:11:49.231: INFO: Pod var-expansion-7545204d-08b8-415b-84cb-693261d237a5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:11:49.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7844" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":68,"skipped":1105,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:11:49.312: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 21 19:11:57.536: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 19:11:57.544: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 19:11:59.544: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 19:11:59.550: INFO: Pod pod-with-prestop-exec-hook still exists
May 21 19:12:01.544: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 21 19:12:01.552: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:01.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3290" for this suite.

• [SLOW TEST:12.283 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":69,"skipped":1111,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:01.596: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-585e269a-7482-4288-a4d7-4c834287d758
STEP: Creating a pod to test consume secrets
May 21 19:12:01.796: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d4cc74e-83bb-47cd-8923-f29f3951bb0d" in namespace "projected-5475" to be "Succeeded or Failed"
May 21 19:12:01.830: INFO: Pod "pod-projected-secrets-9d4cc74e-83bb-47cd-8923-f29f3951bb0d": Phase="Pending", Reason="", readiness=false. Elapsed: 34.419897ms
May 21 19:12:03.837: INFO: Pod "pod-projected-secrets-9d4cc74e-83bb-47cd-8923-f29f3951bb0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041161397s
May 21 19:12:05.844: INFO: Pod "pod-projected-secrets-9d4cc74e-83bb-47cd-8923-f29f3951bb0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047760063s
STEP: Saw pod success
May 21 19:12:05.844: INFO: Pod "pod-projected-secrets-9d4cc74e-83bb-47cd-8923-f29f3951bb0d" satisfied condition "Succeeded or Failed"
May 21 19:12:05.853: INFO: Trying to get logs from node dc-hg-1 pod pod-projected-secrets-9d4cc74e-83bb-47cd-8923-f29f3951bb0d container secret-volume-test: <nil>
STEP: delete the pod
May 21 19:12:05.892: INFO: Waiting for pod pod-projected-secrets-9d4cc74e-83bb-47cd-8923-f29f3951bb0d to disappear
May 21 19:12:05.901: INFO: Pod pod-projected-secrets-9d4cc74e-83bb-47cd-8923-f29f3951bb0d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:05.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5475" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":70,"skipped":1120,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:05.919: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1512
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 19:12:06.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-1967 run e2e-test-httpd-pod --restart=Never --image=mirror.gcr.io/library/httpd:2.4.38-alpine'
May 21 19:12:06.463: INFO: stderr: ""
May 21 19:12:06.463: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
May 21 19:12:06.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-1967 delete pods e2e-test-httpd-pod'
May 21 19:12:14.809: INFO: stderr: ""
May 21 19:12:14.809: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:14.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1967" for this suite.

• [SLOW TEST:8.911 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1509
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":71,"skipped":1120,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:14.831: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
May 21 19:12:19.462: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1203 pod-service-account-f1c7c318-7ac1-4c79-a04f-e25bb83fdd17 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 21 19:12:20.047: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1203 pod-service-account-f1c7c318-7ac1-4c79-a04f-e25bb83fdd17 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 21 19:12:20.868: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1203 pod-service-account-f1c7c318-7ac1-4c79-a04f-e25bb83fdd17 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:21.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1203" for this suite.

• [SLOW TEST:6.576 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":72,"skipped":1132,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:21.408: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
May 21 19:12:21.518: INFO: Waiting up to 5m0s for pod "pod-8ff5ffe9-c36d-4be9-973b-c24c6e131198" in namespace "emptydir-6996" to be "Succeeded or Failed"
May 21 19:12:21.551: INFO: Pod "pod-8ff5ffe9-c36d-4be9-973b-c24c6e131198": Phase="Pending", Reason="", readiness=false. Elapsed: 32.979279ms
May 21 19:12:23.575: INFO: Pod "pod-8ff5ffe9-c36d-4be9-973b-c24c6e131198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056603256s
May 21 19:12:25.580: INFO: Pod "pod-8ff5ffe9-c36d-4be9-973b-c24c6e131198": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062508305s
STEP: Saw pod success
May 21 19:12:25.581: INFO: Pod "pod-8ff5ffe9-c36d-4be9-973b-c24c6e131198" satisfied condition "Succeeded or Failed"
May 21 19:12:25.590: INFO: Trying to get logs from node dc-hg-1 pod pod-8ff5ffe9-c36d-4be9-973b-c24c6e131198 container test-container: <nil>
STEP: delete the pod
May 21 19:12:25.675: INFO: Waiting for pod pod-8ff5ffe9-c36d-4be9-973b-c24c6e131198 to disappear
May 21 19:12:25.691: INFO: Pod pod-8ff5ffe9-c36d-4be9-973b-c24c6e131198 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:25.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6996" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":73,"skipped":1141,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:25.719: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-c9dc5891-2d55-48fa-8d2b-b45d8429d450
STEP: Creating a pod to test consume secrets
May 21 19:12:25.875: INFO: Waiting up to 5m0s for pod "pod-secrets-72e09ac7-5865-4b10-a650-4b566b08b2f2" in namespace "secrets-3462" to be "Succeeded or Failed"
May 21 19:12:25.884: INFO: Pod "pod-secrets-72e09ac7-5865-4b10-a650-4b566b08b2f2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.971102ms
May 21 19:12:27.894: INFO: Pod "pod-secrets-72e09ac7-5865-4b10-a650-4b566b08b2f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018801442s
May 21 19:12:29.899: INFO: Pod "pod-secrets-72e09ac7-5865-4b10-a650-4b566b08b2f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024062042s
STEP: Saw pod success
May 21 19:12:29.899: INFO: Pod "pod-secrets-72e09ac7-5865-4b10-a650-4b566b08b2f2" satisfied condition "Succeeded or Failed"
May 21 19:12:29.904: INFO: Trying to get logs from node dc-hg-1 pod pod-secrets-72e09ac7-5865-4b10-a650-4b566b08b2f2 container secret-volume-test: <nil>
STEP: delete the pod
May 21 19:12:29.997: INFO: Waiting for pod pod-secrets-72e09ac7-5865-4b10-a650-4b566b08b2f2 to disappear
May 21 19:12:30.011: INFO: Pod pod-secrets-72e09ac7-5865-4b10-a650-4b566b08b2f2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:30.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3462" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":74,"skipped":1146,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:30.041: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:12:30.133: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:30.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8107" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":75,"skipped":1147,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:30.890: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:12:35.520: INFO: Waiting up to 5m0s for pod "client-envvars-01815fe2-03fa-4df3-9998-aaf42e441d1c" in namespace "pods-2833" to be "Succeeded or Failed"
May 21 19:12:35.567: INFO: Pod "client-envvars-01815fe2-03fa-4df3-9998-aaf42e441d1c": Phase="Pending", Reason="", readiness=false. Elapsed: 46.953956ms
May 21 19:12:37.587: INFO: Pod "client-envvars-01815fe2-03fa-4df3-9998-aaf42e441d1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066463549s
May 21 19:12:39.596: INFO: Pod "client-envvars-01815fe2-03fa-4df3-9998-aaf42e441d1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075622302s
May 21 19:12:41.758: INFO: Pod "client-envvars-01815fe2-03fa-4df3-9998-aaf42e441d1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.23763262s
STEP: Saw pod success
May 21 19:12:41.758: INFO: Pod "client-envvars-01815fe2-03fa-4df3-9998-aaf42e441d1c" satisfied condition "Succeeded or Failed"
May 21 19:12:41.800: INFO: Trying to get logs from node dc-hg-2 pod client-envvars-01815fe2-03fa-4df3-9998-aaf42e441d1c container env3cont: <nil>
STEP: delete the pod
May 21 19:12:41.933: INFO: Waiting for pod client-envvars-01815fe2-03fa-4df3-9998-aaf42e441d1c to disappear
May 21 19:12:41.965: INFO: Pod client-envvars-01815fe2-03fa-4df3-9998-aaf42e441d1c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:41.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2833" for this suite.

• [SLOW TEST:11.104 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":1150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:41.997: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7392.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7392.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7392.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7392.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7392.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7392.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 19:12:48.231: INFO: DNS probes using dns-7392/dns-test-341bcc47-8dfd-426d-bd48-bb75d3b35b2c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:48.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7392" for this suite.

• [SLOW TEST:6.300 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":77,"skipped":1197,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:48.298: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:12:48.391: INFO: Waiting up to 5m0s for pod "downwardapi-volume-347e5a4c-de21-4778-96ff-3899ee5715fe" in namespace "downward-api-7702" to be "Succeeded or Failed"
May 21 19:12:48.419: INFO: Pod "downwardapi-volume-347e5a4c-de21-4778-96ff-3899ee5715fe": Phase="Pending", Reason="", readiness=false. Elapsed: 28.095827ms
May 21 19:12:50.428: INFO: Pod "downwardapi-volume-347e5a4c-de21-4778-96ff-3899ee5715fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036678953s
May 21 19:12:52.458: INFO: Pod "downwardapi-volume-347e5a4c-de21-4778-96ff-3899ee5715fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066999108s
STEP: Saw pod success
May 21 19:12:52.458: INFO: Pod "downwardapi-volume-347e5a4c-de21-4778-96ff-3899ee5715fe" satisfied condition "Succeeded or Failed"
May 21 19:12:52.468: INFO: Trying to get logs from node dc-hg-2 pod downwardapi-volume-347e5a4c-de21-4778-96ff-3899ee5715fe container client-container: <nil>
STEP: delete the pod
May 21 19:12:52.562: INFO: Waiting for pod downwardapi-volume-347e5a4c-de21-4778-96ff-3899ee5715fe to disappear
May 21 19:12:52.570: INFO: Pod downwardapi-volume-347e5a4c-de21-4778-96ff-3899ee5715fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:52.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7702" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":78,"skipped":1199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:52.597: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
May 21 19:12:52.707: INFO: Waiting up to 5m0s for pod "var-expansion-78074ce3-8c99-4a7b-9348-d0e4cd1dd570" in namespace "var-expansion-1518" to be "Succeeded or Failed"
May 21 19:12:52.717: INFO: Pod "var-expansion-78074ce3-8c99-4a7b-9348-d0e4cd1dd570": Phase="Pending", Reason="", readiness=false. Elapsed: 10.499192ms
May 21 19:12:54.725: INFO: Pod "var-expansion-78074ce3-8c99-4a7b-9348-d0e4cd1dd570": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018488758s
May 21 19:12:56.732: INFO: Pod "var-expansion-78074ce3-8c99-4a7b-9348-d0e4cd1dd570": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025280706s
STEP: Saw pod success
May 21 19:12:56.732: INFO: Pod "var-expansion-78074ce3-8c99-4a7b-9348-d0e4cd1dd570" satisfied condition "Succeeded or Failed"
May 21 19:12:56.739: INFO: Trying to get logs from node dc-hg-1 pod var-expansion-78074ce3-8c99-4a7b-9348-d0e4cd1dd570 container dapi-container: <nil>
STEP: delete the pod
May 21 19:12:56.798: INFO: Waiting for pod var-expansion-78074ce3-8c99-4a7b-9348-d0e4cd1dd570 to disappear
May 21 19:12:56.808: INFO: Pod var-expansion-78074ce3-8c99-4a7b-9348-d0e4cd1dd570 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:12:56.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1518" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":79,"skipped":1223,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:12:56.835: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
May 21 19:12:56.934: INFO: Waiting up to 5m0s for pod "pod-ed148b6d-6767-46e0-9885-4cc27be8f991" in namespace "emptydir-4774" to be "Succeeded or Failed"
May 21 19:12:56.944: INFO: Pod "pod-ed148b6d-6767-46e0-9885-4cc27be8f991": Phase="Pending", Reason="", readiness=false. Elapsed: 9.647059ms
May 21 19:12:58.952: INFO: Pod "pod-ed148b6d-6767-46e0-9885-4cc27be8f991": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017814886s
May 21 19:13:00.988: INFO: Pod "pod-ed148b6d-6767-46e0-9885-4cc27be8f991": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05441622s
STEP: Saw pod success
May 21 19:13:00.989: INFO: Pod "pod-ed148b6d-6767-46e0-9885-4cc27be8f991" satisfied condition "Succeeded or Failed"
May 21 19:13:00.994: INFO: Trying to get logs from node dc-hg-1 pod pod-ed148b6d-6767-46e0-9885-4cc27be8f991 container test-container: <nil>
STEP: delete the pod
May 21 19:13:01.050: INFO: Waiting for pod pod-ed148b6d-6767-46e0-9885-4cc27be8f991 to disappear
May 21 19:13:01.072: INFO: Pod pod-ed148b6d-6767-46e0-9885-4cc27be8f991 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:13:01.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4774" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1234,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:13:01.100: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:13:02.625: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 19:13:04.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221182, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221182, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221182, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221182, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:13:07.682: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:13:07.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7044" for this suite.
STEP: Destroying namespace "webhook-7044-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.142 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":81,"skipped":1239,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:13:08.243: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-ec44e45a-d9d4-4a68-b1ff-1bbf1ff47160
STEP: Creating a pod to test consume configMaps
May 21 19:13:08.370: INFO: Waiting up to 5m0s for pod "pod-configmaps-00a1d045-f086-4743-99b9-39703c93e04e" in namespace "configmap-6230" to be "Succeeded or Failed"
May 21 19:13:08.385: INFO: Pod "pod-configmaps-00a1d045-f086-4743-99b9-39703c93e04e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.542295ms
May 21 19:13:10.398: INFO: Pod "pod-configmaps-00a1d045-f086-4743-99b9-39703c93e04e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02781785s
May 21 19:13:12.405: INFO: Pod "pod-configmaps-00a1d045-f086-4743-99b9-39703c93e04e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034315913s
STEP: Saw pod success
May 21 19:13:12.405: INFO: Pod "pod-configmaps-00a1d045-f086-4743-99b9-39703c93e04e" satisfied condition "Succeeded or Failed"
May 21 19:13:12.410: INFO: Trying to get logs from node dc-hg-2 pod pod-configmaps-00a1d045-f086-4743-99b9-39703c93e04e container configmap-volume-test: <nil>
STEP: delete the pod
May 21 19:13:12.455: INFO: Waiting for pod pod-configmaps-00a1d045-f086-4743-99b9-39703c93e04e to disappear
May 21 19:13:12.472: INFO: Pod pod-configmaps-00a1d045-f086-4743-99b9-39703c93e04e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:13:12.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6230" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":82,"skipped":1246,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:13:12.550: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-601687ea-3ff6-46e3-b44b-c333f9a414a6 in namespace container-probe-1537
May 21 19:13:16.673: INFO: Started pod busybox-601687ea-3ff6-46e3-b44b-c333f9a414a6 in namespace container-probe-1537
STEP: checking the pod's current state and verifying that restartCount is present
May 21 19:13:16.695: INFO: Initial restart count of pod busybox-601687ea-3ff6-46e3-b44b-c333f9a414a6 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:17:17.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1537" for this suite.

• [SLOW TEST:245.199 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":83,"skipped":1262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:17:17.752: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:17:19.537: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 19:17:21.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221439, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221439, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221439, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221439, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:17:23.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221439, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221439, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221439, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221439, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:17:26.582: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:17:27.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-414" for this suite.
STEP: Destroying namespace "webhook-414-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.633 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":84,"skipped":1299,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:17:27.386: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-4cb1f89a-d7bd-4432-a6ca-80386993d491
STEP: Creating a pod to test consume secrets
May 21 19:17:27.554: INFO: Waiting up to 5m0s for pod "pod-secrets-35fc0184-5735-4def-887e-49a0f4ac0dff" in namespace "secrets-5571" to be "Succeeded or Failed"
May 21 19:17:27.629: INFO: Pod "pod-secrets-35fc0184-5735-4def-887e-49a0f4ac0dff": Phase="Pending", Reason="", readiness=false. Elapsed: 74.439309ms
May 21 19:17:29.635: INFO: Pod "pod-secrets-35fc0184-5735-4def-887e-49a0f4ac0dff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080312058s
May 21 19:17:31.643: INFO: Pod "pod-secrets-35fc0184-5735-4def-887e-49a0f4ac0dff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089027212s
May 21 19:17:33.653: INFO: Pod "pod-secrets-35fc0184-5735-4def-887e-49a0f4ac0dff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098660216s
STEP: Saw pod success
May 21 19:17:33.653: INFO: Pod "pod-secrets-35fc0184-5735-4def-887e-49a0f4ac0dff" satisfied condition "Succeeded or Failed"
May 21 19:17:33.663: INFO: Trying to get logs from node dc-hg-1 pod pod-secrets-35fc0184-5735-4def-887e-49a0f4ac0dff container secret-volume-test: <nil>
STEP: delete the pod
May 21 19:17:33.757: INFO: Waiting for pod pod-secrets-35fc0184-5735-4def-887e-49a0f4ac0dff to disappear
May 21 19:17:33.766: INFO: Pod pod-secrets-35fc0184-5735-4def-887e-49a0f4ac0dff no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:17:33.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5571" for this suite.

• [SLOW TEST:6.406 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":85,"skipped":1311,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:17:33.792: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3976.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3976.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 19:17:39.989: INFO: DNS probes using dns-3976/dns-test-b788adf6-16d8-49dd-be7b-6c0f47a1244f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:17:40.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3976" for this suite.

• [SLOW TEST:6.282 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":86,"skipped":1317,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:17:40.075: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:17:48.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1299" for this suite.

• [SLOW TEST:8.158 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1320,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:17:48.233: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:17:48.385: INFO: Waiting up to 5m0s for pod "downwardapi-volume-327d3cd5-78bd-4af3-bee6-8dbe6e4df2e0" in namespace "downward-api-1282" to be "Succeeded or Failed"
May 21 19:17:48.403: INFO: Pod "downwardapi-volume-327d3cd5-78bd-4af3-bee6-8dbe6e4df2e0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.110253ms
May 21 19:17:50.410: INFO: Pod "downwardapi-volume-327d3cd5-78bd-4af3-bee6-8dbe6e4df2e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025764938s
May 21 19:17:52.418: INFO: Pod "downwardapi-volume-327d3cd5-78bd-4af3-bee6-8dbe6e4df2e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033205994s
STEP: Saw pod success
May 21 19:17:52.418: INFO: Pod "downwardapi-volume-327d3cd5-78bd-4af3-bee6-8dbe6e4df2e0" satisfied condition "Succeeded or Failed"
May 21 19:17:52.424: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-327d3cd5-78bd-4af3-bee6-8dbe6e4df2e0 container client-container: <nil>
STEP: delete the pod
May 21 19:17:52.462: INFO: Waiting for pod downwardapi-volume-327d3cd5-78bd-4af3-bee6-8dbe6e4df2e0 to disappear
May 21 19:17:52.508: INFO: Pod downwardapi-volume-327d3cd5-78bd-4af3-bee6-8dbe6e4df2e0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:17:52.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1282" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":88,"skipped":1327,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:17:52.528: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5431
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
May 21 19:17:52.688: INFO: Found 0 stateful pods, waiting for 3
May 21 19:18:02.698: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:18:02.698: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:18:02.699: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 21 19:18:12.699: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:18:12.699: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:18:12.699: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
May 21 19:18:12.758: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 21 19:18:22.833: INFO: Updating stateful set ss2
May 21 19:18:22.845: INFO: Waiting for Pod statefulset-5431/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
STEP: Restoring Pods to the correct revision when they are deleted
May 21 19:18:33.291: INFO: Found 2 stateful pods, waiting for 3
May 21 19:18:43.300: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:18:43.300: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:18:43.300: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 21 19:18:43.345: INFO: Updating stateful set ss2
May 21 19:18:43.372: INFO: Waiting for Pod statefulset-5431/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 19:18:53.419: INFO: Updating stateful set ss2
May 21 19:18:53.450: INFO: Waiting for StatefulSet statefulset-5431/ss2 to complete update
May 21 19:18:53.450: INFO: Waiting for Pod statefulset-5431/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 19:19:03.464: INFO: Waiting for StatefulSet statefulset-5431/ss2 to complete update
May 21 19:19:03.464: INFO: Waiting for Pod statefulset-5431/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 19:19:13.470: INFO: Waiting for StatefulSet statefulset-5431/ss2 to complete update
May 21 19:19:13.470: INFO: Waiting for Pod statefulset-5431/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 19:19:23.466: INFO: Waiting for StatefulSet statefulset-5431/ss2 to complete update
May 21 19:19:33.466: INFO: Waiting for StatefulSet statefulset-5431/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 19:19:43.467: INFO: Deleting all statefulset in ns statefulset-5431
May 21 19:19:43.486: INFO: Scaling statefulset ss2 to 0
May 21 19:20:23.537: INFO: Waiting for statefulset status.replicas updated to 0
May 21 19:20:23.543: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:20:23.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5431" for this suite.

• [SLOW TEST:151.166 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":89,"skipped":1335,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:20:23.695: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 21 19:20:23.834: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-a 9f8a637d-8286-4da6-8597-5d85d5f4b360 1271343 0 2021-05-21 19:20:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 19:20:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:20:23.834: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-a 9f8a637d-8286-4da6-8597-5d85d5f4b360 1271343 0 2021-05-21 19:20:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 19:20:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 21 19:20:33.848: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-a 9f8a637d-8286-4da6-8597-5d85d5f4b360 1271419 0 2021-05-21 19:20:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 19:20:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:20:33.848: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-a 9f8a637d-8286-4da6-8597-5d85d5f4b360 1271419 0 2021-05-21 19:20:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 19:20:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 21 19:20:43.862: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-a 9f8a637d-8286-4da6-8597-5d85d5f4b360 1271426 0 2021-05-21 19:20:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 19:20:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:20:43.863: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-a 9f8a637d-8286-4da6-8597-5d85d5f4b360 1271426 0 2021-05-21 19:20:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 19:20:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 21 19:20:53.877: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-a 9f8a637d-8286-4da6-8597-5d85d5f4b360 1271432 0 2021-05-21 19:20:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 19:20:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:20:53.878: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-a 9f8a637d-8286-4da6-8597-5d85d5f4b360 1271432 0 2021-05-21 19:20:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-21 19:20:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 21 19:21:03.898: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-b 296b3da0-3e63-4305-87f8-c42b3fcc531b 1271439 0 2021-05-21 19:21:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 19:21:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:21:03.899: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-b 296b3da0-3e63-4305-87f8-c42b3fcc531b 1271439 0 2021-05-21 19:21:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 19:21:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 21 19:21:13.912: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-b 296b3da0-3e63-4305-87f8-c42b3fcc531b 1271446 0 2021-05-21 19:21:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 19:21:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 19:21:13.912: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5026 /api/v1/namespaces/watch-5026/configmaps/e2e-watch-test-configmap-b 296b3da0-3e63-4305-87f8-c42b3fcc531b 1271446 0 2021-05-21 19:21:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-21 19:21:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:21:23.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5026" for this suite.

• [SLOW TEST:60.264 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":90,"skipped":1365,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:21:23.960: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:21:26.530: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 19:21:28.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221686, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221686, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221686, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757221686, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:21:31.570: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:21:41.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5056" for this suite.
STEP: Destroying namespace "webhook-5056-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.156 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":91,"skipped":1374,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:21:42.117: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:21:53.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7327" for this suite.

• [SLOW TEST:11.297 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":92,"skipped":1376,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:21:53.415: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
May 21 19:21:53.510: INFO: Waiting up to 5m0s for pod "client-containers-d62e13ee-0fac-488e-adaa-1e9e1b411ec6" in namespace "containers-9527" to be "Succeeded or Failed"
May 21 19:21:53.541: INFO: Pod "client-containers-d62e13ee-0fac-488e-adaa-1e9e1b411ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 30.487656ms
May 21 19:21:55.569: INFO: Pod "client-containers-d62e13ee-0fac-488e-adaa-1e9e1b411ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0585985s
May 21 19:21:57.578: INFO: Pod "client-containers-d62e13ee-0fac-488e-adaa-1e9e1b411ec6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068139638s
STEP: Saw pod success
May 21 19:21:57.579: INFO: Pod "client-containers-d62e13ee-0fac-488e-adaa-1e9e1b411ec6" satisfied condition "Succeeded or Failed"
May 21 19:21:57.585: INFO: Trying to get logs from node dc-hg-1 pod client-containers-d62e13ee-0fac-488e-adaa-1e9e1b411ec6 container test-container: <nil>
STEP: delete the pod
May 21 19:21:57.723: INFO: Waiting for pod client-containers-d62e13ee-0fac-488e-adaa-1e9e1b411ec6 to disappear
May 21 19:21:57.736: INFO: Pod client-containers-d62e13ee-0fac-488e-adaa-1e9e1b411ec6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:21:57.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9527" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1385,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:21:57.780: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
May 21 19:21:57.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-5164 api-versions'
May 21 19:21:58.124: INFO: stderr: ""
May 21 19:21:58.124: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.kiali.io/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:21:58.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5164" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":94,"skipped":1390,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:21:58.149: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1664
STEP: creating service affinity-clusterip in namespace services-1664
STEP: creating replication controller affinity-clusterip in namespace services-1664
I0521 19:21:58.263143      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-1664, replica count: 3
I0521 19:22:01.313961      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 19:22:04.314381      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 19:22:04.326: INFO: Creating new exec pod
May 21 19:22:09.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-1664 exec execpod-affinityw8qt6 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
May 21 19:22:10.533: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 21 19:22:10.533: INFO: stdout: ""
May 21 19:22:10.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-1664 exec execpod-affinityw8qt6 -- /bin/sh -x -c nc -zv -t -w 2 10.10.31.15 80'
May 21 19:22:11.131: INFO: stderr: "+ nc -zv -t -w 2 10.10.31.15 80\nConnection to 10.10.31.15 80 port [tcp/http] succeeded!\n"
May 21 19:22:11.131: INFO: stdout: ""
May 21 19:22:11.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-1664 exec execpod-affinityw8qt6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.31.15:80/ ; done'
May 21 19:22:11.984: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.31.15:80/\n"
May 21 19:22:11.984: INFO: stdout: "\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc\naffinity-clusterip-7q7qc"
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Received response from host: affinity-clusterip-7q7qc
May 21 19:22:11.984: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1664, will wait for the garbage collector to delete the pods
May 21 19:22:12.101: INFO: Deleting ReplicationController affinity-clusterip took: 16.708828ms
May 21 19:22:12.804: INFO: Terminating ReplicationController affinity-clusterip pods took: 702.744432ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:22:28.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1664" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:30.670 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":95,"skipped":1392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:22:28.830: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:22:29.009: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d73a8783-2a84-4bf6-bd00-54dee0629c7d" in namespace "projected-1023" to be "Succeeded or Failed"
May 21 19:22:29.032: INFO: Pod "downwardapi-volume-d73a8783-2a84-4bf6-bd00-54dee0629c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 23.133528ms
May 21 19:22:31.040: INFO: Pod "downwardapi-volume-d73a8783-2a84-4bf6-bd00-54dee0629c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030786306s
May 21 19:22:33.048: INFO: Pod "downwardapi-volume-d73a8783-2a84-4bf6-bd00-54dee0629c7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038723255s
STEP: Saw pod success
May 21 19:22:33.048: INFO: Pod "downwardapi-volume-d73a8783-2a84-4bf6-bd00-54dee0629c7d" satisfied condition "Succeeded or Failed"
May 21 19:22:33.054: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-d73a8783-2a84-4bf6-bd00-54dee0629c7d container client-container: <nil>
STEP: delete the pod
May 21 19:22:33.149: INFO: Waiting for pod downwardapi-volume-d73a8783-2a84-4bf6-bd00-54dee0629c7d to disappear
May 21 19:22:33.155: INFO: Pod downwardapi-volume-d73a8783-2a84-4bf6-bd00-54dee0629c7d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:22:33.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1023" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":96,"skipped":1435,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:22:33.186: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:22:33.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-936 version'
May 21 19:22:33.568: INFO: stderr: ""
May 21 19:22:33.568: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.6\", GitCommit:\"fbf646b339dc52336b55d8ec85c181981b86331a\", GitTreeState:\"clean\", BuildDate:\"2020-12-18T12:09:30Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.6\", GitCommit:\"fbf646b339dc52336b55d8ec85c181981b86331a\", GitTreeState:\"clean\", BuildDate:\"2020-12-18T12:01:36Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:22:33.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-936" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":97,"skipped":1442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:22:33.593: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-kjrq
STEP: Creating a pod to test atomic-volume-subpath
May 21 19:22:33.788: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-kjrq" in namespace "subpath-869" to be "Succeeded or Failed"
May 21 19:22:33.809: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Pending", Reason="", readiness=false. Elapsed: 20.797884ms
May 21 19:22:35.824: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035533015s
May 21 19:22:37.829: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Running", Reason="", readiness=true. Elapsed: 4.041009963s
May 21 19:22:39.837: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Running", Reason="", readiness=true. Elapsed: 6.049051166s
May 21 19:22:41.844: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Running", Reason="", readiness=true. Elapsed: 8.055518762s
May 21 19:22:43.850: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Running", Reason="", readiness=true. Elapsed: 10.062232796s
May 21 19:22:45.857: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Running", Reason="", readiness=true. Elapsed: 12.068733759s
May 21 19:22:47.865: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Running", Reason="", readiness=true. Elapsed: 14.076923838s
May 21 19:22:49.871: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Running", Reason="", readiness=true. Elapsed: 16.083223055s
May 21 19:22:51.881: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Running", Reason="", readiness=true. Elapsed: 18.092743453s
May 21 19:22:53.889: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Running", Reason="", readiness=true. Elapsed: 20.100952823s
May 21 19:22:55.896: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Running", Reason="", readiness=true. Elapsed: 22.107842026s
May 21 19:22:57.905: INFO: Pod "pod-subpath-test-projected-kjrq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.117290543s
STEP: Saw pod success
May 21 19:22:57.905: INFO: Pod "pod-subpath-test-projected-kjrq" satisfied condition "Succeeded or Failed"
May 21 19:22:57.912: INFO: Trying to get logs from node dc-hg-1 pod pod-subpath-test-projected-kjrq container test-container-subpath-projected-kjrq: <nil>
STEP: delete the pod
May 21 19:22:58.071: INFO: Waiting for pod pod-subpath-test-projected-kjrq to disappear
May 21 19:22:58.077: INFO: Pod pod-subpath-test-projected-kjrq no longer exists
STEP: Deleting pod pod-subpath-test-projected-kjrq
May 21 19:22:58.077: INFO: Deleting pod "pod-subpath-test-projected-kjrq" in namespace "subpath-869"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:22:58.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-869" for this suite.

• [SLOW TEST:24.507 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":98,"skipped":1494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:22:58.102: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 21 19:22:58.185: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 19:23:58.236: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:23:58.243: INFO: Starting informer...
STEP: Starting pods...
May 21 19:23:58.301: INFO: Pod1 is running on dc-hg-1. Tainting Node
May 21 19:24:02.556: INFO: Pod2 is running on dc-hg-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 21 19:24:14.763: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 21 19:24:34.763: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:24:34.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3769" for this suite.

• [SLOW TEST:96.730 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":99,"skipped":1576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:24:34.834: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-qxpg
STEP: Creating a pod to test atomic-volume-subpath
May 21 19:24:34.954: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qxpg" in namespace "subpath-3736" to be "Succeeded or Failed"
May 21 19:24:34.965: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.403652ms
May 21 19:24:36.973: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019437996s
May 21 19:24:38.980: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026042913s
May 21 19:24:40.987: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Running", Reason="", readiness=true. Elapsed: 6.033511989s
May 21 19:24:42.994: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Running", Reason="", readiness=true. Elapsed: 8.040670149s
May 21 19:24:45.000: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Running", Reason="", readiness=true. Elapsed: 10.046169332s
May 21 19:24:47.014: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Running", Reason="", readiness=true. Elapsed: 12.059949917s
May 21 19:24:49.020: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Running", Reason="", readiness=true. Elapsed: 14.066625457s
May 21 19:24:51.027: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Running", Reason="", readiness=true. Elapsed: 16.07366377s
May 21 19:24:53.050: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Running", Reason="", readiness=true. Elapsed: 18.096656488s
May 21 19:24:55.058: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Running", Reason="", readiness=true. Elapsed: 20.10416121s
May 21 19:24:57.065: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Running", Reason="", readiness=true. Elapsed: 22.111468126s
May 21 19:24:59.072: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Running", Reason="", readiness=true. Elapsed: 24.117695094s
May 21 19:25:01.078: INFO: Pod "pod-subpath-test-configmap-qxpg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.124688979s
STEP: Saw pod success
May 21 19:25:01.079: INFO: Pod "pod-subpath-test-configmap-qxpg" satisfied condition "Succeeded or Failed"
May 21 19:25:01.083: INFO: Trying to get logs from node dc-hg-1 pod pod-subpath-test-configmap-qxpg container test-container-subpath-configmap-qxpg: <nil>
STEP: delete the pod
May 21 19:25:01.157: INFO: Waiting for pod pod-subpath-test-configmap-qxpg to disappear
May 21 19:25:01.189: INFO: Pod pod-subpath-test-configmap-qxpg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qxpg
May 21 19:25:01.189: INFO: Deleting pod "pod-subpath-test-configmap-qxpg" in namespace "subpath-3736"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:25:01.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3736" for this suite.

• [SLOW TEST:26.388 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":100,"skipped":1612,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:25:01.241: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-06eca210-b52f-4a8b-aa2a-c5865fbd3148 in namespace container-probe-2
May 21 19:25:05.353: INFO: Started pod test-webserver-06eca210-b52f-4a8b-aa2a-c5865fbd3148 in namespace container-probe-2
STEP: checking the pod's current state and verifying that restartCount is present
May 21 19:25:05.358: INFO: Initial restart count of pod test-webserver-06eca210-b52f-4a8b-aa2a-c5865fbd3148 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:29:06.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2" for this suite.

• [SLOW TEST:245.151 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":101,"skipped":1624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:29:06.394: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
May 21 19:29:06.601: INFO: Waiting up to 5m0s for pod "client-containers-cd00eaad-bd32-40ec-80f2-c9ae2c244331" in namespace "containers-6596" to be "Succeeded or Failed"
May 21 19:29:06.622: INFO: Pod "client-containers-cd00eaad-bd32-40ec-80f2-c9ae2c244331": Phase="Pending", Reason="", readiness=false. Elapsed: 20.794441ms
May 21 19:29:08.643: INFO: Pod "client-containers-cd00eaad-bd32-40ec-80f2-c9ae2c244331": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041824803s
May 21 19:29:10.650: INFO: Pod "client-containers-cd00eaad-bd32-40ec-80f2-c9ae2c244331": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049521377s
STEP: Saw pod success
May 21 19:29:10.651: INFO: Pod "client-containers-cd00eaad-bd32-40ec-80f2-c9ae2c244331" satisfied condition "Succeeded or Failed"
May 21 19:29:10.661: INFO: Trying to get logs from node dc-hg-1 pod client-containers-cd00eaad-bd32-40ec-80f2-c9ae2c244331 container test-container: <nil>
STEP: delete the pod
May 21 19:29:10.778: INFO: Waiting for pod client-containers-cd00eaad-bd32-40ec-80f2-c9ae2c244331 to disappear
May 21 19:29:10.788: INFO: Pod client-containers-cd00eaad-bd32-40ec-80f2-c9ae2c244331 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:29:10.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6596" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":102,"skipped":1672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:29:10.804: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
May 21 19:29:10.903: INFO: created test-event-1
May 21 19:29:10.912: INFO: created test-event-2
May 21 19:29:10.922: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 21 19:29:10.929: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 21 19:29:10.960: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:29:10.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9293" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":103,"skipped":1715,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:29:10.984: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:29:11.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1027" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":104,"skipped":1724,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:29:11.201: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 21 19:29:12.007: INFO: Pod name wrapped-volume-race-a4f3e207-f6f1-4c0f-a291-026d93b85ff1: Found 0 pods out of 5
May 21 19:29:17.028: INFO: Pod name wrapped-volume-race-a4f3e207-f6f1-4c0f-a291-026d93b85ff1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a4f3e207-f6f1-4c0f-a291-026d93b85ff1 in namespace emptydir-wrapper-9062, will wait for the garbage collector to delete the pods
May 21 19:29:29.143: INFO: Deleting ReplicationController wrapped-volume-race-a4f3e207-f6f1-4c0f-a291-026d93b85ff1 took: 18.839871ms
May 21 19:29:29.843: INFO: Terminating ReplicationController wrapped-volume-race-a4f3e207-f6f1-4c0f-a291-026d93b85ff1 pods took: 700.461991ms
STEP: Creating RC which spawns configmap-volume pods
May 21 19:29:45.323: INFO: Pod name wrapped-volume-race-5d87dfb9-aeae-4cf1-b23f-ddf6a60a6c8c: Found 0 pods out of 5
May 21 19:29:50.348: INFO: Pod name wrapped-volume-race-5d87dfb9-aeae-4cf1-b23f-ddf6a60a6c8c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5d87dfb9-aeae-4cf1-b23f-ddf6a60a6c8c in namespace emptydir-wrapper-9062, will wait for the garbage collector to delete the pods
May 21 19:30:04.550: INFO: Deleting ReplicationController wrapped-volume-race-5d87dfb9-aeae-4cf1-b23f-ddf6a60a6c8c took: 13.622315ms
May 21 19:30:05.251: INFO: Terminating ReplicationController wrapped-volume-race-5d87dfb9-aeae-4cf1-b23f-ddf6a60a6c8c pods took: 701.091271ms
STEP: Creating RC which spawns configmap-volume pods
May 21 19:30:19.033: INFO: Pod name wrapped-volume-race-00541e28-65fe-4968-8ec3-f82d4a24f77a: Found 0 pods out of 5
May 21 19:30:24.073: INFO: Pod name wrapped-volume-race-00541e28-65fe-4968-8ec3-f82d4a24f77a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-00541e28-65fe-4968-8ec3-f82d4a24f77a in namespace emptydir-wrapper-9062, will wait for the garbage collector to delete the pods
May 21 19:30:40.217: INFO: Deleting ReplicationController wrapped-volume-race-00541e28-65fe-4968-8ec3-f82d4a24f77a took: 19.215261ms
May 21 19:30:40.918: INFO: Terminating ReplicationController wrapped-volume-race-00541e28-65fe-4968-8ec3-f82d4a24f77a pods took: 700.360431ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:30:59.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9062" for this suite.

• [SLOW TEST:108.431 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":105,"skipped":1725,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:30:59.633: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 21 19:31:04.341: INFO: Successfully updated pod "adopt-release-2wt5j"
STEP: Checking that the Job readopts the Pod
May 21 19:31:04.341: INFO: Waiting up to 15m0s for pod "adopt-release-2wt5j" in namespace "job-7328" to be "adopted"
May 21 19:31:04.362: INFO: Pod "adopt-release-2wt5j": Phase="Running", Reason="", readiness=true. Elapsed: 21.114021ms
May 21 19:31:06.381: INFO: Pod "adopt-release-2wt5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.03963077s
May 21 19:31:06.381: INFO: Pod "adopt-release-2wt5j" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 21 19:31:06.907: INFO: Successfully updated pod "adopt-release-2wt5j"
STEP: Checking that the Job releases the Pod
May 21 19:31:06.907: INFO: Waiting up to 15m0s for pod "adopt-release-2wt5j" in namespace "job-7328" to be "released"
May 21 19:31:06.912: INFO: Pod "adopt-release-2wt5j": Phase="Running", Reason="", readiness=true. Elapsed: 5.037996ms
May 21 19:31:08.925: INFO: Pod "adopt-release-2wt5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.018601239s
May 21 19:31:08.925: INFO: Pod "adopt-release-2wt5j" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:31:08.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7328" for this suite.

• [SLOW TEST:9.318 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":106,"skipped":1730,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:31:08.951: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-e107d563-3709-40ac-89f2-72d9d2b45482
STEP: Creating a pod to test consume configMaps
May 21 19:31:09.095: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5487761a-a209-4cc8-8f2f-3a09c8f2e059" in namespace "projected-5334" to be "Succeeded or Failed"
May 21 19:31:09.108: INFO: Pod "pod-projected-configmaps-5487761a-a209-4cc8-8f2f-3a09c8f2e059": Phase="Pending", Reason="", readiness=false. Elapsed: 13.137526ms
May 21 19:31:11.115: INFO: Pod "pod-projected-configmaps-5487761a-a209-4cc8-8f2f-3a09c8f2e059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020468298s
May 21 19:31:13.124: INFO: Pod "pod-projected-configmaps-5487761a-a209-4cc8-8f2f-3a09c8f2e059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02887034s
STEP: Saw pod success
May 21 19:31:13.124: INFO: Pod "pod-projected-configmaps-5487761a-a209-4cc8-8f2f-3a09c8f2e059" satisfied condition "Succeeded or Failed"
May 21 19:31:13.129: INFO: Trying to get logs from node dc-hg-2 pod pod-projected-configmaps-5487761a-a209-4cc8-8f2f-3a09c8f2e059 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 19:31:13.211: INFO: Waiting for pod pod-projected-configmaps-5487761a-a209-4cc8-8f2f-3a09c8f2e059 to disappear
May 21 19:31:13.219: INFO: Pod pod-projected-configmaps-5487761a-a209-4cc8-8f2f-3a09c8f2e059 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:31:13.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5334" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1734,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:31:13.240: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 21 19:31:17.923: INFO: Successfully updated pod "labelsupdatecb90a92c-0cbc-42e0-af2e-0d43f0ea016b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:31:19.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8790" for this suite.

• [SLOW TEST:6.810 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:31:20.050: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-1852ec0e-9504-45bb-b403-5faa34acd0c0
STEP: Creating a pod to test consume configMaps
May 21 19:31:20.264: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3bc2edd6-1355-4491-ad26-54565a719b66" in namespace "projected-6412" to be "Succeeded or Failed"
May 21 19:31:20.269: INFO: Pod "pod-projected-configmaps-3bc2edd6-1355-4491-ad26-54565a719b66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.993953ms
May 21 19:31:22.275: INFO: Pod "pod-projected-configmaps-3bc2edd6-1355-4491-ad26-54565a719b66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011308897s
May 21 19:31:24.290: INFO: Pod "pod-projected-configmaps-3bc2edd6-1355-4491-ad26-54565a719b66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026069535s
STEP: Saw pod success
May 21 19:31:24.290: INFO: Pod "pod-projected-configmaps-3bc2edd6-1355-4491-ad26-54565a719b66" satisfied condition "Succeeded or Failed"
May 21 19:31:24.295: INFO: Trying to get logs from node dc-hg-1 pod pod-projected-configmaps-3bc2edd6-1355-4491-ad26-54565a719b66 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 19:31:24.368: INFO: Waiting for pod pod-projected-configmaps-3bc2edd6-1355-4491-ad26-54565a719b66 to disappear
May 21 19:31:24.424: INFO: Pod pod-projected-configmaps-3bc2edd6-1355-4491-ad26-54565a719b66 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:31:24.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6412" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":109,"skipped":1771,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:31:24.449: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:31:24.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7560" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":110,"skipped":1786,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:31:24.654: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 21 19:31:32.872: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 19:31:32.879: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 19:31:34.880: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 19:31:34.886: INFO: Pod pod-with-poststart-exec-hook still exists
May 21 19:31:36.880: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 21 19:31:36.886: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:31:36.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3637" for this suite.

• [SLOW TEST:12.247 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":111,"skipped":1790,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:31:36.903: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-887e23f7-f467-43fd-b9cd-746b913ddef4
STEP: Creating a pod to test consume configMaps
May 21 19:31:37.012: INFO: Waiting up to 5m0s for pod "pod-configmaps-3806bc4e-1eb1-4cda-b3b0-797060bcb93a" in namespace "configmap-1739" to be "Succeeded or Failed"
May 21 19:31:37.019: INFO: Pod "pod-configmaps-3806bc4e-1eb1-4cda-b3b0-797060bcb93a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.321952ms
May 21 19:31:39.030: INFO: Pod "pod-configmaps-3806bc4e-1eb1-4cda-b3b0-797060bcb93a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017833628s
May 21 19:31:41.035: INFO: Pod "pod-configmaps-3806bc4e-1eb1-4cda-b3b0-797060bcb93a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023395887s
STEP: Saw pod success
May 21 19:31:41.035: INFO: Pod "pod-configmaps-3806bc4e-1eb1-4cda-b3b0-797060bcb93a" satisfied condition "Succeeded or Failed"
May 21 19:31:41.039: INFO: Trying to get logs from node dc-hg-2 pod pod-configmaps-3806bc4e-1eb1-4cda-b3b0-797060bcb93a container configmap-volume-test: <nil>
STEP: delete the pod
May 21 19:31:41.084: INFO: Waiting for pod pod-configmaps-3806bc4e-1eb1-4cda-b3b0-797060bcb93a to disappear
May 21 19:31:41.090: INFO: Pod pod-configmaps-3806bc4e-1eb1-4cda-b3b0-797060bcb93a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:31:41.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1739" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":112,"skipped":1814,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:31:41.111: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:31:52.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4882" for this suite.

• [SLOW TEST:11.190 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":113,"skipped":1820,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:31:52.302: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:31:52.377: INFO: Creating ReplicaSet my-hostname-basic-ac05fa63-d34e-47c8-920f-7a89ac1205cc
May 21 19:31:52.400: INFO: Pod name my-hostname-basic-ac05fa63-d34e-47c8-920f-7a89ac1205cc: Found 0 pods out of 1
May 21 19:31:57.411: INFO: Pod name my-hostname-basic-ac05fa63-d34e-47c8-920f-7a89ac1205cc: Found 1 pods out of 1
May 21 19:31:57.411: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ac05fa63-d34e-47c8-920f-7a89ac1205cc" is running
May 21 19:31:57.419: INFO: Pod "my-hostname-basic-ac05fa63-d34e-47c8-920f-7a89ac1205cc-d2br4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 19:31:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 19:31:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 19:31:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-21 19:31:52 +0000 UTC Reason: Message:}])
May 21 19:31:57.422: INFO: Trying to dial the pod
May 21 19:32:02.439: INFO: Controller my-hostname-basic-ac05fa63-d34e-47c8-920f-7a89ac1205cc: Got expected result from replica 1 [my-hostname-basic-ac05fa63-d34e-47c8-920f-7a89ac1205cc-d2br4]: "my-hostname-basic-ac05fa63-d34e-47c8-920f-7a89ac1205cc-d2br4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:32:02.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3904" for this suite.

• [SLOW TEST:10.162 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":114,"skipped":1833,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:32:02.466: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1385
STEP: creating an pod
May 21 19:32:02.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-530 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 21 19:32:02.906: INFO: stderr: ""
May 21 19:32:02.906: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
May 21 19:32:02.906: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 21 19:32:02.906: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-530" to be "running and ready, or succeeded"
May 21 19:32:02.911: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.93249ms
May 21 19:32:04.917: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01106321s
May 21 19:32:06.926: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.01983021s
May 21 19:32:06.926: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 21 19:32:06.926: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 21 19:32:06.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-530 logs logs-generator logs-generator'
May 21 19:32:07.352: INFO: stderr: ""
May 21 19:32:07.352: INFO: stdout: "I0521 19:32:05.169297       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/c879 294\nI0521 19:32:05.369450       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/szrf 288\nI0521 19:32:05.569518       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/mr8r 211\nI0521 19:32:05.769482       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/g2g 217\nI0521 19:32:05.970168       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/tnp 593\nI0521 19:32:06.169536       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/krn 326\nI0521 19:32:06.369493       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/ncs5 464\nI0521 19:32:06.569454       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/c9w 279\nI0521 19:32:06.771196       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/b69n 435\nI0521 19:32:06.969484       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/xb7s 206\nI0521 19:32:07.169415       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/965 513\n"
STEP: limiting log lines
May 21 19:32:07.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-530 logs logs-generator logs-generator --tail=1'
May 21 19:32:07.781: INFO: stderr: ""
May 21 19:32:07.781: INFO: stdout: "I0521 19:32:07.569531       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/8xz7 461\n"
May 21 19:32:07.781: INFO: got output "I0521 19:32:07.569531       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/8xz7 461\n"
STEP: limiting log bytes
May 21 19:32:07.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-530 logs logs-generator logs-generator --limit-bytes=1'
May 21 19:32:08.226: INFO: stderr: ""
May 21 19:32:08.226: INFO: stdout: "I"
May 21 19:32:08.227: INFO: got output "I"
STEP: exposing timestamps
May 21 19:32:08.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-530 logs logs-generator logs-generator --tail=1 --timestamps'
May 21 19:32:08.546: INFO: stderr: ""
May 21 19:32:08.546: INFO: stdout: "2021-05-21T19:32:08.374299326Z I0521 19:32:08.369724       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/r5j 369\n"
May 21 19:32:08.547: INFO: got output "2021-05-21T19:32:08.374299326Z I0521 19:32:08.369724       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/r5j 369\n"
STEP: restricting to a time range
May 21 19:32:11.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-530 logs logs-generator logs-generator --since=1s'
May 21 19:32:11.903: INFO: stderr: ""
May 21 19:32:11.903: INFO: stdout: "I0521 19:32:10.969444       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/2pjn 332\nI0521 19:32:11.169499       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/n7r2 304\nI0521 19:32:11.369444       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/8h2 258\nI0521 19:32:11.569460       1 logs_generator.go:76] 32 GET /api/v1/namespaces/ns/pods/69qr 574\nI0521 19:32:11.769536       1 logs_generator.go:76] 33 GET /api/v1/namespaces/ns/pods/qzn5 471\n"
May 21 19:32:11.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-530 logs logs-generator logs-generator --since=24h'
May 21 19:32:12.266: INFO: stderr: ""
May 21 19:32:12.266: INFO: stdout: "I0521 19:32:05.169297       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/c879 294\nI0521 19:32:05.369450       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/szrf 288\nI0521 19:32:05.569518       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/mr8r 211\nI0521 19:32:05.769482       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/g2g 217\nI0521 19:32:05.970168       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/tnp 593\nI0521 19:32:06.169536       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/krn 326\nI0521 19:32:06.369493       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/ncs5 464\nI0521 19:32:06.569454       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/c9w 279\nI0521 19:32:06.771196       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/b69n 435\nI0521 19:32:06.969484       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/xb7s 206\nI0521 19:32:07.169415       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/965 513\nI0521 19:32:07.369510       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/qhv 312\nI0521 19:32:07.569531       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/8xz7 461\nI0521 19:32:07.769372       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/5m8 308\nI0521 19:32:07.969477       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/gwl 449\nI0521 19:32:08.171260       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/vqq 278\nI0521 19:32:08.369724       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/r5j 369\nI0521 19:32:08.569456       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/mwgk 393\nI0521 19:32:08.771107       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/bwrq 466\nI0521 19:32:08.969515       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/c7k 592\nI0521 19:32:09.169537       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/pzr 511\nI0521 19:32:09.369517       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/6txb 293\nI0521 19:32:09.569431       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/7kj 405\nI0521 19:32:09.769445       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/9s2 595\nI0521 19:32:09.969364       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/pd5m 582\nI0521 19:32:10.169456       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/dvg4 401\nI0521 19:32:10.369457       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/nr86 594\nI0521 19:32:10.569544       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/7tw 509\nI0521 19:32:10.769448       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/gxtj 338\nI0521 19:32:10.969444       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/2pjn 332\nI0521 19:32:11.169499       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/n7r2 304\nI0521 19:32:11.369444       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/8h2 258\nI0521 19:32:11.569460       1 logs_generator.go:76] 32 GET /api/v1/namespaces/ns/pods/69qr 574\nI0521 19:32:11.769536       1 logs_generator.go:76] 33 GET /api/v1/namespaces/ns/pods/qzn5 471\nI0521 19:32:11.969457       1 logs_generator.go:76] 34 POST /api/v1/namespaces/default/pods/5vp 348\nI0521 19:32:12.171181       1 logs_generator.go:76] 35 PUT /api/v1/namespaces/ns/pods/jjpn 473\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
May 21 19:32:12.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-530 delete pod logs-generator'
May 21 19:32:24.778: INFO: stderr: ""
May 21 19:32:24.778: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:32:24.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-530" for this suite.

• [SLOW TEST:22.337 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":115,"skipped":1839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:32:24.803: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
May 21 19:32:24.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-8380 create -f -'
May 21 19:32:25.629: INFO: stderr: ""
May 21 19:32:25.629: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 19:32:26.636: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 19:32:26.636: INFO: Found 0 / 1
May 21 19:32:27.637: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 19:32:27.637: INFO: Found 0 / 1
May 21 19:32:28.636: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 19:32:28.637: INFO: Found 1 / 1
May 21 19:32:28.637: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 21 19:32:28.642: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 19:32:28.642: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 19:32:28.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-8380 patch pod agnhost-primary-66vp4 -p {"metadata":{"annotations":{"x":"y"}}}'
May 21 19:32:28.974: INFO: stderr: ""
May 21 19:32:28.974: INFO: stdout: "pod/agnhost-primary-66vp4 patched\n"
STEP: checking annotations
May 21 19:32:28.990: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 19:32:28.991: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:32:28.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8380" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":116,"skipped":1864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:32:29.038: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:32:29.135: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a12c2ad-8f8d-417a-9fd9-b9e2fad2d98e" in namespace "projected-2095" to be "Succeeded or Failed"
May 21 19:32:29.143: INFO: Pod "downwardapi-volume-5a12c2ad-8f8d-417a-9fd9-b9e2fad2d98e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.66516ms
May 21 19:32:31.150: INFO: Pod "downwardapi-volume-5a12c2ad-8f8d-417a-9fd9-b9e2fad2d98e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015296831s
May 21 19:32:33.157: INFO: Pod "downwardapi-volume-5a12c2ad-8f8d-417a-9fd9-b9e2fad2d98e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022796824s
STEP: Saw pod success
May 21 19:32:33.158: INFO: Pod "downwardapi-volume-5a12c2ad-8f8d-417a-9fd9-b9e2fad2d98e" satisfied condition "Succeeded or Failed"
May 21 19:32:33.164: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-5a12c2ad-8f8d-417a-9fd9-b9e2fad2d98e container client-container: <nil>
STEP: delete the pod
May 21 19:32:33.203: INFO: Waiting for pod downwardapi-volume-5a12c2ad-8f8d-417a-9fd9-b9e2fad2d98e to disappear
May 21 19:32:33.210: INFO: Pod downwardapi-volume-5a12c2ad-8f8d-417a-9fd9-b9e2fad2d98e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:32:33.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2095" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":117,"skipped":1894,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:32:33.226: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 21 19:32:33.315: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 19:32:33.330: INFO: Waiting for terminating namespaces to be deleted...
May 21 19:32:33.335: INFO: 
Logging pods the apiserver thinks is on node conformance1 before test
May 21 19:32:33.348: INFO: haproxy-ingress-85597cc495-l7rzc from ingress-haproxy started at 2021-04-26 19:00:07 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.348: INFO: 	Container haproxy-ingress ready: true, restart count 2
May 21 19:32:33.348: INFO: ingress-default-backend-5f65f5865b-nldzd from ingress-haproxy started at 2021-04-26 19:00:28 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.348: INFO: 	Container ingress-default-backend ready: true, restart count 2
May 21 19:32:33.348: INFO: calico-kube-controllers-db766f945-4228n from kube-system started at 2021-04-26 19:00:07 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.348: INFO: 	Container calico-kube-controllers ready: true, restart count 2
May 21 19:32:33.348: INFO: calico-node-j5hq5 from kube-system started at 2021-04-23 21:19:36 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.348: INFO: 	Container calico-node ready: true, restart count 2
May 21 19:32:33.348: INFO: coredns-c6fc4f979-bv84n from kube-system started at 2021-04-26 19:00:09 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.348: INFO: 	Container coredns ready: true, restart count 2
May 21 19:32:33.348: INFO: coredns-c6fc4f979-jfxfx from kube-system started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.348: INFO: 	Container coredns ready: true, restart count 2
May 21 19:32:33.348: INFO: metrics-server-6c6c6b5d47-9r52q from kube-system started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.348: INFO: 	Container metrics-server ready: true, restart count 2
May 21 19:32:33.348: INFO: nirmata-cni-installer-zjkpx from nirmata started at 2021-04-26 19:00:42 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.348: INFO: 	Container install-cni ready: true, restart count 2
May 21 19:32:33.348: INFO: nirmata-kube-controller-5cf898966b-md6gc from nirmata started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.348: INFO: 	Container nirmata-kube-controller ready: true, restart count 8
May 21 19:32:33.348: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-47fv4 from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 19:32:33.348: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 19:32:33.348: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 19:32:33.348: INFO: 
Logging pods the apiserver thinks is on node dc-hg-1 before test
May 21 19:32:33.359: INFO: calico-node-6r5n7 from kube-system started at 2021-05-21 02:51:05 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.359: INFO: 	Container calico-node ready: true, restart count 0
May 21 19:32:33.359: INFO: agnhost-primary-66vp4 from kubectl-8380 started at 2021-05-21 19:32:25 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.359: INFO: 	Container agnhost-primary ready: true, restart count 0
May 21 19:32:33.359: INFO: nirmata-cni-installer-xzg69 from nirmata started at 2021-05-21 19:24:35 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.359: INFO: 	Container install-cni ready: true, restart count 0
May 21 19:32:33.359: INFO: sonobuoy from sonobuoy started at 2021-05-21 18:56:18 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.359: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 19:32:33.359: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-xch78 from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 19:32:33.359: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 19:32:33.359: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 19:32:33.360: INFO: 
Logging pods the apiserver thinks is on node dc-hg-2 before test
May 21 19:32:33.374: INFO: calico-node-c7rz4 from kube-system started at 2021-04-27 15:00:32 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.374: INFO: 	Container calico-node ready: true, restart count 2
May 21 19:32:33.374: INFO: nirmata-cni-installer-5m9z5 from nirmata started at 2021-05-20 07:33:18 +0000 UTC (1 container statuses recorded)
May 21 19:32:33.375: INFO: 	Container install-cni ready: true, restart count 0
May 21 19:32:33.375: INFO: sonobuoy-e2e-job-03ad7d58f5964dbf from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 19:32:33.375: INFO: 	Container e2e ready: true, restart count 0
May 21 19:32:33.375: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 19:32:33.375: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-lxlxb from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 19:32:33.375: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 19:32:33.375: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1d207750-11ac-47f6-8202-9dadfc106640 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-1d207750-11ac-47f6-8202-9dadfc106640 off the node dc-hg-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1d207750-11ac-47f6-8202-9dadfc106640
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:37:43.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6681" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:310.375 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":118,"skipped":1900,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:37:43.602: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-df5e3a3c-eac4-4697-a159-7204c4b873fe
STEP: Creating a pod to test consume secrets
May 21 19:37:43.796: INFO: Waiting up to 5m0s for pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4" in namespace "secrets-1521" to be "Succeeded or Failed"
May 21 19:37:43.802: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.492984ms
May 21 19:37:45.809: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013344644s
May 21 19:37:47.815: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019525909s
May 21 19:37:49.831: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034912195s
May 21 19:37:51.837: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041565879s
May 21 19:37:53.844: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047908535s
May 21 19:37:55.853: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.05751262s
May 21 19:37:57.859: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.063649028s
May 21 19:37:59.865: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.069420094s
May 21 19:38:01.871: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.075221538s
May 21 19:38:03.877: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.080880419s
May 21 19:38:05.882: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.086633219s
May 21 19:38:07.888: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.091974833s
May 21 19:38:09.898: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.102742415s
May 21 19:38:11.905: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.108951241s
May 21 19:38:13.910: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 30.114583401s
May 21 19:38:15.917: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 32.12107283s
May 21 19:38:17.923: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Pending", Reason="", readiness=false. Elapsed: 34.12685793s
May 21 19:38:19.929: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.132944074s
STEP: Saw pod success
May 21 19:38:19.929: INFO: Pod "pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4" satisfied condition "Succeeded or Failed"
May 21 19:38:19.933: INFO: Trying to get logs from node dc-hg-2 pod pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4 container secret-volume-test: <nil>
STEP: delete the pod
May 21 19:38:20.007: INFO: Waiting for pod pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4 to disappear
May 21 19:38:20.024: INFO: Pod pod-secrets-c70d3c48-9ab0-4910-b4e3-53163efb88e4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:38:20.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1521" for this suite.

• [SLOW TEST:36.437 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":119,"skipped":1906,"failed":0}
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:38:20.040: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
May 21 19:40:20.687: INFO: Successfully updated pod "var-expansion-c818e705-80d7-4367-9958-3bc9c8a5c609"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 21 19:40:22.728: INFO: Deleting pod "var-expansion-c818e705-80d7-4367-9958-3bc9c8a5c609" in namespace "var-expansion-1319"
May 21 19:40:22.750: INFO: Wait up to 5m0s for pod "var-expansion-c818e705-80d7-4367-9958-3bc9c8a5c609" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:04.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1319" for this suite.

• [SLOW TEST:164.762 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":120,"skipped":1906,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:04.803: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:41:04.887: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Pending, waiting for it to be Running (with Ready = true)
May 21 19:41:06.894: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Pending, waiting for it to be Running (with Ready = true)
May 21 19:41:08.897: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = false)
May 21 19:41:10.893: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = false)
May 21 19:41:12.905: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = false)
May 21 19:41:14.898: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = false)
May 21 19:41:16.893: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = false)
May 21 19:41:18.901: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = false)
May 21 19:41:20.894: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = false)
May 21 19:41:22.893: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = false)
May 21 19:41:24.893: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = false)
May 21 19:41:26.894: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = false)
May 21 19:41:28.893: INFO: The status of Pod test-webserver-d01d489a-45d7-4192-9de2-fd729d8caea1 is Running (Ready = true)
May 21 19:41:28.918: INFO: Container started at 2021-05-21 19:41:07 +0000 UTC, pod became ready at 2021-05-21 19:41:28 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:28.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3980" for this suite.

• [SLOW TEST:24.139 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":1926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:28.944: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
May 21 19:41:29.110: INFO: created test-pod-1
May 21 19:41:29.127: INFO: created test-pod-2
May 21 19:41:29.145: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:29.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4717" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":122,"skipped":1995,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:29.348: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 21 19:41:34.043: INFO: Successfully updated pod "pod-update-4b968ced-de16-480f-ac94-a46a1a9a51a4"
STEP: verifying the updated pod is in kubernetes
May 21 19:41:34.061: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:34.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2898" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":1996,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:34.082: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:41:35.423: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 19:41:37.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757222895, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757222895, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757222895, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757222895, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:41:40.479: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:41:40.485: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7064-crds.webhook.example.com via the AdmissionRegistration API
May 21 19:41:41.072: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:41.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5124" for this suite.
STEP: Destroying namespace "webhook-5124-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.126 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":124,"skipped":2006,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:42.209: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-664d35c0-4d8b-4c0d-9235-5bf93f4ea62a
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:42.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2139" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":125,"skipped":2010,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:42.452: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-e97701c3-a30e-42f7-a912-e74b45b37511
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-e97701c3-a30e-42f7-a912-e74b45b37511
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:48.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6280" for this suite.

• [SLOW TEST:6.455 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":126,"skipped":2029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:48.909: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:41:49.030: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-605dd616-0500-4cc1-b0d7-e1816d376198" in namespace "security-context-test-8157" to be "Succeeded or Failed"
May 21 19:41:49.042: INFO: Pod "alpine-nnp-false-605dd616-0500-4cc1-b0d7-e1816d376198": Phase="Pending", Reason="", readiness=false. Elapsed: 12.506724ms
May 21 19:41:51.051: INFO: Pod "alpine-nnp-false-605dd616-0500-4cc1-b0d7-e1816d376198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021766158s
May 21 19:41:53.057: INFO: Pod "alpine-nnp-false-605dd616-0500-4cc1-b0d7-e1816d376198": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027286727s
May 21 19:41:53.057: INFO: Pod "alpine-nnp-false-605dd616-0500-4cc1-b0d7-e1816d376198" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:53.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8157" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":127,"skipped":2052,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:53.128: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:57.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7165" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":128,"skipped":2054,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:57.308: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 19:41:58.989: INFO: starting watch
STEP: patching
STEP: updating
May 21 19:41:59.033: INFO: waiting for watch events with expected annotations
May 21 19:41:59.033: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:59.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9578" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":129,"skipped":2064,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:59.178: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 21 19:41:59.311: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:41:59.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6885" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":130,"skipped":2066,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:41:59.364: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-4230082e-05b7-48ce-bce0-ffee8f7e3ffd in namespace container-probe-9420
May 21 19:42:03.509: INFO: Started pod liveness-4230082e-05b7-48ce-bce0-ffee8f7e3ffd in namespace container-probe-9420
STEP: checking the pod's current state and verifying that restartCount is present
May 21 19:42:03.515: INFO: Initial restart count of pod liveness-4230082e-05b7-48ce-bce0-ffee8f7e3ffd is 0
May 21 19:42:19.599: INFO: Restart count of pod container-probe-9420/liveness-4230082e-05b7-48ce-bce0-ffee8f7e3ffd is now 1 (16.084435635s elapsed)
May 21 19:42:39.669: INFO: Restart count of pod container-probe-9420/liveness-4230082e-05b7-48ce-bce0-ffee8f7e3ffd is now 2 (36.154547628s elapsed)
May 21 19:42:59.744: INFO: Restart count of pod container-probe-9420/liveness-4230082e-05b7-48ce-bce0-ffee8f7e3ffd is now 3 (56.229463906s elapsed)
May 21 19:43:19.813: INFO: Restart count of pod container-probe-9420/liveness-4230082e-05b7-48ce-bce0-ffee8f7e3ffd is now 4 (1m16.298199586s elapsed)
May 21 19:44:24.032: INFO: Restart count of pod container-probe-9420/liveness-4230082e-05b7-48ce-bce0-ffee8f7e3ffd is now 5 (2m20.51701978s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:44:24.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9420" for this suite.

• [SLOW TEST:144.751 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":131,"skipped":2068,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:44:24.115: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 21 19:44:24.208: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 19:45:24.281: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
May 21 19:45:24.331: INFO: Created pod: pod0-sched-preemption-low-priority
May 21 19:45:24.419: INFO: Created pod: pod1-sched-preemption-medium-priority
May 21 19:45:24.447: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:45:42.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2785" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:78.554 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":132,"skipped":2068,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:45:42.671: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 21 19:45:50.859: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6110 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:45:50.859: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:45:51.171: INFO: Exec stderr: ""
May 21 19:45:51.171: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6110 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:45:51.171: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:45:51.437: INFO: Exec stderr: ""
May 21 19:45:51.437: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6110 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:45:51.437: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:45:51.648: INFO: Exec stderr: ""
May 21 19:45:51.648: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6110 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:45:51.648: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:45:51.922: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 21 19:45:51.922: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6110 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:45:51.922: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:45:52.231: INFO: Exec stderr: ""
May 21 19:45:52.231: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6110 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:45:52.231: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:45:52.472: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 21 19:45:52.472: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6110 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:45:52.472: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:45:52.719: INFO: Exec stderr: ""
May 21 19:45:52.719: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6110 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:45:52.719: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:45:52.936: INFO: Exec stderr: ""
May 21 19:45:52.936: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6110 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:45:52.936: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:45:53.178: INFO: Exec stderr: ""
May 21 19:45:53.179: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6110 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 19:45:53.179: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 19:45:53.435: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:45:53.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6110" for this suite.

• [SLOW TEST:10.780 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2084,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:45:53.452: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 21 19:45:53.607: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:46:03.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-783" for this suite.

• [SLOW TEST:9.710 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":134,"skipped":2088,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:46:03.162: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
May 21 19:46:03.292: INFO: Waiting up to 5m0s for pod "pod-9f3a7cb4-e08a-418f-978f-ae5f4d0f165a" in namespace "emptydir-7938" to be "Succeeded or Failed"
May 21 19:46:03.304: INFO: Pod "pod-9f3a7cb4-e08a-418f-978f-ae5f4d0f165a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.651869ms
May 21 19:46:05.310: INFO: Pod "pod-9f3a7cb4-e08a-418f-978f-ae5f4d0f165a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017773347s
May 21 19:46:07.322: INFO: Pod "pod-9f3a7cb4-e08a-418f-978f-ae5f4d0f165a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029955568s
STEP: Saw pod success
May 21 19:46:07.322: INFO: Pod "pod-9f3a7cb4-e08a-418f-978f-ae5f4d0f165a" satisfied condition "Succeeded or Failed"
May 21 19:46:07.332: INFO: Trying to get logs from node dc-hg-2 pod pod-9f3a7cb4-e08a-418f-978f-ae5f4d0f165a container test-container: <nil>
STEP: delete the pod
May 21 19:46:07.414: INFO: Waiting for pod pod-9f3a7cb4-e08a-418f-978f-ae5f4d0f165a to disappear
May 21 19:46:07.423: INFO: Pod pod-9f3a7cb4-e08a-418f-978f-ae5f4d0f165a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:46:07.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7938" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:46:07.456: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9214.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9214.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 19:46:11.641: INFO: DNS probes using dns-test-1399fd69-a827-4c9a-ac82-a6f632a16f6d succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9214.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9214.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 19:46:15.767: INFO: File wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:15.775: INFO: File jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:15.775: INFO: Lookups using dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c failed for: [wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local]

May 21 19:46:20.783: INFO: File wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:20.788: INFO: File jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:20.788: INFO: Lookups using dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c failed for: [wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local]

May 21 19:46:25.785: INFO: File wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:25.796: INFO: File jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:25.796: INFO: Lookups using dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c failed for: [wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local]

May 21 19:46:30.784: INFO: File wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:30.793: INFO: File jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains '' instead of 'bar.example.com.'
May 21 19:46:30.793: INFO: Lookups using dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c failed for: [wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local]

May 21 19:46:35.782: INFO: File wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:35.792: INFO: File jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:35.792: INFO: Lookups using dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c failed for: [wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local]

May 21 19:46:40.782: INFO: File wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:40.787: INFO: File jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local from pod  dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c contains 'foo.example.com.
' instead of 'bar.example.com.'
May 21 19:46:40.787: INFO: Lookups using dns-9214/dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c failed for: [wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local]

May 21 19:46:45.793: INFO: DNS probes using dns-test-1b5d8922-4506-4bac-835a-406cd61c9d6c succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9214.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9214.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9214.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9214.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 19:46:50.109: INFO: DNS probes using dns-test-122342bf-9334-41e5-a22f-8ac3dcf09ac4 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:46:50.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9214" for this suite.

• [SLOW TEST:42.742 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":136,"skipped":2149,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:46:50.198: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2115
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2115
STEP: Creating statefulset with conflicting port in namespace statefulset-2115
STEP: Waiting until pod test-pod will start running in namespace statefulset-2115
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2115
May 21 19:46:56.666: INFO: Observed stateful pod in namespace: statefulset-2115, name: ss-0, uid: 70cd65ba-45e6-455f-91a7-3ffe58a17cb5, status phase: Pending. Waiting for statefulset controller to delete.
May 21 19:46:56.708: INFO: Observed stateful pod in namespace: statefulset-2115, name: ss-0, uid: 70cd65ba-45e6-455f-91a7-3ffe58a17cb5, status phase: Failed. Waiting for statefulset controller to delete.
May 21 19:46:56.725: INFO: Observed stateful pod in namespace: statefulset-2115, name: ss-0, uid: 70cd65ba-45e6-455f-91a7-3ffe58a17cb5, status phase: Failed. Waiting for statefulset controller to delete.
May 21 19:46:56.750: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2115
STEP: Removing pod with conflicting port in namespace statefulset-2115
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2115 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 19:47:01.026: INFO: Deleting all statefulset in ns statefulset-2115
May 21 19:47:01.031: INFO: Scaling statefulset ss to 0
May 21 19:47:11.065: INFO: Waiting for statefulset status.replicas updated to 0
May 21 19:47:11.070: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:47:11.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2115" for this suite.

• [SLOW TEST:20.955 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":137,"skipped":2149,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:47:11.153: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:47:18.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-902" for this suite.

• [SLOW TEST:7.110 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":138,"skipped":2165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:47:18.264: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
May 21 19:47:18.404: INFO: Waiting up to 5m0s for pod "var-expansion-cf073688-5747-4167-a7a8-ddb56fdb9d78" in namespace "var-expansion-2835" to be "Succeeded or Failed"
May 21 19:47:18.410: INFO: Pod "var-expansion-cf073688-5747-4167-a7a8-ddb56fdb9d78": Phase="Pending", Reason="", readiness=false. Elapsed: 5.754532ms
May 21 19:47:20.417: INFO: Pod "var-expansion-cf073688-5747-4167-a7a8-ddb56fdb9d78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012822537s
May 21 19:47:22.427: INFO: Pod "var-expansion-cf073688-5747-4167-a7a8-ddb56fdb9d78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022897068s
STEP: Saw pod success
May 21 19:47:22.427: INFO: Pod "var-expansion-cf073688-5747-4167-a7a8-ddb56fdb9d78" satisfied condition "Succeeded or Failed"
May 21 19:47:22.432: INFO: Trying to get logs from node dc-hg-1 pod var-expansion-cf073688-5747-4167-a7a8-ddb56fdb9d78 container dapi-container: <nil>
STEP: delete the pod
May 21 19:47:22.586: INFO: Waiting for pod var-expansion-cf073688-5747-4167-a7a8-ddb56fdb9d78 to disappear
May 21 19:47:22.595: INFO: Pod var-expansion-cf073688-5747-4167-a7a8-ddb56fdb9d78 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:47:22.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2835" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":139,"skipped":2189,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:47:22.631: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:47:22.726: INFO: Creating deployment "webserver-deployment"
May 21 19:47:22.738: INFO: Waiting for observed generation 1
May 21 19:47:24.764: INFO: Waiting for all required pods to come up
May 21 19:47:24.777: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 21 19:47:30.822: INFO: Waiting for deployment "webserver-deployment" to complete
May 21 19:47:30.832: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 21 19:47:30.848: INFO: Updating deployment webserver-deployment
May 21 19:47:30.848: INFO: Waiting for observed generation 2
May 21 19:47:32.870: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 21 19:47:32.880: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 21 19:47:32.904: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 21 19:47:32.935: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 21 19:47:32.935: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 21 19:47:32.952: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 21 19:47:32.969: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 21 19:47:32.969: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 21 19:47:32.987: INFO: Updating deployment webserver-deployment
May 21 19:47:32.987: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 21 19:47:33.079: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 21 19:47:33.155: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 21 19:47:35.534: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4879 /apis/apps/v1/namespaces/deployment-4879/deployments/webserver-deployment ffa2ad1a-aa66-4573-a83e-8ff23edd97f2 1275792 3 2021-05-21 19:47:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 19:47:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 19:47:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006346718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-21 19:47:33 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-21 19:47:33 +0000 UTC,LastTransitionTime:2021-05-21 19:47:22 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 21 19:47:35.682: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-4879 /apis/apps/v1/namespaces/deployment-4879/replicasets/webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 1275771 3 2021-05-21 19:47:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ffa2ad1a-aa66-4573-a83e-8ff23edd97f2 0xc006346b87 0xc006346b88}] []  [{kube-controller-manager Update apps/v1 2021-05-21 19:47:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ffa2ad1a-aa66-4573-a83e-8ff23edd97f2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006346c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 19:47:35.682: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 21 19:47:35.682: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-867f44f6fb  deployment-4879 /apis/apps/v1/namespaces/deployment-4879/replicasets/webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 1275787 3 2021-05-21 19:47:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ffa2ad1a-aa66-4573-a83e-8ff23edd97f2 0xc006346c67 0xc006346c68}] []  [{kube-controller-manager Update apps/v1 2021-05-21 19:47:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ffa2ad1a-aa66-4573-a83e-8ff23edd97f2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 867f44f6fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006346cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 21 19:47:36.376: INFO: Pod "webserver-deployment-795d758f88-54mlv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-54mlv webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-54mlv d313eeff-5bae-4b41-b5fd-dc0bb3620c58 1275761 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc006347227 0xc006347228}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.380: INFO: Pod "webserver-deployment-795d758f88-68rzh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-68rzh webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-68rzh 7953596d-1837-4427-861f-0b6e80b311d1 1275776 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc0063473d7 0xc0063473d8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.382: INFO: Pod "webserver-deployment-795d758f88-9kngv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9kngv webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-9kngv 27a3599b-7fea-48b5-8ee3-b0c073298762 1275664 0 2021-05-21 19:47:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc006347587 0xc006347588}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:,StartTime:2021-05-21 19:47:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.382: INFO: Pod "webserver-deployment-795d758f88-ctc2v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ctc2v webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-ctc2v 8b61a8e4-7fd9-4ecf-be30-382ac43ea89a 1275821 0 2021-05-21 19:47:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.161.147/32 cni.projectcalico.org/podIPs:192.168.161.147/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc006347737 0xc006347738}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 19:47:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:,StartTime:2021-05-21 19:47:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.396: INFO: Pod "webserver-deployment-795d758f88-gvd98" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gvd98 webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-gvd98 11b20c44-ef36-4ba7-a081-5a7ec639ed70 1275783 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc006347f17 0xc006347f18}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.415: INFO: Pod "webserver-deployment-795d758f88-hk8mr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hk8mr webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-hk8mr fbc00f2c-a61c-4c85-83b5-ab5210e1f25d 1275772 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc00317a0d7 0xc00317a0d8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.429: INFO: Pod "webserver-deployment-795d758f88-p5dcv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-p5dcv webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-p5dcv d97814d9-4ad6-4392-ad18-114fcec21b56 1275760 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc00317a217 0xc00317a218}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.429: INFO: Pod "webserver-deployment-795d758f88-p67c9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-p67c9 webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-p67c9 d1a6283f-899a-4cb9-b6f1-ca6ad39cd12c 1275726 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc00317a3c7 0xc00317a3c8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.446: INFO: Pod "webserver-deployment-795d758f88-pfmvb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pfmvb webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-pfmvb cb81d082-e637-4517-923a-3be7aa47d357 1275786 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc00317a6a7 0xc00317a6a8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.447: INFO: Pod "webserver-deployment-795d758f88-qgbnh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qgbnh webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-qgbnh 483e0f1a-8e66-4de6-8b0d-f50f75fdc409 1275803 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc00317a937 0xc00317a938}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.451: INFO: Pod "webserver-deployment-795d758f88-t4jj4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-t4jj4 webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-t4jj4 f26d90cc-a5d9-4f6e-ba4e-0af52968ec0b 1275744 0 2021-05-21 19:47:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.209.73/32 cni.projectcalico.org/podIPs:192.168.209.73/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc00317ac67 0xc00317ac68}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:,StartTime:2021-05-21 19:47:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.456: INFO: Pod "webserver-deployment-795d758f88-wwxnf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wwxnf webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-wwxnf 95017f19-f8e5-4c3c-913b-fb03f9efb438 1275703 0 2021-05-21 19:47:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.161.146/32 cni.projectcalico.org/podIPs:192.168.161.146/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc00317afa7 0xc00317afa8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-21 19:47:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:,StartTime:2021-05-21 19:47:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.457: INFO: Pod "webserver-deployment-795d758f88-zcjq8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zcjq8 webserver-deployment-795d758f88- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-795d758f88-zcjq8 d417bf8f-99de-44ee-9689-d631a4daac7a 1275689 0 2021-05-21 19:47:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3db95810-6a51-479d-aa2f-44c8bb944d0d 0xc00317b267 0xc00317b268}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3db95810-6a51-479d-aa2f-44c8bb944d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:,StartTime:2021-05-21 19:47:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.461: INFO: Pod "webserver-deployment-867f44f6fb-2c2sj" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-2c2sj webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-2c2sj 73f55be0-6861-444d-8297-1245925a1b8f 1275785 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc00317b5a7 0xc00317b5a8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.465: INFO: Pod "webserver-deployment-867f44f6fb-2szqb" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-2szqb webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-2szqb 145ec0f6-1c02-44d2-a786-cf23ef857790 1275617 0 2021-05-21 19:47:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[cni.projectcalico.org/podIP:192.168.161.144/32 cni.projectcalico.org/podIPs:192.168.161.144/32] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc00317b747 0xc00317b748}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:47:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:47:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.161.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:192.168.161.144,StartTime:2021-05-21 19:47:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:47:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1808119bd497cb4af0bff5b1f0e8a476a07da0b3c8fa501a22503b799db1e1e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.161.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.465: INFO: Pod "webserver-deployment-867f44f6fb-4z7b2" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-4z7b2 webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-4z7b2 1664f47f-2327-41ba-a4ec-6b297c0de911 1275633 0 2021-05-21 19:47:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[cni.projectcalico.org/podIP:192.168.209.101/32 cni.projectcalico.org/podIPs:192.168.209.101/32] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc00317b907 0xc00317b908}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:47:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:47:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.209.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:192.168.209.101,StartTime:2021-05-21 19:47:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:47:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://eb2cef6fff73499d78d7dc932c42c257a9a780d97b4886ba45f165f25fa7ad5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.209.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.480: INFO: Pod "webserver-deployment-867f44f6fb-7mnzw" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-7mnzw webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-7mnzw cf75682a-5003-4b30-9f0b-64544ac3e8e5 1275619 0 2021-05-21 19:47:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[cni.projectcalico.org/podIP:192.168.106.131/32 cni.projectcalico.org/podIPs:192.168.106.131/32] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc00317bc27 0xc00317bc28}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:47:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:47:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.106.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:192.168.106.131,StartTime:2021-05-21 19:47:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:47:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c01c22540c241ae524202e82f51712461d07148eb1cebe5083495d6c3ff96da2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.106.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.545: INFO: Pod "webserver-deployment-867f44f6fb-852sm" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-852sm webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-852sm af64ca4f-99c0-49eb-8d38-e3596142cb33 1275768 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc00317bec7 0xc00317bec8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.738: INFO: Pod "webserver-deployment-867f44f6fb-92gq9" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-92gq9 webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-92gq9 a791b41a-1142-46c7-8797-a38dc7fd52a6 1275742 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0007be1a7 0xc0007be1a8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:36.865: INFO: Pod "webserver-deployment-867f44f6fb-bn9v9" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-bn9v9 webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-bn9v9 6be3a233-458c-465d-bbf7-3dbb77dca385 1275775 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0007be6b7 0xc0007be6b8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.047: INFO: Pod "webserver-deployment-867f44f6fb-cvhtq" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-cvhtq webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-cvhtq 4fb0f794-13ff-4ae5-9133-eb50047a150c 1275780 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0007bf267 0xc0007bf268}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.092: INFO: Pod "webserver-deployment-867f44f6fb-gvgzn" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-gvgzn webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-gvgzn 0550e9ee-383c-487a-9c95-ccdef7fbbc91 1275824 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0007bf567 0xc0007bf568}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:,StartTime:2021-05-21 19:47:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.131: INFO: Pod "webserver-deployment-867f44f6fb-hh98w" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-hh98w webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-hh98w f19bc2e6-cd64-4642-9288-d251e032dcde 1275621 0 2021-05-21 19:47:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[cni.projectcalico.org/podIP:192.168.161.140/32 cni.projectcalico.org/podIPs:192.168.161.140/32] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0007bf797 0xc0007bf798}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:47:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:47:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.161.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:192.168.161.140,StartTime:2021-05-21 19:47:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:47:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://48b0f089aaa733f60275122246cb756abc95a9cf621ca34d68773405f3f24970,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.161.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.210: INFO: Pod "webserver-deployment-867f44f6fb-m8cj6" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-m8cj6 webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-m8cj6 ab47c861-5307-4e3a-bd8f-249ad39ddc50 1275795 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0007bfa47 0xc0007bfa48}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.238: INFO: Pod "webserver-deployment-867f44f6fb-p5gtt" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-p5gtt webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-p5gtt 1e67de25-c72a-45c9-bf9d-7afc949c7642 1275636 0 2021-05-21 19:47:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[cni.projectcalico.org/podIP:192.168.209.66/32 cni.projectcalico.org/podIPs:192.168.209.66/32] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0007bfc37 0xc0007bfc38}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:47:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:47:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.209.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:192.168.209.66,StartTime:2021-05-21 19:47:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:47:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4d75d188bf5756b211ee596dd6dc65c5cdde61b44d8e0783366131c562f76b78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.209.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.241: INFO: Pod "webserver-deployment-867f44f6fb-qcpsm" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-qcpsm webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-qcpsm fa3737ce-c11f-4222-bf0f-64bf7fe53fd8 1275614 0 2021-05-21 19:47:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[cni.projectcalico.org/podIP:192.168.161.145/32 cni.projectcalico.org/podIPs:192.168.161.145/32] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0035d6027 0xc0035d6028}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:47:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:47:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.161.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:192.168.161.145,StartTime:2021-05-21 19:47:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:47:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ce8b8f1a25d41af505c90e7e05c579f499121d048c185abcc041186508719338,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.161.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.256: INFO: Pod "webserver-deployment-867f44f6fb-rslmf" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-rslmf webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-rslmf 8f58f774-d19b-4ba6-80da-b3a5d167c66e 1275767 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0035d61e7 0xc0035d61e8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.302: INFO: Pod "webserver-deployment-867f44f6fb-snf55" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-snf55 webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-snf55 64b30555-e5b4-41b9-8f12-f0e3b73c4d49 1275813 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0035d6317 0xc0035d6318}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.86,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.305: INFO: Pod "webserver-deployment-867f44f6fb-t4tbh" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-t4tbh webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-t4tbh bd0ebd7d-ce9c-42d0-9da2-49af6c4bb72d 1275724 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0035d64a7 0xc0035d64a8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.305: INFO: Pod "webserver-deployment-867f44f6fb-t9bw5" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-t9bw5 webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-t9bw5 28f34b09-ba63-4882-8920-f001a67ab94d 1275791 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0035d6957 0xc0035d6958}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.312: INFO: Pod "webserver-deployment-867f44f6fb-vw7fm" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-vw7fm webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-vw7fm 92452030-aeac-41a7-8424-563e86fc81df 1275615 0 2021-05-21 19:47:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[cni.projectcalico.org/podIP:192.168.106.171/32 cni.projectcalico.org/podIPs:192.168.106.171/32] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0035d6bc7 0xc0035d6bc8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:47:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:47:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.106.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:192.168.106.171,StartTime:2021-05-21 19:47:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:47:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7ca1d948b99deb286309cbb7a9c8473f44c82b87f9177fd7a5c0d9f2488b43c0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.106.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.313: INFO: Pod "webserver-deployment-867f44f6fb-wt5bb" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-wt5bb webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-wt5bb c2bcf916-9c8f-46c9-9ad7-364b91cec67d 1275743 0 2021-05-21 19:47:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0035d6ea7 0xc0035d6ea8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 19:47:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:,StartTime:2021-05-21 19:47:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 19:47:37.410: INFO: Pod "webserver-deployment-867f44f6fb-zm2f2" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-zm2f2 webserver-deployment-867f44f6fb- deployment-4879 /api/v1/namespaces/deployment-4879/pods/webserver-deployment-867f44f6fb-zm2f2 0133bfa9-cc64-4b48-b6f6-b6e61cb4ed7e 1275583 0 2021-05-21 19:47:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[cni.projectcalico.org/podIP:192.168.161.143/32 cni.projectcalico.org/podIPs:192.168.161.143/32] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 4748a86a-2827-429f-8807-45c06625437b 0xc0035d70a7 0xc0035d70a8}] []  [{kube-controller-manager Update v1 2021-05-21 19:47:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4748a86a-2827-429f-8807-45c06625437b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:47:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:47:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.161.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-77gx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-77gx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-77gx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:47:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:192.168.161.143,StartTime:2021-05-21 19:47:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:47:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://63c8084869ff8672b083356c8186079a60f6f8d6128692a884f9e4d459d52989,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.161.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:47:37.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4879" for this suite.

• [SLOW TEST:14.898 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":140,"skipped":2191,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:47:37.556: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 21 19:47:37.840: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:48:02.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3562" for this suite.

• [SLOW TEST:24.535 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":141,"skipped":2202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:48:02.093: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:48:02.251: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 21 19:48:07.297: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 19:48:07.297: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 21 19:48:09.304: INFO: Creating deployment "test-rollover-deployment"
May 21 19:48:09.321: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 21 19:48:11.342: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 21 19:48:11.368: INFO: Ensure that both replica sets have 1 created replica
May 21 19:48:11.430: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 21 19:48:11.463: INFO: Updating deployment test-rollover-deployment
May 21 19:48:11.463: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 21 19:48:13.479: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 21 19:48:13.491: INFO: Make sure deployment "test-rollover-deployment" is complete
May 21 19:48:13.505: INFO: all replica sets need to contain the pod-template-hash label
May 21 19:48:13.505: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223291, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:48:15.526: INFO: all replica sets need to contain the pod-template-hash label
May 21 19:48:15.526: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223291, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:48:17.520: INFO: all replica sets need to contain the pod-template-hash label
May 21 19:48:17.520: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223295, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:48:19.522: INFO: all replica sets need to contain the pod-template-hash label
May 21 19:48:19.522: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223295, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:48:21.516: INFO: all replica sets need to contain the pod-template-hash label
May 21 19:48:21.516: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223295, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:48:23.516: INFO: all replica sets need to contain the pod-template-hash label
May 21 19:48:23.516: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223295, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:48:25.521: INFO: all replica sets need to contain the pod-template-hash label
May 21 19:48:25.521: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223295, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223289, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:48:27.522: INFO: 
May 21 19:48:27.522: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 21 19:48:27.538: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5937 /apis/apps/v1/namespaces/deployment-5937/deployments/test-rollover-deployment b51bcbb0-3792-4fd4-897b-0bc44d097d05 1276435 2 2021-05-21 19:48:09 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 19:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 19:48:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c7bbf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-21 19:48:09 +0000 UTC,LastTransitionTime:2021-05-21 19:48:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-05-21 19:48:25 +0000 UTC,LastTransitionTime:2021-05-21 19:48:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 21 19:48:27.546: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-5937 /apis/apps/v1/namespaces/deployment-5937/replicasets/test-rollover-deployment-5797c7764 0c06ce15-f273-45ba-9dac-b8a774ac9eb7 1276424 2 2021-05-21 19:48:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b51bcbb0-3792-4fd4-897b-0bc44d097d05 0xc000735cb0 0xc000735cb1}] []  [{kube-controller-manager Update apps/v1 2021-05-21 19:48:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b51bcbb0-3792-4fd4-897b-0bc44d097d05\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000735e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 19:48:27.546: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 21 19:48:27.546: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5937 /apis/apps/v1/namespaces/deployment-5937/replicasets/test-rollover-controller 98953874-2785-402a-afc9-ea44068837bd 1276434 2 2021-05-21 19:48:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b51bcbb0-3792-4fd4-897b-0bc44d097d05 0xc000735ba7 0xc000735ba8}] []  [{e2e.test Update apps/v1 2021-05-21 19:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 19:48:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b51bcbb0-3792-4fd4-897b-0bc44d097d05\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000735c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 19:48:27.546: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-5937 /apis/apps/v1/namespaces/deployment-5937/replicasets/test-rollover-deployment-78bc8b888c d5af554c-fd39-4cbd-844a-52ec65266b87 1276396 2 2021-05-21 19:48:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b51bcbb0-3792-4fd4-897b-0bc44d097d05 0xc0008fe057 0xc0008fe058}] []  [{kube-controller-manager Update apps/v1 2021-05-21 19:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b51bcbb0-3792-4fd4-897b-0bc44d097d05\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008fe228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 19:48:27.554: INFO: Pod "test-rollover-deployment-5797c7764-k4hxr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-k4hxr test-rollover-deployment-5797c7764- deployment-5937 /api/v1/namespaces/deployment-5937/pods/test-rollover-deployment-5797c7764-k4hxr 5263941f-9b68-4075-b74d-6119853de359 1276415 0 2021-05-21 19:48:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:192.168.106.157/32 cni.projectcalico.org/podIPs:192.168.106.157/32] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 0c06ce15-f273-45ba-9dac-b8a774ac9eb7 0xc003b6e120 0xc003b6e121}] []  [{kube-controller-manager Update v1 2021-05-21 19:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0c06ce15-f273-45ba-9dac-b8a774ac9eb7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:48:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:48:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.106.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brdl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brdl7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brdl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:48:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:48:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:48:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:48:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:192.168.106.157,StartTime:2021-05-21 19:48:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:48:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://ca74d6ef5ebbd107bc3dd45451bd5ac48400217a1d06928f4f9767aaef6dfdb6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.106.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:48:27.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5937" for this suite.

• [SLOW TEST:25.507 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":142,"skipped":2273,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:48:27.601: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:48:30.009: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 19:48:32.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223310, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223310, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223310, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223310, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:48:35.068: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:48:35.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5518" for this suite.
STEP: Destroying namespace "webhook-5518-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.960 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":143,"skipped":2289,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:48:35.562: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:48:37.150: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 19:48:39.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223317, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223317, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223317, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223317, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:48:41.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223317, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223317, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223317, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223317, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:48:44.204: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:48:44.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-309" for this suite.
STEP: Destroying namespace "webhook-309-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.989 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":144,"skipped":2295,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:48:44.552: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-725d73c5-fbbe-4ec0-942a-5114cbf4e0c0
STEP: Creating a pod to test consume configMaps
May 21 19:48:44.742: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a2bca99a-5220-425a-9c43-93999ed3580a" in namespace "projected-1536" to be "Succeeded or Failed"
May 21 19:48:44.765: INFO: Pod "pod-projected-configmaps-a2bca99a-5220-425a-9c43-93999ed3580a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.059408ms
May 21 19:48:46.771: INFO: Pod "pod-projected-configmaps-a2bca99a-5220-425a-9c43-93999ed3580a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02954206s
May 21 19:48:48.779: INFO: Pod "pod-projected-configmaps-a2bca99a-5220-425a-9c43-93999ed3580a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037287108s
STEP: Saw pod success
May 21 19:48:48.779: INFO: Pod "pod-projected-configmaps-a2bca99a-5220-425a-9c43-93999ed3580a" satisfied condition "Succeeded or Failed"
May 21 19:48:48.785: INFO: Trying to get logs from node dc-hg-1 pod pod-projected-configmaps-a2bca99a-5220-425a-9c43-93999ed3580a container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 19:48:48.850: INFO: Waiting for pod pod-projected-configmaps-a2bca99a-5220-425a-9c43-93999ed3580a to disappear
May 21 19:48:48.857: INFO: Pod pod-projected-configmaps-a2bca99a-5220-425a-9c43-93999ed3580a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:48:48.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1536" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2297,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:48:48.896: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 21 19:48:53.698: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c025eaf6-06b1-409c-9849-2318e70347da"
May 21 19:48:53.698: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c025eaf6-06b1-409c-9849-2318e70347da" in namespace "pods-1043" to be "terminated due to deadline exceeded"
May 21 19:48:53.709: INFO: Pod "pod-update-activedeadlineseconds-c025eaf6-06b1-409c-9849-2318e70347da": Phase="Running", Reason="", readiness=true. Elapsed: 10.89621ms
May 21 19:48:55.724: INFO: Pod "pod-update-activedeadlineseconds-c025eaf6-06b1-409c-9849-2318e70347da": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.026160037s
May 21 19:48:55.724: INFO: Pod "pod-update-activedeadlineseconds-c025eaf6-06b1-409c-9849-2318e70347da" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:48:55.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1043" for this suite.

• [SLOW TEST:6.852 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":146,"skipped":2308,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:48:55.750: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-8e4f6537-4c0c-4b36-b288-a77f2cf10433
STEP: Creating a pod to test consume configMaps
May 21 19:48:55.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-4bc4b0df-af96-4503-96b5-8a49d8f3fbf8" in namespace "configmap-3854" to be "Succeeded or Failed"
May 21 19:48:55.990: INFO: Pod "pod-configmaps-4bc4b0df-af96-4503-96b5-8a49d8f3fbf8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.191908ms
May 21 19:48:57.997: INFO: Pod "pod-configmaps-4bc4b0df-af96-4503-96b5-8a49d8f3fbf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023602648s
May 21 19:49:00.004: INFO: Pod "pod-configmaps-4bc4b0df-af96-4503-96b5-8a49d8f3fbf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030420523s
STEP: Saw pod success
May 21 19:49:00.004: INFO: Pod "pod-configmaps-4bc4b0df-af96-4503-96b5-8a49d8f3fbf8" satisfied condition "Succeeded or Failed"
May 21 19:49:00.010: INFO: Trying to get logs from node dc-hg-1 pod pod-configmaps-4bc4b0df-af96-4503-96b5-8a49d8f3fbf8 container configmap-volume-test: <nil>
STEP: delete the pod
May 21 19:49:00.063: INFO: Waiting for pod pod-configmaps-4bc4b0df-af96-4503-96b5-8a49d8f3fbf8 to disappear
May 21 19:49:00.071: INFO: Pod pod-configmaps-4bc4b0df-af96-4503-96b5-8a49d8f3fbf8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:49:00.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3854" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":147,"skipped":2332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:49:00.104: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 21 19:49:04.747: INFO: Successfully updated pod "labelsupdatefde6e31b-c677-455a-9029-8f6c8be9e460"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:49:06.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-730" for this suite.

• [SLOW TEST:6.719 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:49:06.824: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
May 21 19:49:06.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 create -f -'
May 21 19:49:08.661: INFO: stderr: ""
May 21 19:49:08.661: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 19:49:08.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 19:49:09.051: INFO: stderr: ""
May 21 19:49:09.051: INFO: stdout: "update-demo-nautilus-9llmt update-demo-nautilus-ddkgs "
May 21 19:49:09.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-9llmt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:49:09.321: INFO: stderr: ""
May 21 19:49:09.321: INFO: stdout: ""
May 21 19:49:09.321: INFO: update-demo-nautilus-9llmt is created but not running
May 21 19:49:14.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 19:49:14.603: INFO: stderr: ""
May 21 19:49:14.603: INFO: stdout: "update-demo-nautilus-9llmt update-demo-nautilus-ddkgs "
May 21 19:49:14.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-9llmt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:49:14.903: INFO: stderr: ""
May 21 19:49:14.903: INFO: stdout: "true"
May 21 19:49:14.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-9llmt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 19:49:15.217: INFO: stderr: ""
May 21 19:49:15.217: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 19:49:15.217: INFO: validating pod update-demo-nautilus-9llmt
May 21 19:49:15.228: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 19:49:15.228: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 19:49:15.228: INFO: update-demo-nautilus-9llmt is verified up and running
May 21 19:49:15.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-ddkgs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:49:15.593: INFO: stderr: ""
May 21 19:49:15.593: INFO: stdout: "true"
May 21 19:49:15.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-ddkgs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 19:49:15.904: INFO: stderr: ""
May 21 19:49:15.904: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 19:49:15.904: INFO: validating pod update-demo-nautilus-ddkgs
May 21 19:49:15.924: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 19:49:15.925: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 19:49:15.925: INFO: update-demo-nautilus-ddkgs is verified up and running
STEP: scaling down the replication controller
May 21 19:49:15.937: INFO: scanned /root for discovery docs: <nil>
May 21 19:49:15.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 21 19:49:17.537: INFO: stderr: ""
May 21 19:49:17.537: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 19:49:17.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 19:49:17.851: INFO: stderr: ""
May 21 19:49:17.852: INFO: stdout: "update-demo-nautilus-9llmt update-demo-nautilus-ddkgs "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 21 19:49:22.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 19:49:23.164: INFO: stderr: ""
May 21 19:49:23.164: INFO: stdout: "update-demo-nautilus-ddkgs "
May 21 19:49:23.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-ddkgs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:49:23.446: INFO: stderr: ""
May 21 19:49:23.446: INFO: stdout: "true"
May 21 19:49:23.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-ddkgs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 19:49:23.766: INFO: stderr: ""
May 21 19:49:23.766: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 19:49:23.766: INFO: validating pod update-demo-nautilus-ddkgs
May 21 19:49:23.780: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 19:49:23.780: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 19:49:23.780: INFO: update-demo-nautilus-ddkgs is verified up and running
STEP: scaling up the replication controller
May 21 19:49:23.787: INFO: scanned /root for discovery docs: <nil>
May 21 19:49:23.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 21 19:49:25.277: INFO: stderr: ""
May 21 19:49:25.277: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 21 19:49:25.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 19:49:25.826: INFO: stderr: ""
May 21 19:49:25.826: INFO: stdout: "update-demo-nautilus-ddkgs update-demo-nautilus-wwhhv "
May 21 19:49:25.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-ddkgs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:49:26.556: INFO: stderr: ""
May 21 19:49:26.556: INFO: stdout: "true"
May 21 19:49:26.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-ddkgs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 19:49:26.898: INFO: stderr: ""
May 21 19:49:26.898: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 19:49:26.898: INFO: validating pod update-demo-nautilus-ddkgs
May 21 19:49:26.909: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 19:49:26.909: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 19:49:26.910: INFO: update-demo-nautilus-ddkgs is verified up and running
May 21 19:49:26.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-wwhhv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:49:27.291: INFO: stderr: ""
May 21 19:49:27.291: INFO: stdout: ""
May 21 19:49:27.291: INFO: update-demo-nautilus-wwhhv is created but not running
May 21 19:49:32.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 21 19:49:32.612: INFO: stderr: ""
May 21 19:49:32.612: INFO: stdout: "update-demo-nautilus-ddkgs update-demo-nautilus-wwhhv "
May 21 19:49:32.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-ddkgs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:49:32.920: INFO: stderr: ""
May 21 19:49:32.920: INFO: stdout: "true"
May 21 19:49:32.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-ddkgs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 19:49:33.232: INFO: stderr: ""
May 21 19:49:33.232: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 19:49:33.232: INFO: validating pod update-demo-nautilus-ddkgs
May 21 19:49:33.241: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 19:49:33.242: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 19:49:33.242: INFO: update-demo-nautilus-ddkgs is verified up and running
May 21 19:49:33.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-wwhhv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 21 19:49:33.534: INFO: stderr: ""
May 21 19:49:33.534: INFO: stdout: "true"
May 21 19:49:33.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods update-demo-nautilus-wwhhv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 21 19:49:33.799: INFO: stderr: ""
May 21 19:49:33.799: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 21 19:49:33.799: INFO: validating pod update-demo-nautilus-wwhhv
May 21 19:49:33.807: INFO: got data: {
  "image": "nautilus.jpg"
}

May 21 19:49:33.807: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 21 19:49:33.808: INFO: update-demo-nautilus-wwhhv is verified up and running
STEP: using delete to clean up resources
May 21 19:49:33.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 delete --grace-period=0 --force -f -'
May 21 19:49:34.082: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 19:49:34.082: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 21 19:49:34.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get rc,svc -l name=update-demo --no-headers'
May 21 19:49:34.423: INFO: stderr: "No resources found in kubectl-9887 namespace.\n"
May 21 19:49:34.423: INFO: stdout: ""
May 21 19:49:34.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9887 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 19:49:34.767: INFO: stderr: ""
May 21 19:49:34.767: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:49:34.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9887" for this suite.

• [SLOW TEST:27.964 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":149,"skipped":2403,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:49:34.788: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:49:37.539: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 19:49:39.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223377, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223377, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223377, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223377, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:49:42.601: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:49:43.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6889" for this suite.
STEP: Destroying namespace "webhook-6889-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.580 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":150,"skipped":2403,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:49:43.369: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4892
STEP: creating service affinity-clusterip-transition in namespace services-4892
STEP: creating replication controller affinity-clusterip-transition in namespace services-4892
I0521 19:49:43.593734      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-4892, replica count: 3
I0521 19:49:46.648742      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 19:49:49.649199      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 19:49:49.664: INFO: Creating new exec pod
May 21 19:49:54.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-4892 exec execpod-affinityvgmxh -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
May 21 19:49:55.356: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 21 19:49:55.356: INFO: stdout: ""
May 21 19:49:55.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-4892 exec execpod-affinityvgmxh -- /bin/sh -x -c nc -zv -t -w 2 10.10.5.15 80'
May 21 19:49:55.941: INFO: stderr: "+ nc -zv -t -w 2 10.10.5.15 80\nConnection to 10.10.5.15 80 port [tcp/http] succeeded!\n"
May 21 19:49:55.941: INFO: stdout: ""
May 21 19:49:55.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-4892 exec execpod-affinityvgmxh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.5.15:80/ ; done'
May 21 19:49:56.969: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n"
May 21 19:49:56.969: INFO: stdout: "\naffinity-clusterip-transition-6xn5b\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-6xn5b\naffinity-clusterip-transition-6xn5b\naffinity-clusterip-transition-6xn5b\naffinity-clusterip-transition-6xn5b\naffinity-clusterip-transition-6xn5b\naffinity-clusterip-transition-ggcd9\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-6xn5b\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-ggcd9\naffinity-clusterip-transition-6xn5b\naffinity-clusterip-transition-ggcd9"
May 21 19:49:56.969: INFO: Received response from host: affinity-clusterip-transition-6xn5b
May 21 19:49:56.969: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:56.969: INFO: Received response from host: affinity-clusterip-transition-6xn5b
May 21 19:49:56.969: INFO: Received response from host: affinity-clusterip-transition-6xn5b
May 21 19:49:56.969: INFO: Received response from host: affinity-clusterip-transition-6xn5b
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-6xn5b
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-6xn5b
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-ggcd9
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-6xn5b
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-ggcd9
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-6xn5b
May 21 19:49:56.970: INFO: Received response from host: affinity-clusterip-transition-ggcd9
May 21 19:49:56.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-4892 exec execpod-affinityvgmxh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.5.15:80/ ; done'
May 21 19:49:57.917: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.5.15:80/\n"
May 21 19:49:57.917: INFO: stdout: "\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85\naffinity-clusterip-transition-rfn85"
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.917: INFO: Received response from host: affinity-clusterip-transition-rfn85
May 21 19:49:57.918: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4892, will wait for the garbage collector to delete the pods
May 21 19:49:58.042: INFO: Deleting ReplicationController affinity-clusterip-transition took: 15.137593ms
May 21 19:49:58.242: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.407768ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:50:03.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4892" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:20.425 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":151,"skipped":2421,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:50:03.794: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 21 19:50:03.927: INFO: Waiting up to 5m0s for pod "downward-api-28a70e15-ea1b-4166-9a3e-89bb8c440d81" in namespace "downward-api-7323" to be "Succeeded or Failed"
May 21 19:50:03.945: INFO: Pod "downward-api-28a70e15-ea1b-4166-9a3e-89bb8c440d81": Phase="Pending", Reason="", readiness=false. Elapsed: 18.755667ms
May 21 19:50:05.964: INFO: Pod "downward-api-28a70e15-ea1b-4166-9a3e-89bb8c440d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037293046s
May 21 19:50:07.972: INFO: Pod "downward-api-28a70e15-ea1b-4166-9a3e-89bb8c440d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045195545s
STEP: Saw pod success
May 21 19:50:07.972: INFO: Pod "downward-api-28a70e15-ea1b-4166-9a3e-89bb8c440d81" satisfied condition "Succeeded or Failed"
May 21 19:50:07.980: INFO: Trying to get logs from node dc-hg-1 pod downward-api-28a70e15-ea1b-4166-9a3e-89bb8c440d81 container dapi-container: <nil>
STEP: delete the pod
May 21 19:50:08.031: INFO: Waiting for pod downward-api-28a70e15-ea1b-4166-9a3e-89bb8c440d81 to disappear
May 21 19:50:08.037: INFO: Pod downward-api-28a70e15-ea1b-4166-9a3e-89bb8c440d81 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:50:08.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7323" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":152,"skipped":2422,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:50:08.058: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-ec31dc42-441c-4482-b3a9-4b243314f200
STEP: Creating a pod to test consume secrets
May 21 19:50:08.245: INFO: Waiting up to 5m0s for pod "pod-secrets-68b22298-59c6-4042-9b1f-5c95987b1faf" in namespace "secrets-7316" to be "Succeeded or Failed"
May 21 19:50:08.259: INFO: Pod "pod-secrets-68b22298-59c6-4042-9b1f-5c95987b1faf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.682311ms
May 21 19:50:10.271: INFO: Pod "pod-secrets-68b22298-59c6-4042-9b1f-5c95987b1faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026181987s
May 21 19:50:12.278: INFO: Pod "pod-secrets-68b22298-59c6-4042-9b1f-5c95987b1faf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032756257s
May 21 19:50:14.286: INFO: Pod "pod-secrets-68b22298-59c6-4042-9b1f-5c95987b1faf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040492826s
STEP: Saw pod success
May 21 19:50:14.286: INFO: Pod "pod-secrets-68b22298-59c6-4042-9b1f-5c95987b1faf" satisfied condition "Succeeded or Failed"
May 21 19:50:14.292: INFO: Trying to get logs from node dc-hg-1 pod pod-secrets-68b22298-59c6-4042-9b1f-5c95987b1faf container secret-volume-test: <nil>
STEP: delete the pod
May 21 19:50:14.343: INFO: Waiting for pod pod-secrets-68b22298-59c6-4042-9b1f-5c95987b1faf to disappear
May 21 19:50:14.349: INFO: Pod pod-secrets-68b22298-59c6-4042-9b1f-5c95987b1faf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:50:14.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7316" for this suite.
STEP: Destroying namespace "secret-namespace-3284" for this suite.

• [SLOW TEST:6.327 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":153,"skipped":2430,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:50:14.386: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
May 21 19:50:14.499: INFO: Waiting up to 5m0s for pod "client-containers-35a8a264-3464-4726-bc9e-d40408caa014" in namespace "containers-8638" to be "Succeeded or Failed"
May 21 19:50:14.522: INFO: Pod "client-containers-35a8a264-3464-4726-bc9e-d40408caa014": Phase="Pending", Reason="", readiness=false. Elapsed: 22.93621ms
May 21 19:50:16.528: INFO: Pod "client-containers-35a8a264-3464-4726-bc9e-d40408caa014": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029508022s
May 21 19:50:18.534: INFO: Pod "client-containers-35a8a264-3464-4726-bc9e-d40408caa014": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035422816s
STEP: Saw pod success
May 21 19:50:18.534: INFO: Pod "client-containers-35a8a264-3464-4726-bc9e-d40408caa014" satisfied condition "Succeeded or Failed"
May 21 19:50:18.539: INFO: Trying to get logs from node dc-hg-1 pod client-containers-35a8a264-3464-4726-bc9e-d40408caa014 container test-container: <nil>
STEP: delete the pod
May 21 19:50:18.586: INFO: Waiting for pod client-containers-35a8a264-3464-4726-bc9e-d40408caa014 to disappear
May 21 19:50:18.599: INFO: Pod client-containers-35a8a264-3464-4726-bc9e-d40408caa014 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:50:18.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8638" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":154,"skipped":2437,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:50:18.647: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-a8173a9f-e8cb-409d-bcc3-46eaab5de5be
STEP: Creating a pod to test consume configMaps
May 21 19:50:18.770: INFO: Waiting up to 5m0s for pod "pod-configmaps-3ec6abdb-ba43-4bac-b27f-eb41bee998a1" in namespace "configmap-9587" to be "Succeeded or Failed"
May 21 19:50:18.790: INFO: Pod "pod-configmaps-3ec6abdb-ba43-4bac-b27f-eb41bee998a1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.087548ms
May 21 19:50:20.801: INFO: Pod "pod-configmaps-3ec6abdb-ba43-4bac-b27f-eb41bee998a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030687403s
May 21 19:50:22.814: INFO: Pod "pod-configmaps-3ec6abdb-ba43-4bac-b27f-eb41bee998a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043826878s
STEP: Saw pod success
May 21 19:50:22.814: INFO: Pod "pod-configmaps-3ec6abdb-ba43-4bac-b27f-eb41bee998a1" satisfied condition "Succeeded or Failed"
May 21 19:50:22.833: INFO: Trying to get logs from node dc-hg-1 pod pod-configmaps-3ec6abdb-ba43-4bac-b27f-eb41bee998a1 container configmap-volume-test: <nil>
STEP: delete the pod
May 21 19:50:22.890: INFO: Waiting for pod pod-configmaps-3ec6abdb-ba43-4bac-b27f-eb41bee998a1 to disappear
May 21 19:50:22.914: INFO: Pod pod-configmaps-3ec6abdb-ba43-4bac-b27f-eb41bee998a1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:50:22.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9587" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":155,"skipped":2439,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:50:22.988: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:50:25.156: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 19:50:27.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223425, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223425, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223425, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223425, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:50:30.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:50:42.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7404" for this suite.
STEP: Destroying namespace "webhook-7404-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.726 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":156,"skipped":2463,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:50:42.715: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1656
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
May 21 19:50:42.879: INFO: Found 0 stateful pods, waiting for 3
May 21 19:50:52.889: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:50:52.889: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:50:52.889: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 21 19:51:02.887: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:51:02.887: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:51:02.887: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 21 19:51:02.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1656 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 19:51:03.503: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 19:51:03.503: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 19:51:03.503: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
May 21 19:51:13.594: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 21 19:51:23.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1656 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 19:51:24.283: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 19:51:24.283: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 19:51:24.283: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 19:51:34.329: INFO: Waiting for StatefulSet statefulset-1656/ss2 to complete update
May 21 19:51:34.329: INFO: Waiting for Pod statefulset-1656/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 19:51:34.329: INFO: Waiting for Pod statefulset-1656/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 19:51:34.329: INFO: Waiting for Pod statefulset-1656/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 19:51:44.350: INFO: Waiting for StatefulSet statefulset-1656/ss2 to complete update
May 21 19:51:44.350: INFO: Waiting for Pod statefulset-1656/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 19:51:44.350: INFO: Waiting for Pod statefulset-1656/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
May 21 19:51:54.348: INFO: Waiting for StatefulSet statefulset-1656/ss2 to complete update
May 21 19:51:54.348: INFO: Waiting for Pod statefulset-1656/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
STEP: Rolling back to a previous revision
May 21 19:52:04.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1656 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 19:52:04.922: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 19:52:04.922: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 19:52:04.922: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 19:52:14.976: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 21 19:52:25.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1656 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 19:52:25.563: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 19:52:25.563: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 19:52:25.563: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 19:52:35.607: INFO: Waiting for StatefulSet statefulset-1656/ss2 to complete update
May 21 19:52:35.607: INFO: Waiting for Pod statefulset-1656/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
May 21 19:52:35.607: INFO: Waiting for Pod statefulset-1656/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
May 21 19:52:35.607: INFO: Waiting for Pod statefulset-1656/ss2-2 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
May 21 19:52:45.683: INFO: Waiting for StatefulSet statefulset-1656/ss2 to complete update
May 21 19:52:45.684: INFO: Waiting for Pod statefulset-1656/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
May 21 19:52:55.620: INFO: Waiting for StatefulSet statefulset-1656/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 19:53:05.619: INFO: Deleting all statefulset in ns statefulset-1656
May 21 19:53:05.624: INFO: Scaling statefulset ss2 to 0
May 21 19:53:35.690: INFO: Waiting for statefulset status.replicas updated to 0
May 21 19:53:35.697: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:53:35.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1656" for this suite.

• [SLOW TEST:173.111 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":157,"skipped":2478,"failed":0}
SSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:53:35.827: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 19:53:35.982: INFO: starting watch
STEP: patching
STEP: updating
May 21 19:53:36.011: INFO: waiting for watch events with expected annotations
May 21 19:53:36.011: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:53:36.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3714" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":158,"skipped":2485,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:53:36.136: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:53:36.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4453" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":159,"skipped":2493,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:53:36.356: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
May 21 19:53:36.446: INFO: Major version: 1
STEP: Confirm minor version
May 21 19:53:36.446: INFO: cleanMinorVersion: 19
May 21 19:53:36.446: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:53:36.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4035" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":160,"skipped":2500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:53:36.470: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 21 19:53:41.675: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:53:41.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7038" for this suite.

• [SLOW TEST:5.623 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":161,"skipped":2547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:53:42.095: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
May 21 19:53:42.399: INFO: Waiting up to 5m0s for pod "pod-be281ba1-c5a6-4342-9825-5d0e5458e413" in namespace "emptydir-3445" to be "Succeeded or Failed"
May 21 19:53:42.463: INFO: Pod "pod-be281ba1-c5a6-4342-9825-5d0e5458e413": Phase="Pending", Reason="", readiness=false. Elapsed: 63.921456ms
May 21 19:53:44.479: INFO: Pod "pod-be281ba1-c5a6-4342-9825-5d0e5458e413": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080326303s
May 21 19:53:46.486: INFO: Pod "pod-be281ba1-c5a6-4342-9825-5d0e5458e413": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087003522s
May 21 19:53:48.510: INFO: Pod "pod-be281ba1-c5a6-4342-9825-5d0e5458e413": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.111336059s
STEP: Saw pod success
May 21 19:53:48.510: INFO: Pod "pod-be281ba1-c5a6-4342-9825-5d0e5458e413" satisfied condition "Succeeded or Failed"
May 21 19:53:48.522: INFO: Trying to get logs from node dc-hg-2 pod pod-be281ba1-c5a6-4342-9825-5d0e5458e413 container test-container: <nil>
STEP: delete the pod
May 21 19:53:48.635: INFO: Waiting for pod pod-be281ba1-c5a6-4342-9825-5d0e5458e413 to disappear
May 21 19:53:48.644: INFO: Pod pod-be281ba1-c5a6-4342-9825-5d0e5458e413 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:53:48.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3445" for this suite.

• [SLOW TEST:6.573 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":162,"skipped":2569,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:53:48.668: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-79a73b2f-9d50-4076-940e-233cff4d9519
STEP: Creating a pod to test consume secrets
May 21 19:53:48.773: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ab22d896-275c-4273-968b-30979bcfec98" in namespace "projected-9555" to be "Succeeded or Failed"
May 21 19:53:48.780: INFO: Pod "pod-projected-secrets-ab22d896-275c-4273-968b-30979bcfec98": Phase="Pending", Reason="", readiness=false. Elapsed: 7.478669ms
May 21 19:53:50.789: INFO: Pod "pod-projected-secrets-ab22d896-275c-4273-968b-30979bcfec98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015719709s
May 21 19:53:52.795: INFO: Pod "pod-projected-secrets-ab22d896-275c-4273-968b-30979bcfec98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022384324s
STEP: Saw pod success
May 21 19:53:52.795: INFO: Pod "pod-projected-secrets-ab22d896-275c-4273-968b-30979bcfec98" satisfied condition "Succeeded or Failed"
May 21 19:53:52.807: INFO: Trying to get logs from node dc-hg-1 pod pod-projected-secrets-ab22d896-275c-4273-968b-30979bcfec98 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 19:53:52.879: INFO: Waiting for pod pod-projected-secrets-ab22d896-275c-4273-968b-30979bcfec98 to disappear
May 21 19:53:52.887: INFO: Pod pod-projected-secrets-ab22d896-275c-4273-968b-30979bcfec98 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:53:52.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9555" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":163,"skipped":2570,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:53:52.931: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 21 19:53:53.223: INFO: Waiting up to 5m0s for pod "pod-94245c5a-3f3f-46bc-8389-0a0b5d7b3666" in namespace "emptydir-5817" to be "Succeeded or Failed"
May 21 19:53:53.245: INFO: Pod "pod-94245c5a-3f3f-46bc-8389-0a0b5d7b3666": Phase="Pending", Reason="", readiness=false. Elapsed: 21.937652ms
May 21 19:53:55.252: INFO: Pod "pod-94245c5a-3f3f-46bc-8389-0a0b5d7b3666": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02909677s
May 21 19:53:57.259: INFO: Pod "pod-94245c5a-3f3f-46bc-8389-0a0b5d7b3666": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03630084s
STEP: Saw pod success
May 21 19:53:57.259: INFO: Pod "pod-94245c5a-3f3f-46bc-8389-0a0b5d7b3666" satisfied condition "Succeeded or Failed"
May 21 19:53:57.266: INFO: Trying to get logs from node dc-hg-2 pod pod-94245c5a-3f3f-46bc-8389-0a0b5d7b3666 container test-container: <nil>
STEP: delete the pod
May 21 19:53:57.324: INFO: Waiting for pod pod-94245c5a-3f3f-46bc-8389-0a0b5d7b3666 to disappear
May 21 19:53:57.340: INFO: Pod pod-94245c5a-3f3f-46bc-8389-0a0b5d7b3666 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:53:57.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5817" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2578,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:53:57.363: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-6b43038f-5a40-48fc-8e00-baee637c1b21 in namespace container-probe-4345
May 21 19:54:01.492: INFO: Started pod liveness-6b43038f-5a40-48fc-8e00-baee637c1b21 in namespace container-probe-4345
STEP: checking the pod's current state and verifying that restartCount is present
May 21 19:54:01.497: INFO: Initial restart count of pod liveness-6b43038f-5a40-48fc-8e00-baee637c1b21 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:58:02.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4345" for this suite.

• [SLOW TEST:245.313 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":165,"skipped":2586,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:58:02.676: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 21 19:58:06.826: INFO: &Pod{ObjectMeta:{send-events-3a1393c1-cb8d-4175-833d-ee62685a3272  events-2703 /api/v1/namespaces/events-2703/pods/send-events-3a1393c1-cb8d-4175-833d-ee62685a3272 1c399a56-45f6-4cd8-ae76-5849d0b4ac37 1278466 0 2021-05-21 19:58:02 +0000 UTC <nil> <nil> map[name:foo time:779018677] map[cni.projectcalico.org/podIP:192.168.161.178/32 cni.projectcalico.org/podIPs:192.168.161.178/32] [] []  [{e2e.test Update v1 2021-05-21 19:58:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:58:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.161.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nrlwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nrlwb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nrlwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:58:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:58:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:58:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:192.168.161.178,StartTime:2021-05-21 19:58:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:58:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://f93181c805bfa7ec7ec89ecad528b496762e0513e93f179603a971323a1f7a14,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.161.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 21 19:58:08.834: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 21 19:58:10.842: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:58:10.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2703" for this suite.

• [SLOW TEST:8.247 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":166,"skipped":2591,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:58:10.926: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:58:11.025: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 21 19:58:11.045: INFO: Pod name sample-pod: Found 0 pods out of 1
May 21 19:58:16.057: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 19:58:16.057: INFO: Creating deployment "test-rolling-update-deployment"
May 21 19:58:16.077: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 21 19:58:16.099: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 21 19:58:18.111: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 21 19:58:18.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223896, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223896, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223896, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223896, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:58:20.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223896, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223896, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223896, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223896, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 19:58:22.131: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 21 19:58:22.161: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5154 /apis/apps/v1/namespaces/deployment-5154/deployments/test-rolling-update-deployment 5bdc3a39-8d71-4d9d-b489-2410f668662c 1278574 1 2021-05-21 19:58:16 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-21 19:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 19:58:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bbee38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-21 19:58:16 +0000 UTC,LastTransitionTime:2021-05-21 19:58:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-05-21 19:58:20 +0000 UTC,LastTransitionTime:2021-05-21 19:58:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 21 19:58:22.174: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-5154 /apis/apps/v1/namespaces/deployment-5154/replicasets/test-rolling-update-deployment-c4cb8d6d9 11300979-1ed0-448c-b9e0-b61b65038cc7 1278561 1 2021-05-21 19:58:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5bdc3a39-8d71-4d9d-b489-2410f668662c 0xc004bbf3a0 0xc004bbf3a1}] []  [{kube-controller-manager Update apps/v1 2021-05-21 19:58:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5bdc3a39-8d71-4d9d-b489-2410f668662c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bbf418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 19:58:22.174: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 21 19:58:22.175: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5154 /apis/apps/v1/namespaces/deployment-5154/replicasets/test-rolling-update-controller 8c77b2a6-3d17-4acc-9314-b3408a1b43da 1278573 2 2021-05-21 19:58:11 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5bdc3a39-8d71-4d9d-b489-2410f668662c 0xc004bbf297 0xc004bbf298}] []  [{e2e.test Update apps/v1 2021-05-21 19:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 19:58:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5bdc3a39-8d71-4d9d-b489-2410f668662c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004bbf338 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 19:58:22.184: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-dd67q" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-dd67q test-rolling-update-deployment-c4cb8d6d9- deployment-5154 /api/v1/namespaces/deployment-5154/pods/test-rolling-update-deployment-c4cb8d6d9-dd67q 07c0479d-be9e-434e-8947-443b5dc55062 1278560 0 2021-05-21 19:58:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:192.168.161.179/32 cni.projectcalico.org/podIPs:192.168.161.179/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 11300979-1ed0-448c-b9e0-b61b65038cc7 0xc003536000 0xc003536001}] []  [{kube-controller-manager Update v1 2021-05-21 19:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11300979-1ed0-448c-b9e0-b61b65038cc7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 19:58:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 19:58:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.161.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v279w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v279w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v279w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:58:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:58:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 19:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:192.168.161.179,StartTime:2021-05-21 19:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 19:58:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://3ddd414886fdfd2bb58408b0675d89ff2ba5b0fb476ff1af335ebff58cc77ac7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.161.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:58:22.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5154" for this suite.

• [SLOW TEST:11.297 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":167,"skipped":2619,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:58:22.252: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
May 21 19:58:28.515: INFO: Pod pod-hostip-05a45118-b73d-43a3-a022-fa44839db061 has hostIP: 10.10.1.136
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:58:28.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8273" for this suite.

• [SLOW TEST:6.296 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":168,"skipped":2640,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:58:28.561: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 21 19:58:28.673: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 19:58:28.687: INFO: Waiting for terminating namespaces to be deleted...
May 21 19:58:28.694: INFO: 
Logging pods the apiserver thinks is on node conformance1 before test
May 21 19:58:28.718: INFO: haproxy-ingress-85597cc495-l7rzc from ingress-haproxy started at 2021-04-26 19:00:07 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.718: INFO: 	Container haproxy-ingress ready: true, restart count 2
May 21 19:58:28.718: INFO: ingress-default-backend-5f65f5865b-nldzd from ingress-haproxy started at 2021-04-26 19:00:28 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.718: INFO: 	Container ingress-default-backend ready: true, restart count 2
May 21 19:58:28.718: INFO: calico-kube-controllers-db766f945-4228n from kube-system started at 2021-04-26 19:00:07 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.718: INFO: 	Container calico-kube-controllers ready: true, restart count 2
May 21 19:58:28.718: INFO: calico-node-j5hq5 from kube-system started at 2021-04-23 21:19:36 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.718: INFO: 	Container calico-node ready: true, restart count 2
May 21 19:58:28.718: INFO: coredns-c6fc4f979-bv84n from kube-system started at 2021-04-26 19:00:09 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.718: INFO: 	Container coredns ready: true, restart count 2
May 21 19:58:28.718: INFO: coredns-c6fc4f979-jfxfx from kube-system started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.718: INFO: 	Container coredns ready: true, restart count 2
May 21 19:58:28.718: INFO: metrics-server-6c6c6b5d47-9r52q from kube-system started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.718: INFO: 	Container metrics-server ready: true, restart count 2
May 21 19:58:28.718: INFO: nirmata-cni-installer-zjkpx from nirmata started at 2021-04-26 19:00:42 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.718: INFO: 	Container install-cni ready: true, restart count 2
May 21 19:58:28.718: INFO: nirmata-kube-controller-5cf898966b-md6gc from nirmata started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.718: INFO: 	Container nirmata-kube-controller ready: true, restart count 8
May 21 19:58:28.718: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-47fv4 from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 19:58:28.718: INFO: 	Container sonobuoy-worker ready: false, restart count 4
May 21 19:58:28.718: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 19:58:28.718: INFO: 
Logging pods the apiserver thinks is on node dc-hg-1 before test
May 21 19:58:28.733: INFO: send-events-3a1393c1-cb8d-4175-833d-ee62685a3272 from events-2703 started at 2021-05-21 19:58:02 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.733: INFO: 	Container p ready: true, restart count 0
May 21 19:58:28.733: INFO: calico-node-6r5n7 from kube-system started at 2021-05-21 02:51:05 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.733: INFO: 	Container calico-node ready: true, restart count 0
May 21 19:58:28.733: INFO: nirmata-cni-installer-xzg69 from nirmata started at 2021-05-21 19:24:35 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.733: INFO: 	Container install-cni ready: true, restart count 0
May 21 19:58:28.733: INFO: sonobuoy from sonobuoy started at 2021-05-21 18:56:18 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.733: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 19:58:28.733: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-xch78 from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 19:58:28.733: INFO: 	Container sonobuoy-worker ready: false, restart count 1
May 21 19:58:28.733: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 19:58:28.733: INFO: 
Logging pods the apiserver thinks is on node dc-hg-2 before test
May 21 19:58:28.760: INFO: calico-node-c7rz4 from kube-system started at 2021-04-27 15:00:32 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.760: INFO: 	Container calico-node ready: true, restart count 2
May 21 19:58:28.760: INFO: nirmata-cni-installer-5m9z5 from nirmata started at 2021-05-20 07:33:18 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.760: INFO: 	Container install-cni ready: true, restart count 0
May 21 19:58:28.760: INFO: pod-hostip-05a45118-b73d-43a3-a022-fa44839db061 from pods-8273 started at 2021-05-21 19:58:22 +0000 UTC (1 container statuses recorded)
May 21 19:58:28.760: INFO: 	Container test ready: true, restart count 0
May 21 19:58:28.760: INFO: sonobuoy-e2e-job-03ad7d58f5964dbf from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 19:58:28.760: INFO: 	Container e2e ready: true, restart count 0
May 21 19:58:28.760: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 19:58:28.760: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-lxlxb from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 19:58:28.760: INFO: 	Container sonobuoy-worker ready: false, restart count 4
May 21 19:58:28.760: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-41c369ff-deba-4c50-b4c2-fc93880d9361 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-41c369ff-deba-4c50-b4c2-fc93880d9361 off the node dc-hg-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-41c369ff-deba-4c50-b4c2-fc93880d9361
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:58:36.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-769" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.456 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":169,"skipped":2651,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:58:37.018: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 19:58:39.162: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 19:58:41.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223919, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223919, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223919, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757223919, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 19:58:44.225: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:58:44.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8971" for this suite.
STEP: Destroying namespace "webhook-8971-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.643 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":170,"skipped":2651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:58:44.664: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8982
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8982
I0521 19:58:44.933824      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8982, replica count: 2
I0521 19:58:47.984649      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 19:58:50.985: INFO: Creating new exec pod
I0521 19:58:50.985117      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 19:58:56.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8982 exec execpodtcb9n -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 21 19:58:56.730: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 21 19:58:56.730: INFO: stdout: ""
May 21 19:58:56.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8982 exec execpodtcb9n -- /bin/sh -x -c nc -zv -t -w 2 10.10.114.163 80'
May 21 19:58:57.371: INFO: stderr: "+ nc -zv -t -w 2 10.10.114.163 80\nConnection to 10.10.114.163 80 port [tcp/http] succeeded!\n"
May 21 19:58:57.371: INFO: stdout: ""
May 21 19:58:57.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8982 exec execpodtcb9n -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.86 30560'
May 21 19:58:57.926: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.86 30560\nConnection to 10.10.1.86 30560 port [tcp/30560] succeeded!\n"
May 21 19:58:57.926: INFO: stdout: ""
May 21 19:58:57.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-8982 exec execpodtcb9n -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.136 30560'
May 21 19:58:58.430: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.136 30560\nConnection to 10.10.1.136 30560 port [tcp/30560] succeeded!\n"
May 21 19:58:58.430: INFO: stdout: ""
May 21 19:58:58.430: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:58:58.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8982" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:13.885 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":171,"skipped":2696,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:58:58.549: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-ceefcff3-dc62-431c-9741-f4b943fe0aec
STEP: Creating a pod to test consume secrets
May 21 19:58:58.735: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8833bca6-3847-4b60-a640-a5987d07fb4d" in namespace "projected-9763" to be "Succeeded or Failed"
May 21 19:58:58.752: INFO: Pod "pod-projected-secrets-8833bca6-3847-4b60-a640-a5987d07fb4d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.792814ms
May 21 19:59:00.759: INFO: Pod "pod-projected-secrets-8833bca6-3847-4b60-a640-a5987d07fb4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023664827s
May 21 19:59:02.767: INFO: Pod "pod-projected-secrets-8833bca6-3847-4b60-a640-a5987d07fb4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031883323s
STEP: Saw pod success
May 21 19:59:02.767: INFO: Pod "pod-projected-secrets-8833bca6-3847-4b60-a640-a5987d07fb4d" satisfied condition "Succeeded or Failed"
May 21 19:59:02.774: INFO: Trying to get logs from node dc-hg-1 pod pod-projected-secrets-8833bca6-3847-4b60-a640-a5987d07fb4d container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 19:59:02.867: INFO: Waiting for pod pod-projected-secrets-8833bca6-3847-4b60-a640-a5987d07fb4d to disappear
May 21 19:59:02.874: INFO: Pod pod-projected-secrets-8833bca6-3847-4b60-a640-a5987d07fb4d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:59:02.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9763" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2708,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:59:02.892: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 19:59:03.092: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eaa9f8d3-fc63-49dd-8f8d-1fd8c6472585" in namespace "downward-api-9962" to be "Succeeded or Failed"
May 21 19:59:03.101: INFO: Pod "downwardapi-volume-eaa9f8d3-fc63-49dd-8f8d-1fd8c6472585": Phase="Pending", Reason="", readiness=false. Elapsed: 9.408361ms
May 21 19:59:05.115: INFO: Pod "downwardapi-volume-eaa9f8d3-fc63-49dd-8f8d-1fd8c6472585": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022622532s
May 21 19:59:07.124: INFO: Pod "downwardapi-volume-eaa9f8d3-fc63-49dd-8f8d-1fd8c6472585": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032089148s
STEP: Saw pod success
May 21 19:59:07.124: INFO: Pod "downwardapi-volume-eaa9f8d3-fc63-49dd-8f8d-1fd8c6472585" satisfied condition "Succeeded or Failed"
May 21 19:59:07.147: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-eaa9f8d3-fc63-49dd-8f8d-1fd8c6472585 container client-container: <nil>
STEP: delete the pod
May 21 19:59:07.230: INFO: Waiting for pod downwardapi-volume-eaa9f8d3-fc63-49dd-8f8d-1fd8c6472585 to disappear
May 21 19:59:07.240: INFO: Pod downwardapi-volume-eaa9f8d3-fc63-49dd-8f8d-1fd8c6472585 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:59:07.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9962" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2718,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:59:07.277: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:59:23.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6662" for this suite.

• [SLOW TEST:16.324 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":174,"skipped":2731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:59:23.606: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5513.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5513.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5513.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5513.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5513.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5513.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 19:59:29.925: INFO: DNS probes using dns-5513/dns-test-08ec49af-d4ec-4090-9936-fd7930b0e2e7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:59:30.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5513" for this suite.

• [SLOW TEST:6.556 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":175,"skipped":2759,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:59:30.162: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-06a7c5d8-2e33-4afd-98a8-9f95969fb85e
STEP: Creating a pod to test consume configMaps
May 21 19:59:30.308: INFO: Waiting up to 5m0s for pod "pod-configmaps-0dfb270f-e69c-42e2-a403-8b568b43fd6f" in namespace "configmap-2910" to be "Succeeded or Failed"
May 21 19:59:30.342: INFO: Pod "pod-configmaps-0dfb270f-e69c-42e2-a403-8b568b43fd6f": Phase="Pending", Reason="", readiness=false. Elapsed: 33.03432ms
May 21 19:59:32.351: INFO: Pod "pod-configmaps-0dfb270f-e69c-42e2-a403-8b568b43fd6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04223626s
May 21 19:59:34.357: INFO: Pod "pod-configmaps-0dfb270f-e69c-42e2-a403-8b568b43fd6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048164744s
May 21 19:59:36.363: INFO: Pod "pod-configmaps-0dfb270f-e69c-42e2-a403-8b568b43fd6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054559576s
STEP: Saw pod success
May 21 19:59:36.363: INFO: Pod "pod-configmaps-0dfb270f-e69c-42e2-a403-8b568b43fd6f" satisfied condition "Succeeded or Failed"
May 21 19:59:36.369: INFO: Trying to get logs from node dc-hg-1 pod pod-configmaps-0dfb270f-e69c-42e2-a403-8b568b43fd6f container configmap-volume-test: <nil>
STEP: delete the pod
May 21 19:59:36.452: INFO: Waiting for pod pod-configmaps-0dfb270f-e69c-42e2-a403-8b568b43fd6f to disappear
May 21 19:59:36.466: INFO: Pod pod-configmaps-0dfb270f-e69c-42e2-a403-8b568b43fd6f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:59:36.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2910" for this suite.

• [SLOW TEST:6.346 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":176,"skipped":2765,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:59:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:59:36.621: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9698
I0521 19:59:36.652363      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9698, replica count: 1
I0521 19:59:37.703545      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 19:59:38.703922      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 19:59:39.705846      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 19:59:39.825: INFO: Created: latency-svc-xqqcb
May 21 19:59:39.856: INFO: Got endpoints: latency-svc-xqqcb [50.737706ms]
May 21 19:59:39.940: INFO: Created: latency-svc-7gr8t
May 21 19:59:39.981: INFO: Created: latency-svc-vjxq2
May 21 19:59:39.981: INFO: Got endpoints: latency-svc-7gr8t [124.165138ms]
May 21 19:59:40.016: INFO: Created: latency-svc-7whqg
May 21 19:59:40.044: INFO: Got endpoints: latency-svc-vjxq2 [184.258598ms]
May 21 19:59:40.053: INFO: Created: latency-svc-45rdq
May 21 19:59:40.119: INFO: Created: latency-svc-4kj4z
May 21 19:59:40.125: INFO: Got endpoints: latency-svc-7whqg [263.795943ms]
May 21 19:59:40.126: INFO: Got endpoints: latency-svc-45rdq [268.56971ms]
May 21 19:59:40.195: INFO: Got endpoints: latency-svc-4kj4z [336.648348ms]
May 21 19:59:40.206: INFO: Created: latency-svc-n4m7f
May 21 19:59:40.238: INFO: Got endpoints: latency-svc-n4m7f [379.856456ms]
May 21 19:59:40.239: INFO: Created: latency-svc-phwxz
May 21 19:59:40.269: INFO: Created: latency-svc-4dpgl
May 21 19:59:40.288: INFO: Created: latency-svc-vkrw6
May 21 19:59:40.300: INFO: Got endpoints: latency-svc-phwxz [440.648211ms]
May 21 19:59:40.313: INFO: Got endpoints: latency-svc-4dpgl [453.48155ms]
May 21 19:59:40.383: INFO: Got endpoints: latency-svc-vkrw6 [522.802727ms]
May 21 19:59:40.384: INFO: Created: latency-svc-pwbgg
May 21 19:59:40.422: INFO: Created: latency-svc-79p8t
May 21 19:59:40.440: INFO: Got endpoints: latency-svc-pwbgg [579.56494ms]
May 21 19:59:40.491: INFO: Created: latency-svc-vxdl9
May 21 19:59:40.525: INFO: Created: latency-svc-cbpbj
May 21 19:59:40.577: INFO: Got endpoints: latency-svc-vxdl9 [716.876013ms]
May 21 19:59:40.577: INFO: Got endpoints: latency-svc-79p8t [717.813993ms]
May 21 19:59:40.579: INFO: Created: latency-svc-wwtjg
May 21 19:59:40.587: INFO: Got endpoints: latency-svc-cbpbj [727.118745ms]
May 21 19:59:40.603: INFO: Created: latency-svc-jrc7m
May 21 19:59:40.613: INFO: Got endpoints: latency-svc-wwtjg [751.981566ms]
May 21 19:59:40.626: INFO: Created: latency-svc-pdh6p
May 21 19:59:40.636: INFO: Got endpoints: latency-svc-jrc7m [775.793986ms]
May 21 19:59:40.645: INFO: Created: latency-svc-jsmwf
May 21 19:59:40.678: INFO: Created: latency-svc-65jk8
May 21 19:59:40.679: INFO: Got endpoints: latency-svc-pdh6p [697.199752ms]
May 21 19:59:40.681: INFO: Got endpoints: latency-svc-jsmwf [637.561065ms]
May 21 19:59:40.687: INFO: Created: latency-svc-hp9gd
May 21 19:59:40.689: INFO: Got endpoints: latency-svc-65jk8 [562.827279ms]
May 21 19:59:40.743: INFO: Got endpoints: latency-svc-hp9gd [618.247183ms]
May 21 19:59:40.755: INFO: Created: latency-svc-92mpz
May 21 19:59:40.761: INFO: Got endpoints: latency-svc-92mpz [566.556952ms]
May 21 19:59:40.784: INFO: Created: latency-svc-jtjp8
May 21 19:59:40.802: INFO: Created: latency-svc-gwsvn
May 21 19:59:40.809: INFO: Got endpoints: latency-svc-jtjp8 [570.30966ms]
May 21 19:59:40.825: INFO: Created: latency-svc-m6gxw
May 21 19:59:40.827: INFO: Got endpoints: latency-svc-gwsvn [526.876039ms]
May 21 19:59:40.862: INFO: Created: latency-svc-pvxcc
May 21 19:59:40.862: INFO: Got endpoints: latency-svc-m6gxw [549.686609ms]
May 21 19:59:40.914: INFO: Got endpoints: latency-svc-pvxcc [530.991527ms]
May 21 19:59:40.935: INFO: Created: latency-svc-k9jnf
May 21 19:59:40.959: INFO: Got endpoints: latency-svc-k9jnf [518.753438ms]
May 21 19:59:40.998: INFO: Created: latency-svc-gcljf
May 21 19:59:41.015: INFO: Got endpoints: latency-svc-gcljf [437.092214ms]
May 21 19:59:41.043: INFO: Created: latency-svc-54klg
May 21 19:59:41.089: INFO: Got endpoints: latency-svc-54klg [511.686747ms]
May 21 19:59:41.098: INFO: Created: latency-svc-bdz24
May 21 19:59:41.098: INFO: Created: latency-svc-5wjj8
May 21 19:59:41.143: INFO: Created: latency-svc-9sjqp
May 21 19:59:41.182: INFO: Created: latency-svc-9p6jr
May 21 19:59:41.202: INFO: Got endpoints: latency-svc-9sjqp [566.061461ms]
May 21 19:59:41.203: INFO: Got endpoints: latency-svc-bdz24 [615.972919ms]
May 21 19:59:41.203: INFO: Got endpoints: latency-svc-5wjj8 [590.653563ms]
May 21 19:59:41.214: INFO: Created: latency-svc-qwn5l
May 21 19:59:41.237: INFO: Got endpoints: latency-svc-9p6jr [558.245218ms]
May 21 19:59:41.239: INFO: Got endpoints: latency-svc-qwn5l [549.292ms]
May 21 19:59:41.265: INFO: Created: latency-svc-xpx62
May 21 19:59:41.286: INFO: Created: latency-svc-5tl7n
May 21 19:59:41.307: INFO: Got endpoints: latency-svc-xpx62 [626.050508ms]
May 21 19:59:41.309: INFO: Created: latency-svc-82c55
May 21 19:59:41.327: INFO: Created: latency-svc-sndz2
May 21 19:59:41.330: INFO: Got endpoints: latency-svc-5tl7n [586.358818ms]
May 21 19:59:41.352: INFO: Got endpoints: latency-svc-82c55 [590.969798ms]
May 21 19:59:41.357: INFO: Created: latency-svc-f6ngs
May 21 19:59:41.386: INFO: Created: latency-svc-6vb87
May 21 19:59:41.391: INFO: Got endpoints: latency-svc-f6ngs [564.262076ms]
May 21 19:59:41.392: INFO: Got endpoints: latency-svc-sndz2 [583.129395ms]
May 21 19:59:41.429: INFO: Got endpoints: latency-svc-6vb87 [566.022915ms]
May 21 19:59:41.429: INFO: Created: latency-svc-dtlsw
May 21 19:59:41.470: INFO: Got endpoints: latency-svc-dtlsw [555.710651ms]
May 21 19:59:41.484: INFO: Created: latency-svc-ghrt8
May 21 19:59:41.549: INFO: Created: latency-svc-zcrnr
May 21 19:59:41.587: INFO: Got endpoints: latency-svc-ghrt8 [628.669785ms]
May 21 19:59:41.605: INFO: Created: latency-svc-6x28s
May 21 19:59:41.615: INFO: Got endpoints: latency-svc-zcrnr [600.188707ms]
May 21 19:59:41.658: INFO: Got endpoints: latency-svc-6x28s [568.451355ms]
May 21 19:59:41.660: INFO: Created: latency-svc-ppk2h
May 21 19:59:41.705: INFO: Created: latency-svc-n8wlx
May 21 19:59:41.734: INFO: Created: latency-svc-x24tk
May 21 19:59:41.748: INFO: Created: latency-svc-twqsm
May 21 19:59:41.786: INFO: Got endpoints: latency-svc-ppk2h [583.632317ms]
May 21 19:59:41.786: INFO: Got endpoints: latency-svc-n8wlx [582.545534ms]
May 21 19:59:41.788: INFO: Created: latency-svc-654vw
May 21 19:59:41.843: INFO: Got endpoints: latency-svc-twqsm [605.80729ms]
May 21 19:59:41.843: INFO: Got endpoints: latency-svc-654vw [604.55454ms]
May 21 19:59:41.844: INFO: Got endpoints: latency-svc-x24tk [640.173022ms]
May 21 19:59:41.879: INFO: Created: latency-svc-bc5pt
May 21 19:59:42.007: INFO: Created: latency-svc-dxsnc
May 21 19:59:42.013: INFO: Got endpoints: latency-svc-bc5pt [705.223807ms]
May 21 19:59:42.060: INFO: Got endpoints: latency-svc-dxsnc [730.098166ms]
May 21 19:59:42.068: INFO: Created: latency-svc-t2klp
May 21 19:59:42.068: INFO: Got endpoints: latency-svc-t2klp [715.727166ms]
May 21 19:59:42.133: INFO: Created: latency-svc-cqxpq
May 21 19:59:42.194: INFO: Got endpoints: latency-svc-cqxpq [803.158563ms]
May 21 19:59:42.218: INFO: Created: latency-svc-vxxqp
May 21 19:59:42.268: INFO: Created: latency-svc-hffhs
May 21 19:59:42.282: INFO: Created: latency-svc-nwlzx
May 21 19:59:42.283: INFO: Got endpoints: latency-svc-vxxqp [890.716687ms]
May 21 19:59:42.303: INFO: Created: latency-svc-s8h4s
May 21 19:59:42.315: INFO: Got endpoints: latency-svc-hffhs [844.745533ms]
May 21 19:59:42.319: INFO: Got endpoints: latency-svc-nwlzx [889.958178ms]
May 21 19:59:42.345: INFO: Created: latency-svc-xlqkm
May 21 19:59:42.348: INFO: Got endpoints: latency-svc-s8h4s [759.967361ms]
May 21 19:59:42.375: INFO: Got endpoints: latency-svc-xlqkm [759.870392ms]
May 21 19:59:42.380: INFO: Created: latency-svc-5rqhx
May 21 19:59:42.413: INFO: Got endpoints: latency-svc-5rqhx [754.973654ms]
May 21 19:59:42.448: INFO: Created: latency-svc-f2wjr
May 21 19:59:42.458: INFO: Got endpoints: latency-svc-f2wjr [672.745707ms]
May 21 19:59:42.472: INFO: Created: latency-svc-dx9p8
May 21 19:59:42.501: INFO: Got endpoints: latency-svc-dx9p8 [714.45888ms]
May 21 19:59:42.509: INFO: Created: latency-svc-fp4qn
May 21 19:59:42.514: INFO: Created: latency-svc-7cksd
May 21 19:59:42.548: INFO: Created: latency-svc-gmkwc
May 21 19:59:42.564: INFO: Got endpoints: latency-svc-fp4qn [720.660919ms]
May 21 19:59:42.564: INFO: Got endpoints: latency-svc-7cksd [719.942273ms]
May 21 19:59:42.574: INFO: Created: latency-svc-48dj8
May 21 19:59:42.577: INFO: Got endpoints: latency-svc-gmkwc [733.555084ms]
May 21 19:59:42.619: INFO: Created: latency-svc-jkfvf
May 21 19:59:42.638: INFO: Got endpoints: latency-svc-48dj8 [625.030131ms]
May 21 19:59:42.654: INFO: Got endpoints: latency-svc-jkfvf [585.856765ms]
May 21 19:59:42.660: INFO: Created: latency-svc-tttqj
May 21 19:59:42.691: INFO: Got endpoints: latency-svc-tttqj [630.836247ms]
May 21 19:59:42.699: INFO: Created: latency-svc-xnb56
May 21 19:59:42.726: INFO: Created: latency-svc-tgj2p
May 21 19:59:42.738: INFO: Got endpoints: latency-svc-xnb56 [544.185711ms]
May 21 19:59:42.760: INFO: Created: latency-svc-pqn6t
May 21 19:59:42.772: INFO: Got endpoints: latency-svc-tgj2p [489.081645ms]
May 21 19:59:42.800: INFO: Got endpoints: latency-svc-pqn6t [484.648783ms]
May 21 19:59:42.802: INFO: Created: latency-svc-dfgk4
May 21 19:59:42.819: INFO: Created: latency-svc-9mrlt
May 21 19:59:42.861: INFO: Created: latency-svc-gnxst
May 21 19:59:42.899: INFO: Got endpoints: latency-svc-9mrlt [551.376034ms]
May 21 19:59:42.900: INFO: Got endpoints: latency-svc-dfgk4 [581.174857ms]
May 21 19:59:42.902: INFO: Created: latency-svc-s4gbt
May 21 19:59:42.919: INFO: Got endpoints: latency-svc-gnxst [543.826751ms]
May 21 19:59:43.024: INFO: Got endpoints: latency-svc-s4gbt [610.923913ms]
May 21 19:59:43.041: INFO: Created: latency-svc-d6qwf
May 21 19:59:43.108: INFO: Got endpoints: latency-svc-d6qwf [649.305367ms]
May 21 19:59:43.130: INFO: Created: latency-svc-ht8s4
May 21 19:59:43.220: INFO: Created: latency-svc-9vrrl
May 21 19:59:43.291: INFO: Got endpoints: latency-svc-9vrrl [727.561944ms]
May 21 19:59:43.291: INFO: Got endpoints: latency-svc-ht8s4 [790.248158ms]
May 21 19:59:43.293: INFO: Created: latency-svc-2th89
May 21 19:59:43.363: INFO: Created: latency-svc-tqq2m
May 21 19:59:43.369: INFO: Got endpoints: latency-svc-2th89 [792.413818ms]
May 21 19:59:43.383: INFO: Created: latency-svc-6pbdn
May 21 19:59:43.391: INFO: Got endpoints: latency-svc-tqq2m [826.821353ms]
May 21 19:59:43.420: INFO: Created: latency-svc-wjqhr
May 21 19:59:43.455: INFO: Got endpoints: latency-svc-6pbdn [816.897213ms]
May 21 19:59:43.459: INFO: Created: latency-svc-9mc4b
May 21 19:59:43.463: INFO: Got endpoints: latency-svc-wjqhr [808.941532ms]
May 21 19:59:43.492: INFO: Got endpoints: latency-svc-9mc4b [801.028926ms]
May 21 19:59:43.501: INFO: Created: latency-svc-cmgr7
May 21 19:59:43.531: INFO: Created: latency-svc-vkf2q
May 21 19:59:43.538: INFO: Got endpoints: latency-svc-cmgr7 [799.444533ms]
May 21 19:59:43.583: INFO: Got endpoints: latency-svc-vkf2q [811.605096ms]
May 21 19:59:43.608: INFO: Created: latency-svc-fb7df
May 21 19:59:43.675: INFO: Created: latency-svc-sn6q9
May 21 19:59:43.755: INFO: Got endpoints: latency-svc-fb7df [954.827207ms]
May 21 19:59:43.773: INFO: Created: latency-svc-clx6c
May 21 19:59:43.800: INFO: Got endpoints: latency-svc-sn6q9 [880.824614ms]
May 21 19:59:43.831: INFO: Got endpoints: latency-svc-clx6c [931.558166ms]
May 21 19:59:43.907: INFO: Created: latency-svc-txxn8
May 21 19:59:43.907: INFO: Got endpoints: latency-svc-txxn8 [1.007218167s]
May 21 19:59:43.918: INFO: Created: latency-svc-wr8qq
May 21 19:59:43.927: INFO: Got endpoints: latency-svc-wr8qq [903.118745ms]
May 21 19:59:43.962: INFO: Created: latency-svc-rdwnp
May 21 19:59:43.962: INFO: Created: latency-svc-b2vrt
May 21 19:59:44.000: INFO: Created: latency-svc-ltmlb
May 21 19:59:44.009: INFO: Got endpoints: latency-svc-rdwnp [717.729116ms]
May 21 19:59:44.010: INFO: Got endpoints: latency-svc-b2vrt [902.062276ms]
May 21 19:59:44.023: INFO: Got endpoints: latency-svc-ltmlb [730.98438ms]
May 21 19:59:44.035: INFO: Created: latency-svc-2blhm
May 21 19:59:44.047: INFO: Created: latency-svc-sk2sx
May 21 19:59:44.078: INFO: Got endpoints: latency-svc-2blhm [708.739109ms]
May 21 19:59:44.084: INFO: Created: latency-svc-hrjqd
May 21 19:59:44.205: INFO: Got endpoints: latency-svc-hrjqd [741.982793ms]
May 21 19:59:44.206: INFO: Got endpoints: latency-svc-sk2sx [814.88059ms]
May 21 19:59:44.228: INFO: Created: latency-svc-jqmnx
May 21 19:59:44.243: INFO: Got endpoints: latency-svc-jqmnx [787.864629ms]
May 21 19:59:44.247: INFO: Created: latency-svc-hgv7h
May 21 19:59:44.292: INFO: Got endpoints: latency-svc-hgv7h [799.864871ms]
May 21 19:59:44.300: INFO: Created: latency-svc-mrds7
May 21 19:59:44.321: INFO: Created: latency-svc-fjn22
May 21 19:59:44.338: INFO: Got endpoints: latency-svc-fjn22 [754.001109ms]
May 21 19:59:44.356: INFO: Created: latency-svc-2kvpk
May 21 19:59:44.372: INFO: Got endpoints: latency-svc-mrds7 [833.935078ms]
May 21 19:59:44.390: INFO: Created: latency-svc-lgl7p
May 21 19:59:44.392: INFO: Got endpoints: latency-svc-2kvpk [636.75829ms]
May 21 19:59:44.424: INFO: Got endpoints: latency-svc-lgl7p [624.347845ms]
May 21 19:59:44.467: INFO: Created: latency-svc-gs7v5
May 21 19:59:44.530: INFO: Created: latency-svc-gtxct
May 21 19:59:44.556: INFO: Got endpoints: latency-svc-gs7v5 [725.1847ms]
May 21 19:59:44.614: INFO: Got endpoints: latency-svc-gtxct [706.748746ms]
May 21 19:59:44.618: INFO: Created: latency-svc-rmbwt
May 21 19:59:44.643: INFO: Created: latency-svc-q4h87
May 21 19:59:44.662: INFO: Got endpoints: latency-svc-rmbwt [735.387415ms]
May 21 19:59:44.679: INFO: Created: latency-svc-rjwgw
May 21 19:59:44.699: INFO: Got endpoints: latency-svc-q4h87 [689.75063ms]
May 21 19:59:44.757: INFO: Created: latency-svc-tqtp6
May 21 19:59:44.768: INFO: Got endpoints: latency-svc-rjwgw [758.207034ms]
May 21 19:59:44.770: INFO: Created: latency-svc-rxhmv
May 21 19:59:44.824: INFO: Got endpoints: latency-svc-tqtp6 [800.502129ms]
May 21 19:59:44.858: INFO: Got endpoints: latency-svc-rxhmv [779.362833ms]
May 21 19:59:44.911: INFO: Created: latency-svc-p5xf4
May 21 19:59:44.924: INFO: Created: latency-svc-lx8qv
May 21 19:59:44.988: INFO: Created: latency-svc-g2lj8
May 21 19:59:45.017: INFO: Got endpoints: latency-svc-lx8qv [811.323792ms]
May 21 19:59:45.018: INFO: Got endpoints: latency-svc-p5xf4 [812.985446ms]
May 21 19:59:45.122: INFO: Got endpoints: latency-svc-g2lj8 [879.216761ms]
May 21 19:59:45.137: INFO: Created: latency-svc-gz2ms
May 21 19:59:45.181: INFO: Created: latency-svc-vj6vn
May 21 19:59:45.209: INFO: Got endpoints: latency-svc-gz2ms [917.692375ms]
May 21 19:59:45.283: INFO: Got endpoints: latency-svc-vj6vn [945.206076ms]
May 21 19:59:45.318: INFO: Created: latency-svc-862p7
May 21 19:59:45.396: INFO: Created: latency-svc-8rtkx
May 21 19:59:45.566: INFO: Got endpoints: latency-svc-8rtkx [1.174778625s]
May 21 19:59:45.566: INFO: Got endpoints: latency-svc-862p7 [1.194411756s]
May 21 19:59:45.630: INFO: Created: latency-svc-dxc2l
May 21 19:59:45.681: INFO: Created: latency-svc-2zvv9
May 21 19:59:45.717: INFO: Got endpoints: latency-svc-dxc2l [1.292502948s]
May 21 19:59:45.769: INFO: Created: latency-svc-f8rgk
May 21 19:59:45.798: INFO: Got endpoints: latency-svc-2zvv9 [1.241713013s]
May 21 19:59:45.922: INFO: Created: latency-svc-v89jr
May 21 19:59:46.057: INFO: Got endpoints: latency-svc-f8rgk [1.44315021s]
May 21 19:59:46.059: INFO: Got endpoints: latency-svc-v89jr [1.39642878s]
May 21 19:59:46.061: INFO: Created: latency-svc-d76w4
May 21 19:59:46.061: INFO: Got endpoints: latency-svc-d76w4 [1.361884517s]
May 21 19:59:46.198: INFO: Created: latency-svc-dbp4n
May 21 19:59:46.226: INFO: Got endpoints: latency-svc-dbp4n [1.457380435s]
May 21 19:59:46.300: INFO: Created: latency-svc-psvzt
May 21 19:59:46.310: INFO: Got endpoints: latency-svc-psvzt [1.486271495s]
May 21 19:59:46.335: INFO: Created: latency-svc-p4jsz
May 21 19:59:46.349: INFO: Created: latency-svc-779ld
May 21 19:59:46.360: INFO: Created: latency-svc-jtmfh
May 21 19:59:46.390: INFO: Got endpoints: latency-svc-p4jsz [1.372072389s]
May 21 19:59:46.392: INFO: Got endpoints: latency-svc-779ld [1.534664309s]
May 21 19:59:46.424: INFO: Got endpoints: latency-svc-jtmfh [1.406807707s]
May 21 19:59:46.435: INFO: Created: latency-svc-qrww9
May 21 19:59:46.445: INFO: Created: latency-svc-gpvxg
May 21 19:59:46.470: INFO: Got endpoints: latency-svc-qrww9 [1.348148592s]
May 21 19:59:46.560: INFO: Got endpoints: latency-svc-gpvxg [1.35058538s]
May 21 19:59:46.564: INFO: Created: latency-svc-m9sgp
May 21 19:59:46.617: INFO: Created: latency-svc-96gx6
May 21 19:59:46.634: INFO: Got endpoints: latency-svc-m9sgp [1.35098173s]
May 21 19:59:46.651: INFO: Created: latency-svc-2gfcp
May 21 19:59:46.675: INFO: Created: latency-svc-9625f
May 21 19:59:46.704: INFO: Created: latency-svc-4rb8c
May 21 19:59:46.708: INFO: Got endpoints: latency-svc-96gx6 [1.141295573s]
May 21 19:59:46.740: INFO: Created: latency-svc-v6qdk
May 21 19:59:46.773: INFO: Got endpoints: latency-svc-2gfcp [1.206699745s]
May 21 19:59:46.773: INFO: Got endpoints: latency-svc-4rb8c [975.273721ms]
May 21 19:59:46.774: INFO: Got endpoints: latency-svc-9625f [1.05707019s]
May 21 19:59:46.812: INFO: Got endpoints: latency-svc-v6qdk [754.624206ms]
May 21 19:59:46.826: INFO: Created: latency-svc-7rvhb
May 21 19:59:46.907: INFO: Created: latency-svc-bsgw5
May 21 19:59:46.974: INFO: Created: latency-svc-ptcft
May 21 19:59:47.030: INFO: Got endpoints: latency-svc-bsgw5 [969.03702ms]
May 21 19:59:47.035: INFO: Got endpoints: latency-svc-7rvhb [976.368988ms]
May 21 19:59:47.192: INFO: Created: latency-svc-njmv4
May 21 19:59:47.193: INFO: Created: latency-svc-hnkzh
May 21 19:59:47.195: INFO: Created: latency-svc-tcm7s
May 21 19:59:47.195: INFO: Got endpoints: latency-svc-ptcft [969.249767ms]
May 21 19:59:47.215: INFO: Created: latency-svc-5kp42
May 21 19:59:47.215: INFO: Got endpoints: latency-svc-5kp42 [904.106524ms]
May 21 19:59:47.235: INFO: Got endpoints: latency-svc-hnkzh [842.971339ms]
May 21 19:59:47.236: INFO: Got endpoints: latency-svc-tcm7s [846.088336ms]
May 21 19:59:47.273: INFO: Created: latency-svc-cblt4
May 21 19:59:47.273: INFO: Got endpoints: latency-svc-njmv4 [848.617741ms]
May 21 19:59:47.289: INFO: Created: latency-svc-mx97j
May 21 19:59:47.322: INFO: Created: latency-svc-p8z6b
May 21 19:59:47.367: INFO: Got endpoints: latency-svc-cblt4 [806.354128ms]
May 21 19:59:47.367: INFO: Got endpoints: latency-svc-mx97j [897.104661ms]
May 21 19:59:47.411: INFO: Got endpoints: latency-svc-p8z6b [777.465617ms]
May 21 19:59:47.460: INFO: Created: latency-svc-rmvs7
May 21 19:59:47.480: INFO: Got endpoints: latency-svc-rmvs7 [772.419199ms]
May 21 19:59:47.491: INFO: Created: latency-svc-rqmxr
May 21 19:59:47.525: INFO: Created: latency-svc-twcrd
May 21 19:59:47.531: INFO: Got endpoints: latency-svc-rqmxr [757.973945ms]
May 21 19:59:47.597: INFO: Got endpoints: latency-svc-twcrd [823.163128ms]
May 21 19:59:47.611: INFO: Created: latency-svc-h4rcj
May 21 19:59:47.751: INFO: Got endpoints: latency-svc-h4rcj [977.645155ms]
May 21 19:59:47.755: INFO: Created: latency-svc-f8gqx
May 21 19:59:47.783: INFO: Created: latency-svc-lmwmd
May 21 19:59:47.790: INFO: Got endpoints: latency-svc-f8gqx [977.710087ms]
May 21 19:59:47.803: INFO: Got endpoints: latency-svc-lmwmd [772.550512ms]
May 21 19:59:47.864: INFO: Created: latency-svc-7h8hh
May 21 19:59:47.907: INFO: Got endpoints: latency-svc-7h8hh [871.771074ms]
May 21 19:59:47.943: INFO: Created: latency-svc-dchtw
May 21 19:59:48.002: INFO: Created: latency-svc-prd5s
May 21 19:59:48.003: INFO: Created: latency-svc-drqcj
May 21 19:59:48.079: INFO: Got endpoints: latency-svc-prd5s [864.422937ms]
May 21 19:59:48.080: INFO: Got endpoints: latency-svc-dchtw [884.541794ms]
May 21 19:59:48.080: INFO: Got endpoints: latency-svc-drqcj [843.941866ms]
May 21 19:59:48.100: INFO: Created: latency-svc-nfz4l
May 21 19:59:48.126: INFO: Created: latency-svc-lhft2
May 21 19:59:48.151: INFO: Got endpoints: latency-svc-nfz4l [915.372659ms]
May 21 19:59:48.158: INFO: Got endpoints: latency-svc-lhft2 [884.71545ms]
May 21 19:59:48.195: INFO: Created: latency-svc-sb7sp
May 21 19:59:48.223: INFO: Created: latency-svc-8jp7g
May 21 19:59:48.225: INFO: Got endpoints: latency-svc-sb7sp [857.845779ms]
May 21 19:59:48.255: INFO: Created: latency-svc-rkqxl
May 21 19:59:48.283: INFO: Got endpoints: latency-svc-8jp7g [871.86534ms]
May 21 19:59:48.289: INFO: Got endpoints: latency-svc-rkqxl [921.898317ms]
May 21 19:59:48.339: INFO: Created: latency-svc-rpgjl
May 21 19:59:48.341: INFO: Created: latency-svc-2zqn7
May 21 19:59:48.361: INFO: Created: latency-svc-t58jk
May 21 19:59:48.373: INFO: Got endpoints: latency-svc-2zqn7 [892.138453ms]
May 21 19:59:48.399: INFO: Created: latency-svc-x5tf5
May 21 19:59:48.416: INFO: Got endpoints: latency-svc-rpgjl [884.680045ms]
May 21 19:59:48.441: INFO: Created: latency-svc-gq4zs
May 21 19:59:48.450: INFO: Got endpoints: latency-svc-t58jk [852.870826ms]
May 21 19:59:48.477: INFO: Created: latency-svc-6vsqz
May 21 19:59:48.532: INFO: Created: latency-svc-sshzr
May 21 19:59:48.532: INFO: Got endpoints: latency-svc-gq4zs [742.230203ms]
May 21 19:59:48.532: INFO: Got endpoints: latency-svc-x5tf5 [781.230839ms]
May 21 19:59:48.560: INFO: Got endpoints: latency-svc-6vsqz [757.485824ms]
May 21 19:59:48.571: INFO: Got endpoints: latency-svc-sshzr [663.670065ms]
May 21 19:59:48.586: INFO: Created: latency-svc-dq5qt
May 21 19:59:48.603: INFO: Got endpoints: latency-svc-dq5qt [524.041846ms]
May 21 19:59:48.639: INFO: Created: latency-svc-x44pf
May 21 19:59:48.669: INFO: Got endpoints: latency-svc-x44pf [588.33624ms]
May 21 19:59:48.681: INFO: Created: latency-svc-v65nb
May 21 19:59:48.714: INFO: Created: latency-svc-blc6m
May 21 19:59:48.723: INFO: Got endpoints: latency-svc-v65nb [643.168321ms]
May 21 19:59:48.728: INFO: Got endpoints: latency-svc-blc6m [576.732084ms]
May 21 19:59:48.755: INFO: Created: latency-svc-4hx6w
May 21 19:59:48.772: INFO: Created: latency-svc-9jbkt
May 21 19:59:48.780: INFO: Got endpoints: latency-svc-4hx6w [621.841425ms]
May 21 19:59:48.800: INFO: Got endpoints: latency-svc-9jbkt [575.586728ms]
May 21 19:59:48.810: INFO: Created: latency-svc-knffm
May 21 19:59:48.853: INFO: Created: latency-svc-ghcwn
May 21 19:59:48.857: INFO: Got endpoints: latency-svc-knffm [573.776613ms]
May 21 19:59:48.893: INFO: Created: latency-svc-m548q
May 21 19:59:48.915: INFO: Got endpoints: latency-svc-ghcwn [625.296242ms]
May 21 19:59:48.989: INFO: Created: latency-svc-xhncd
May 21 19:59:49.000: INFO: Got endpoints: latency-svc-m548q [626.988627ms]
May 21 19:59:49.067: INFO: Got endpoints: latency-svc-xhncd [650.879742ms]
May 21 19:59:49.103: INFO: Created: latency-svc-jz5kl
May 21 19:59:49.126: INFO: Created: latency-svc-t54qk
May 21 19:59:49.171: INFO: Got endpoints: latency-svc-jz5kl [720.533437ms]
May 21 19:59:49.192: INFO: Created: latency-svc-lvglf
May 21 19:59:49.197: INFO: Got endpoints: latency-svc-t54qk [664.935161ms]
May 21 19:59:49.208: INFO: Got endpoints: latency-svc-lvglf [675.204533ms]
May 21 19:59:49.239: INFO: Created: latency-svc-8lcxv
May 21 19:59:49.277: INFO: Got endpoints: latency-svc-8lcxv [716.472763ms]
May 21 19:59:49.292: INFO: Created: latency-svc-sj2jb
May 21 19:59:49.308: INFO: Created: latency-svc-hrrkk
May 21 19:59:49.424: INFO: Got endpoints: latency-svc-hrrkk [820.822545ms]
May 21 19:59:49.425: INFO: Got endpoints: latency-svc-sj2jb [854.293475ms]
May 21 19:59:49.428: INFO: Created: latency-svc-cll7d
May 21 19:59:49.451: INFO: Got endpoints: latency-svc-cll7d [782.493587ms]
May 21 19:59:49.489: INFO: Created: latency-svc-v6l9h
May 21 19:59:49.515: INFO: Created: latency-svc-j4mlx
May 21 19:59:49.530: INFO: Got endpoints: latency-svc-v6l9h [806.377565ms]
May 21 19:59:49.569: INFO: Got endpoints: latency-svc-j4mlx [841.051268ms]
May 21 19:59:49.575: INFO: Created: latency-svc-lcmwb
May 21 19:59:49.609: INFO: Got endpoints: latency-svc-lcmwb [828.691755ms]
May 21 19:59:49.618: INFO: Created: latency-svc-z6tsb
May 21 19:59:49.639: INFO: Got endpoints: latency-svc-z6tsb [838.572862ms]
May 21 19:59:49.668: INFO: Created: latency-svc-62c7r
May 21 19:59:49.700: INFO: Created: latency-svc-kvx92
May 21 19:59:49.739: INFO: Got endpoints: latency-svc-62c7r [881.92851ms]
May 21 19:59:49.740: INFO: Created: latency-svc-64qm4
May 21 19:59:49.785: INFO: Got endpoints: latency-svc-kvx92 [870.10267ms]
May 21 19:59:49.785: INFO: Got endpoints: latency-svc-64qm4 [785.252309ms]
May 21 19:59:49.812: INFO: Created: latency-svc-s85bj
May 21 19:59:49.870: INFO: Created: latency-svc-n4t8b
May 21 19:59:49.912: INFO: Got endpoints: latency-svc-s85bj [844.53299ms]
May 21 19:59:49.949: INFO: Got endpoints: latency-svc-n4t8b [778.206049ms]
May 21 19:59:49.980: INFO: Created: latency-svc-6wlmv
May 21 19:59:50.004: INFO: Got endpoints: latency-svc-6wlmv [806.99395ms]
May 21 19:59:50.032: INFO: Created: latency-svc-qvsrg
May 21 19:59:50.033: INFO: Created: latency-svc-6vvhf
May 21 19:59:50.070: INFO: Created: latency-svc-zfvz5
May 21 19:59:50.131: INFO: Got endpoints: latency-svc-qvsrg [854.075345ms]
May 21 19:59:50.155: INFO: Got endpoints: latency-svc-zfvz5 [730.71151ms]
May 21 19:59:50.155: INFO: Got endpoints: latency-svc-6vvhf [947.448323ms]
May 21 19:59:50.172: INFO: Created: latency-svc-f6n8t
May 21 19:59:50.250: INFO: Created: latency-svc-pwx2b
May 21 19:59:50.253: INFO: Got endpoints: latency-svc-f6n8t [827.130261ms]
May 21 19:59:50.315: INFO: Got endpoints: latency-svc-pwx2b [863.24913ms]
May 21 19:59:50.409: INFO: Created: latency-svc-qjnjz
May 21 19:59:50.546: INFO: Got endpoints: latency-svc-qjnjz [1.016437357s]
May 21 19:59:50.598: INFO: Created: latency-svc-nrp74
May 21 19:59:50.626: INFO: Got endpoints: latency-svc-nrp74 [1.0573373s]
May 21 19:59:50.644: INFO: Created: latency-svc-8plcm
May 21 19:59:50.690: INFO: Got endpoints: latency-svc-8plcm [1.080913049s]
May 21 19:59:50.699: INFO: Created: latency-svc-nx7h2
May 21 19:59:50.700: INFO: Created: latency-svc-x7px8
May 21 19:59:50.750: INFO: Got endpoints: latency-svc-nx7h2 [1.110697512s]
May 21 19:59:50.812: INFO: Got endpoints: latency-svc-x7px8 [1.073094769s]
May 21 19:59:50.813: INFO: Latencies: [124.165138ms 184.258598ms 263.795943ms 268.56971ms 336.648348ms 379.856456ms 437.092214ms 440.648211ms 453.48155ms 484.648783ms 489.081645ms 511.686747ms 518.753438ms 522.802727ms 524.041846ms 526.876039ms 530.991527ms 543.826751ms 544.185711ms 549.292ms 549.686609ms 551.376034ms 555.710651ms 558.245218ms 562.827279ms 564.262076ms 566.022915ms 566.061461ms 566.556952ms 568.451355ms 570.30966ms 573.776613ms 575.586728ms 576.732084ms 579.56494ms 581.174857ms 582.545534ms 583.129395ms 583.632317ms 585.856765ms 586.358818ms 588.33624ms 590.653563ms 590.969798ms 600.188707ms 604.55454ms 605.80729ms 610.923913ms 615.972919ms 618.247183ms 621.841425ms 624.347845ms 625.030131ms 625.296242ms 626.050508ms 626.988627ms 628.669785ms 630.836247ms 636.75829ms 637.561065ms 640.173022ms 643.168321ms 649.305367ms 650.879742ms 663.670065ms 664.935161ms 672.745707ms 675.204533ms 689.75063ms 697.199752ms 705.223807ms 706.748746ms 708.739109ms 714.45888ms 715.727166ms 716.472763ms 716.876013ms 717.729116ms 717.813993ms 719.942273ms 720.533437ms 720.660919ms 725.1847ms 727.118745ms 727.561944ms 730.098166ms 730.71151ms 730.98438ms 733.555084ms 735.387415ms 741.982793ms 742.230203ms 751.981566ms 754.001109ms 754.624206ms 754.973654ms 757.485824ms 757.973945ms 758.207034ms 759.870392ms 759.967361ms 772.419199ms 772.550512ms 775.793986ms 777.465617ms 778.206049ms 779.362833ms 781.230839ms 782.493587ms 785.252309ms 787.864629ms 790.248158ms 792.413818ms 799.444533ms 799.864871ms 800.502129ms 801.028926ms 803.158563ms 806.354128ms 806.377565ms 806.99395ms 808.941532ms 811.323792ms 811.605096ms 812.985446ms 814.88059ms 816.897213ms 820.822545ms 823.163128ms 826.821353ms 827.130261ms 828.691755ms 833.935078ms 838.572862ms 841.051268ms 842.971339ms 843.941866ms 844.53299ms 844.745533ms 846.088336ms 848.617741ms 852.870826ms 854.075345ms 854.293475ms 857.845779ms 863.24913ms 864.422937ms 870.10267ms 871.771074ms 871.86534ms 879.216761ms 880.824614ms 881.92851ms 884.541794ms 884.680045ms 884.71545ms 889.958178ms 890.716687ms 892.138453ms 897.104661ms 902.062276ms 903.118745ms 904.106524ms 915.372659ms 917.692375ms 921.898317ms 931.558166ms 945.206076ms 947.448323ms 954.827207ms 969.03702ms 969.249767ms 975.273721ms 976.368988ms 977.645155ms 977.710087ms 1.007218167s 1.016437357s 1.05707019s 1.0573373s 1.073094769s 1.080913049s 1.110697512s 1.141295573s 1.174778625s 1.194411756s 1.206699745s 1.241713013s 1.292502948s 1.348148592s 1.35058538s 1.35098173s 1.361884517s 1.372072389s 1.39642878s 1.406807707s 1.44315021s 1.457380435s 1.486271495s 1.534664309s]
May 21 19:59:50.813: INFO: 50 %ile: 759.967361ms
May 21 19:59:50.813: INFO: 90 %ile: 1.073094769s
May 21 19:59:50.813: INFO: 99 %ile: 1.486271495s
May 21 19:59:50.813: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:59:50.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9698" for this suite.

• [SLOW TEST:14.359 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":177,"skipped":2774,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:59:50.868: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 19:59:52.764: INFO: Checking APIGroup: apiregistration.k8s.io
May 21 19:59:52.766: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 21 19:59:52.766: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.766: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 21 19:59:52.766: INFO: Checking APIGroup: extensions
May 21 19:59:52.770: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 21 19:59:52.770: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 21 19:59:52.770: INFO: extensions/v1beta1 matches extensions/v1beta1
May 21 19:59:52.770: INFO: Checking APIGroup: apps
May 21 19:59:52.771: INFO: PreferredVersion.GroupVersion: apps/v1
May 21 19:59:52.771: INFO: Versions found [{apps/v1 v1}]
May 21 19:59:52.771: INFO: apps/v1 matches apps/v1
May 21 19:59:52.771: INFO: Checking APIGroup: events.k8s.io
May 21 19:59:52.773: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 21 19:59:52.773: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.773: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 21 19:59:52.773: INFO: Checking APIGroup: authentication.k8s.io
May 21 19:59:52.775: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 21 19:59:52.775: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.775: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 21 19:59:52.775: INFO: Checking APIGroup: authorization.k8s.io
May 21 19:59:52.776: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 21 19:59:52.776: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.776: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 21 19:59:52.776: INFO: Checking APIGroup: autoscaling
May 21 19:59:52.778: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 21 19:59:52.778: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 21 19:59:52.778: INFO: autoscaling/v1 matches autoscaling/v1
May 21 19:59:52.778: INFO: Checking APIGroup: batch
May 21 19:59:52.780: INFO: PreferredVersion.GroupVersion: batch/v1
May 21 19:59:52.780: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 21 19:59:52.780: INFO: batch/v1 matches batch/v1
May 21 19:59:52.780: INFO: Checking APIGroup: certificates.k8s.io
May 21 19:59:52.781: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 21 19:59:52.781: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.781: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 21 19:59:52.781: INFO: Checking APIGroup: networking.k8s.io
May 21 19:59:52.783: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 21 19:59:52.783: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.783: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 21 19:59:52.783: INFO: Checking APIGroup: policy
May 21 19:59:52.787: INFO: PreferredVersion.GroupVersion: policy/v1beta1
May 21 19:59:52.787: INFO: Versions found [{policy/v1beta1 v1beta1}]
May 21 19:59:52.787: INFO: policy/v1beta1 matches policy/v1beta1
May 21 19:59:52.787: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 21 19:59:52.790: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 21 19:59:52.790: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.790: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 21 19:59:52.790: INFO: Checking APIGroup: storage.k8s.io
May 21 19:59:52.794: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 21 19:59:52.794: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.794: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 21 19:59:52.794: INFO: Checking APIGroup: admissionregistration.k8s.io
May 21 19:59:52.797: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 21 19:59:52.797: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.797: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 21 19:59:52.797: INFO: Checking APIGroup: apiextensions.k8s.io
May 21 19:59:52.800: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 21 19:59:52.800: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.801: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 21 19:59:52.801: INFO: Checking APIGroup: scheduling.k8s.io
May 21 19:59:52.802: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 21 19:59:52.802: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.802: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 21 19:59:52.802: INFO: Checking APIGroup: coordination.k8s.io
May 21 19:59:52.805: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 21 19:59:52.805: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.805: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 21 19:59:52.805: INFO: Checking APIGroup: node.k8s.io
May 21 19:59:52.806: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
May 21 19:59:52.807: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.807: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
May 21 19:59:52.807: INFO: Checking APIGroup: discovery.k8s.io
May 21 19:59:52.808: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
May 21 19:59:52.808: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.808: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
May 21 19:59:52.808: INFO: Checking APIGroup: crd.projectcalico.org
May 21 19:59:52.811: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May 21 19:59:52.811: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May 21 19:59:52.811: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
May 21 19:59:52.811: INFO: Checking APIGroup: monitoring.kiali.io
May 21 19:59:52.813: INFO: PreferredVersion.GroupVersion: monitoring.kiali.io/v1alpha1
May 21 19:59:52.813: INFO: Versions found [{monitoring.kiali.io/v1alpha1 v1alpha1}]
May 21 19:59:52.813: INFO: monitoring.kiali.io/v1alpha1 matches monitoring.kiali.io/v1alpha1
May 21 19:59:52.813: INFO: Checking APIGroup: metrics.k8s.io
May 21 19:59:52.815: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May 21 19:59:52.815: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May 21 19:59:52.815: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:59:52.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2300" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":178,"skipped":2779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:59:52.846: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 19:59:59.409: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 19:59:59.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9689" for this suite.

• [SLOW TEST:6.754 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":179,"skipped":2834,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 19:59:59.620: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-39cc12ee-b66e-420e-a54f-8b048adae036
STEP: Creating a pod to test consume configMaps
May 21 20:00:00.718: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c54fe9e-efc0-4ea2-a00c-7796ff951a13" in namespace "configmap-1483" to be "Succeeded or Failed"
May 21 20:00:00.872: INFO: Pod "pod-configmaps-8c54fe9e-efc0-4ea2-a00c-7796ff951a13": Phase="Pending", Reason="", readiness=false. Elapsed: 154.367188ms
May 21 20:00:02.913: INFO: Pod "pod-configmaps-8c54fe9e-efc0-4ea2-a00c-7796ff951a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.194664869s
May 21 20:00:04.966: INFO: Pod "pod-configmaps-8c54fe9e-efc0-4ea2-a00c-7796ff951a13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247844971s
May 21 20:00:07.088: INFO: Pod "pod-configmaps-8c54fe9e-efc0-4ea2-a00c-7796ff951a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.37061889s
STEP: Saw pod success
May 21 20:00:07.089: INFO: Pod "pod-configmaps-8c54fe9e-efc0-4ea2-a00c-7796ff951a13" satisfied condition "Succeeded or Failed"
May 21 20:00:07.122: INFO: Trying to get logs from node dc-hg-2 pod pod-configmaps-8c54fe9e-efc0-4ea2-a00c-7796ff951a13 container configmap-volume-test: <nil>
STEP: delete the pod
May 21 20:00:07.400: INFO: Waiting for pod pod-configmaps-8c54fe9e-efc0-4ea2-a00c-7796ff951a13 to disappear
May 21 20:00:07.446: INFO: Pod pod-configmaps-8c54fe9e-efc0-4ea2-a00c-7796ff951a13 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:00:07.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1483" for this suite.

• [SLOW TEST:8.254 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":180,"skipped":2842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:00:07.874: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 20:00:10.686: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 20:00:12.797: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224011, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224011, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224011, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224010, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 20:00:14.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224011, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224011, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224011, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224010, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 20:00:17.839: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
May 21 20:00:18.839: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
May 21 20:00:19.839: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
May 21 20:00:20.839: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:00:20.853: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9267-crds.webhook.example.com via the AdmissionRegistration API
May 21 20:00:21.464: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:00:22.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8177" for this suite.
STEP: Destroying namespace "webhook-8177-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.084 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":181,"skipped":2877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:00:23.961: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:00:40.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2011" for this suite.

• [SLOW TEST:16.997 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":182,"skipped":2909,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:00:40.958: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
May 21 20:00:41.051: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:01:31.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1280" for this suite.

• [SLOW TEST:50.608 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":183,"skipped":2918,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:01:31.567: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 21 20:01:31.712: INFO: Waiting up to 5m0s for pod "pod-09b9dab9-6775-4961-8045-8ecb5d3eef8f" in namespace "emptydir-9375" to be "Succeeded or Failed"
May 21 20:01:31.764: INFO: Pod "pod-09b9dab9-6775-4961-8045-8ecb5d3eef8f": Phase="Pending", Reason="", readiness=false. Elapsed: 51.954138ms
May 21 20:01:33.771: INFO: Pod "pod-09b9dab9-6775-4961-8045-8ecb5d3eef8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058243608s
May 21 20:01:35.778: INFO: Pod "pod-09b9dab9-6775-4961-8045-8ecb5d3eef8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065220005s
STEP: Saw pod success
May 21 20:01:35.778: INFO: Pod "pod-09b9dab9-6775-4961-8045-8ecb5d3eef8f" satisfied condition "Succeeded or Failed"
May 21 20:01:35.782: INFO: Trying to get logs from node dc-hg-1 pod pod-09b9dab9-6775-4961-8045-8ecb5d3eef8f container test-container: <nil>
STEP: delete the pod
May 21 20:01:35.847: INFO: Waiting for pod pod-09b9dab9-6775-4961-8045-8ecb5d3eef8f to disappear
May 21 20:01:35.853: INFO: Pod pod-09b9dab9-6775-4961-8045-8ecb5d3eef8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:01:35.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9375" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":184,"skipped":2925,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:01:35.882: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:01:36.005: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:01:45.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6753" for this suite.

• [SLOW TEST:9.395 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":185,"skipped":2940,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:01:45.277: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:02:45.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9765" for this suite.

• [SLOW TEST:60.202 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":186,"skipped":2952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:02:45.488: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
May 21 20:02:45.557: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 21 20:02:45.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 create -f -'
May 21 20:02:46.995: INFO: stderr: ""
May 21 20:02:46.995: INFO: stdout: "service/agnhost-replica created\n"
May 21 20:02:46.995: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 21 20:02:46.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 create -f -'
May 21 20:02:47.822: INFO: stderr: ""
May 21 20:02:47.822: INFO: stdout: "service/agnhost-primary created\n"
May 21 20:02:47.823: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 21 20:02:47.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 create -f -'
May 21 20:02:48.591: INFO: stderr: ""
May 21 20:02:48.591: INFO: stdout: "service/frontend created\n"
May 21 20:02:48.592: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 21 20:02:48.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 create -f -'
May 21 20:02:49.216: INFO: stderr: ""
May 21 20:02:49.216: INFO: stdout: "deployment.apps/frontend created\n"
May 21 20:02:49.217: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 21 20:02:49.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 create -f -'
May 21 20:02:50.070: INFO: stderr: ""
May 21 20:02:50.070: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 21 20:02:50.070: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 21 20:02:50.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 create -f -'
May 21 20:02:51.208: INFO: stderr: ""
May 21 20:02:51.208: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 21 20:02:51.208: INFO: Waiting for all frontend pods to be Running.
May 21 20:02:56.259: INFO: Waiting for frontend to serve content.
May 21 20:02:56.301: INFO: Trying to add a new entry to the guestbook.
May 21 20:02:56.343: INFO: Verifying that added entry can be retrieved.
May 21 20:02:56.364: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
May 21 20:03:01.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
May 21 20:03:01.745: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 20:03:01.745: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 21 20:03:01.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
May 21 20:03:02.077: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 20:03:02.077: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 21 20:03:02.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
May 21 20:03:02.467: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 20:03:02.467: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 21 20:03:02.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
May 21 20:03:02.724: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 20:03:02.724: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 21 20:03:02.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
May 21 20:03:03.124: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 20:03:03.124: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 21 20:03:03.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
May 21 20:03:03.582: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 20:03:03.582: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:03:03.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7862" for this suite.

• [SLOW TEST:18.186 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":187,"skipped":3078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:03:03.676: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 21 20:03:08.772: INFO: Successfully updated pod "annotationupdate1d8ad964-ca59-44c5-96ed-658de0cd3ce8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:03:12.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-801" for this suite.

• [SLOW TEST:9.183 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":188,"skipped":3115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:03:12.860: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:03:26.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5477" for this suite.

• [SLOW TEST:13.260 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":189,"skipped":3147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:03:26.123: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 21 20:03:34.308: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 20:03:34.324: INFO: Pod pod-with-poststart-http-hook still exists
May 21 20:03:36.324: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 20:03:36.332: INFO: Pod pod-with-poststart-http-hook still exists
May 21 20:03:38.324: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 21 20:03:38.333: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:03:38.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8793" for this suite.

• [SLOW TEST:12.233 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":190,"skipped":3254,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:03:38.356: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 20:03:40.728: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 20:03:42.754: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224220, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224220, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224220, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224220, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 20:03:45.793: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 21 20:03:49.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=webhook-8938 attach --namespace=webhook-8938 to-be-attached-pod -i -c=container1'
May 21 20:03:50.247: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:03:50.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8938" for this suite.
STEP: Destroying namespace "webhook-8938-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.333 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":191,"skipped":3271,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:03:50.690: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-809a99e4-0c2b-4a2d-a60c-c15dbaec38fe
STEP: Creating a pod to test consume secrets
May 21 20:03:50.797: INFO: Waiting up to 5m0s for pod "pod-secrets-9e5826f7-4726-472b-9899-9bee6ff40e6e" in namespace "secrets-9500" to be "Succeeded or Failed"
May 21 20:03:50.804: INFO: Pod "pod-secrets-9e5826f7-4726-472b-9899-9bee6ff40e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.723497ms
May 21 20:03:52.812: INFO: Pod "pod-secrets-9e5826f7-4726-472b-9899-9bee6ff40e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014980674s
May 21 20:03:54.819: INFO: Pod "pod-secrets-9e5826f7-4726-472b-9899-9bee6ff40e6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022246364s
STEP: Saw pod success
May 21 20:03:54.819: INFO: Pod "pod-secrets-9e5826f7-4726-472b-9899-9bee6ff40e6e" satisfied condition "Succeeded or Failed"
May 21 20:03:54.823: INFO: Trying to get logs from node dc-hg-1 pod pod-secrets-9e5826f7-4726-472b-9899-9bee6ff40e6e container secret-volume-test: <nil>
STEP: delete the pod
May 21 20:03:54.869: INFO: Waiting for pod pod-secrets-9e5826f7-4726-472b-9899-9bee6ff40e6e to disappear
May 21 20:03:54.883: INFO: Pod pod-secrets-9e5826f7-4726-472b-9899-9bee6ff40e6e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:03:54.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9500" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":192,"skipped":3291,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:03:54.905: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 20:03:55.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8bfd206-aafa-498c-876f-32bac0a8b1fc" in namespace "projected-7137" to be "Succeeded or Failed"
May 21 20:03:55.040: INFO: Pod "downwardapi-volume-a8bfd206-aafa-498c-876f-32bac0a8b1fc": Phase="Pending", Reason="", readiness=false. Elapsed: 35.448459ms
May 21 20:03:57.055: INFO: Pod "downwardapi-volume-a8bfd206-aafa-498c-876f-32bac0a8b1fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049979992s
May 21 20:03:59.061: INFO: Pod "downwardapi-volume-a8bfd206-aafa-498c-876f-32bac0a8b1fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056552433s
STEP: Saw pod success
May 21 20:03:59.061: INFO: Pod "downwardapi-volume-a8bfd206-aafa-498c-876f-32bac0a8b1fc" satisfied condition "Succeeded or Failed"
May 21 20:03:59.069: INFO: Trying to get logs from node dc-hg-2 pod downwardapi-volume-a8bfd206-aafa-498c-876f-32bac0a8b1fc container client-container: <nil>
STEP: delete the pod
May 21 20:03:59.145: INFO: Waiting for pod downwardapi-volume-a8bfd206-aafa-498c-876f-32bac0a8b1fc to disappear
May 21 20:03:59.158: INFO: Pod downwardapi-volume-a8bfd206-aafa-498c-876f-32bac0a8b1fc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:03:59.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7137" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:03:59.188: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 21 20:03:59.319: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 20:03:59.338: INFO: Waiting for terminating namespaces to be deleted...
May 21 20:03:59.344: INFO: 
Logging pods the apiserver thinks is on node conformance1 before test
May 21 20:03:59.357: INFO: haproxy-ingress-85597cc495-l7rzc from ingress-haproxy started at 2021-04-26 19:00:07 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.357: INFO: 	Container haproxy-ingress ready: true, restart count 2
May 21 20:03:59.357: INFO: ingress-default-backend-5f65f5865b-nldzd from ingress-haproxy started at 2021-04-26 19:00:28 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.357: INFO: 	Container ingress-default-backend ready: true, restart count 2
May 21 20:03:59.357: INFO: calico-kube-controllers-db766f945-4228n from kube-system started at 2021-04-26 19:00:07 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.357: INFO: 	Container calico-kube-controllers ready: true, restart count 2
May 21 20:03:59.357: INFO: calico-node-j5hq5 from kube-system started at 2021-04-23 21:19:36 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.357: INFO: 	Container calico-node ready: true, restart count 2
May 21 20:03:59.357: INFO: coredns-c6fc4f979-bv84n from kube-system started at 2021-04-26 19:00:09 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.357: INFO: 	Container coredns ready: true, restart count 2
May 21 20:03:59.357: INFO: coredns-c6fc4f979-jfxfx from kube-system started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.357: INFO: 	Container coredns ready: true, restart count 2
May 21 20:03:59.357: INFO: metrics-server-6c6c6b5d47-9r52q from kube-system started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.357: INFO: 	Container metrics-server ready: true, restart count 2
May 21 20:03:59.357: INFO: nirmata-cni-installer-zjkpx from nirmata started at 2021-04-26 19:00:42 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.357: INFO: 	Container install-cni ready: true, restart count 2
May 21 20:03:59.357: INFO: nirmata-kube-controller-5cf898966b-md6gc from nirmata started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.357: INFO: 	Container nirmata-kube-controller ready: true, restart count 8
May 21 20:03:59.357: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-47fv4 from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:03:59.357: INFO: 	Container sonobuoy-worker ready: false, restart count 6
May 21 20:03:59.358: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 20:03:59.358: INFO: 
Logging pods the apiserver thinks is on node dc-hg-1 before test
May 21 20:03:59.369: INFO: calico-node-6r5n7 from kube-system started at 2021-05-21 02:51:05 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.369: INFO: 	Container calico-node ready: true, restart count 0
May 21 20:03:59.369: INFO: nirmata-cni-installer-xzg69 from nirmata started at 2021-05-21 19:24:35 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.369: INFO: 	Container install-cni ready: true, restart count 0
May 21 20:03:59.369: INFO: sonobuoy from sonobuoy started at 2021-05-21 18:56:18 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.369: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 20:03:59.369: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-xch78 from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:03:59.369: INFO: 	Container sonobuoy-worker ready: false, restart count 5
May 21 20:03:59.369: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 20:03:59.369: INFO: 
Logging pods the apiserver thinks is on node dc-hg-2 before test
May 21 20:03:59.380: INFO: calico-node-c7rz4 from kube-system started at 2021-04-27 15:00:32 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.380: INFO: 	Container calico-node ready: true, restart count 2
May 21 20:03:59.380: INFO: nirmata-cni-installer-5m9z5 from nirmata started at 2021-05-20 07:33:18 +0000 UTC (1 container statuses recorded)
May 21 20:03:59.380: INFO: 	Container install-cni ready: true, restart count 0
May 21 20:03:59.380: INFO: sonobuoy-e2e-job-03ad7d58f5964dbf from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:03:59.380: INFO: 	Container e2e ready: true, restart count 0
May 21 20:03:59.380: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 20:03:59.380: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-lxlxb from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:03:59.380: INFO: 	Container sonobuoy-worker ready: false, restart count 6
May 21 20:03:59.380: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0b9c3243-812b-4cb9-a32b-21d8fe4a9c7a 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-0b9c3243-812b-4cb9-a32b-21d8fe4a9c7a off the node dc-hg-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0b9c3243-812b-4cb9-a32b-21d8fe4a9c7a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:04:15.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2010" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:16.462 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":194,"skipped":3337,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:04:15.650: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 20:04:20.125: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:04:20.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2467" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":195,"skipped":3344,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:04:20.205: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
May 21 20:04:20.296: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-3632 proxy --unix-socket=/tmp/kubectl-proxy-unix156778947/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:04:20.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3632" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":196,"skipped":3344,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:04:20.734: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:04:20.851: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 20:04:29.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-135 --namespace=crd-publish-openapi-135 create -f -'
May 21 20:04:31.160: INFO: stderr: ""
May 21 20:04:31.160: INFO: stdout: "e2e-test-crd-publish-openapi-4964-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 21 20:04:31.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-135 --namespace=crd-publish-openapi-135 delete e2e-test-crd-publish-openapi-4964-crds test-cr'
May 21 20:04:31.478: INFO: stderr: ""
May 21 20:04:31.478: INFO: stdout: "e2e-test-crd-publish-openapi-4964-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 21 20:04:31.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-135 --namespace=crd-publish-openapi-135 apply -f -'
May 21 20:04:32.190: INFO: stderr: ""
May 21 20:04:32.190: INFO: stdout: "e2e-test-crd-publish-openapi-4964-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 21 20:04:32.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-135 --namespace=crd-publish-openapi-135 delete e2e-test-crd-publish-openapi-4964-crds test-cr'
May 21 20:04:32.522: INFO: stderr: ""
May 21 20:04:32.522: INFO: stdout: "e2e-test-crd-publish-openapi-4964-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 21 20:04:32.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-135 explain e2e-test-crd-publish-openapi-4964-crds'
May 21 20:04:33.286: INFO: stderr: ""
May 21 20:04:33.286: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4964-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:04:41.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-135" for this suite.

• [SLOW TEST:21.268 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":197,"skipped":3345,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:04:42.002: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:06:42.134: INFO: Deleting pod "var-expansion-ab541c5b-abd9-4a34-bef0-73c30ed6f429" in namespace "var-expansion-6368"
May 21 20:06:42.148: INFO: Wait up to 5m0s for pod "var-expansion-ab541c5b-abd9-4a34-bef0-73c30ed6f429" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:06:44.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6368" for this suite.

• [SLOW TEST:122.193 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":198,"skipped":3359,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:06:44.195: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-6fc02d47-3325-4765-81cc-948583bc24e8
STEP: Creating secret with name s-test-opt-upd-51103de5-2a87-4916-af08-bd4cdbf322ed
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6fc02d47-3325-4765-81cc-948583bc24e8
STEP: Updating secret s-test-opt-upd-51103de5-2a87-4916-af08-bd4cdbf322ed
STEP: Creating secret with name s-test-opt-create-4efb7ae9-014e-48ae-8c6b-afd3879f78e3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:08:09.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7095" for this suite.

• [SLOW TEST:85.357 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":199,"skipped":3370,"failed":0}
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:08:09.553: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:08:09.658: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e4f5ae7b-c796-493b-a464-11b7f5fd2be6" in namespace "security-context-test-6088" to be "Succeeded or Failed"
May 21 20:08:09.676: INFO: Pod "busybox-user-65534-e4f5ae7b-c796-493b-a464-11b7f5fd2be6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.754277ms
May 21 20:08:11.689: INFO: Pod "busybox-user-65534-e4f5ae7b-c796-493b-a464-11b7f5fd2be6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030705946s
May 21 20:08:13.708: INFO: Pod "busybox-user-65534-e4f5ae7b-c796-493b-a464-11b7f5fd2be6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049906171s
May 21 20:08:13.708: INFO: Pod "busybox-user-65534-e4f5ae7b-c796-493b-a464-11b7f5fd2be6" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:08:13.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6088" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":200,"skipped":3370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:08:13.753: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 20:08:16.916: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 20:08:18.962: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224496, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224496, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224497, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224496, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 20:08:21.982: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:08:21.987: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2128-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:08:23.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6490" for this suite.
STEP: Destroying namespace "webhook-6490-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.897 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":201,"skipped":3395,"failed":0}
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:08:23.650: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-15f39ff2-e6e1-4476-9efe-cff1310570bc
STEP: Creating secret with name secret-projected-all-test-volume-0d2a36da-cbed-4b40-a5bb-b2ef87dc1183
STEP: Creating a pod to test Check all projections for projected volume plugin
May 21 20:08:23.919: INFO: Waiting up to 5m0s for pod "projected-volume-473227f1-ca04-4f25-af7f-27e1279da4f5" in namespace "projected-9537" to be "Succeeded or Failed"
May 21 20:08:23.957: INFO: Pod "projected-volume-473227f1-ca04-4f25-af7f-27e1279da4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 37.497608ms
May 21 20:08:25.968: INFO: Pod "projected-volume-473227f1-ca04-4f25-af7f-27e1279da4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049358494s
May 21 20:08:27.978: INFO: Pod "projected-volume-473227f1-ca04-4f25-af7f-27e1279da4f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058685561s
STEP: Saw pod success
May 21 20:08:27.978: INFO: Pod "projected-volume-473227f1-ca04-4f25-af7f-27e1279da4f5" satisfied condition "Succeeded or Failed"
May 21 20:08:28.021: INFO: Trying to get logs from node dc-hg-1 pod projected-volume-473227f1-ca04-4f25-af7f-27e1279da4f5 container projected-all-volume-test: <nil>
STEP: delete the pod
May 21 20:08:28.171: INFO: Waiting for pod projected-volume-473227f1-ca04-4f25-af7f-27e1279da4f5 to disappear
May 21 20:08:28.205: INFO: Pod projected-volume-473227f1-ca04-4f25-af7f-27e1279da4f5 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:08:28.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9537" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":202,"skipped":3395,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:08:28.248: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-ed1bc778-e22c-4767-b744-e7e9eb1a7098
STEP: Creating configMap with name cm-test-opt-upd-1476fa44-6826-4903-a4fa-78030b242116
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ed1bc778-e22c-4767-b744-e7e9eb1a7098
STEP: Updating configmap cm-test-opt-upd-1476fa44-6826-4903-a4fa-78030b242116
STEP: Creating configMap with name cm-test-opt-create-233a6682-7efc-45fc-b847-cc66c04e2c38
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:08:36.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5552" for this suite.

• [SLOW TEST:8.607 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":203,"skipped":3409,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:08:36.856: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 20:08:36.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bc2dee2-a40f-4fc9-ad6e-21d50b9b7cd6" in namespace "projected-9381" to be "Succeeded or Failed"
May 21 20:08:36.966: INFO: Pod "downwardapi-volume-7bc2dee2-a40f-4fc9-ad6e-21d50b9b7cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.01513ms
May 21 20:08:38.978: INFO: Pod "downwardapi-volume-7bc2dee2-a40f-4fc9-ad6e-21d50b9b7cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024822375s
May 21 20:08:40.983: INFO: Pod "downwardapi-volume-7bc2dee2-a40f-4fc9-ad6e-21d50b9b7cd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030519658s
STEP: Saw pod success
May 21 20:08:40.983: INFO: Pod "downwardapi-volume-7bc2dee2-a40f-4fc9-ad6e-21d50b9b7cd6" satisfied condition "Succeeded or Failed"
May 21 20:08:40.992: INFO: Trying to get logs from node dc-hg-2 pod downwardapi-volume-7bc2dee2-a40f-4fc9-ad6e-21d50b9b7cd6 container client-container: <nil>
STEP: delete the pod
May 21 20:08:41.079: INFO: Waiting for pod downwardapi-volume-7bc2dee2-a40f-4fc9-ad6e-21d50b9b7cd6 to disappear
May 21 20:08:41.096: INFO: Pod downwardapi-volume-7bc2dee2-a40f-4fc9-ad6e-21d50b9b7cd6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:08:41.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9381" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":204,"skipped":3421,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:08:41.127: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 20:08:41.262: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e37dde2-a8ba-470f-be61-77498874786a" in namespace "projected-7036" to be "Succeeded or Failed"
May 21 20:08:41.272: INFO: Pod "downwardapi-volume-6e37dde2-a8ba-470f-be61-77498874786a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.395966ms
May 21 20:08:43.280: INFO: Pod "downwardapi-volume-6e37dde2-a8ba-470f-be61-77498874786a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017734672s
May 21 20:08:45.286: INFO: Pod "downwardapi-volume-6e37dde2-a8ba-470f-be61-77498874786a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024421066s
STEP: Saw pod success
May 21 20:08:45.287: INFO: Pod "downwardapi-volume-6e37dde2-a8ba-470f-be61-77498874786a" satisfied condition "Succeeded or Failed"
May 21 20:08:45.293: INFO: Trying to get logs from node dc-hg-2 pod downwardapi-volume-6e37dde2-a8ba-470f-be61-77498874786a container client-container: <nil>
STEP: delete the pod
May 21 20:08:45.359: INFO: Waiting for pod downwardapi-volume-6e37dde2-a8ba-470f-be61-77498874786a to disappear
May 21 20:08:45.368: INFO: Pod downwardapi-volume-6e37dde2-a8ba-470f-be61-77498874786a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:08:45.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7036" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":205,"skipped":3428,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:08:45.389: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-5048
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 20:08:45.469: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 20:08:45.617: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:08:47.664: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:08:49.656: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:08:51.625: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:08:53.623: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:08:55.624: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:08:57.623: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:08:59.626: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:09:01.623: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:09:03.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:09:05.624: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:09:07.629: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:09:09.624: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 20:09:09.641: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 21 20:09:09.662: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 21 20:09:13.723: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.161.146:8080/dial?request=hostname&protocol=udp&host=192.168.209.87&port=8081&tries=1'] Namespace:pod-network-test-5048 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:09:13.723: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:09:14.016: INFO: Waiting for responses: map[]
May 21 20:09:14.024: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.161.146:8080/dial?request=hostname&protocol=udp&host=192.168.161.149&port=8081&tries=1'] Namespace:pod-network-test-5048 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:09:14.024: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:09:14.257: INFO: Waiting for responses: map[]
May 21 20:09:14.263: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.161.146:8080/dial?request=hostname&protocol=udp&host=192.168.106.173&port=8081&tries=1'] Namespace:pod-network-test-5048 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:09:14.263: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:09:14.496: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:09:14.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5048" for this suite.

• [SLOW TEST:29.144 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3428,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:09:14.533: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
May 21 20:09:14.596: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
May 21 20:09:17.765: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 21 20:09:20.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224557, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224557, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224557, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224557, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 20:09:22.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224557, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224557, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224557, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224557, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 20:09:27.172: INFO: Waited 3.073697308s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:09:29.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7132" for this suite.

• [SLOW TEST:14.835 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":207,"skipped":3438,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:09:29.368: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 21 20:09:36.273: INFO: Successfully updated pod "annotationupdatec72c14fb-cd06-47e8-aebf-732655dc5911"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:09:40.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6391" for this suite.

• [SLOW TEST:11.014 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":208,"skipped":3440,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:09:40.383: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-8dfad06d-aaf3-44ab-b342-ee8edadb84fb
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:09:44.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2854" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":209,"skipped":3441,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:09:44.724: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:09:44.867: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 21 20:09:44.899: INFO: Number of nodes with available pods: 0
May 21 20:09:44.899: INFO: Node conformance1 is running more than one daemon pod
May 21 20:09:45.999: INFO: Number of nodes with available pods: 0
May 21 20:09:45.999: INFO: Node conformance1 is running more than one daemon pod
May 21 20:09:46.925: INFO: Number of nodes with available pods: 0
May 21 20:09:46.925: INFO: Node conformance1 is running more than one daemon pod
May 21 20:09:47.955: INFO: Number of nodes with available pods: 0
May 21 20:09:47.955: INFO: Node conformance1 is running more than one daemon pod
May 21 20:09:48.946: INFO: Number of nodes with available pods: 0
May 21 20:09:48.946: INFO: Node conformance1 is running more than one daemon pod
May 21 20:09:50.001: INFO: Number of nodes with available pods: 2
May 21 20:09:50.001: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:09:50.922: INFO: Number of nodes with available pods: 3
May 21 20:09:50.922: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 21 20:09:51.042: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:51.042: INFO: Wrong image for pod: daemon-set-rn8pz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:51.042: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:52.083: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:52.083: INFO: Wrong image for pod: daemon-set-rn8pz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:52.083: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:53.071: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:53.071: INFO: Wrong image for pod: daemon-set-rn8pz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:53.071: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:54.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:54.070: INFO: Wrong image for pod: daemon-set-rn8pz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:54.070: INFO: Pod daemon-set-rn8pz is not available
May 21 20:09:54.070: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:55.072: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:55.072: INFO: Wrong image for pod: daemon-set-rn8pz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:55.072: INFO: Pod daemon-set-rn8pz is not available
May 21 20:09:55.072: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:56.079: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:56.079: INFO: Wrong image for pod: daemon-set-rn8pz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:56.079: INFO: Pod daemon-set-rn8pz is not available
May 21 20:09:56.079: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:57.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:57.070: INFO: Wrong image for pod: daemon-set-rn8pz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:57.070: INFO: Pod daemon-set-rn8pz is not available
May 21 20:09:57.070: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:58.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:58.070: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:58.070: INFO: Pod daemon-set-xk5rh is not available
May 21 20:09:59.126: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:59.126: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:09:59.127: INFO: Pod daemon-set-xk5rh is not available
May 21 20:10:00.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:00.070: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:00.070: INFO: Pod daemon-set-xk5rh is not available
May 21 20:10:01.073: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:01.073: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:01.073: INFO: Pod daemon-set-xk5rh is not available
May 21 20:10:02.081: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:02.082: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:02.082: INFO: Pod daemon-set-xk5rh is not available
May 21 20:10:03.157: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:03.157: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:04.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:04.070: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:05.071: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:05.071: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:05.071: INFO: Pod daemon-set-wmn65 is not available
May 21 20:10:06.072: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:06.072: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:06.072: INFO: Pod daemon-set-wmn65 is not available
May 21 20:10:07.073: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:07.073: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:07.073: INFO: Pod daemon-set-wmn65 is not available
May 21 20:10:08.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:08.070: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:08.070: INFO: Pod daemon-set-wmn65 is not available
May 21 20:10:09.071: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:09.071: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:09.071: INFO: Pod daemon-set-wmn65 is not available
May 21 20:10:10.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:10.070: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:10.070: INFO: Pod daemon-set-wmn65 is not available
May 21 20:10:11.068: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:11.068: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:11.068: INFO: Pod daemon-set-wmn65 is not available
May 21 20:10:12.069: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:12.069: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:12.069: INFO: Pod daemon-set-wmn65 is not available
May 21 20:10:13.078: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:13.078: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:13.078: INFO: Pod daemon-set-wmn65 is not available
May 21 20:10:14.068: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:14.068: INFO: Wrong image for pod: daemon-set-wmn65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:14.068: INFO: Pod daemon-set-wmn65 is not available
May 21 20:10:15.109: INFO: Pod daemon-set-2bn2c is not available
May 21 20:10:15.109: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:16.068: INFO: Pod daemon-set-2bn2c is not available
May 21 20:10:16.068: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:17.073: INFO: Pod daemon-set-2bn2c is not available
May 21 20:10:17.073: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:18.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:19.069: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:20.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:20.070: INFO: Pod daemon-set-rdl4r is not available
May 21 20:10:21.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:21.070: INFO: Pod daemon-set-rdl4r is not available
May 21 20:10:22.073: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:22.073: INFO: Pod daemon-set-rdl4r is not available
May 21 20:10:23.075: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:23.075: INFO: Pod daemon-set-rdl4r is not available
May 21 20:10:24.068: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:24.068: INFO: Pod daemon-set-rdl4r is not available
May 21 20:10:25.071: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:25.071: INFO: Pod daemon-set-rdl4r is not available
May 21 20:10:26.070: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:26.070: INFO: Pod daemon-set-rdl4r is not available
May 21 20:10:27.071: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:27.071: INFO: Pod daemon-set-rdl4r is not available
May 21 20:10:28.069: INFO: Wrong image for pod: daemon-set-rdl4r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
May 21 20:10:28.070: INFO: Pod daemon-set-rdl4r is not available
May 21 20:10:29.073: INFO: Pod daemon-set-8r6kl is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 21 20:10:29.116: INFO: Number of nodes with available pods: 2
May 21 20:10:29.116: INFO: Node dc-hg-2 is running more than one daemon pod
May 21 20:10:30.134: INFO: Number of nodes with available pods: 2
May 21 20:10:30.134: INFO: Node dc-hg-2 is running more than one daemon pod
May 21 20:10:31.139: INFO: Number of nodes with available pods: 2
May 21 20:10:31.139: INFO: Node dc-hg-2 is running more than one daemon pod
May 21 20:10:32.136: INFO: Number of nodes with available pods: 2
May 21 20:10:32.136: INFO: Node dc-hg-2 is running more than one daemon pod
May 21 20:10:33.155: INFO: Number of nodes with available pods: 3
May 21 20:10:33.155: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3177, will wait for the garbage collector to delete the pods
May 21 20:10:33.257: INFO: Deleting DaemonSet.extensions daemon-set took: 11.322331ms
May 21 20:10:33.958: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.376095ms
May 21 20:10:48.766: INFO: Number of nodes with available pods: 0
May 21 20:10:48.767: INFO: Number of running nodes: 0, number of available pods: 0
May 21 20:10:48.773: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3177/daemonsets","resourceVersion":"1283534"},"items":null}

May 21 20:10:48.779: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3177/pods","resourceVersion":"1283534"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:10:48.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3177" for this suite.

• [SLOW TEST:64.108 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":210,"skipped":3464,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:10:48.833: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
May 21 20:10:48.960: INFO: namespace kubectl-7685
May 21 20:10:48.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7685 create -f -'
May 21 20:10:49.850: INFO: stderr: ""
May 21 20:10:49.851: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 20:10:50.857: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 20:10:50.857: INFO: Found 0 / 1
May 21 20:10:51.860: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 20:10:51.860: INFO: Found 0 / 1
May 21 20:10:52.857: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 20:10:52.858: INFO: Found 1 / 1
May 21 20:10:52.858: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 21 20:10:52.866: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 20:10:52.866: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 20:10:52.866: INFO: wait on agnhost-primary startup in kubectl-7685 
May 21 20:10:52.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7685 logs agnhost-primary-k6vvp agnhost-primary'
May 21 20:10:53.215: INFO: stderr: ""
May 21 20:10:53.216: INFO: stdout: "Paused\n"
STEP: exposing RC
May 21 20:10:53.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7685 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 21 20:10:53.581: INFO: stderr: ""
May 21 20:10:53.582: INFO: stdout: "service/rm2 exposed\n"
May 21 20:10:53.614: INFO: Service rm2 in namespace kubectl-7685 found.
STEP: exposing service
May 21 20:10:55.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-7685 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 21 20:10:56.151: INFO: stderr: ""
May 21 20:10:56.151: INFO: stdout: "service/rm3 exposed\n"
May 21 20:10:56.166: INFO: Service rm3 in namespace kubectl-7685 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:10:58.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7685" for this suite.

• [SLOW TEST:9.372 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":211,"skipped":3481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:10:58.207: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-pjkl
STEP: Creating a pod to test atomic-volume-subpath
May 21 20:10:58.359: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-pjkl" in namespace "subpath-7112" to be "Succeeded or Failed"
May 21 20:10:58.391: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Pending", Reason="", readiness=false. Elapsed: 31.556769ms
May 21 20:11:00.403: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044184139s
May 21 20:11:02.410: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Running", Reason="", readiness=true. Elapsed: 4.050979487s
May 21 20:11:04.417: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Running", Reason="", readiness=true. Elapsed: 6.058178877s
May 21 20:11:06.424: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Running", Reason="", readiness=true. Elapsed: 8.064998204s
May 21 20:11:08.432: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Running", Reason="", readiness=true. Elapsed: 10.072492476s
May 21 20:11:10.448: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Running", Reason="", readiness=true. Elapsed: 12.088445975s
May 21 20:11:12.475: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Running", Reason="", readiness=true. Elapsed: 14.115931818s
May 21 20:11:14.481: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Running", Reason="", readiness=true. Elapsed: 16.122115151s
May 21 20:11:16.487: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Running", Reason="", readiness=true. Elapsed: 18.127983897s
May 21 20:11:18.493: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Running", Reason="", readiness=true. Elapsed: 20.133950701s
May 21 20:11:20.499: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Running", Reason="", readiness=true. Elapsed: 22.139377116s
May 21 20:11:22.505: INFO: Pod "pod-subpath-test-downwardapi-pjkl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.145577784s
STEP: Saw pod success
May 21 20:11:22.505: INFO: Pod "pod-subpath-test-downwardapi-pjkl" satisfied condition "Succeeded or Failed"
May 21 20:11:22.509: INFO: Trying to get logs from node dc-hg-2 pod pod-subpath-test-downwardapi-pjkl container test-container-subpath-downwardapi-pjkl: <nil>
STEP: delete the pod
May 21 20:11:22.584: INFO: Waiting for pod pod-subpath-test-downwardapi-pjkl to disappear
May 21 20:11:22.599: INFO: Pod pod-subpath-test-downwardapi-pjkl no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-pjkl
May 21 20:11:22.599: INFO: Deleting pod "pod-subpath-test-downwardapi-pjkl" in namespace "subpath-7112"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:11:22.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7112" for this suite.

• [SLOW TEST:24.422 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":212,"skipped":3519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:11:22.629: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:11:22.730: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-46237d56-73e4-44bf-8eb9-fd3db271fc98" in namespace "security-context-test-6527" to be "Succeeded or Failed"
May 21 20:11:22.740: INFO: Pod "busybox-privileged-false-46237d56-73e4-44bf-8eb9-fd3db271fc98": Phase="Pending", Reason="", readiness=false. Elapsed: 9.581122ms
May 21 20:11:24.746: INFO: Pod "busybox-privileged-false-46237d56-73e4-44bf-8eb9-fd3db271fc98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016247673s
May 21 20:11:26.755: INFO: Pod "busybox-privileged-false-46237d56-73e4-44bf-8eb9-fd3db271fc98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025112042s
May 21 20:11:26.755: INFO: Pod "busybox-privileged-false-46237d56-73e4-44bf-8eb9-fd3db271fc98" satisfied condition "Succeeded or Failed"
May 21 20:11:26.795: INFO: Got logs for pod "busybox-privileged-false-46237d56-73e4-44bf-8eb9-fd3db271fc98": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:11:26.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6527" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":213,"skipped":3541,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:11:26.820: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:11:26.898: INFO: Creating deployment "test-recreate-deployment"
May 21 20:11:26.917: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 21 20:11:26.935: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 21 20:11:28.948: INFO: Waiting deployment "test-recreate-deployment" to complete
May 21 20:11:28.959: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224686, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224686, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224687, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224686, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 20:11:30.967: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 21 20:11:30.993: INFO: Updating deployment test-recreate-deployment
May 21 20:11:30.993: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 21 20:11:31.248: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1344 /apis/apps/v1/namespaces/deployment-1344/deployments/test-recreate-deployment f2c9cb64-b5fc-4d10-bc9b-709156aaaf2a 1283768 2 2021-05-21 20:11:26 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-21 20:11:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 20:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006347128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-21 20:11:31 +0000 UTC,LastTransitionTime:2021-05-21 20:11:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f756bcb56" is progressing.,LastUpdateTime:2021-05-21 20:11:31 +0000 UTC,LastTransitionTime:2021-05-21 20:11:26 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 21 20:11:31.271: INFO: New ReplicaSet "test-recreate-deployment-f756bcb56" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f756bcb56  deployment-1344 /apis/apps/v1/namespaces/deployment-1344/replicasets/test-recreate-deployment-f756bcb56 6d4104c0-055b-41e8-82da-0c56e3b62dda 1283766 1 2021-05-21 20:11:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f2c9cb64-b5fc-4d10-bc9b-709156aaaf2a 0xc006347630 0xc006347631}] []  [{kube-controller-manager Update apps/v1 2021-05-21 20:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2c9cb64-b5fc-4d10-bc9b-709156aaaf2a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f756bcb56,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0063476a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 20:11:31.271: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 21 20:11:31.272: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-1344 /apis/apps/v1/namespaces/deployment-1344/replicasets/test-recreate-deployment-c96cf48f bae86cf0-67e6-46fa-b3b3-71fef0113b7c 1283758 2 2021-05-21 20:11:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f2c9cb64-b5fc-4d10-bc9b-709156aaaf2a 0xc00634753f 0xc006347550}] []  [{kube-controller-manager Update apps/v1 2021-05-21 20:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2c9cb64-b5fc-4d10-bc9b-709156aaaf2a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0063475c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 20:11:31.283: INFO: Pod "test-recreate-deployment-f756bcb56-bqg65" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f756bcb56-bqg65 test-recreate-deployment-f756bcb56- deployment-1344 /api/v1/namespaces/deployment-1344/pods/test-recreate-deployment-f756bcb56-bqg65 cd00088e-b26e-43cc-a0a8-b5933ec3b7e5 1283769 0 2021-05-21 20:11:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f756bcb56 6d4104c0-055b-41e8-82da-0c56e3b62dda 0xc006347b80 0xc006347b81}] []  [{kube-controller-manager Update v1 2021-05-21 20:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d4104c0-055b-41e8-82da-0c56e3b62dda\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-21 20:11:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5jdwc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5jdwc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5jdwc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 20:11:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 20:11:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 20:11:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 20:11:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.125,PodIP:,StartTime:2021-05-21 20:11:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:11:31.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1344" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":214,"skipped":3549,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:11:31.332: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
May 21 20:11:31.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-9974 cluster-info'
May 21 20:11:31.798: INFO: stderr: ""
May 21 20:11:31.798: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.10.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:11:31.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9974" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":215,"skipped":3556,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:11:31.848: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:11:32.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6968" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":216,"skipped":3556,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:11:32.177: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-bc0c33da-46d8-4a59-b0d7-3194b4d66507
STEP: Creating a pod to test consume secrets
May 21 20:11:32.426: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3d6a9a14-9236-43ab-ad72-51a1505c75bd" in namespace "projected-9741" to be "Succeeded or Failed"
May 21 20:11:32.455: INFO: Pod "pod-projected-secrets-3d6a9a14-9236-43ab-ad72-51a1505c75bd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.251263ms
May 21 20:11:34.460: INFO: Pod "pod-projected-secrets-3d6a9a14-9236-43ab-ad72-51a1505c75bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033483178s
May 21 20:11:36.468: INFO: Pod "pod-projected-secrets-3d6a9a14-9236-43ab-ad72-51a1505c75bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041710111s
STEP: Saw pod success
May 21 20:11:36.468: INFO: Pod "pod-projected-secrets-3d6a9a14-9236-43ab-ad72-51a1505c75bd" satisfied condition "Succeeded or Failed"
May 21 20:11:36.474: INFO: Trying to get logs from node dc-hg-2 pod pod-projected-secrets-3d6a9a14-9236-43ab-ad72-51a1505c75bd container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 20:11:36.549: INFO: Waiting for pod pod-projected-secrets-3d6a9a14-9236-43ab-ad72-51a1505c75bd to disappear
May 21 20:11:36.579: INFO: Pod pod-projected-secrets-3d6a9a14-9236-43ab-ad72-51a1505c75bd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:11:36.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9741" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":217,"skipped":3566,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:11:36.632: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
May 21 20:12:16.933: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 21 20:12:16.933: INFO: Deleting pod "simpletest.rc-2zsln" in namespace "gc-6641"
W0521 20:12:16.933141      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 20:12:16.933205      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 20:12:16.933223      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 21 20:12:17.001: INFO: Deleting pod "simpletest.rc-4qtqk" in namespace "gc-6641"
May 21 20:12:17.161: INFO: Deleting pod "simpletest.rc-6x6f7" in namespace "gc-6641"
May 21 20:12:17.241: INFO: Deleting pod "simpletest.rc-d8lpp" in namespace "gc-6641"
May 21 20:12:17.400: INFO: Deleting pod "simpletest.rc-dsv5j" in namespace "gc-6641"
May 21 20:12:17.497: INFO: Deleting pod "simpletest.rc-f2vg7" in namespace "gc-6641"
May 21 20:12:17.617: INFO: Deleting pod "simpletest.rc-p6877" in namespace "gc-6641"
May 21 20:12:17.737: INFO: Deleting pod "simpletest.rc-pbr6w" in namespace "gc-6641"
May 21 20:12:17.835: INFO: Deleting pod "simpletest.rc-xfnmj" in namespace "gc-6641"
May 21 20:12:17.933: INFO: Deleting pod "simpletest.rc-xqdqf" in namespace "gc-6641"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:12:17.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6641" for this suite.

• [SLOW TEST:41.395 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":218,"skipped":3585,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:12:18.074: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6071
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6071
STEP: creating replication controller externalsvc in namespace services-6071
I0521 20:12:18.608916      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6071, replica count: 2
I0521 20:12:21.666080      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 20:12:24.666563      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 21 20:12:24.743: INFO: Creating new exec pod
May 21 20:12:28.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-6071 exec execpodwqqpb -- /bin/sh -x -c nslookup nodeport-service.services-6071.svc.cluster.local'
May 21 20:12:29.455: INFO: stderr: "+ nslookup nodeport-service.services-6071.svc.cluster.local\n"
May 21 20:12:29.455: INFO: stdout: "Server:\t\t10.10.0.10\nAddress:\t10.10.0.10#53\n\nnodeport-service.services-6071.svc.cluster.local\tcanonical name = externalsvc.services-6071.svc.cluster.local.\nName:\texternalsvc.services-6071.svc.cluster.local\nAddress: 10.10.107.146\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6071, will wait for the garbage collector to delete the pods
May 21 20:12:29.527: INFO: Deleting ReplicationController externalsvc took: 15.55198ms
May 21 20:12:30.132: INFO: Terminating ReplicationController externalsvc pods took: 605.014534ms
May 21 20:12:35.270: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:12:35.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6071" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:17.351 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":219,"skipped":3617,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:12:35.407: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 21 20:12:35.495: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 21 20:13:09.254: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:13:18.010: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:13:52.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5514" for this suite.

• [SLOW TEST:77.038 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":220,"skipped":3618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:13:52.446: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 20:13:55.611: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 21 20:13:57.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224835, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224835, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224835, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224835, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 20:13:59.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224835, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224835, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224835, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224835, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 20:14:02.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:14:02.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1680" for this suite.
STEP: Destroying namespace "webhook-1680-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.768 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":221,"skipped":3655,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:14:03.215: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:14:03.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6085" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":222,"skipped":3656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:14:03.376: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
May 21 20:14:13.712: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:14:13.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0521 20:14:13.711951      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 20:14:13.712019      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 20:14:13.712049      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7377" for this suite.

• [SLOW TEST:10.378 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":223,"skipped":3680,"failed":0}
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:14:13.755: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 21 20:14:13.889: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 21 20:14:13.899: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 21 20:14:13.899: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 21 20:14:13.919: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 21 20:14:13.919: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 21 20:14:13.949: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 21 20:14:13.949: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 21 20:14:21.075: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:14:21.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-1853" for this suite.

• [SLOW TEST:7.376 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":224,"skipped":3680,"failed":0}
SSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:14:21.132: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 21 20:14:21.291: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 21 20:14:21.299: INFO: starting watch
STEP: patching
STEP: updating
May 21 20:14:21.333: INFO: waiting for watch events with expected annotations
May 21 20:14:21.333: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:14:21.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6748" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":225,"skipped":3684,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:14:21.720: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 21 20:14:23.253: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 21 20:14:25.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224863, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224863, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224863, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224863, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 20:14:27.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224863, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224863, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224863, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757224863, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 20:14:30.322: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:14:30.329: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:14:31.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3167" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.360 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":226,"skipped":3685,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:14:32.080: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-8jbv
STEP: Creating a pod to test atomic-volume-subpath
May 21 20:14:32.227: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8jbv" in namespace "subpath-9227" to be "Succeeded or Failed"
May 21 20:14:32.241: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Pending", Reason="", readiness=false. Elapsed: 14.557459ms
May 21 20:14:34.248: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02132422s
May 21 20:14:36.263: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Running", Reason="", readiness=true. Elapsed: 4.035952913s
May 21 20:14:38.269: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Running", Reason="", readiness=true. Elapsed: 6.04261915s
May 21 20:14:40.280: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Running", Reason="", readiness=true. Elapsed: 8.053848625s
May 21 20:14:42.288: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Running", Reason="", readiness=true. Elapsed: 10.061552239s
May 21 20:14:44.294: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Running", Reason="", readiness=true. Elapsed: 12.067856902s
May 21 20:14:46.303: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Running", Reason="", readiness=true. Elapsed: 14.076726385s
May 21 20:14:48.311: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Running", Reason="", readiness=true. Elapsed: 16.084139739s
May 21 20:14:50.318: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Running", Reason="", readiness=true. Elapsed: 18.090957439s
May 21 20:14:52.325: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Running", Reason="", readiness=true. Elapsed: 20.098146439s
May 21 20:14:54.332: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Running", Reason="", readiness=true. Elapsed: 22.105361401s
May 21 20:14:56.341: INFO: Pod "pod-subpath-test-configmap-8jbv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.114153632s
STEP: Saw pod success
May 21 20:14:56.341: INFO: Pod "pod-subpath-test-configmap-8jbv" satisfied condition "Succeeded or Failed"
May 21 20:14:56.346: INFO: Trying to get logs from node dc-hg-1 pod pod-subpath-test-configmap-8jbv container test-container-subpath-configmap-8jbv: <nil>
STEP: delete the pod
May 21 20:14:56.439: INFO: Waiting for pod pod-subpath-test-configmap-8jbv to disappear
May 21 20:14:56.445: INFO: Pod pod-subpath-test-configmap-8jbv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8jbv
May 21 20:14:56.446: INFO: Deleting pod "pod-subpath-test-configmap-8jbv" in namespace "subpath-9227"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:14:56.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9227" for this suite.

• [SLOW TEST:24.390 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":227,"skipped":3685,"failed":0}
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:14:56.471: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:14:56.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2439" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":228,"skipped":3685,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:14:56.661: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:14:56.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8313" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":229,"skipped":3691,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:14:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-23bbf9bd-33c7-4cbf-a82e-9efcfde180e8 in namespace container-probe-3344
May 21 20:15:00.901: INFO: Started pod busybox-23bbf9bd-33c7-4cbf-a82e-9efcfde180e8 in namespace container-probe-3344
STEP: checking the pod's current state and verifying that restartCount is present
May 21 20:15:00.909: INFO: Initial restart count of pod busybox-23bbf9bd-33c7-4cbf-a82e-9efcfde180e8 is 0
May 21 20:15:53.113: INFO: Restart count of pod container-probe-3344/busybox-23bbf9bd-33c7-4cbf-a82e-9efcfde180e8 is now 1 (52.204218219s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:15:53.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3344" for this suite.

• [SLOW TEST:56.481 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":230,"skipped":3700,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:15:53.264: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 21 20:15:53.398: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7237 /api/v1/namespaces/watch-7237/configmaps/e2e-watch-test-watch-closed 8cfd947b-0833-4dfb-80fc-f334e79f9b82 1284902 0 2021-05-21 20:15:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 20:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 20:15:53.398: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7237 /api/v1/namespaces/watch-7237/configmaps/e2e-watch-test-watch-closed 8cfd947b-0833-4dfb-80fc-f334e79f9b82 1284903 0 2021-05-21 20:15:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 20:15:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 21 20:15:53.438: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7237 /api/v1/namespaces/watch-7237/configmaps/e2e-watch-test-watch-closed 8cfd947b-0833-4dfb-80fc-f334e79f9b82 1284904 0 2021-05-21 20:15:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 20:15:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 21 20:15:53.438: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7237 /api/v1/namespaces/watch-7237/configmaps/e2e-watch-test-watch-closed 8cfd947b-0833-4dfb-80fc-f334e79f9b82 1284905 0 2021-05-21 20:15:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-21 20:15:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:15:53.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7237" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":231,"skipped":3704,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:15:53.459: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 20:15:58.652: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:15:58.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1457" for this suite.

• [SLOW TEST:5.301 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":232,"skipped":3707,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:15:58.761: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:16:03.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2459" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":233,"skipped":3721,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:16:03.283: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:16:03.410: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 21 20:16:08.417: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 21 20:16:08.417: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 21 20:16:08.481: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3183 /apis/apps/v1/namespaces/deployment-3183/deployments/test-cleanup-deployment 4e2ce858-3375-4d85-9690-ca9eaee808c5 1285008 1 2021-05-21 20:16:08 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-05-21 20:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0063f77c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 21 20:16:08.515: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-3183 /apis/apps/v1/namespaces/deployment-3183/replicasets/test-cleanup-deployment-5d446bdd47 2ee75c51-6ee3-40fc-9dab-3b69962079e5 1285010 1 2021-05-21 20:16:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 4e2ce858-3375-4d85-9690-ca9eaee808c5 0xc0062a8737 0xc0062a8738}] []  [{kube-controller-manager Update apps/v1 2021-05-21 20:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e2ce858-3375-4d85-9690-ca9eaee808c5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062a87c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 21 20:16:08.515: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 21 20:16:08.516: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3183 /apis/apps/v1/namespaces/deployment-3183/replicasets/test-cleanup-controller 4a25023c-a12c-43ea-9efd-1b628eb4f345 1285009 1 2021-05-21 20:16:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4e2ce858-3375-4d85-9690-ca9eaee808c5 0xc0062a860f 0xc0062a8620}] []  [{e2e.test Update apps/v1 2021-05-21 20:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-21 20:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4e2ce858-3375-4d85-9690-ca9eaee808c5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0062a86b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 21 20:16:08.561: INFO: Pod "test-cleanup-controller-cd7dt" is available:
&Pod{ObjectMeta:{test-cleanup-controller-cd7dt test-cleanup-controller- deployment-3183 /api/v1/namespaces/deployment-3183/pods/test-cleanup-controller-cd7dt 07d5a084-d02c-40e4-aa46-933afb6c7820 1285006 0 2021-05-21 20:16:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:192.168.106.182/32 cni.projectcalico.org/podIPs:192.168.106.182/32] [{apps/v1 ReplicaSet test-cleanup-controller 4a25023c-a12c-43ea-9efd-1b628eb4f345 0xc0063f7c97 0xc0063f7c98}] []  [{kube-controller-manager Update v1 2021-05-21 20:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a25023c-a12c-43ea-9efd-1b628eb4f345\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-21 20:16:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-21 20:16:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.106.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4c6qk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4c6qk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4c6qk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 20:16:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 20:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 20:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 20:16:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.1.136,PodIP:192.168.106.182,StartTime:2021-05-21 20:16:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-21 20:16:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3a00fa75e317cf7e7031ac1794d22b072ecad2f2ff478704d34304862a6f5447,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.106.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 20:16:08.561: INFO: Pod "test-cleanup-deployment-5d446bdd47-f488t" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-f488t test-cleanup-deployment-5d446bdd47- deployment-3183 /api/v1/namespaces/deployment-3183/pods/test-cleanup-deployment-5d446bdd47-f488t 44e9dafa-dd5d-4b29-b55b-ea3020661cf9 1285013 0 2021-05-21 20:16:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 2ee75c51-6ee3-40fc-9dab-3b69962079e5 0xc0063f7e67 0xc0063f7e68}] []  [{kube-controller-manager Update v1 2021-05-21 20:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ee75c51-6ee3-40fc-9dab-3b69962079e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4c6qk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4c6qk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4c6qk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dc-hg-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-21 20:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:16:08.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3183" for this suite.

• [SLOW TEST:5.348 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":234,"skipped":3733,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:16:08.632: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:16:08.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2273" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":235,"skipped":3740,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:16:08.836: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-1878
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1878
STEP: Deleting pre-stop pod
May 21 20:16:22.146: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:16:22.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1878" for this suite.

• [SLOW TEST:13.409 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":236,"skipped":3746,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:16:22.246: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-b094b197-71c7-4cac-9e1c-ef20f6e27d01
STEP: Creating a pod to test consume configMaps
May 21 20:16:22.409: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b89eeef-e0b4-455e-aada-ed7e206024ff" in namespace "projected-3181" to be "Succeeded or Failed"
May 21 20:16:22.428: INFO: Pod "pod-projected-configmaps-4b89eeef-e0b4-455e-aada-ed7e206024ff": Phase="Pending", Reason="", readiness=false. Elapsed: 18.319861ms
May 21 20:16:24.438: INFO: Pod "pod-projected-configmaps-4b89eeef-e0b4-455e-aada-ed7e206024ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028576558s
May 21 20:16:26.455: INFO: Pod "pod-projected-configmaps-4b89eeef-e0b4-455e-aada-ed7e206024ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045652958s
STEP: Saw pod success
May 21 20:16:26.455: INFO: Pod "pod-projected-configmaps-4b89eeef-e0b4-455e-aada-ed7e206024ff" satisfied condition "Succeeded or Failed"
May 21 20:16:26.460: INFO: Trying to get logs from node dc-hg-1 pod pod-projected-configmaps-4b89eeef-e0b4-455e-aada-ed7e206024ff container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 20:16:26.507: INFO: Waiting for pod pod-projected-configmaps-4b89eeef-e0b4-455e-aada-ed7e206024ff to disappear
May 21 20:16:26.511: INFO: Pod pod-projected-configmaps-4b89eeef-e0b4-455e-aada-ed7e206024ff no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:16:26.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3181" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":237,"skipped":3746,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:16:26.530: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:16:26.687: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Creating first CR 
May 21 20:16:27.382: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T20:16:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T20:16:27Z]] name:name1 resourceVersion:1285166 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:aefcee3d-a01f-4c20-884c-1484ed2b763b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 21 20:16:37.394: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T20:16:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T20:16:37Z]] name:name2 resourceVersion:1285195 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:8b993eff-be37-4f40-b812-df9ec468972c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 21 20:16:47.406: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T20:16:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T20:16:47Z]] name:name1 resourceVersion:1285209 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:aefcee3d-a01f-4c20-884c-1484ed2b763b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 21 20:16:57.422: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T20:16:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T20:16:57Z]] name:name2 resourceVersion:1285223 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:8b993eff-be37-4f40-b812-df9ec468972c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 21 20:17:07.439: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T20:16:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T20:16:47Z]] name:name1 resourceVersion:1285231 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:aefcee3d-a01f-4c20-884c-1484ed2b763b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 21 20:17:17.453: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-21T20:16:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-21T20:16:57Z]] name:name2 resourceVersion:1285238 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:8b993eff-be37-4f40-b812-df9ec468972c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:17:28.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2691" for this suite.

• [SLOW TEST:61.510 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":238,"skipped":3747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:17:28.041: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-7262
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7262 to expose endpoints map[]
May 21 20:17:28.229: INFO: successfully validated that service endpoint-test2 in namespace services-7262 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7262
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7262 to expose endpoints map[pod1:[80]]
May 21 20:17:32.380: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
May 21 20:17:33.391: INFO: successfully validated that service endpoint-test2 in namespace services-7262 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-7262
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7262 to expose endpoints map[pod1:[80] pod2:[80]]
May 21 20:17:37.473: INFO: successfully validated that service endpoint-test2 in namespace services-7262 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-7262
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7262 to expose endpoints map[pod2:[80]]
May 21 20:17:37.535: INFO: successfully validated that service endpoint-test2 in namespace services-7262 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-7262
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7262 to expose endpoints map[]
May 21 20:17:38.582: INFO: successfully validated that service endpoint-test2 in namespace services-7262 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:17:38.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7262" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.672 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":239,"skipped":3771,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:17:38.715: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:17:43.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7125" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":240,"skipped":3785,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:17:43.293: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7596
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7596
I0521 20:17:43.455111      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7596, replica count: 2
I0521 20:17:46.506038      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 20:17:49.506: INFO: Creating new exec pod
I0521 20:17:49.506427      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 20:17:54.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-7596 exec execpod4wkxk -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 21 20:17:55.802: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 21 20:17:55.802: INFO: stdout: ""
May 21 20:17:55.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-7596 exec execpod4wkxk -- /bin/sh -x -c nc -zv -t -w 2 10.10.161.248 80'
May 21 20:17:56.558: INFO: stderr: "+ nc -zv -t -w 2 10.10.161.248 80\nConnection to 10.10.161.248 80 port [tcp/http] succeeded!\n"
May 21 20:17:56.558: INFO: stdout: ""
May 21 20:17:56.558: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:17:56.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7596" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:13.411 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":241,"skipped":3799,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:17:56.745: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2185, will wait for the garbage collector to delete the pods
May 21 20:18:00.936: INFO: Deleting Job.batch foo took: 26.831385ms
May 21 20:18:01.537: INFO: Terminating Job.batch foo pods took: 600.339142ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:18:44.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2185" for this suite.

• [SLOW TEST:48.119 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":242,"skipped":3815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:18:44.867: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 21 20:18:44.978: INFO: Pod name pod-release: Found 0 pods out of 1
May 21 20:18:49.984: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:18:51.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5684" for this suite.

• [SLOW TEST:6.194 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":243,"skipped":3845,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:18:51.062: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
May 21 20:18:57.514: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:18:57.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0521 20:18:57.514143      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0521 20:18:57.514199      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0521 20:18:57.514217      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7711" for this suite.

• [SLOW TEST:6.517 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":244,"skipped":3858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:18:57.592: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5801
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-5801
May 21 20:18:58.075: INFO: Found 0 stateful pods, waiting for 1
May 21 20:19:08.094: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 20:19:08.147: INFO: Deleting all statefulset in ns statefulset-5801
May 21 20:19:08.153: INFO: Scaling statefulset ss to 0
May 21 20:19:28.221: INFO: Waiting for statefulset status.replicas updated to 0
May 21 20:19:28.226: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:19:28.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5801" for this suite.

• [SLOW TEST:30.741 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":245,"skipped":3905,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:19:28.335: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-811afb3b-6aa2-43c8-abd3-8c165915b405
STEP: Creating a pod to test consume configMaps
May 21 20:19:28.450: INFO: Waiting up to 5m0s for pod "pod-configmaps-0c7a0d1e-237d-4c3f-8f6b-d41cd24e1ae9" in namespace "configmap-7613" to be "Succeeded or Failed"
May 21 20:19:28.460: INFO: Pod "pod-configmaps-0c7a0d1e-237d-4c3f-8f6b-d41cd24e1ae9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.802045ms
May 21 20:19:30.469: INFO: Pod "pod-configmaps-0c7a0d1e-237d-4c3f-8f6b-d41cd24e1ae9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018879552s
May 21 20:19:32.478: INFO: Pod "pod-configmaps-0c7a0d1e-237d-4c3f-8f6b-d41cd24e1ae9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027974994s
STEP: Saw pod success
May 21 20:19:32.478: INFO: Pod "pod-configmaps-0c7a0d1e-237d-4c3f-8f6b-d41cd24e1ae9" satisfied condition "Succeeded or Failed"
May 21 20:19:32.483: INFO: Trying to get logs from node dc-hg-1 pod pod-configmaps-0c7a0d1e-237d-4c3f-8f6b-d41cd24e1ae9 container configmap-volume-test: <nil>
STEP: delete the pod
May 21 20:19:32.556: INFO: Waiting for pod pod-configmaps-0c7a0d1e-237d-4c3f-8f6b-d41cd24e1ae9 to disappear
May 21 20:19:32.562: INFO: Pod pod-configmaps-0c7a0d1e-237d-4c3f-8f6b-d41cd24e1ae9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:19:32.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7613" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":246,"skipped":3927,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:19:32.586: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-6f31ef26-aac3-4c01-bf84-4d5c4a104943
STEP: Creating a pod to test consume configMaps
May 21 20:19:32.677: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a6ac3d01-0536-4f29-993a-ad5810ce60ba" in namespace "projected-169" to be "Succeeded or Failed"
May 21 20:19:32.698: INFO: Pod "pod-projected-configmaps-a6ac3d01-0536-4f29-993a-ad5810ce60ba": Phase="Pending", Reason="", readiness=false. Elapsed: 21.225756ms
May 21 20:19:34.709: INFO: Pod "pod-projected-configmaps-a6ac3d01-0536-4f29-993a-ad5810ce60ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032034411s
May 21 20:19:36.724: INFO: Pod "pod-projected-configmaps-a6ac3d01-0536-4f29-993a-ad5810ce60ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047084519s
STEP: Saw pod success
May 21 20:19:36.724: INFO: Pod "pod-projected-configmaps-a6ac3d01-0536-4f29-993a-ad5810ce60ba" satisfied condition "Succeeded or Failed"
May 21 20:19:36.729: INFO: Trying to get logs from node dc-hg-1 pod pod-projected-configmaps-a6ac3d01-0536-4f29-993a-ad5810ce60ba container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 21 20:19:36.780: INFO: Waiting for pod pod-projected-configmaps-a6ac3d01-0536-4f29-993a-ad5810ce60ba to disappear
May 21 20:19:36.787: INFO: Pod pod-projected-configmaps-a6ac3d01-0536-4f29-993a-ad5810ce60ba no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:19:36.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-169" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":3937,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:19:36.809: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:19:37.006: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3cf530d0-2649-42a1-8adf-f892cf4d6521", Controller:(*bool)(0xc003537682), BlockOwnerDeletion:(*bool)(0xc003537683)}}
May 21 20:19:37.035: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1bafbf68-a429-4715-b7d1-ece3dceffcb0", Controller:(*bool)(0xc00317a112), BlockOwnerDeletion:(*bool)(0xc00317a113)}}
May 21 20:19:37.075: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f5536070-d388-4019-a06b-5774d2b6fe43", Controller:(*bool)(0xc003537916), BlockOwnerDeletion:(*bool)(0xc003537917)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:19:42.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-668" for this suite.

• [SLOW TEST:5.322 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":248,"skipped":3991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:19:42.132: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:19:42.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3832" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":249,"skipped":4020,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:19:42.242: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:19:54.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3841" for this suite.

• [SLOW TEST:12.126 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":250,"skipped":4033,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:19:54.369: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1546
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 20:19:54.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-4261 run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 21 20:19:55.007: INFO: stderr: ""
May 21 20:19:55.007: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 21 20:20:00.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-4261 get pod e2e-test-httpd-pod -o json'
May 21 20:20:00.378: INFO: stderr: ""
May 21 20:20:00.379: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.161.187/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.161.187/32\"\n        },\n        \"creationTimestamp\": \"2021-05-21T20:19:54Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-21T20:19:54Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-21T20:19:56Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"192.168.161.187\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-21T20:19:57Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4261\",\n        \"resourceVersion\": \"1286329\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4261/pods/e2e-test-httpd-pod\",\n        \"uid\": \"ef69c2e0-6d34-4fe2-9d97-45f3bcf1df7b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-6mmps\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"dc-hg-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-6mmps\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-6mmps\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T20:19:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T20:19:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T20:19:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-21T20:19:54Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://f5e8c1bcb5b9ac75c2f50b09aa138175f984bdeafab6cb1011c9723632b71f9d\",\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-21T20:19:57Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.1.125\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.161.187\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.161.187\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-21T20:19:55Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 21 20:20:00.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-4261 replace -f -'
May 21 20:20:01.167: INFO: stderr: ""
May 21 20:20:01.167: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
May 21 20:20:01.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-4261 delete pods e2e-test-httpd-pod'
May 21 20:20:05.108: INFO: stderr: ""
May 21 20:20:05.108: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:20:05.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4261" for this suite.

• [SLOW TEST:10.756 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":251,"skipped":4050,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:20:05.125: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2380
May 21 20:20:07.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-2380 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 21 20:20:07.981: INFO: rc: 7
May 21 20:20:08.001: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 20:20:08.006: INFO: Pod kube-proxy-mode-detector still exists
May 21 20:20:10.007: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 20:20:10.014: INFO: Pod kube-proxy-mode-detector still exists
May 21 20:20:12.007: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 21 20:20:12.013: INFO: Pod kube-proxy-mode-detector no longer exists
May 21 20:20:12.013: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-2380 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-2380
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2380
I0521 20:20:12.065286      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2380, replica count: 3
I0521 20:20:15.115895      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 20:20:18.116256      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 20:20:18.141: INFO: Creating new exec pod
May 21 20:20:23.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-2380 exec execpod-affinity9jwkc -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
May 21 20:20:23.774: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 21 20:20:23.774: INFO: stdout: ""
May 21 20:20:23.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-2380 exec execpod-affinity9jwkc -- /bin/sh -x -c nc -zv -t -w 2 10.10.177.110 80'
May 21 20:20:24.424: INFO: stderr: "+ nc -zv -t -w 2 10.10.177.110 80\nConnection to 10.10.177.110 80 port [tcp/http] succeeded!\n"
May 21 20:20:24.424: INFO: stdout: ""
May 21 20:20:24.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-2380 exec execpod-affinity9jwkc -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.136 31469'
May 21 20:20:25.126: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.136 31469\nConnection to 10.10.1.136 31469 port [tcp/31469] succeeded!\n"
May 21 20:20:25.126: INFO: stdout: ""
May 21 20:20:25.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-2380 exec execpod-affinity9jwkc -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.125 31469'
May 21 20:20:25.694: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.125 31469\nConnection to 10.10.1.125 31469 port [tcp/31469] succeeded!\n"
May 21 20:20:25.694: INFO: stdout: ""
May 21 20:20:25.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-2380 exec execpod-affinity9jwkc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.86:31469/ ; done'
May 21 20:20:26.884: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n"
May 21 20:20:26.885: INFO: stdout: "\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k\naffinity-nodeport-timeout-w7b6k"
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Received response from host: affinity-nodeport-timeout-w7b6k
May 21 20:20:26.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-2380 exec execpod-affinity9jwkc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.1.86:31469/'
May 21 20:20:27.594: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n"
May 21 20:20:27.594: INFO: stdout: "affinity-nodeport-timeout-w7b6k"
May 21 20:20:42.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-2380 exec execpod-affinity9jwkc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.1.86:31469/'
May 21 20:20:43.206: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n"
May 21 20:20:43.206: INFO: stdout: "affinity-nodeport-timeout-w7b6k"
May 21 20:20:58.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-2380 exec execpod-affinity9jwkc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.1.86:31469/'
May 21 20:20:58.770: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.1.86:31469/\n"
May 21 20:20:58.770: INFO: stdout: "affinity-nodeport-timeout-9tfpw"
May 21 20:20:58.770: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2380, will wait for the garbage collector to delete the pods
May 21 20:20:58.930: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 36.910686ms
May 21 20:20:59.630: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 700.576652ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:21:14.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2380" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:69.837 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":252,"skipped":4054,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:21:14.963: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 20:21:15.081: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50a8bec5-59f6-479b-a55c-9481443b2978" in namespace "downward-api-3655" to be "Succeeded or Failed"
May 21 20:21:15.109: INFO: Pod "downwardapi-volume-50a8bec5-59f6-479b-a55c-9481443b2978": Phase="Pending", Reason="", readiness=false. Elapsed: 27.815132ms
May 21 20:21:17.121: INFO: Pod "downwardapi-volume-50a8bec5-59f6-479b-a55c-9481443b2978": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040111826s
May 21 20:21:19.138: INFO: Pod "downwardapi-volume-50a8bec5-59f6-479b-a55c-9481443b2978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057517062s
STEP: Saw pod success
May 21 20:21:19.138: INFO: Pod "downwardapi-volume-50a8bec5-59f6-479b-a55c-9481443b2978" satisfied condition "Succeeded or Failed"
May 21 20:21:19.143: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-50a8bec5-59f6-479b-a55c-9481443b2978 container client-container: <nil>
STEP: delete the pod
May 21 20:21:19.213: INFO: Waiting for pod downwardapi-volume-50a8bec5-59f6-479b-a55c-9481443b2978 to disappear
May 21 20:21:19.224: INFO: Pod downwardapi-volume-50a8bec5-59f6-479b-a55c-9481443b2978 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:21:19.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3655" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":253,"skipped":4056,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:21:19.248: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-3497/configmap-test-b07d9e5e-654b-4b71-8cff-2dc8152fb517
STEP: Creating a pod to test consume configMaps
May 21 20:21:19.349: INFO: Waiting up to 5m0s for pod "pod-configmaps-76c4920e-830f-4a79-a3a5-9f924e60484f" in namespace "configmap-3497" to be "Succeeded or Failed"
May 21 20:21:19.361: INFO: Pod "pod-configmaps-76c4920e-830f-4a79-a3a5-9f924e60484f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.3991ms
May 21 20:21:21.368: INFO: Pod "pod-configmaps-76c4920e-830f-4a79-a3a5-9f924e60484f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019460361s
May 21 20:21:23.375: INFO: Pod "pod-configmaps-76c4920e-830f-4a79-a3a5-9f924e60484f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025655948s
STEP: Saw pod success
May 21 20:21:23.375: INFO: Pod "pod-configmaps-76c4920e-830f-4a79-a3a5-9f924e60484f" satisfied condition "Succeeded or Failed"
May 21 20:21:23.380: INFO: Trying to get logs from node dc-hg-1 pod pod-configmaps-76c4920e-830f-4a79-a3a5-9f924e60484f container env-test: <nil>
STEP: delete the pod
May 21 20:21:23.433: INFO: Waiting for pod pod-configmaps-76c4920e-830f-4a79-a3a5-9f924e60484f to disappear
May 21 20:21:23.444: INFO: Pod pod-configmaps-76c4920e-830f-4a79-a3a5-9f924e60484f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:21:23.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3497" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":254,"skipped":4060,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:21:23.468: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8185
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 20:21:23.570: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 20:21:23.785: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:21:25.791: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:21:27.800: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:21:29.791: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:21:31.792: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:21:33.791: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:21:35.791: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:21:37.792: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:21:39.791: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 20:21:39.810: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 20:21:41.817: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 20:21:43.817: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 21 20:21:45.830: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 21 20:21:45.845: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 21 20:21:49.972: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.209.85:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8185 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:21:49.972: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:21:50.223: INFO: Found all expected endpoints: [netserver-0]
May 21 20:21:50.241: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.161.191:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8185 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:21:50.241: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:21:50.552: INFO: Found all expected endpoints: [netserver-1]
May 21 20:21:50.559: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.106.136:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8185 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:21:50.559: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:21:50.825: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:21:50.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8185" for this suite.

• [SLOW TEST:27.398 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":255,"skipped":4087,"failed":0}
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:21:50.866: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:21:55.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-544" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":256,"skipped":4087,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:21:55.067: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:22:26.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-510" for this suite.

• [SLOW TEST:31.814 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":257,"skipped":4092,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:22:26.881: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 21 20:22:31.040: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:22:31.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9960" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:22:31.133: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1307
STEP: creating the pod
May 21 20:22:31.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-339 create -f -'
May 21 20:22:31.995: INFO: stderr: ""
May 21 20:22:31.995: INFO: stdout: "pod/pause created\n"
May 21 20:22:31.995: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 21 20:22:31.995: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-339" to be "running and ready"
May 21 20:22:32.040: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 45.320256ms
May 21 20:22:34.046: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051414611s
May 21 20:22:36.056: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.060709402s
May 21 20:22:36.056: INFO: Pod "pause" satisfied condition "running and ready"
May 21 20:22:36.056: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
May 21 20:22:36.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-339 label pods pause testing-label=testing-label-value'
May 21 20:22:36.516: INFO: stderr: ""
May 21 20:22:36.516: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 21 20:22:36.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-339 get pod pause -L testing-label'
May 21 20:22:36.856: INFO: stderr: ""
May 21 20:22:36.856: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 21 20:22:36.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-339 label pods pause testing-label-'
May 21 20:22:37.236: INFO: stderr: ""
May 21 20:22:37.236: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 21 20:22:37.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-339 get pod pause -L testing-label'
May 21 20:22:37.545: INFO: stderr: ""
May 21 20:22:37.545: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1313
STEP: using delete to clean up resources
May 21 20:22:37.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-339 delete --grace-period=0 --force -f -'
May 21 20:22:37.836: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 21 20:22:37.836: INFO: stdout: "pod \"pause\" force deleted\n"
May 21 20:22:37.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-339 get rc,svc -l name=pause --no-headers'
May 21 20:22:38.122: INFO: stderr: "No resources found in kubectl-339 namespace.\n"
May 21 20:22:38.122: INFO: stdout: ""
May 21 20:22:38.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-339 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 21 20:22:38.398: INFO: stderr: ""
May 21 20:22:38.398: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:22:38.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-339" for this suite.

• [SLOW TEST:7.298 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1305
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":259,"skipped":4136,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:22:38.432: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:22:38.532: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:22:39.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3875" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":260,"skipped":4142,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:22:39.955: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 20:22:41.304: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 20:22:43.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225361, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225361, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225361, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225361, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 20:22:46.428: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
May 21 20:22:46.653: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
May 21 20:22:46.846: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:22:46.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9199" for this suite.
STEP: Destroying namespace "webhook-9199-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.373 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":261,"skipped":4145,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:22:47.329: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:23:06.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3472" for this suite.
STEP: Destroying namespace "nsdeletetest-58" for this suite.
May 21 20:23:06.129: INFO: Namespace nsdeletetest-58 was already deleted
STEP: Destroying namespace "nsdeletetest-271" for this suite.

• [SLOW TEST:18.823 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":262,"skipped":4149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:23:06.154: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:23:10.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2987" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":263,"skipped":4218,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:23:10.357: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 20:23:10.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-6500 run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 21 20:23:10.867: INFO: stderr: ""
May 21 20:23:10.867: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 21 20:23:10.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-6500 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "mirror.gcr.io/library/busybox:1.29"}]}} --dry-run=server'
May 21 20:23:12.024: INFO: stderr: ""
May 21 20:23:12.024: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/httpd:2.4.38-alpine
May 21 20:23:12.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-6500 delete pods e2e-test-httpd-pod'
May 21 20:23:18.754: INFO: stderr: ""
May 21 20:23:18.754: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:23:18.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6500" for this suite.

• [SLOW TEST:8.434 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:902
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":264,"skipped":4248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:23:18.793: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 21 20:23:19.037: INFO: Waiting up to 5m0s for pod "downward-api-1954d469-53a4-4603-8142-22d9fba71822" in namespace "downward-api-8030" to be "Succeeded or Failed"
May 21 20:23:19.052: INFO: Pod "downward-api-1954d469-53a4-4603-8142-22d9fba71822": Phase="Pending", Reason="", readiness=false. Elapsed: 15.07605ms
May 21 20:23:21.061: INFO: Pod "downward-api-1954d469-53a4-4603-8142-22d9fba71822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023385394s
May 21 20:23:23.074: INFO: Pod "downward-api-1954d469-53a4-4603-8142-22d9fba71822": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036329596s
STEP: Saw pod success
May 21 20:23:23.074: INFO: Pod "downward-api-1954d469-53a4-4603-8142-22d9fba71822" satisfied condition "Succeeded or Failed"
May 21 20:23:23.093: INFO: Trying to get logs from node dc-hg-1 pod downward-api-1954d469-53a4-4603-8142-22d9fba71822 container dapi-container: <nil>
STEP: delete the pod
May 21 20:23:23.187: INFO: Waiting for pod downward-api-1954d469-53a4-4603-8142-22d9fba71822 to disappear
May 21 20:23:23.200: INFO: Pod downward-api-1954d469-53a4-4603-8142-22d9fba71822 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:23:23.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8030" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":265,"skipped":4273,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:23:23.223: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
May 21 20:23:23.326: INFO: Waiting up to 5m0s for pod "pod-8978a629-e7ee-4ea8-ad29-d73237ef0083" in namespace "emptydir-7590" to be "Succeeded or Failed"
May 21 20:23:23.343: INFO: Pod "pod-8978a629-e7ee-4ea8-ad29-d73237ef0083": Phase="Pending", Reason="", readiness=false. Elapsed: 16.672077ms
May 21 20:23:25.349: INFO: Pod "pod-8978a629-e7ee-4ea8-ad29-d73237ef0083": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022962784s
May 21 20:23:27.355: INFO: Pod "pod-8978a629-e7ee-4ea8-ad29-d73237ef0083": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028898921s
STEP: Saw pod success
May 21 20:23:27.355: INFO: Pod "pod-8978a629-e7ee-4ea8-ad29-d73237ef0083" satisfied condition "Succeeded or Failed"
May 21 20:23:27.360: INFO: Trying to get logs from node dc-hg-1 pod pod-8978a629-e7ee-4ea8-ad29-d73237ef0083 container test-container: <nil>
STEP: delete the pod
May 21 20:23:27.404: INFO: Waiting for pod pod-8978a629-e7ee-4ea8-ad29-d73237ef0083 to disappear
May 21 20:23:27.410: INFO: Pod pod-8978a629-e7ee-4ea8-ad29-d73237ef0083 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:23:27.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7590" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":266,"skipped":4273,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:23:27.425: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 21 20:23:35.645: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 20:23:35.674: INFO: Pod pod-with-prestop-http-hook still exists
May 21 20:23:37.674: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 20:23:37.690: INFO: Pod pod-with-prestop-http-hook still exists
May 21 20:23:39.674: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 20:23:39.681: INFO: Pod pod-with-prestop-http-hook still exists
May 21 20:23:41.674: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 20:23:41.695: INFO: Pod pod-with-prestop-http-hook still exists
May 21 20:23:43.674: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 20:23:43.727: INFO: Pod pod-with-prestop-http-hook still exists
May 21 20:23:45.674: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 21 20:23:45.681: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:23:45.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8842" for this suite.

• [SLOW TEST:18.296 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4274,"failed":0}
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:23:45.723: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-815/configmap-test-29eb32f0-7312-451d-aad8-d356f926f6ef
STEP: Creating a pod to test consume configMaps
May 21 20:23:45.885: INFO: Waiting up to 5m0s for pod "pod-configmaps-55af0ad3-7496-4db8-b86d-aa69f6ba9000" in namespace "configmap-815" to be "Succeeded or Failed"
May 21 20:23:45.902: INFO: Pod "pod-configmaps-55af0ad3-7496-4db8-b86d-aa69f6ba9000": Phase="Pending", Reason="", readiness=false. Elapsed: 17.168176ms
May 21 20:23:47.911: INFO: Pod "pod-configmaps-55af0ad3-7496-4db8-b86d-aa69f6ba9000": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02586922s
May 21 20:23:49.917: INFO: Pod "pod-configmaps-55af0ad3-7496-4db8-b86d-aa69f6ba9000": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032437316s
STEP: Saw pod success
May 21 20:23:49.917: INFO: Pod "pod-configmaps-55af0ad3-7496-4db8-b86d-aa69f6ba9000" satisfied condition "Succeeded or Failed"
May 21 20:23:49.923: INFO: Trying to get logs from node dc-hg-2 pod pod-configmaps-55af0ad3-7496-4db8-b86d-aa69f6ba9000 container env-test: <nil>
STEP: delete the pod
May 21 20:23:49.994: INFO: Waiting for pod pod-configmaps-55af0ad3-7496-4db8-b86d-aa69f6ba9000 to disappear
May 21 20:23:50.000: INFO: Pod pod-configmaps-55af0ad3-7496-4db8-b86d-aa69f6ba9000 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:23:50.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-815" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":268,"skipped":4274,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:23:50.032: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 21 20:23:54.174: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1822 PodName:var-expansion-7fc07e4e-5646-4080-99da-da06197219da ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:23:54.174: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: test for file in mounted path
May 21 20:23:54.409: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1822 PodName:var-expansion-7fc07e4e-5646-4080-99da-da06197219da ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:23:54.409: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: updating the annotation value
May 21 20:23:55.133: INFO: Successfully updated pod "var-expansion-7fc07e4e-5646-4080-99da-da06197219da"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 21 20:23:55.152: INFO: Deleting pod "var-expansion-7fc07e4e-5646-4080-99da-da06197219da" in namespace "var-expansion-1822"
May 21 20:23:55.190: INFO: Wait up to 5m0s for pod "var-expansion-7fc07e4e-5646-4080-99da-da06197219da" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:24:39.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1822" for this suite.

• [SLOW TEST:49.195 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":269,"skipped":4274,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:24:39.228: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 20:24:39.392: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50ebe4ca-b1a5-444b-81ae-59c767543e96" in namespace "projected-488" to be "Succeeded or Failed"
May 21 20:24:39.421: INFO: Pod "downwardapi-volume-50ebe4ca-b1a5-444b-81ae-59c767543e96": Phase="Pending", Reason="", readiness=false. Elapsed: 28.962723ms
May 21 20:24:41.427: INFO: Pod "downwardapi-volume-50ebe4ca-b1a5-444b-81ae-59c767543e96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034827865s
May 21 20:24:43.434: INFO: Pod "downwardapi-volume-50ebe4ca-b1a5-444b-81ae-59c767543e96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041571043s
STEP: Saw pod success
May 21 20:24:43.434: INFO: Pod "downwardapi-volume-50ebe4ca-b1a5-444b-81ae-59c767543e96" satisfied condition "Succeeded or Failed"
May 21 20:24:43.440: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-50ebe4ca-b1a5-444b-81ae-59c767543e96 container client-container: <nil>
STEP: delete the pod
May 21 20:24:43.486: INFO: Waiting for pod downwardapi-volume-50ebe4ca-b1a5-444b-81ae-59c767543e96 to disappear
May 21 20:24:43.492: INFO: Pod downwardapi-volume-50ebe4ca-b1a5-444b-81ae-59c767543e96 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:24:43.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-488" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4281,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:24:43.523: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-4356
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 20:24:43.679: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 20:24:43.902: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:24:45.919: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:24:47.909: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:24:49.908: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:24:51.910: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:24:53.912: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:24:55.910: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:24:57.910: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:24:59.910: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:25:01.909: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:25:03.909: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:25:05.910: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 20:25:05.923: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 21 20:25:05.936: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 21 20:25:09.997: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.161.147:8080/dial?request=hostname&protocol=http&host=192.168.209.86&port=8080&tries=1'] Namespace:pod-network-test-4356 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:25:09.997: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:25:10.246: INFO: Waiting for responses: map[]
May 21 20:25:10.251: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.161.147:8080/dial?request=hostname&protocol=http&host=192.168.161.152&port=8080&tries=1'] Namespace:pod-network-test-4356 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:25:10.251: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:25:10.463: INFO: Waiting for responses: map[]
May 21 20:25:10.469: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.161.147:8080/dial?request=hostname&protocol=http&host=192.168.106.131&port=8080&tries=1'] Namespace:pod-network-test-4356 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:25:10.469: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:25:10.701: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:25:10.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4356" for this suite.

• [SLOW TEST:27.203 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":271,"skipped":4288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:25:10.728: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 21 20:25:10.844: INFO: Created pod &Pod{ObjectMeta:{dns-3105  dns-3105 /api/v1/namespaces/dns-3105/pods/dns-3105 817199dc-78f6-4715-a7b3-690be036c3b9 1287618 0 2021-05-21 20:25:10 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-05-21 20:25:10 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-l52l6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-l52l6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-l52l6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 21 20:25:10.865: INFO: The status of Pod dns-3105 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:25:12.871: INFO: The status of Pod dns-3105 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:25:14.871: INFO: The status of Pod dns-3105 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 21 20:25:14.871: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3105 PodName:dns-3105 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:25:14.871: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Verifying customized DNS server is configured on pod...
May 21 20:25:15.146: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3105 PodName:dns-3105 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:25:15.146: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:25:15.431: INFO: Deleting pod dns-3105...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:25:15.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3105" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":272,"skipped":4317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:25:15.535: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-f7ee3530-a957-4a07-88e7-e623e231a058
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:25:15.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2755" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":273,"skipped":4382,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:25:15.673: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 21 20:25:15.811: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 20:26:15.892: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:26:15.900: INFO: Starting informer...
STEP: Starting pod...
May 21 20:26:16.168: INFO: Pod is running on dc-hg-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 21 20:26:16.239: INFO: Pod wasn't evicted. Proceeding
May 21 20:26:16.239: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 21 20:27:31.346: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:27:31.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6411" for this suite.

• [SLOW TEST:135.698 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":274,"skipped":4398,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:27:31.372: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 21 20:27:31.453: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 20:27:31.478: INFO: Waiting for terminating namespaces to be deleted...
May 21 20:27:31.485: INFO: 
Logging pods the apiserver thinks is on node conformance1 before test
May 21 20:27:31.497: INFO: haproxy-ingress-85597cc495-l7rzc from ingress-haproxy started at 2021-04-26 19:00:07 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.497: INFO: 	Container haproxy-ingress ready: true, restart count 2
May 21 20:27:31.497: INFO: ingress-default-backend-5f65f5865b-nldzd from ingress-haproxy started at 2021-04-26 19:00:28 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.497: INFO: 	Container ingress-default-backend ready: true, restart count 2
May 21 20:27:31.497: INFO: calico-kube-controllers-db766f945-4228n from kube-system started at 2021-04-26 19:00:07 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.497: INFO: 	Container calico-kube-controllers ready: true, restart count 2
May 21 20:27:31.497: INFO: calico-node-j5hq5 from kube-system started at 2021-04-23 21:19:36 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.497: INFO: 	Container calico-node ready: true, restart count 2
May 21 20:27:31.497: INFO: coredns-c6fc4f979-bv84n from kube-system started at 2021-04-26 19:00:09 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.497: INFO: 	Container coredns ready: true, restart count 2
May 21 20:27:31.497: INFO: coredns-c6fc4f979-jfxfx from kube-system started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.497: INFO: 	Container coredns ready: true, restart count 2
May 21 20:27:31.497: INFO: metrics-server-6c6c6b5d47-9r52q from kube-system started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.497: INFO: 	Container metrics-server ready: true, restart count 2
May 21 20:27:31.497: INFO: nirmata-cni-installer-zjkpx from nirmata started at 2021-04-26 19:00:42 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.497: INFO: 	Container install-cni ready: true, restart count 2
May 21 20:27:31.497: INFO: nirmata-kube-controller-5cf898966b-md6gc from nirmata started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.497: INFO: 	Container nirmata-kube-controller ready: true, restart count 8
May 21 20:27:31.497: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-47fv4 from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:27:31.497: INFO: 	Container sonobuoy-worker ready: false, restart count 10
May 21 20:27:31.497: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 20:27:31.497: INFO: 
Logging pods the apiserver thinks is on node dc-hg-1 before test
May 21 20:27:31.510: INFO: calico-node-6r5n7 from kube-system started at 2021-05-21 02:51:05 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.510: INFO: 	Container calico-node ready: true, restart count 0
May 21 20:27:31.510: INFO: nirmata-cni-installer-zm9dd from nirmata started at 2021-05-21 20:26:54 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.510: INFO: 	Container install-cni ready: true, restart count 0
May 21 20:27:31.510: INFO: sonobuoy from sonobuoy started at 2021-05-21 18:56:18 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.510: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 20:27:31.510: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-xch78 from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:27:31.510: INFO: 	Container sonobuoy-worker ready: false, restart count 10
May 21 20:27:31.510: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 20:27:31.510: INFO: taint-eviction-4 from taint-single-pod-6411 started at 2021-05-21 20:26:15 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.510: INFO: 	Container pause ready: true, restart count 0
May 21 20:27:31.510: INFO: 
Logging pods the apiserver thinks is on node dc-hg-2 before test
May 21 20:27:31.520: INFO: calico-node-c7rz4 from kube-system started at 2021-04-27 15:00:32 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.520: INFO: 	Container calico-node ready: true, restart count 2
May 21 20:27:31.520: INFO: nirmata-cni-installer-5m9z5 from nirmata started at 2021-05-20 07:33:18 +0000 UTC (1 container statuses recorded)
May 21 20:27:31.520: INFO: 	Container install-cni ready: true, restart count 0
May 21 20:27:31.521: INFO: sonobuoy-e2e-job-03ad7d58f5964dbf from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:27:31.521: INFO: 	Container e2e ready: true, restart count 0
May 21 20:27:31.521: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 20:27:31.521: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-lxlxb from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:27:31.521: INFO: 	Container sonobuoy-worker ready: false, restart count 10
May 21 20:27:31.521: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16812ed948ab43d9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:27:32.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9993" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":275,"skipped":4402,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:27:32.607: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 21 20:27:34.228: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 21 20:27:36.289: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225654, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225654, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225654, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225654, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 21 20:27:38.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225654, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225654, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225654, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757225654, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 21 20:27:41.312: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:27:41.318: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:27:42.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5790" for this suite.
STEP: Destroying namespace "webhook-5790-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.337 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":276,"skipped":4406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:27:42.945: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:29:43.226: INFO: Deleting pod "var-expansion-07a8a78b-1272-4ff4-9cd6-ef1907fbffe6" in namespace "var-expansion-876"
May 21 20:29:43.256: INFO: Wait up to 5m0s for pod "var-expansion-07a8a78b-1272-4ff4-9cd6-ef1907fbffe6" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:29:47.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-876" for this suite.

• [SLOW TEST:124.379 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":277,"skipped":4430,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:29:47.325: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8380
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 21 20:29:47.408: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 21 20:29:47.553: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:29:49.566: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:29:51.558: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 21 20:29:53.575: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:29:55.560: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:29:57.559: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:29:59.561: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:30:01.559: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:30:03.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:30:05.560: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:30:07.560: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:30:09.558: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 21 20:30:11.564: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 21 20:30:11.580: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 21 20:30:11.595: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 21 20:30:15.707: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.209.93 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8380 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:30:15.707: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:30:16.969: INFO: Found all expected endpoints: [netserver-0]
May 21 20:30:16.974: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.161.151 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8380 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:30:16.975: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:30:18.192: INFO: Found all expected endpoints: [netserver-1]
May 21 20:30:18.198: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.106.137 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8380 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 21 20:30:18.198: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:30:19.448: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:30:19.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8380" for this suite.

• [SLOW TEST:32.150 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":278,"skipped":4437,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:30:19.475: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 21 20:30:19.573: INFO: Waiting up to 5m0s for pod "pod-17a00d67-e370-4a2f-86ec-6ee161d44a5b" in namespace "emptydir-44" to be "Succeeded or Failed"
May 21 20:30:19.584: INFO: Pod "pod-17a00d67-e370-4a2f-86ec-6ee161d44a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.234752ms
May 21 20:30:21.591: INFO: Pod "pod-17a00d67-e370-4a2f-86ec-6ee161d44a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017895813s
May 21 20:30:23.598: INFO: Pod "pod-17a00d67-e370-4a2f-86ec-6ee161d44a5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024895333s
STEP: Saw pod success
May 21 20:30:23.598: INFO: Pod "pod-17a00d67-e370-4a2f-86ec-6ee161d44a5b" satisfied condition "Succeeded or Failed"
May 21 20:30:23.606: INFO: Trying to get logs from node dc-hg-2 pod pod-17a00d67-e370-4a2f-86ec-6ee161d44a5b container test-container: <nil>
STEP: delete the pod
May 21 20:30:23.718: INFO: Waiting for pod pod-17a00d67-e370-4a2f-86ec-6ee161d44a5b to disappear
May 21 20:30:23.739: INFO: Pod pod-17a00d67-e370-4a2f-86ec-6ee161d44a5b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:30:23.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-44" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":279,"skipped":4444,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:30:23.761: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:30:40.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8216" for this suite.

• [SLOW TEST:16.412 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":280,"skipped":4463,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:30:40.175: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-394
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-394
STEP: creating replication controller externalsvc in namespace services-394
I0521 20:30:40.399599      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-394, replica count: 2
I0521 20:30:43.450573      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 20:30:46.451102      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 21 20:30:46.482: INFO: Creating new exec pod
May 21 20:30:50.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-394 exec execpodp5d5j -- /bin/sh -x -c nslookup clusterip-service.services-394.svc.cluster.local'
May 21 20:30:51.997: INFO: stderr: "+ nslookup clusterip-service.services-394.svc.cluster.local\n"
May 21 20:30:51.997: INFO: stdout: "Server:\t\t10.10.0.10\nAddress:\t10.10.0.10#53\n\nclusterip-service.services-394.svc.cluster.local\tcanonical name = externalsvc.services-394.svc.cluster.local.\nName:\texternalsvc.services-394.svc.cluster.local\nAddress: 10.10.111.197\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-394, will wait for the garbage collector to delete the pods
May 21 20:30:52.067: INFO: Deleting ReplicationController externalsvc took: 13.590072ms
May 21 20:30:52.768: INFO: Terminating ReplicationController externalsvc pods took: 700.877293ms
May 21 20:31:08.874: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:31:09.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-394" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:28.913 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":281,"skipped":4468,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:31:09.089: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 21 20:31:09.179: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:31:16.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8778" for this suite.

• [SLOW TEST:7.308 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":282,"skipped":4476,"failed":0}
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:31:16.397: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
May 21 20:31:16.520: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-5699 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:31:16.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5699" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":283,"skipped":4476,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:31:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
May 21 20:31:17.057: INFO: Waiting up to 5m0s for pod "var-expansion-ff456476-05aa-438f-a06b-d94b69e0c1f8" in namespace "var-expansion-6475" to be "Succeeded or Failed"
May 21 20:31:17.067: INFO: Pod "var-expansion-ff456476-05aa-438f-a06b-d94b69e0c1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.833504ms
May 21 20:31:19.082: INFO: Pod "var-expansion-ff456476-05aa-438f-a06b-d94b69e0c1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025451872s
May 21 20:31:21.091: INFO: Pod "var-expansion-ff456476-05aa-438f-a06b-d94b69e0c1f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033727341s
STEP: Saw pod success
May 21 20:31:21.091: INFO: Pod "var-expansion-ff456476-05aa-438f-a06b-d94b69e0c1f8" satisfied condition "Succeeded or Failed"
May 21 20:31:21.098: INFO: Trying to get logs from node dc-hg-1 pod var-expansion-ff456476-05aa-438f-a06b-d94b69e0c1f8 container dapi-container: <nil>
STEP: delete the pod
May 21 20:31:21.169: INFO: Waiting for pod var-expansion-ff456476-05aa-438f-a06b-d94b69e0c1f8 to disappear
May 21 20:31:21.174: INFO: Pod var-expansion-ff456476-05aa-438f-a06b-d94b69e0c1f8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:31:21.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6475" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":284,"skipped":4478,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:31:21.196: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-821a7286-948c-4f6a-81eb-2854421d616b
STEP: Creating secret with name s-test-opt-upd-d1d241c1-4ec4-42f3-803d-38eb667aad38
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-821a7286-948c-4f6a-81eb-2854421d616b
STEP: Updating secret s-test-opt-upd-d1d241c1-4ec4-42f3-803d-38eb667aad38
STEP: Creating secret with name s-test-opt-create-3c5df433-348d-4ea2-96dd-9c7f217dced2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:31:31.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8950" for this suite.

• [SLOW TEST:10.368 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":285,"skipped":4482,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:31:31.565: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:31:31.683: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:31:32.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7467" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":286,"skipped":4482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:31:32.814: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 21 20:31:32.926: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 21 20:31:32.957: INFO: Waiting for terminating namespaces to be deleted...
May 21 20:31:32.967: INFO: 
Logging pods the apiserver thinks is on node conformance1 before test
May 21 20:31:32.982: INFO: haproxy-ingress-85597cc495-l7rzc from ingress-haproxy started at 2021-04-26 19:00:07 +0000 UTC (1 container statuses recorded)
May 21 20:31:32.982: INFO: 	Container haproxy-ingress ready: true, restart count 2
May 21 20:31:32.982: INFO: ingress-default-backend-5f65f5865b-nldzd from ingress-haproxy started at 2021-04-26 19:00:28 +0000 UTC (1 container statuses recorded)
May 21 20:31:32.982: INFO: 	Container ingress-default-backend ready: true, restart count 2
May 21 20:31:32.982: INFO: calico-kube-controllers-db766f945-4228n from kube-system started at 2021-04-26 19:00:07 +0000 UTC (1 container statuses recorded)
May 21 20:31:32.982: INFO: 	Container calico-kube-controllers ready: true, restart count 2
May 21 20:31:32.982: INFO: calico-node-j5hq5 from kube-system started at 2021-04-23 21:19:36 +0000 UTC (1 container statuses recorded)
May 21 20:31:32.982: INFO: 	Container calico-node ready: true, restart count 2
May 21 20:31:32.982: INFO: coredns-c6fc4f979-bv84n from kube-system started at 2021-04-26 19:00:09 +0000 UTC (1 container statuses recorded)
May 21 20:31:32.982: INFO: 	Container coredns ready: true, restart count 2
May 21 20:31:32.982: INFO: coredns-c6fc4f979-jfxfx from kube-system started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 20:31:32.982: INFO: 	Container coredns ready: true, restart count 2
May 21 20:31:32.982: INFO: metrics-server-6c6c6b5d47-9r52q from kube-system started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 20:31:32.982: INFO: 	Container metrics-server ready: true, restart count 2
May 21 20:31:32.982: INFO: nirmata-cni-installer-zjkpx from nirmata started at 2021-04-26 19:00:42 +0000 UTC (1 container statuses recorded)
May 21 20:31:32.982: INFO: 	Container install-cni ready: true, restart count 2
May 21 20:31:32.982: INFO: nirmata-kube-controller-5cf898966b-md6gc from nirmata started at 2021-04-26 19:00:06 +0000 UTC (1 container statuses recorded)
May 21 20:31:32.982: INFO: 	Container nirmata-kube-controller ready: true, restart count 8
May 21 20:31:32.982: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-47fv4 from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:31:32.982: INFO: 	Container sonobuoy-worker ready: false, restart count 11
May 21 20:31:32.982: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 20:31:32.982: INFO: 
Logging pods the apiserver thinks is on node dc-hg-1 before test
May 21 20:31:33.004: INFO: calico-node-6r5n7 from kube-system started at 2021-05-21 02:51:05 +0000 UTC (1 container statuses recorded)
May 21 20:31:33.004: INFO: 	Container calico-node ready: true, restart count 0
May 21 20:31:33.004: INFO: nirmata-cni-installer-zm9dd from nirmata started at 2021-05-21 20:26:54 +0000 UTC (1 container statuses recorded)
May 21 20:31:33.004: INFO: 	Container install-cni ready: true, restart count 0
May 21 20:31:33.004: INFO: pod-projected-secrets-565e7ac9-70a5-4099-b331-bf7733f55ffd from projected-8950 started at 2021-05-21 20:31:21 +0000 UTC (3 container statuses recorded)
May 21 20:31:33.004: INFO: 	Container creates-volume-test ready: true, restart count 0
May 21 20:31:33.004: INFO: 	Container dels-volume-test ready: true, restart count 0
May 21 20:31:33.004: INFO: 	Container upds-volume-test ready: true, restart count 0
May 21 20:31:33.004: INFO: sonobuoy from sonobuoy started at 2021-05-21 18:56:18 +0000 UTC (1 container statuses recorded)
May 21 20:31:33.004: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 21 20:31:33.004: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-xch78 from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:31:33.004: INFO: 	Container sonobuoy-worker ready: false, restart count 11
May 21 20:31:33.004: INFO: 	Container systemd-logs ready: true, restart count 0
May 21 20:31:33.004: INFO: 
Logging pods the apiserver thinks is on node dc-hg-2 before test
May 21 20:31:33.035: INFO: calico-node-c7rz4 from kube-system started at 2021-04-27 15:00:32 +0000 UTC (1 container statuses recorded)
May 21 20:31:33.035: INFO: 	Container calico-node ready: true, restart count 2
May 21 20:31:33.035: INFO: nirmata-cni-installer-5m9z5 from nirmata started at 2021-05-20 07:33:18 +0000 UTC (1 container statuses recorded)
May 21 20:31:33.035: INFO: 	Container install-cni ready: true, restart count 0
May 21 20:31:33.035: INFO: sonobuoy-e2e-job-03ad7d58f5964dbf from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:31:33.035: INFO: 	Container e2e ready: true, restart count 0
May 21 20:31:33.035: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 21 20:31:33.035: INFO: sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-lxlxb from sonobuoy started at 2021-05-21 18:56:21 +0000 UTC (2 container statuses recorded)
May 21 20:31:33.035: INFO: 	Container sonobuoy-worker ready: false, restart count 11
May 21 20:31:33.035: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node conformance1
STEP: verifying the node has the label node dc-hg-1
STEP: verifying the node has the label node dc-hg-2
May 21 20:31:33.188: INFO: Pod haproxy-ingress-85597cc495-l7rzc requesting resource cpu=10m on Node conformance1
May 21 20:31:33.188: INFO: Pod ingress-default-backend-5f65f5865b-nldzd requesting resource cpu=250m on Node conformance1
May 21 20:31:33.188: INFO: Pod calico-kube-controllers-db766f945-4228n requesting resource cpu=0m on Node conformance1
May 21 20:31:33.188: INFO: Pod calico-node-6r5n7 requesting resource cpu=250m on Node dc-hg-1
May 21 20:31:33.188: INFO: Pod calico-node-c7rz4 requesting resource cpu=250m on Node dc-hg-2
May 21 20:31:33.188: INFO: Pod calico-node-j5hq5 requesting resource cpu=250m on Node conformance1
May 21 20:31:33.188: INFO: Pod coredns-c6fc4f979-bv84n requesting resource cpu=100m on Node conformance1
May 21 20:31:33.188: INFO: Pod coredns-c6fc4f979-jfxfx requesting resource cpu=100m on Node conformance1
May 21 20:31:33.188: INFO: Pod metrics-server-6c6c6b5d47-9r52q requesting resource cpu=0m on Node conformance1
May 21 20:31:33.188: INFO: Pod nirmata-cni-installer-5m9z5 requesting resource cpu=250m on Node dc-hg-2
May 21 20:31:33.188: INFO: Pod nirmata-cni-installer-zjkpx requesting resource cpu=250m on Node conformance1
May 21 20:31:33.188: INFO: Pod nirmata-cni-installer-zm9dd requesting resource cpu=250m on Node dc-hg-1
May 21 20:31:33.188: INFO: Pod nirmata-kube-controller-5cf898966b-md6gc requesting resource cpu=250m on Node conformance1
May 21 20:31:33.188: INFO: Pod pod-projected-secrets-565e7ac9-70a5-4099-b331-bf7733f55ffd requesting resource cpu=0m on Node dc-hg-1
May 21 20:31:33.188: INFO: Pod sonobuoy requesting resource cpu=0m on Node dc-hg-1
May 21 20:31:33.188: INFO: Pod sonobuoy-e2e-job-03ad7d58f5964dbf requesting resource cpu=0m on Node dc-hg-2
May 21 20:31:33.188: INFO: Pod sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-47fv4 requesting resource cpu=0m on Node conformance1
May 21 20:31:33.188: INFO: Pod sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-lxlxb requesting resource cpu=0m on Node dc-hg-2
May 21 20:31:33.188: INFO: Pod sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-xch78 requesting resource cpu=0m on Node dc-hg-1
STEP: Starting Pods to consume most of the cluster CPU.
May 21 20:31:33.188: INFO: Creating a pod which consumes cpu=553m on Node conformance1
May 21 20:31:33.222: INFO: Creating a pod which consumes cpu=1050m on Node dc-hg-1
May 21 20:31:33.262: INFO: Creating a pod which consumes cpu=1050m on Node dc-hg-2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-199f6f68-b727-457d-bf13-da0b63fdc1aa.16812f118ef3376f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6065/filler-pod-199f6f68-b727-457d-bf13-da0b63fdc1aa to dc-hg-1]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-199f6f68-b727-457d-bf13-da0b63fdc1aa.16812f11d6d7c592], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "default-token-mxbql" : failed to sync secret cache: timed out waiting for the condition]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-199f6f68-b727-457d-bf13-da0b63fdc1aa.16812f126bee4ca7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-199f6f68-b727-457d-bf13-da0b63fdc1aa.16812f1275869a1f], Reason = [Created], Message = [Created container filler-pod-199f6f68-b727-457d-bf13-da0b63fdc1aa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-199f6f68-b727-457d-bf13-da0b63fdc1aa.16812f12957b41c5], Reason = [Started], Message = [Started container filler-pod-199f6f68-b727-457d-bf13-da0b63fdc1aa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35af195a-63ec-4960-9cfc-683b4adeb896.16812f118d8ba9b9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6065/filler-pod-35af195a-63ec-4960-9cfc-683b4adeb896 to conformance1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35af195a-63ec-4960-9cfc-683b4adeb896.16812f1267447cce], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35af195a-63ec-4960-9cfc-683b4adeb896.16812f126dcd1003], Reason = [Created], Message = [Created container filler-pod-35af195a-63ec-4960-9cfc-683b4adeb896]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35af195a-63ec-4960-9cfc-683b4adeb896.16812f128ab603c0], Reason = [Started], Message = [Started container filler-pod-35af195a-63ec-4960-9cfc-683b4adeb896]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7eb16e5c-8521-477f-8013-3a757b98e3a8.16812f1192944e2f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6065/filler-pod-7eb16e5c-8521-477f-8013-3a757b98e3a8 to dc-hg-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7eb16e5c-8521-477f-8013-3a757b98e3a8.16812f1247c00edd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7eb16e5c-8521-477f-8013-3a757b98e3a8.16812f124fcd800d], Reason = [Created], Message = [Created container filler-pod-7eb16e5c-8521-477f-8013-3a757b98e3a8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7eb16e5c-8521-477f-8013-3a757b98e3a8.16812f12657adda8], Reason = [Started], Message = [Started container filler-pod-7eb16e5c-8521-477f-8013-3a757b98e3a8]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16812f12fa7c7882], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node conformance1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node dc-hg-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node dc-hg-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:31:40.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6065" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:7.762 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":287,"skipped":4507,"failed":0}
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:31:40.575: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:31:44.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-19" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":288,"skipped":4507,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:31:45.106: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:31:45.332: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 21 20:31:45.365: INFO: Number of nodes with available pods: 0
May 21 20:31:45.365: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 21 20:31:45.423: INFO: Number of nodes with available pods: 0
May 21 20:31:45.423: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:46.447: INFO: Number of nodes with available pods: 0
May 21 20:31:46.448: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:47.435: INFO: Number of nodes with available pods: 0
May 21 20:31:47.435: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:48.433: INFO: Number of nodes with available pods: 0
May 21 20:31:48.433: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:49.429: INFO: Number of nodes with available pods: 0
May 21 20:31:49.429: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:50.437: INFO: Number of nodes with available pods: 1
May 21 20:31:50.437: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 21 20:31:50.513: INFO: Number of nodes with available pods: 1
May 21 20:31:50.513: INFO: Number of running nodes: 0, number of available pods: 1
May 21 20:31:51.518: INFO: Number of nodes with available pods: 0
May 21 20:31:51.518: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 21 20:31:51.541: INFO: Number of nodes with available pods: 0
May 21 20:31:51.541: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:52.549: INFO: Number of nodes with available pods: 0
May 21 20:31:52.549: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:53.550: INFO: Number of nodes with available pods: 0
May 21 20:31:53.550: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:54.553: INFO: Number of nodes with available pods: 0
May 21 20:31:54.553: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:55.548: INFO: Number of nodes with available pods: 0
May 21 20:31:55.548: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:56.549: INFO: Number of nodes with available pods: 0
May 21 20:31:56.549: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:57.549: INFO: Number of nodes with available pods: 0
May 21 20:31:57.549: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:58.549: INFO: Number of nodes with available pods: 0
May 21 20:31:58.549: INFO: Node dc-hg-1 is running more than one daemon pod
May 21 20:31:59.548: INFO: Number of nodes with available pods: 1
May 21 20:31:59.548: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-607, will wait for the garbage collector to delete the pods
May 21 20:31:59.629: INFO: Deleting DaemonSet.extensions daemon-set took: 13.430774ms
May 21 20:32:00.230: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.369244ms
May 21 20:32:03.036: INFO: Number of nodes with available pods: 0
May 21 20:32:03.036: INFO: Number of running nodes: 0, number of available pods: 0
May 21 20:32:03.045: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-607/daemonsets","resourceVersion":"1288820"},"items":null}

May 21 20:32:03.049: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-607/pods","resourceVersion":"1288820"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:32:03.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-607" for this suite.

• [SLOW TEST:18.154 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":289,"skipped":4510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:32:03.262: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7253
STEP: creating service affinity-nodeport-transition in namespace services-7253
STEP: creating replication controller affinity-nodeport-transition in namespace services-7253
I0521 20:32:03.427116      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-7253, replica count: 3
I0521 20:32:06.478012      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0521 20:32:09.478341      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 21 20:32:09.509: INFO: Creating new exec pod
May 21 20:32:14.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-7253 exec execpod-affinitybdm9c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 21 20:32:15.354: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 21 20:32:15.354: INFO: stdout: ""
May 21 20:32:15.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-7253 exec execpod-affinitybdm9c -- /bin/sh -x -c nc -zv -t -w 2 10.10.10.65 80'
May 21 20:32:16.071: INFO: stderr: "+ nc -zv -t -w 2 10.10.10.65 80\nConnection to 10.10.10.65 80 port [tcp/http] succeeded!\n"
May 21 20:32:16.072: INFO: stdout: ""
May 21 20:32:16.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-7253 exec execpod-affinitybdm9c -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.125 31032'
May 21 20:32:16.808: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.125 31032\nConnection to 10.10.1.125 31032 port [tcp/31032] succeeded!\n"
May 21 20:32:16.808: INFO: stdout: ""
May 21 20:32:16.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-7253 exec execpod-affinitybdm9c -- /bin/sh -x -c nc -zv -t -w 2 10.10.1.136 31032'
May 21 20:32:17.493: INFO: stderr: "+ nc -zv -t -w 2 10.10.1.136 31032\nConnection to 10.10.1.136 31032 port [tcp/31032] succeeded!\n"
May 21 20:32:17.493: INFO: stdout: ""
May 21 20:32:17.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-7253 exec execpod-affinitybdm9c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.86:31032/ ; done'
May 21 20:32:18.422: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n"
May 21 20:32:18.423: INFO: stdout: "\naffinity-nodeport-transition-68574\naffinity-nodeport-transition-z8zw9\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-z8zw9\naffinity-nodeport-transition-z8zw9\naffinity-nodeport-transition-68574\naffinity-nodeport-transition-z8zw9\naffinity-nodeport-transition-68574\naffinity-nodeport-transition-68574\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-68574\naffinity-nodeport-transition-z8zw9\naffinity-nodeport-transition-z8zw9\naffinity-nodeport-transition-68574\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-68574"
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-68574
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-z8zw9
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-z8zw9
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-z8zw9
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-68574
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-z8zw9
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-68574
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-68574
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-68574
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-z8zw9
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-z8zw9
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-68574
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:18.423: INFO: Received response from host: affinity-nodeport-transition-68574
May 21 20:32:18.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=services-7253 exec execpod-affinitybdm9c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.1.86:31032/ ; done'
May 21 20:32:19.379: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.1.86:31032/\n"
May 21 20:32:19.379: INFO: stdout: "\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d\naffinity-nodeport-transition-rsf5d"
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Received response from host: affinity-nodeport-transition-rsf5d
May 21 20:32:19.379: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7253, will wait for the garbage collector to delete the pods
May 21 20:32:19.477: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.554138ms
May 21 20:32:20.281: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 804.454045ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:32:27.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7253" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:24.633 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":290,"skipped":4569,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:32:27.901: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 21 20:32:27.981: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
May 21 20:32:36.873: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:33:10.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-584" for this suite.

• [SLOW TEST:43.047 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":291,"skipped":4582,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:33:10.946: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
May 21 20:33:11.607: INFO: created pod pod-service-account-defaultsa
May 21 20:33:11.607: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 21 20:33:11.625: INFO: created pod pod-service-account-mountsa
May 21 20:33:11.625: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 21 20:33:11.668: INFO: created pod pod-service-account-nomountsa
May 21 20:33:11.668: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 21 20:33:11.712: INFO: created pod pod-service-account-defaultsa-mountspec
May 21 20:33:11.712: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 21 20:33:11.746: INFO: created pod pod-service-account-mountsa-mountspec
May 21 20:33:11.746: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 21 20:33:11.783: INFO: created pod pod-service-account-nomountsa-mountspec
May 21 20:33:11.783: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 21 20:33:11.824: INFO: created pod pod-service-account-defaultsa-nomountspec
May 21 20:33:11.824: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 21 20:33:11.851: INFO: created pod pod-service-account-mountsa-nomountspec
May 21 20:33:11.851: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 21 20:33:11.894: INFO: created pod pod-service-account-nomountsa-nomountspec
May 21 20:33:11.894: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:33:11.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9487" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":292,"skipped":4583,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:33:11.956: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1068
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-1068
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1068
May 21 20:33:12.098: INFO: Found 0 stateful pods, waiting for 1
May 21 20:33:22.104: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 21 20:33:22.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 20:33:22.721: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 20:33:22.721: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 20:33:22.721: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 20:33:22.730: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 21 20:33:32.740: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 20:33:32.740: INFO: Waiting for statefulset status.replicas updated to 0
May 21 20:33:32.789: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 21 20:33:32.789: INFO: ss-0  dc-hg-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  }]
May 21 20:33:32.789: INFO: 
May 21 20:33:32.789: INFO: StatefulSet ss has not reached scale 3, at 1
May 21 20:33:33.800: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986052068s
May 21 20:33:34.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975031703s
May 21 20:33:35.868: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.945049471s
May 21 20:33:36.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.911602393s
May 21 20:33:37.887: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.902696291s
May 21 20:33:38.895: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.893061926s
May 21 20:33:39.901: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.885143255s
May 21 20:33:40.909: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.878726269s
May 21 20:33:41.919: INFO: Verifying statefulset ss doesn't scale past 3 for another 870.983764ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1068
May 21 20:33:42.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:33:43.521: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 20:33:43.521: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 20:33:43.521: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 20:33:43.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:33:44.132: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 21 20:33:44.133: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 20:33:44.133: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 20:33:44.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:33:44.662: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 21 20:33:44.662: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 20:33:44.662: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 20:33:44.670: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 20:33:44.670: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 20:33:44.670: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 21 20:33:44.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 20:33:45.273: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 20:33:45.273: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 20:33:45.273: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 20:33:45.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 20:33:45.894: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 20:33:45.894: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 20:33:45.894: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 20:33:45.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 20:33:46.867: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 20:33:46.867: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 20:33:46.867: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 20:33:46.867: INFO: Waiting for statefulset status.replicas updated to 0
May 21 20:33:46.873: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 21 20:33:56.895: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 20:33:56.895: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 21 20:33:56.895: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 21 20:33:56.939: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 21 20:33:56.939: INFO: ss-0  dc-hg-1       Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  }]
May 21 20:33:56.939: INFO: ss-1  dc-hg-2       Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:33:56.939: INFO: ss-2  conformance1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:33:56.940: INFO: 
May 21 20:33:56.940: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 20:33:57.949: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 21 20:33:57.950: INFO: ss-0  dc-hg-1       Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  }]
May 21 20:33:57.950: INFO: ss-1  dc-hg-2       Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:33:57.950: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:33:57.950: INFO: 
May 21 20:33:57.950: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 20:33:58.967: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 21 20:33:58.967: INFO: ss-0  dc-hg-1       Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  }]
May 21 20:33:58.967: INFO: ss-1  dc-hg-2       Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:33:58.967: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:33:58.968: INFO: 
May 21 20:33:58.968: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 20:33:59.978: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 21 20:33:59.978: INFO: ss-0  dc-hg-1       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  }]
May 21 20:33:59.978: INFO: ss-1  dc-hg-2       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:33:59.978: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:33:59.978: INFO: 
May 21 20:33:59.978: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 20:34:00.985: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 21 20:34:00.985: INFO: ss-0  dc-hg-1       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  }]
May 21 20:34:00.985: INFO: ss-1  dc-hg-2       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:00.985: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:00.985: INFO: 
May 21 20:34:00.985: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 20:34:01.992: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 21 20:34:01.992: INFO: ss-0  dc-hg-1       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  }]
May 21 20:34:01.992: INFO: ss-1  dc-hg-2       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:01.992: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:01.992: INFO: 
May 21 20:34:01.992: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 20:34:03.000: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 21 20:34:03.000: INFO: ss-0  dc-hg-1       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  }]
May 21 20:34:03.000: INFO: ss-1  dc-hg-2       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:03.000: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:03.000: INFO: 
May 21 20:34:03.000: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 20:34:04.008: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 21 20:34:04.008: INFO: ss-0  dc-hg-1       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:12 +0000 UTC  }]
May 21 20:34:04.008: INFO: ss-1  dc-hg-2       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:04.008: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:04.008: INFO: 
May 21 20:34:04.008: INFO: StatefulSet ss has not reached scale 0, at 3
May 21 20:34:05.020: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 21 20:34:05.020: INFO: ss-1  dc-hg-2       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:05.020: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:05.020: INFO: 
May 21 20:34:05.020: INFO: StatefulSet ss has not reached scale 0, at 2
May 21 20:34:06.029: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 21 20:34:06.029: INFO: ss-1  dc-hg-2       Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:06.029: INFO: ss-2  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-21 20:33:32 +0000 UTC  }]
May 21 20:34:06.029: INFO: 
May 21 20:34:06.029: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1068
May 21 20:34:07.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:34:07.502: INFO: rc: 1
May 21 20:34:07.502: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 21 20:34:17.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:34:17.812: INFO: rc: 1
May 21 20:34:17.812: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:34:27.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:34:28.082: INFO: rc: 1
May 21 20:34:28.082: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:34:38.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:34:38.362: INFO: rc: 1
May 21 20:34:38.362: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:34:48.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:34:48.689: INFO: rc: 1
May 21 20:34:48.690: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:34:58.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:34:58.996: INFO: rc: 1
May 21 20:34:58.996: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:35:08.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:35:09.268: INFO: rc: 1
May 21 20:35:09.268: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:35:19.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:35:19.580: INFO: rc: 1
May 21 20:35:19.581: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:35:29.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:35:29.914: INFO: rc: 1
May 21 20:35:29.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:35:39.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:35:40.187: INFO: rc: 1
May 21 20:35:40.187: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:35:50.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:35:50.490: INFO: rc: 1
May 21 20:35:50.490: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:36:00.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:36:00.803: INFO: rc: 1
May 21 20:36:00.804: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:36:10.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:36:11.081: INFO: rc: 1
May 21 20:36:11.081: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:36:21.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:36:21.359: INFO: rc: 1
May 21 20:36:21.359: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:36:31.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:36:31.640: INFO: rc: 1
May 21 20:36:31.640: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:36:41.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:36:41.937: INFO: rc: 1
May 21 20:36:41.937: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:36:51.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:36:52.324: INFO: rc: 1
May 21 20:36:52.325: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:37:02.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:37:02.609: INFO: rc: 1
May 21 20:37:02.609: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:37:12.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:37:12.876: INFO: rc: 1
May 21 20:37:12.876: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:37:22.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:37:23.197: INFO: rc: 1
May 21 20:37:23.197: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:37:33.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:37:33.526: INFO: rc: 1
May 21 20:37:33.526: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:37:43.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:37:43.875: INFO: rc: 1
May 21 20:37:43.876: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:37:53.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:37:54.207: INFO: rc: 1
May 21 20:37:54.207: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:38:04.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:38:04.546: INFO: rc: 1
May 21 20:38:04.546: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:38:14.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:38:14.917: INFO: rc: 1
May 21 20:38:14.917: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:38:24.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:38:25.270: INFO: rc: 1
May 21 20:38:25.270: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:38:35.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:38:35.596: INFO: rc: 1
May 21 20:38:35.596: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:38:45.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:38:45.912: INFO: rc: 1
May 21 20:38:45.912: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:38:55.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:38:56.368: INFO: rc: 1
May 21 20:38:56.368: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:39:06.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:39:06.704: INFO: rc: 1
May 21 20:39:06.704: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May 21 20:39:16.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:39:17.133: INFO: rc: 1
May 21 20:39:17.133: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
May 21 20:39:17.133: INFO: Scaling statefulset ss to 0
May 21 20:39:17.156: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 20:39:17.164: INFO: Deleting all statefulset in ns statefulset-1068
May 21 20:39:17.169: INFO: Scaling statefulset ss to 0
May 21 20:39:17.193: INFO: Waiting for statefulset status.replicas updated to 0
May 21 20:39:17.202: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:39:17.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1068" for this suite.

• [SLOW TEST:365.340 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":293,"skipped":4590,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:39:17.307: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 21 20:39:17.380: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:39:25.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5121" for this suite.

• [SLOW TEST:8.070 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":294,"skipped":4602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:39:25.380: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 21 20:39:25.474: INFO: Waiting up to 5m0s for pod "downward-api-3e9b109a-4259-475c-85ec-4477e056c368" in namespace "downward-api-9894" to be "Succeeded or Failed"
May 21 20:39:25.486: INFO: Pod "downward-api-3e9b109a-4259-475c-85ec-4477e056c368": Phase="Pending", Reason="", readiness=false. Elapsed: 11.779504ms
May 21 20:39:27.494: INFO: Pod "downward-api-3e9b109a-4259-475c-85ec-4477e056c368": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020112196s
May 21 20:39:29.500: INFO: Pod "downward-api-3e9b109a-4259-475c-85ec-4477e056c368": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026077023s
STEP: Saw pod success
May 21 20:39:29.500: INFO: Pod "downward-api-3e9b109a-4259-475c-85ec-4477e056c368" satisfied condition "Succeeded or Failed"
May 21 20:39:29.506: INFO: Trying to get logs from node dc-hg-2 pod downward-api-3e9b109a-4259-475c-85ec-4477e056c368 container dapi-container: <nil>
STEP: delete the pod
May 21 20:39:29.585: INFO: Waiting for pod downward-api-3e9b109a-4259-475c-85ec-4477e056c368 to disappear
May 21 20:39:29.604: INFO: Pod downward-api-3e9b109a-4259-475c-85ec-4477e056c368 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:39:29.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9894" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":295,"skipped":4635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:39:29.627: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 20:39:29.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-449241dd-2c29-431c-a636-31dd0cd26167" in namespace "projected-9856" to be "Succeeded or Failed"
May 21 20:39:29.839: INFO: Pod "downwardapi-volume-449241dd-2c29-431c-a636-31dd0cd26167": Phase="Pending", Reason="", readiness=false. Elapsed: 5.573064ms
May 21 20:39:31.846: INFO: Pod "downwardapi-volume-449241dd-2c29-431c-a636-31dd0cd26167": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011698553s
May 21 20:39:33.852: INFO: Pod "downwardapi-volume-449241dd-2c29-431c-a636-31dd0cd26167": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018119608s
STEP: Saw pod success
May 21 20:39:33.852: INFO: Pod "downwardapi-volume-449241dd-2c29-431c-a636-31dd0cd26167" satisfied condition "Succeeded or Failed"
May 21 20:39:33.856: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-449241dd-2c29-431c-a636-31dd0cd26167 container client-container: <nil>
STEP: delete the pod
May 21 20:39:33.934: INFO: Waiting for pod downwardapi-volume-449241dd-2c29-431c-a636-31dd0cd26167 to disappear
May 21 20:39:33.949: INFO: Pod downwardapi-volume-449241dd-2c29-431c-a636-31dd0cd26167 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:39:33.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9856" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":296,"skipped":4724,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:39:33.963: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-90c7c694-fc3f-4607-99ea-6e6cfec32516
STEP: Creating a pod to test consume secrets
May 21 20:39:34.063: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-96dba7a8-51e3-4be3-9a48-2fd37ad883de" in namespace "projected-3248" to be "Succeeded or Failed"
May 21 20:39:34.069: INFO: Pod "pod-projected-secrets-96dba7a8-51e3-4be3-9a48-2fd37ad883de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.552739ms
May 21 20:39:36.074: INFO: Pod "pod-projected-secrets-96dba7a8-51e3-4be3-9a48-2fd37ad883de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010973552s
May 21 20:39:38.080: INFO: Pod "pod-projected-secrets-96dba7a8-51e3-4be3-9a48-2fd37ad883de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017441436s
STEP: Saw pod success
May 21 20:39:38.081: INFO: Pod "pod-projected-secrets-96dba7a8-51e3-4be3-9a48-2fd37ad883de" satisfied condition "Succeeded or Failed"
May 21 20:39:38.089: INFO: Trying to get logs from node dc-hg-1 pod pod-projected-secrets-96dba7a8-51e3-4be3-9a48-2fd37ad883de container projected-secret-volume-test: <nil>
STEP: delete the pod
May 21 20:39:38.168: INFO: Waiting for pod pod-projected-secrets-96dba7a8-51e3-4be3-9a48-2fd37ad883de to disappear
May 21 20:39:38.176: INFO: Pod pod-projected-secrets-96dba7a8-51e3-4be3-9a48-2fd37ad883de no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:39:38.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3248" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":297,"skipped":4735,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:39:38.193: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:39:38.255: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 21 20:39:47.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-5410 --namespace=crd-publish-openapi-5410 create -f -'
May 21 20:39:48.761: INFO: stderr: ""
May 21 20:39:48.762: INFO: stdout: "e2e-test-crd-publish-openapi-6804-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 21 20:39:48.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-5410 --namespace=crd-publish-openapi-5410 delete e2e-test-crd-publish-openapi-6804-crds test-cr'
May 21 20:39:49.060: INFO: stderr: ""
May 21 20:39:49.060: INFO: stdout: "e2e-test-crd-publish-openapi-6804-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 21 20:39:49.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-5410 --namespace=crd-publish-openapi-5410 apply -f -'
May 21 20:39:49.776: INFO: stderr: ""
May 21 20:39:49.777: INFO: stdout: "e2e-test-crd-publish-openapi-6804-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 21 20:39:49.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-5410 --namespace=crd-publish-openapi-5410 delete e2e-test-crd-publish-openapi-6804-crds test-cr'
May 21 20:39:50.061: INFO: stderr: ""
May 21 20:39:50.061: INFO: stdout: "e2e-test-crd-publish-openapi-6804-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 21 20:39:50.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=crd-publish-openapi-5410 explain e2e-test-crd-publish-openapi-6804-crds'
May 21 20:39:50.780: INFO: stderr: ""
May 21 20:39:50.780: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6804-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:39:59.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5410" for this suite.

• [SLOW TEST:21.234 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":298,"skipped":4764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:39:59.428: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 21 20:39:59.533: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82d835cc-6097-4973-bbcf-1387b19b7cb1" in namespace "downward-api-4846" to be "Succeeded or Failed"
May 21 20:39:59.549: INFO: Pod "downwardapi-volume-82d835cc-6097-4973-bbcf-1387b19b7cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.525751ms
May 21 20:40:01.561: INFO: Pod "downwardapi-volume-82d835cc-6097-4973-bbcf-1387b19b7cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028111907s
May 21 20:40:03.575: INFO: Pod "downwardapi-volume-82d835cc-6097-4973-bbcf-1387b19b7cb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042422378s
STEP: Saw pod success
May 21 20:40:03.575: INFO: Pod "downwardapi-volume-82d835cc-6097-4973-bbcf-1387b19b7cb1" satisfied condition "Succeeded or Failed"
May 21 20:40:03.580: INFO: Trying to get logs from node dc-hg-1 pod downwardapi-volume-82d835cc-6097-4973-bbcf-1387b19b7cb1 container client-container: <nil>
STEP: delete the pod
May 21 20:40:03.651: INFO: Waiting for pod downwardapi-volume-82d835cc-6097-4973-bbcf-1387b19b7cb1 to disappear
May 21 20:40:03.664: INFO: Pod downwardapi-volume-82d835cc-6097-4973-bbcf-1387b19b7cb1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:40:03.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4846" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":299,"skipped":4798,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:40:03.712: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-wn7b
STEP: Creating a pod to test atomic-volume-subpath
May 21 20:40:03.852: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wn7b" in namespace "subpath-9057" to be "Succeeded or Failed"
May 21 20:40:03.858: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052122ms
May 21 20:40:05.864: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012403795s
May 21 20:40:07.872: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Running", Reason="", readiness=true. Elapsed: 4.019738062s
May 21 20:40:09.878: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Running", Reason="", readiness=true. Elapsed: 6.025872619s
May 21 20:40:11.883: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Running", Reason="", readiness=true. Elapsed: 8.031344976s
May 21 20:40:13.890: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Running", Reason="", readiness=true. Elapsed: 10.037860629s
May 21 20:40:15.899: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Running", Reason="", readiness=true. Elapsed: 12.046737937s
May 21 20:40:17.905: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Running", Reason="", readiness=true. Elapsed: 14.052768216s
May 21 20:40:19.912: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Running", Reason="", readiness=true. Elapsed: 16.059652396s
May 21 20:40:21.920: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Running", Reason="", readiness=true. Elapsed: 18.067851054s
May 21 20:40:23.925: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Running", Reason="", readiness=true. Elapsed: 20.072972334s
May 21 20:40:25.935: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Running", Reason="", readiness=true. Elapsed: 22.082968608s
May 21 20:40:27.941: INFO: Pod "pod-subpath-test-secret-wn7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.088697026s
STEP: Saw pod success
May 21 20:40:27.941: INFO: Pod "pod-subpath-test-secret-wn7b" satisfied condition "Succeeded or Failed"
May 21 20:40:27.946: INFO: Trying to get logs from node dc-hg-1 pod pod-subpath-test-secret-wn7b container test-container-subpath-secret-wn7b: <nil>
STEP: delete the pod
May 21 20:40:27.995: INFO: Waiting for pod pod-subpath-test-secret-wn7b to disappear
May 21 20:40:27.999: INFO: Pod pod-subpath-test-secret-wn7b no longer exists
STEP: Deleting pod pod-subpath-test-secret-wn7b
May 21 20:40:27.999: INFO: Deleting pod "pod-subpath-test-secret-wn7b" in namespace "subpath-9057"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:40:28.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9057" for this suite.

• [SLOW TEST:24.312 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":300,"skipped":4803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:40:28.043: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 21 20:40:28.134: INFO: Waiting up to 1m0s for all nodes to be ready
May 21 20:41:28.189: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:41:28.196: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 21 20:41:32.349: INFO: found a healthy node: dc-hg-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:41:48.552: INFO: pods created so far: [1 1 1]
May 21 20:41:48.553: INFO: length of pods created so far: 3
May 21 20:42:00.570: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:42:07.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-354" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:42:07.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3820" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:99.791 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":301,"skipped":4897,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:42:07.835: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1635
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1635
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1635
May 21 20:42:07.979: INFO: Found 0 stateful pods, waiting for 1
May 21 20:42:17.986: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 21 20:42:17.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 20:42:18.609: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 20:42:18.610: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 20:42:18.610: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 20:42:18.618: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 21 20:42:28.624: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 20:42:28.624: INFO: Waiting for statefulset status.replicas updated to 0
May 21 20:42:28.690: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998847s
May 21 20:42:29.697: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.985875438s
May 21 20:42:30.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.979153096s
May 21 20:42:31.710: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973721382s
May 21 20:42:32.716: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.96650139s
May 21 20:42:33.722: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.960190004s
May 21 20:42:34.729: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.954485259s
May 21 20:42:35.736: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.947051829s
May 21 20:42:36.742: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.940370535s
May 21 20:42:37.749: INFO: Verifying statefulset ss doesn't scale past 1 for another 934.594869ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1635
May 21 20:42:38.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:42:39.304: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 20:42:39.304: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 20:42:39.304: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 20:42:39.311: INFO: Found 1 stateful pods, waiting for 3
May 21 20:42:49.319: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 21 20:42:49.319: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 21 20:42:49.319: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 21 20:42:49.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 20:42:49.870: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 20:42:49.870: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 20:42:49.870: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 20:42:49.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 20:42:50.532: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 20:42:50.532: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 20:42:50.532: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 20:42:50.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 21 20:42:51.030: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 21 20:42:51.030: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 21 20:42:51.030: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 21 20:42:51.030: INFO: Waiting for statefulset status.replicas updated to 0
May 21 20:42:51.035: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 21 20:43:01.047: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 21 20:43:01.047: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 21 20:43:01.047: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 21 20:43:01.072: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999274s
May 21 20:43:02.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987015645s
May 21 20:43:03.092: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980149206s
May 21 20:43:04.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.967192842s
May 21 20:43:05.109: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.957726242s
May 21 20:43:06.126: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.94696513s
May 21 20:43:07.140: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.9335327s
May 21 20:43:08.147: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.919346747s
May 21 20:43:09.154: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.911725058s
May 21 20:43:10.162: INFO: Verifying statefulset ss doesn't scale past 3 for another 905.133169ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1635
May 21 20:43:11.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:43:11.693: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 20:43:11.694: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 20:43:11.694: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 20:43:11.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:43:12.269: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 21 20:43:12.269: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 21 20:43:12.269: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 21 20:43:12.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:43:12.899: INFO: rc: 1
May 21 20:43:12.899: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
May 21 20:43:22.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:43:23.199: INFO: rc: 1
May 21 20:43:23.199: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:43:33.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:43:33.490: INFO: rc: 1
May 21 20:43:33.490: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:43:43.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:43:43.775: INFO: rc: 1
May 21 20:43:43.775: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:43:53.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:43:54.064: INFO: rc: 1
May 21 20:43:54.065: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:44:04.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:44:04.345: INFO: rc: 1
May 21 20:44:04.345: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:44:14.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:44:14.633: INFO: rc: 1
May 21 20:44:14.634: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:44:24.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:44:24.923: INFO: rc: 1
May 21 20:44:24.923: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:44:34.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:44:35.189: INFO: rc: 1
May 21 20:44:35.189: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:44:45.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:44:45.474: INFO: rc: 1
May 21 20:44:45.474: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:44:55.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:44:55.791: INFO: rc: 1
May 21 20:44:55.791: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:45:05.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:45:06.443: INFO: rc: 1
May 21 20:45:06.444: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:45:16.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:45:16.805: INFO: rc: 1
May 21 20:45:16.805: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:45:26.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:45:27.209: INFO: rc: 1
May 21 20:45:27.209: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:45:37.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:45:37.503: INFO: rc: 1
May 21 20:45:37.503: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:45:47.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:45:47.805: INFO: rc: 1
May 21 20:45:47.806: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:45:57.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:45:58.086: INFO: rc: 1
May 21 20:45:58.086: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:46:08.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:46:08.355: INFO: rc: 1
May 21 20:46:08.355: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:46:18.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:46:18.630: INFO: rc: 1
May 21 20:46:18.631: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:46:28.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:46:28.976: INFO: rc: 1
May 21 20:46:28.976: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:46:38.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:46:39.278: INFO: rc: 1
May 21 20:46:39.279: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:46:49.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:46:49.555: INFO: rc: 1
May 21 20:46:49.555: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:46:59.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:46:59.832: INFO: rc: 1
May 21 20:46:59.832: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:47:09.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:47:10.094: INFO: rc: 1
May 21 20:47:10.094: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:47:20.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:47:20.384: INFO: rc: 1
May 21 20:47:20.384: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:47:30.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:47:30.665: INFO: rc: 1
May 21 20:47:30.665: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:47:40.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:47:40.936: INFO: rc: 1
May 21 20:47:40.936: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:47:50.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:47:51.230: INFO: rc: 1
May 21 20:47:51.230: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:48:01.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:48:01.579: INFO: rc: 1
May 21 20:48:01.579: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:48:11.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:48:11.875: INFO: rc: 1
May 21 20:48:11.875: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 21 20:48:21.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=statefulset-1635 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 21 20:48:22.145: INFO: rc: 1
May 21 20:48:22.145: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
May 21 20:48:22.145: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 21 20:48:22.165: INFO: Deleting all statefulset in ns statefulset-1635
May 21 20:48:22.176: INFO: Scaling statefulset ss to 0
May 21 20:48:22.196: INFO: Waiting for statefulset status.replicas updated to 0
May 21 20:48:22.200: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:48:22.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1635" for this suite.

• [SLOW TEST:374.434 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":302,"skipped":4914,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:48:22.270: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:48:22.375: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-d441c412-bdcc-455d-af8f-3a30028820d8" in namespace "security-context-test-1939" to be "Succeeded or Failed"
May 21 20:48:22.387: INFO: Pod "busybox-readonly-false-d441c412-bdcc-455d-af8f-3a30028820d8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.570391ms
May 21 20:48:24.393: INFO: Pod "busybox-readonly-false-d441c412-bdcc-455d-af8f-3a30028820d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018116883s
May 21 20:48:26.401: INFO: Pod "busybox-readonly-false-d441c412-bdcc-455d-af8f-3a30028820d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025886151s
May 21 20:48:28.411: INFO: Pod "busybox-readonly-false-d441c412-bdcc-455d-af8f-3a30028820d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035530287s
May 21 20:48:28.411: INFO: Pod "busybox-readonly-false-d441c412-bdcc-455d-af8f-3a30028820d8" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:48:28.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1939" for this suite.

• [SLOW TEST:6.170 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with readOnlyRootFilesystem
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:166
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":303,"skipped":4924,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:48:28.441: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-397.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-397.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-397.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-397.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-397.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-397.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-397.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 27.239.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.239.27_udp@PTR;check="$$(dig +tcp +noall +answer +search 27.239.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.239.27_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-397.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-397.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-397.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-397.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-397.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-397.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-397.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 27.239.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.239.27_udp@PTR;check="$$(dig +tcp +noall +answer +search 27.239.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.239.27_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 21 20:48:32.725: INFO: Unable to read wheezy_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:32.735: INFO: Unable to read wheezy_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:32.746: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:32.760: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:32.815: INFO: Unable to read jessie_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:32.823: INFO: Unable to read jessie_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:32.830: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:32.836: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:32.874: INFO: Lookups using dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c failed for: [wheezy_udp@dns-test-service.dns-397.svc.cluster.local wheezy_tcp@dns-test-service.dns-397.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_udp@dns-test-service.dns-397.svc.cluster.local jessie_tcp@dns-test-service.dns-397.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local]

May 21 20:48:37.883: INFO: Unable to read wheezy_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:37.890: INFO: Unable to read wheezy_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:37.895: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:37.901: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:37.955: INFO: Unable to read jessie_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:37.964: INFO: Unable to read jessie_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:37.971: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:37.979: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:38.023: INFO: Lookups using dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c failed for: [wheezy_udp@dns-test-service.dns-397.svc.cluster.local wheezy_tcp@dns-test-service.dns-397.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_udp@dns-test-service.dns-397.svc.cluster.local jessie_tcp@dns-test-service.dns-397.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local]

May 21 20:48:42.886: INFO: Unable to read wheezy_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:42.895: INFO: Unable to read wheezy_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:42.903: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:42.911: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:42.970: INFO: Unable to read jessie_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:42.979: INFO: Unable to read jessie_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:43.024: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:43.036: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:43.157: INFO: Lookups using dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c failed for: [wheezy_udp@dns-test-service.dns-397.svc.cluster.local wheezy_tcp@dns-test-service.dns-397.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_udp@dns-test-service.dns-397.svc.cluster.local jessie_tcp@dns-test-service.dns-397.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local]

May 21 20:48:47.881: INFO: Unable to read wheezy_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:47.889: INFO: Unable to read wheezy_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:47.896: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:47.903: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:47.956: INFO: Unable to read jessie_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:47.962: INFO: Unable to read jessie_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:47.970: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:47.978: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:48.027: INFO: Lookups using dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c failed for: [wheezy_udp@dns-test-service.dns-397.svc.cluster.local wheezy_tcp@dns-test-service.dns-397.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_udp@dns-test-service.dns-397.svc.cluster.local jessie_tcp@dns-test-service.dns-397.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local]

May 21 20:48:52.881: INFO: Unable to read wheezy_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:52.888: INFO: Unable to read wheezy_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:52.895: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:52.904: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:52.977: INFO: Unable to read jessie_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:52.986: INFO: Unable to read jessie_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:52.995: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:53.011: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:53.131: INFO: Lookups using dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c failed for: [wheezy_udp@dns-test-service.dns-397.svc.cluster.local wheezy_tcp@dns-test-service.dns-397.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_udp@dns-test-service.dns-397.svc.cluster.local jessie_tcp@dns-test-service.dns-397.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local]

May 21 20:48:57.882: INFO: Unable to read wheezy_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:57.889: INFO: Unable to read wheezy_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:57.897: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:57.904: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:57.952: INFO: Unable to read jessie_udp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:57.959: INFO: Unable to read jessie_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:57.966: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:57.972: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:48:58.018: INFO: Lookups using dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c failed for: [wheezy_udp@dns-test-service.dns-397.svc.cluster.local wheezy_tcp@dns-test-service.dns-397.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_udp@dns-test-service.dns-397.svc.cluster.local jessie_tcp@dns-test-service.dns-397.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local]

May 21 20:49:02.999: INFO: Unable to read jessie_tcp@dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:49:03.007: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:49:03.018: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local from pod dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c: the server could not find the requested resource (get pods dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c)
May 21 20:49:03.132: INFO: Lookups using dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c failed for: [jessie_tcp@dns-test-service.dns-397.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-397.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-397.svc.cluster.local]

May 21 20:49:08.053: INFO: DNS probes using dns-397/dns-test-20cb5a51-74f0-48b8-b9b3-824a086c718c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:49:08.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-397" for this suite.

• [SLOW TEST:39.913 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":304,"skipped":4928,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 21 20:49:08.355: INFO: >>> kubeConfig: /tmp/kubeconfig-321270312
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 21 20:49:08.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-8674 create -f -'
May 21 20:49:09.371: INFO: stderr: ""
May 21 20:49:09.371: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 21 20:49:09.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-8674 create -f -'
May 21 20:49:10.122: INFO: stderr: ""
May 21 20:49:10.123: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 21 20:49:11.147: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 20:49:11.147: INFO: Found 0 / 1
May 21 20:49:12.158: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 20:49:12.158: INFO: Found 0 / 1
May 21 20:49:13.131: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 20:49:13.131: INFO: Found 0 / 1
May 21 20:49:14.142: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 20:49:14.142: INFO: Found 1 / 1
May 21 20:49:14.142: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 21 20:49:14.170: INFO: Selector matched 1 pods for map[app:agnhost]
May 21 20:49:14.170: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 21 20:49:14.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-8674 describe pod agnhost-primary-tsq7f'
May 21 20:49:14.508: INFO: stderr: ""
May 21 20:49:14.508: INFO: stdout: "Name:         agnhost-primary-tsq7f\nNamespace:    kubectl-8674\nPriority:     0\nNode:         dc-hg-1/10.10.1.125\nStart Time:   Fri, 21 May 2021 20:49:09 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 192.168.161.175/32\n              cni.projectcalico.org/podIPs: 192.168.161.175/32\nStatus:       Running\nIP:           192.168.161.175\nIPs:\n  IP:           192.168.161.175\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://c65e0fff954506db4af47d81e8c2934e184b519fe8283d11e977fe0432673f34\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 21 May 2021 20:49:13 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f7hqc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-f7hqc:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-f7hqc\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason       Age   From               Message\n  ----     ------       ----  ----               -------\n  Normal   Scheduled    5s    default-scheduler  Successfully assigned kubectl-8674/agnhost-primary-tsq7f to dc-hg-1\n  Warning  FailedMount  4s    kubelet            MountVolume.SetUp failed for volume \"default-token-f7hqc\" : failed to sync secret cache: timed out waiting for the condition\n  Normal   Pulled       2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal   Created      2s    kubelet            Created container agnhost-primary\n  Normal   Started      1s    kubelet            Started container agnhost-primary\n"
May 21 20:49:14.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-8674 describe rc agnhost-primary'
May 21 20:49:14.873: INFO: stderr: ""
May 21 20:49:14.873: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8674\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-tsq7f\n"
May 21 20:49:14.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-8674 describe service agnhost-primary'
May 21 20:49:15.186: INFO: stderr: ""
May 21 20:49:15.186: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8674\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.10.132.142\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.161.175:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 21 20:49:15.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-8674 describe node conformance1'
May 21 20:49:15.583: INFO: stderr: ""
May 21 20:49:15.584: INFO: stdout: "Name:               conformance1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=conformance1\n                    kubernetes.io/os=linux\n                    nirmata.io/cluster.name=v1196-conformance\n                    nirmata.io/cluster.role=control-plane\n                    nirmata.io/node.name=conformance1\n                    node-role.kubernetes.io/master=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.10.1.86/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.209.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 23 Apr 2021 21:18:16 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  conformance1\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 21 May 2021 20:49:05 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 30 Apr 2021 16:10:26 +0000   Fri, 30 Apr 2021 16:10:26 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 21 May 2021 20:46:32 +0000   Fri, 23 Apr 2021 21:18:08 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 21 May 2021 20:46:32 +0000   Fri, 23 Apr 2021 21:18:08 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 21 May 2021 20:46:32 +0000   Fri, 23 Apr 2021 21:18:08 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 21 May 2021 20:46:32 +0000   Fri, 23 Apr 2021 21:22:29 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.1.86\n  Hostname:    conformance1\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    31166436Ki\n  example.com/fakecpu:  1k\n  hugepages-2Mi:        0\n  memory:               8149868Ki\n  pods:                 110\nAllocatable:\n  cpu:                  2\n  ephemeral-storage:    28722987371\n  example.com/fakecpu:  1k\n  hugepages-2Mi:        0\n  memory:               8047468Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 cf9039846e817bf110c3933d5c3e0c56\n  System UUID:                6965D524-A02C-12ED-9FCA-91C7F0430F4E\n  Boot ID:                    3ad67c41-c4a5-4f7d-8885-6253d4f348b2\n  Kernel Version:             4.4.0-131-generic\n  OS Image:                   Debian GNU/Linux 9 (stretch)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.6\n  Kubelet Version:            v1.19.6\n  Kube-Proxy Version:         v1.19.6\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-haproxy             haproxy-ingress-85597cc495-l7rzc                           10m (0%)      0 (0%)      20Mi (0%)        100Mi (1%)     25d\n  ingress-haproxy             ingress-default-backend-5f65f5865b-nldzd                   250m (12%)    0 (0%)      200Mi (2%)       250Mi (3%)     25d\n  kube-system                 calico-kube-controllers-db766f945-4228n                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         25d\n  kube-system                 calico-node-j5hq5                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         27d\n  kube-system                 coredns-c6fc4f979-bv84n                                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     25d\n  kube-system                 coredns-c6fc4f979-jfxfx                                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     25d\n  kube-system                 metrics-server-6c6c6b5d47-9r52q                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         25d\n  nirmata                     nirmata-cni-installer-zjkpx                                250m (12%)    0 (0%)      100Mi (1%)       200Mi (2%)     25d\n  nirmata                     nirmata-kube-controller-5cf898966b-md6gc                   250m (12%)    0 (0%)      200Mi (2%)       250Mi (3%)     25d\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-6efb9371790741ba-fzpvv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests     Limits\n  --------             --------     ------\n  cpu                  1210m (60%)  0 (0%)\n  memory               660Mi (8%)   1140Mi (14%)\n  ephemeral-storage    0 (0%)       0 (0%)\n  hugepages-2Mi        0 (0%)       0 (0%)\n  example.com/fakecpu  0            0\nEvents:                <none>\n"
May 21 20:49:15.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-321270312 --namespace=kubectl-8674 describe namespace kubectl-8674'
May 21 20:49:15.941: INFO: stderr: ""
May 21 20:49:15.941: INFO: stdout: "Name:         kubectl-8674\nLabels:       e2e-framework=kubectl\n              e2e-run=b9feb580-a75b-4ebc-bcf2-fe1f1cbe618d\nAnnotations:  nirmata.io: {\"environment.modelId\":\"0a7770d7-49fd-467c-b7cf-5ab3aad3f7b2\",\"environment.name\":\"kubectl-8674-v1196-conformance\"}\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 21 20:49:15.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8674" for this suite.

• [SLOW TEST:7.613 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":305,"skipped":4931,"failed":0}
SSMay 21 20:49:15.968: INFO: Running AfterSuite actions on all nodes
May 21 20:49:15.968: INFO: Running AfterSuite actions on node 1
May 21 20:49:15.968: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4933,"failed":0}

Ran 305 of 5238 Specs in 6766.984 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4933 Skipped
PASS

Ginkgo ran 1 suite in 1h52m52.154465175s
Test Suite Passed
