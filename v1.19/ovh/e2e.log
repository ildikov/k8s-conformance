I1105 17:25:33.010059      20 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-230475851
I1105 17:25:33.010186      20 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1105 17:25:33.010402      20 e2e.go:129] Starting e2e run "d4797c05-1fba-4ca2-bb90-f0b59d3c3acd" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1604597131 - Will randomize all specs
Will run 305 of 5232 specs

Nov  5 17:25:33.038: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 17:25:33.041: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E1105 17:25:33.041755      20 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
Nov  5 17:25:33.093: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov  5 17:25:33.169: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov  5 17:25:33.169: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Nov  5 17:25:33.169: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov  5 17:25:33.189: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Nov  5 17:25:33.189: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Nov  5 17:25:33.189: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'wormhole' (0 seconds elapsed)
Nov  5 17:25:33.189: INFO: e2e test version: v1.19.2
Nov  5 17:25:33.194: INFO: kube-apiserver version: v1.19.2
Nov  5 17:25:33.194: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 17:25:33.210: INFO: Cluster IP family: ipv4
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:25:33.214: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
Nov  5 17:25:33.323: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Nov  5 17:25:33.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f -'
Nov  5 17:25:34.744: INFO: stderr: ""
Nov  5 17:25:34.744: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Nov  5 17:25:34.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 diff -f -'
Nov  5 17:25:35.228: INFO: rc: 1
Nov  5 17:25:35.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete -f -'
Nov  5 17:25:35.380: INFO: stderr: ""
Nov  5 17:25:35.380: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:25:35.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7674" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":1,"skipped":11,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:25:35.414: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov  5 17:25:35.499: INFO: Waiting up to 5m0s for pod "pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6" in namespace "emptydir-1978" to be "Succeeded or Failed"
Nov  5 17:25:35.507: INFO: Pod "pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.444ms
Nov  5 17:25:37.517: INFO: Pod "pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017539515s
Nov  5 17:25:39.531: INFO: Pod "pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031187879s
Nov  5 17:25:41.540: INFO: Pod "pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040606716s
Nov  5 17:25:43.548: INFO: Pod "pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048663507s
Nov  5 17:25:45.558: INFO: Pod "pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.058516494s
Nov  5 17:25:47.571: INFO: Pod "pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.071429313s
Nov  5 17:25:49.581: INFO: Pod "pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.081115233s
STEP: Saw pod success
Nov  5 17:25:49.581: INFO: Pod "pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6" satisfied condition "Succeeded or Failed"
Nov  5 17:25:49.589: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6 container test-container: <nil>
STEP: delete the pod
Nov  5 17:25:49.702: INFO: Waiting for pod pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6 to disappear
Nov  5 17:25:49.708: INFO: Pod pod-8cf0e181-8d4e-4bae-8d52-30d89168dcf6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:25:49.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1978" for this suite.

• [SLOW TEST:14.322 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":2,"skipped":27,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:25:49.736: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1965
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov  5 17:25:49.824: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov  5 17:25:49.880: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 17:25:51.917: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 17:25:53.888: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 17:25:55.889: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 17:25:57.889: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:25:59.891: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:26:01.890: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:26:03.892: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:26:05.889: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:26:07.892: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:26:09.888: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:26:11.889: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov  5 17:26:11.919: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov  5 17:26:16.003: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.0.5:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1965 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 17:26:16.004: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 17:26:16.232: INFO: Found all expected endpoints: [netserver-0]
Nov  5 17:26:16.239: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.1.8:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1965 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 17:26:16.239: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 17:26:16.449: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:26:16.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1965" for this suite.

• [SLOW TEST:26.737 seconds]
[sig-network] Networking
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":3,"skipped":46,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:26:16.474: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:26:16.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5588" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":4,"skipped":55,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:26:16.784: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 17:26:16.869: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov  5 17:26:17.930: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:26:18.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2019" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":5,"skipped":76,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:26:18.979: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1105 17:26:20.721947      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1105 17:26:20.722081      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1105 17:26:20.722522      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov  5 17:26:20.722: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:26:20.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4152" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":6,"skipped":93,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:26:20.793: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7474
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7474
STEP: creating replication controller externalsvc in namespace services-7474
I1105 17:26:21.003637      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7474, replica count: 2
I1105 17:26:24.053976      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:26:27.054314      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:26:30.054531      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:26:33.054712      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov  5 17:26:33.098: INFO: Creating new exec pod
Nov  5 17:26:37.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-7474 execpodxf4gs -- /bin/sh -x -c nslookup clusterip-service.services-7474.svc.cluster.local'
Nov  5 17:26:37.502: INFO: stderr: "+ nslookup clusterip-service.services-7474.svc.cluster.local\n"
Nov  5 17:26:37.502: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nclusterip-service.services-7474.svc.cluster.local\tcanonical name = externalsvc.services-7474.svc.cluster.local.\nName:\texternalsvc.services-7474.svc.cluster.local\nAddress: 10.3.153.43\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7474, will wait for the garbage collector to delete the pods
Nov  5 17:26:37.578: INFO: Deleting ReplicationController externalsvc took: 17.681153ms
Nov  5 17:26:38.378: INFO: Terminating ReplicationController externalsvc pods took: 800.185639ms
Nov  5 17:26:49.819: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:26:49.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7474" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:29.072 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":7,"skipped":104,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:26:49.868: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 17:26:49.956: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov  5 17:26:53.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-3252 create -f -'
Nov  5 17:26:55.276: INFO: stderr: ""
Nov  5 17:26:55.276: INFO: stdout: "e2e-test-crd-publish-openapi-1948-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov  5 17:26:55.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-3252 delete e2e-test-crd-publish-openapi-1948-crds test-cr'
Nov  5 17:26:55.425: INFO: stderr: ""
Nov  5 17:26:55.425: INFO: stdout: "e2e-test-crd-publish-openapi-1948-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov  5 17:26:55.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-3252 apply -f -'
Nov  5 17:26:55.729: INFO: stderr: ""
Nov  5 17:26:55.729: INFO: stdout: "e2e-test-crd-publish-openapi-1948-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov  5 17:26:55.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-3252 delete e2e-test-crd-publish-openapi-1948-crds test-cr'
Nov  5 17:26:55.874: INFO: stderr: ""
Nov  5 17:26:55.874: INFO: stdout: "e2e-test-crd-publish-openapi-1948-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov  5 17:26:55.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 explain e2e-test-crd-publish-openapi-1948-crds'
Nov  5 17:26:56.144: INFO: stderr: ""
Nov  5 17:26:56.144: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1948-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:27:00.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3252" for this suite.

• [SLOW TEST:10.237 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":8,"skipped":114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:27:00.107: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2763
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov  5 17:27:00.191: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov  5 17:27:00.241: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 17:27:02.249: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 17:27:04.250: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:27:06.252: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:27:08.251: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:27:10.255: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:27:12.253: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:27:14.253: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov  5 17:27:14.270: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov  5 17:27:16.291: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov  5 17:27:18.280: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov  5 17:27:22.335: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.15:8080/dial?request=hostname&protocol=udp&host=10.2.0.9&port=8081&tries=1'] Namespace:pod-network-test-2763 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 17:27:22.335: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 17:27:22.532: INFO: Waiting for responses: map[]
Nov  5 17:27:22.540: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.15:8080/dial?request=hostname&protocol=udp&host=10.2.1.14&port=8081&tries=1'] Namespace:pod-network-test-2763 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 17:27:22.540: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 17:27:22.743: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:27:22.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2763" for this suite.

• [SLOW TEST:22.661 seconds]
[sig-network] Networking
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":9,"skipped":143,"failed":0}
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:27:22.769: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov  5 17:27:22.868: INFO: Pod name pod-release: Found 0 pods out of 1
Nov  5 17:27:27.877: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:27:27.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9624" for this suite.

• [SLOW TEST:5.176 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":10,"skipped":143,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:27:27.949: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov  5 17:27:28.044: INFO: Waiting up to 5m0s for pod "downward-api-baa59c75-9cba-4aa5-a00b-99266a3706fc" in namespace "downward-api-3634" to be "Succeeded or Failed"
Nov  5 17:27:28.054: INFO: Pod "downward-api-baa59c75-9cba-4aa5-a00b-99266a3706fc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.876086ms
Nov  5 17:27:30.064: INFO: Pod "downward-api-baa59c75-9cba-4aa5-a00b-99266a3706fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019271887s
Nov  5 17:27:32.075: INFO: Pod "downward-api-baa59c75-9cba-4aa5-a00b-99266a3706fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030672889s
Nov  5 17:27:34.083: INFO: Pod "downward-api-baa59c75-9cba-4aa5-a00b-99266a3706fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038454536s
STEP: Saw pod success
Nov  5 17:27:34.083: INFO: Pod "downward-api-baa59c75-9cba-4aa5-a00b-99266a3706fc" satisfied condition "Succeeded or Failed"
Nov  5 17:27:34.091: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downward-api-baa59c75-9cba-4aa5-a00b-99266a3706fc container dapi-container: <nil>
STEP: delete the pod
Nov  5 17:27:34.177: INFO: Waiting for pod downward-api-baa59c75-9cba-4aa5-a00b-99266a3706fc to disappear
Nov  5 17:27:34.185: INFO: Pod downward-api-baa59c75-9cba-4aa5-a00b-99266a3706fc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:27:34.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3634" for this suite.

• [SLOW TEST:6.254 seconds]
[sig-node] Downward API
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":11,"skipped":145,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:27:34.207: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 17:27:35.237: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 17:27:37.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194055, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194055, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194055, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194055, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 17:27:40.290: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:27:40.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8213" for this suite.
STEP: Destroying namespace "webhook-8213-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.399 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":12,"skipped":191,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:27:40.609: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-833fdba1-1147-4075-aa32-38379ce51329
STEP: Creating a pod to test consume secrets
Nov  5 17:27:40.736: INFO: Waiting up to 5m0s for pod "pod-secrets-62c95d0f-b675-42d7-83b1-bb1f6dbc0c2f" in namespace "secrets-5617" to be "Succeeded or Failed"
Nov  5 17:27:40.749: INFO: Pod "pod-secrets-62c95d0f-b675-42d7-83b1-bb1f6dbc0c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.90342ms
Nov  5 17:27:42.762: INFO: Pod "pod-secrets-62c95d0f-b675-42d7-83b1-bb1f6dbc0c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026595157s
Nov  5 17:27:44.773: INFO: Pod "pod-secrets-62c95d0f-b675-42d7-83b1-bb1f6dbc0c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037596647s
STEP: Saw pod success
Nov  5 17:27:44.774: INFO: Pod "pod-secrets-62c95d0f-b675-42d7-83b1-bb1f6dbc0c2f" satisfied condition "Succeeded or Failed"
Nov  5 17:27:44.782: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-secrets-62c95d0f-b675-42d7-83b1-bb1f6dbc0c2f container secret-volume-test: <nil>
STEP: delete the pod
Nov  5 17:27:44.829: INFO: Waiting for pod pod-secrets-62c95d0f-b675-42d7-83b1-bb1f6dbc0c2f to disappear
Nov  5 17:27:44.836: INFO: Pod pod-secrets-62c95d0f-b675-42d7-83b1-bb1f6dbc0c2f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:27:44.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5617" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:27:44.860: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:27:45.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5520" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":14,"skipped":228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:27:45.091: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 17:27:45.877: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 17:27:47.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194066, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194066, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194066, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194065, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 17:27:50.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:27:51.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6331" for this suite.
STEP: Destroying namespace "webhook-6331-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.372 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":15,"skipped":267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:27:51.465: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1105 17:28:31.621813      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1105 17:28:31.621849      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1105 17:28:31.621859      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov  5 17:28:31.621: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov  5 17:28:31.622: INFO: Deleting pod "simpletest.rc-7f5gd" in namespace "gc-4415"
Nov  5 17:28:31.652: INFO: Deleting pod "simpletest.rc-ckffh" in namespace "gc-4415"
Nov  5 17:28:31.692: INFO: Deleting pod "simpletest.rc-dk4kn" in namespace "gc-4415"
Nov  5 17:28:31.719: INFO: Deleting pod "simpletest.rc-lxrhp" in namespace "gc-4415"
Nov  5 17:28:31.750: INFO: Deleting pod "simpletest.rc-mr4x4" in namespace "gc-4415"
Nov  5 17:28:31.777: INFO: Deleting pod "simpletest.rc-p4k79" in namespace "gc-4415"
Nov  5 17:28:31.816: INFO: Deleting pod "simpletest.rc-pdcnm" in namespace "gc-4415"
Nov  5 17:28:31.851: INFO: Deleting pod "simpletest.rc-sfkds" in namespace "gc-4415"
Nov  5 17:28:31.876: INFO: Deleting pod "simpletest.rc-wb9sv" in namespace "gc-4415"
Nov  5 17:28:32.195: INFO: Deleting pod "simpletest.rc-wfzpq" in namespace "gc-4415"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:28:32.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4415" for this suite.

• [SLOW TEST:40.788 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":16,"skipped":292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:28:32.254: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov  5 17:28:32.384: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9902 /api/v1/namespaces/watch-9902/configmaps/e2e-watch-test-label-changed a0c9f415-983e-4845-a53d-eb0e74757e1b 1646461427 0 2020-11-05 17:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-05 17:28:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 17:28:32.384: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9902 /api/v1/namespaces/watch-9902/configmaps/e2e-watch-test-label-changed a0c9f415-983e-4845-a53d-eb0e74757e1b 1646461433 0 2020-11-05 17:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-05 17:28:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 17:28:32.385: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9902 /api/v1/namespaces/watch-9902/configmaps/e2e-watch-test-label-changed a0c9f415-983e-4845-a53d-eb0e74757e1b 1646461437 0 2020-11-05 17:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-05 17:28:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov  5 17:28:42.462: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9902 /api/v1/namespaces/watch-9902/configmaps/e2e-watch-test-label-changed a0c9f415-983e-4845-a53d-eb0e74757e1b 1646464830 0 2020-11-05 17:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-05 17:28:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 17:28:42.463: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9902 /api/v1/namespaces/watch-9902/configmaps/e2e-watch-test-label-changed a0c9f415-983e-4845-a53d-eb0e74757e1b 1646464833 0 2020-11-05 17:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-05 17:28:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 17:28:42.463: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9902 /api/v1/namespaces/watch-9902/configmaps/e2e-watch-test-label-changed a0c9f415-983e-4845-a53d-eb0e74757e1b 1646464836 0 2020-11-05 17:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-05 17:28:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:28:42.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9902" for this suite.

• [SLOW TEST:10.235 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":17,"skipped":320,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:28:42.491: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 17:28:42.591: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9403fd9c-b2d0-4d1f-a39e-720e8342c0a9" in namespace "downward-api-7257" to be "Succeeded or Failed"
Nov  5 17:28:42.600: INFO: Pod "downwardapi-volume-9403fd9c-b2d0-4d1f-a39e-720e8342c0a9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.096431ms
Nov  5 17:28:44.610: INFO: Pod "downwardapi-volume-9403fd9c-b2d0-4d1f-a39e-720e8342c0a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018649537s
Nov  5 17:28:46.626: INFO: Pod "downwardapi-volume-9403fd9c-b2d0-4d1f-a39e-720e8342c0a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035088027s
STEP: Saw pod success
Nov  5 17:28:46.626: INFO: Pod "downwardapi-volume-9403fd9c-b2d0-4d1f-a39e-720e8342c0a9" satisfied condition "Succeeded or Failed"
Nov  5 17:28:46.640: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-9403fd9c-b2d0-4d1f-a39e-720e8342c0a9 container client-container: <nil>
STEP: delete the pod
Nov  5 17:28:46.686: INFO: Waiting for pod downwardapi-volume-9403fd9c-b2d0-4d1f-a39e-720e8342c0a9 to disappear
Nov  5 17:28:46.694: INFO: Pod downwardapi-volume-9403fd9c-b2d0-4d1f-a39e-720e8342c0a9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:28:46.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7257" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":18,"skipped":323,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:28:46.720: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Nov  5 17:28:46.824: INFO: Waiting up to 5m0s for pod "var-expansion-85cf0c2c-8ae3-4d8d-9a54-ee2502c913e9" in namespace "var-expansion-6483" to be "Succeeded or Failed"
Nov  5 17:28:46.836: INFO: Pod "var-expansion-85cf0c2c-8ae3-4d8d-9a54-ee2502c913e9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.56535ms
Nov  5 17:28:48.846: INFO: Pod "var-expansion-85cf0c2c-8ae3-4d8d-9a54-ee2502c913e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021366242s
Nov  5 17:28:50.856: INFO: Pod "var-expansion-85cf0c2c-8ae3-4d8d-9a54-ee2502c913e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03152081s
Nov  5 17:28:52.874: INFO: Pod "var-expansion-85cf0c2c-8ae3-4d8d-9a54-ee2502c913e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050201275s
STEP: Saw pod success
Nov  5 17:28:52.875: INFO: Pod "var-expansion-85cf0c2c-8ae3-4d8d-9a54-ee2502c913e9" satisfied condition "Succeeded or Failed"
Nov  5 17:28:52.886: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod var-expansion-85cf0c2c-8ae3-4d8d-9a54-ee2502c913e9 container dapi-container: <nil>
STEP: delete the pod
Nov  5 17:28:52.935: INFO: Waiting for pod var-expansion-85cf0c2c-8ae3-4d8d-9a54-ee2502c913e9 to disappear
Nov  5 17:28:52.942: INFO: Pod var-expansion-85cf0c2c-8ae3-4d8d-9a54-ee2502c913e9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:28:52.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6483" for this suite.

• [SLOW TEST:6.245 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":19,"skipped":329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:28:52.969: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Nov  5 17:28:53.064: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:29:15.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5988" for this suite.

• [SLOW TEST:22.444 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":20,"skipped":354,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:29:15.417: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-08c0264d-3064-487c-a5c3-07ba5f333ce6
STEP: Creating a pod to test consume secrets
Nov  5 17:29:15.520: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c9f534b6-30c4-4445-af17-03eed202cab9" in namespace "projected-9348" to be "Succeeded or Failed"
Nov  5 17:29:15.528: INFO: Pod "pod-projected-secrets-c9f534b6-30c4-4445-af17-03eed202cab9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.536189ms
Nov  5 17:29:17.539: INFO: Pod "pod-projected-secrets-c9f534b6-30c4-4445-af17-03eed202cab9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018361277s
Nov  5 17:29:19.549: INFO: Pod "pod-projected-secrets-c9f534b6-30c4-4445-af17-03eed202cab9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028772252s
STEP: Saw pod success
Nov  5 17:29:19.549: INFO: Pod "pod-projected-secrets-c9f534b6-30c4-4445-af17-03eed202cab9" satisfied condition "Succeeded or Failed"
Nov  5 17:29:19.560: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-secrets-c9f534b6-30c4-4445-af17-03eed202cab9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov  5 17:29:19.607: INFO: Waiting for pod pod-projected-secrets-c9f534b6-30c4-4445-af17-03eed202cab9 to disappear
Nov  5 17:29:19.615: INFO: Pod pod-projected-secrets-c9f534b6-30c4-4445-af17-03eed202cab9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:29:19.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9348" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":21,"skipped":368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:29:19.637: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov  5 17:29:21.566: INFO: starting watch
STEP: patching
STEP: updating
Nov  5 17:29:21.589: INFO: waiting for watch events with expected annotations
Nov  5 17:29:21.589: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:29:21.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2530" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":22,"skipped":396,"failed":0}
SSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:29:21.744: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:29:21.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6302" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":23,"skipped":401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:29:21.933: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-37bbaab5-fbb4-4eba-a787-8020b0a2e0bd
STEP: Creating a pod to test consume secrets
Nov  5 17:29:22.048: INFO: Waiting up to 5m0s for pod "pod-secrets-6ed77641-641c-4391-ac35-4d5afd932a76" in namespace "secrets-7919" to be "Succeeded or Failed"
Nov  5 17:29:22.062: INFO: Pod "pod-secrets-6ed77641-641c-4391-ac35-4d5afd932a76": Phase="Pending", Reason="", readiness=false. Elapsed: 13.244317ms
Nov  5 17:29:24.071: INFO: Pod "pod-secrets-6ed77641-641c-4391-ac35-4d5afd932a76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022539309s
Nov  5 17:29:26.081: INFO: Pod "pod-secrets-6ed77641-641c-4391-ac35-4d5afd932a76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031772648s
STEP: Saw pod success
Nov  5 17:29:26.081: INFO: Pod "pod-secrets-6ed77641-641c-4391-ac35-4d5afd932a76" satisfied condition "Succeeded or Failed"
Nov  5 17:29:26.184: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-secrets-6ed77641-641c-4391-ac35-4d5afd932a76 container secret-volume-test: <nil>
STEP: delete the pod
Nov  5 17:29:26.240: INFO: Waiting for pod pod-secrets-6ed77641-641c-4391-ac35-4d5afd932a76 to disappear
Nov  5 17:29:26.246: INFO: Pod pod-secrets-6ed77641-641c-4391-ac35-4d5afd932a76 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:29:26.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7919" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":24,"skipped":449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:29:26.287: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-ccf98ae5-f9a0-41a6-9c27-0fc0c92304b6 in namespace container-probe-8749
Nov  5 17:29:32.426: INFO: Started pod busybox-ccf98ae5-f9a0-41a6-9c27-0fc0c92304b6 in namespace container-probe-8749
STEP: checking the pod's current state and verifying that restartCount is present
Nov  5 17:29:32.433: INFO: Initial restart count of pod busybox-ccf98ae5-f9a0-41a6-9c27-0fc0c92304b6 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:33:33.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8749" for this suite.

• [SLOW TEST:246.880 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":25,"skipped":495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:33:33.168: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-rzd5
STEP: Creating a pod to test atomic-volume-subpath
Nov  5 17:33:33.279: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rzd5" in namespace "subpath-9020" to be "Succeeded or Failed"
Nov  5 17:33:33.290: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.935988ms
Nov  5 17:33:35.299: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019969445s
Nov  5 17:33:37.313: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Running", Reason="", readiness=true. Elapsed: 4.034202192s
Nov  5 17:33:39.321: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Running", Reason="", readiness=true. Elapsed: 6.042210275s
Nov  5 17:33:41.330: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Running", Reason="", readiness=true. Elapsed: 8.05095066s
Nov  5 17:33:43.341: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Running", Reason="", readiness=true. Elapsed: 10.062131574s
Nov  5 17:33:45.353: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Running", Reason="", readiness=true. Elapsed: 12.073588891s
Nov  5 17:33:47.364: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Running", Reason="", readiness=true. Elapsed: 14.085203833s
Nov  5 17:33:49.375: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Running", Reason="", readiness=true. Elapsed: 16.095794352s
Nov  5 17:33:51.383: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Running", Reason="", readiness=true. Elapsed: 18.104120963s
Nov  5 17:33:53.396: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Running", Reason="", readiness=true. Elapsed: 20.116743511s
Nov  5 17:33:55.404: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Running", Reason="", readiness=true. Elapsed: 22.12464489s
Nov  5 17:33:57.415: INFO: Pod "pod-subpath-test-projected-rzd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.135553155s
STEP: Saw pod success
Nov  5 17:33:57.415: INFO: Pod "pod-subpath-test-projected-rzd5" satisfied condition "Succeeded or Failed"
Nov  5 17:33:57.423: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-subpath-test-projected-rzd5 container test-container-subpath-projected-rzd5: <nil>
STEP: delete the pod
Nov  5 17:33:57.528: INFO: Waiting for pod pod-subpath-test-projected-rzd5 to disappear
Nov  5 17:33:57.535: INFO: Pod pod-subpath-test-projected-rzd5 no longer exists
STEP: Deleting pod pod-subpath-test-projected-rzd5
Nov  5 17:33:57.535: INFO: Deleting pod "pod-subpath-test-projected-rzd5" in namespace "subpath-9020"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:33:57.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9020" for this suite.

• [SLOW TEST:24.405 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":26,"skipped":550,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:33:57.576: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 17:33:57.685: INFO: Waiting up to 5m0s for pod "downwardapi-volume-292547a8-63bb-4dc2-8830-a0d269dab325" in namespace "projected-9983" to be "Succeeded or Failed"
Nov  5 17:33:57.695: INFO: Pod "downwardapi-volume-292547a8-63bb-4dc2-8830-a0d269dab325": Phase="Pending", Reason="", readiness=false. Elapsed: 10.406445ms
Nov  5 17:33:59.706: INFO: Pod "downwardapi-volume-292547a8-63bb-4dc2-8830-a0d269dab325": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020922662s
Nov  5 17:34:01.715: INFO: Pod "downwardapi-volume-292547a8-63bb-4dc2-8830-a0d269dab325": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029881s
STEP: Saw pod success
Nov  5 17:34:01.715: INFO: Pod "downwardapi-volume-292547a8-63bb-4dc2-8830-a0d269dab325" satisfied condition "Succeeded or Failed"
Nov  5 17:34:01.722: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-292547a8-63bb-4dc2-8830-a0d269dab325 container client-container: <nil>
STEP: delete the pod
Nov  5 17:34:01.762: INFO: Waiting for pod downwardapi-volume-292547a8-63bb-4dc2-8830-a0d269dab325 to disappear
Nov  5 17:34:01.769: INFO: Pod downwardapi-volume-292547a8-63bb-4dc2-8830-a0d269dab325 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:34:01.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9983" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":27,"skipped":567,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:34:01.792: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-0fc6b89f-92be-4e27-92b4-922b72568795
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:34:01.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2928" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":28,"skipped":579,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:34:01.919: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-d5b82e39-6206-476d-bc8d-22e41a6aeb05
STEP: Creating a pod to test consume configMaps
Nov  5 17:34:02.025: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f6e5506d-c5e9-4d42-8fbb-014883d59397" in namespace "projected-1584" to be "Succeeded or Failed"
Nov  5 17:34:02.034: INFO: Pod "pod-projected-configmaps-f6e5506d-c5e9-4d42-8fbb-014883d59397": Phase="Pending", Reason="", readiness=false. Elapsed: 8.712319ms
Nov  5 17:34:04.042: INFO: Pod "pod-projected-configmaps-f6e5506d-c5e9-4d42-8fbb-014883d59397": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016418417s
Nov  5 17:34:06.055: INFO: Pod "pod-projected-configmaps-f6e5506d-c5e9-4d42-8fbb-014883d59397": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029309727s
STEP: Saw pod success
Nov  5 17:34:06.055: INFO: Pod "pod-projected-configmaps-f6e5506d-c5e9-4d42-8fbb-014883d59397" satisfied condition "Succeeded or Failed"
Nov  5 17:34:06.066: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-configmaps-f6e5506d-c5e9-4d42-8fbb-014883d59397 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 17:34:06.109: INFO: Waiting for pod pod-projected-configmaps-f6e5506d-c5e9-4d42-8fbb-014883d59397 to disappear
Nov  5 17:34:06.117: INFO: Pod pod-projected-configmaps-f6e5506d-c5e9-4d42-8fbb-014883d59397 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:34:06.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1584" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":29,"skipped":592,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:34:06.142: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-566ed1e9-b7d8-4c64-a220-afe51557bc82
STEP: Creating secret with name s-test-opt-upd-2fab3c1c-12e7-40ca-9904-86eee01f8e2d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-566ed1e9-b7d8-4c64-a220-afe51557bc82
STEP: Updating secret s-test-opt-upd-2fab3c1c-12e7-40ca-9904-86eee01f8e2d
STEP: Creating secret with name s-test-opt-create-e871aa58-ac6d-48b5-9d48-a45c2679ecfb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:34:14.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9991" for this suite.

• [SLOW TEST:8.360 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":30,"skipped":598,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:34:14.508: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:34:14.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1032" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":31,"skipped":618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:34:14.698: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:34:20.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9136" for this suite.

• [SLOW TEST:6.243 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command in a pod
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":32,"skipped":646,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:34:20.942: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a87cd2c5-4227-4dd6-b6ca-6e62279a88e7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a87cd2c5-4227-4dd6-b6ca-6e62279a88e7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:35:30.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2496" for this suite.

• [SLOW TEST:69.421 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":33,"skipped":651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:35:30.365: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:35:30.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8852" for this suite.
STEP: Destroying namespace "nspatchtest-bcc1a8c0-4eb9-4bd9-a741-1a1c4872f547-4490" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":34,"skipped":696,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:35:30.607: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 17:35:30.717: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-8e99df7c-9c36-4cf6-bd65-ff45c2746990" in namespace "security-context-test-347" to be "Succeeded or Failed"
Nov  5 17:35:30.725: INFO: Pod "busybox-readonly-false-8e99df7c-9c36-4cf6-bd65-ff45c2746990": Phase="Pending", Reason="", readiness=false. Elapsed: 7.567934ms
Nov  5 17:35:32.736: INFO: Pod "busybox-readonly-false-8e99df7c-9c36-4cf6-bd65-ff45c2746990": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018809844s
Nov  5 17:35:34.754: INFO: Pod "busybox-readonly-false-8e99df7c-9c36-4cf6-bd65-ff45c2746990": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03677112s
Nov  5 17:35:34.754: INFO: Pod "busybox-readonly-false-8e99df7c-9c36-4cf6-bd65-ff45c2746990" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:35:34.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-347" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":35,"skipped":721,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:35:34.783: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Nov  5 17:35:34.865: INFO: Waiting up to 1m0s for all nodes to be ready
Nov  5 17:36:34.900: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 17:36:34.908: INFO: Starting informer...
STEP: Starting pod...
Nov  5 17:36:35.141: INFO: Pod is running on node-a7de9392-4cc3-4830-84ff-49e7615b291e. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Nov  5 17:36:35.181: INFO: Pod wasn't evicted. Proceeding
Nov  5 17:36:35.181: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Nov  5 17:37:50.248: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:37:50.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1497" for this suite.

• [SLOW TEST:135.493 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":36,"skipped":727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:37:50.276: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov  5 17:37:50.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6826'
Nov  5 17:37:51.546: INFO: stderr: ""
Nov  5 17:37:51.546: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Nov  5 17:37:51.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pod e2e-test-httpd-pod -o json --namespace=kubectl-6826'
Nov  5 17:37:51.666: INFO: stderr: ""
Nov  5 17:37:51.666: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-11-05T17:37:51Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-05T17:37:51Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-05T17:37:51Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6826\",\n        \"resourceVersion\": \"1646642941\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6826/pods/e2e-test-httpd-pod\",\n        \"uid\": \"e746fb86-d6c0-40c1-a41c-b72b96d01607\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-kv6qf\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node-a7de9392-4cc3-4830-84ff-49e7615b291e\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-kv6qf\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-kv6qf\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-05T17:37:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-05T17:37:51Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-05T17:37:51Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-05T17:37:51Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"51.91.151.116\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-05T17:37:51Z\"\n    }\n}\n"
Nov  5 17:37:51.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 replace -f - --dry-run server --namespace=kubectl-6826'
Nov  5 17:37:52.065: INFO: stderr: "W1105 17:37:51.738981      86 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Nov  5 17:37:52.065: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Nov  5 17:37:52.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete pods e2e-test-httpd-pod --namespace=kubectl-6826'
Nov  5 17:37:54.856: INFO: stderr: ""
Nov  5 17:37:54.856: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:37:54.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6826" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":37,"skipped":753,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:37:54.885: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov  5 17:37:54.968: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 17:37:58.845: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:38:14.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7123" for this suite.

• [SLOW TEST:19.516 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":38,"skipped":754,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:38:14.401: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov  5 17:38:14.503: INFO: Waiting up to 5m0s for pod "pod-e61e2089-35cb-40b6-91b3-c695320870f4" in namespace "emptydir-136" to be "Succeeded or Failed"
Nov  5 17:38:14.513: INFO: Pod "pod-e61e2089-35cb-40b6-91b3-c695320870f4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.737347ms
Nov  5 17:38:16.522: INFO: Pod "pod-e61e2089-35cb-40b6-91b3-c695320870f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019242919s
Nov  5 17:38:18.536: INFO: Pod "pod-e61e2089-35cb-40b6-91b3-c695320870f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032623097s
STEP: Saw pod success
Nov  5 17:38:18.536: INFO: Pod "pod-e61e2089-35cb-40b6-91b3-c695320870f4" satisfied condition "Succeeded or Failed"
Nov  5 17:38:18.544: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-e61e2089-35cb-40b6-91b3-c695320870f4 container test-container: <nil>
STEP: delete the pod
Nov  5 17:38:18.655: INFO: Waiting for pod pod-e61e2089-35cb-40b6-91b3-c695320870f4 to disappear
Nov  5 17:38:18.664: INFO: Pod pod-e61e2089-35cb-40b6-91b3-c695320870f4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:38:18.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-136" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":768,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:38:18.698: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 17:38:18.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf0196e0-0bdb-4d83-b306-70f45ca4402e" in namespace "projected-8265" to be "Succeeded or Failed"
Nov  5 17:38:18.806: INFO: Pod "downwardapi-volume-cf0196e0-0bdb-4d83-b306-70f45ca4402e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.378787ms
Nov  5 17:38:20.818: INFO: Pod "downwardapi-volume-cf0196e0-0bdb-4d83-b306-70f45ca4402e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021181859s
Nov  5 17:38:22.829: INFO: Pod "downwardapi-volume-cf0196e0-0bdb-4d83-b306-70f45ca4402e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032175172s
STEP: Saw pod success
Nov  5 17:38:22.829: INFO: Pod "downwardapi-volume-cf0196e0-0bdb-4d83-b306-70f45ca4402e" satisfied condition "Succeeded or Failed"
Nov  5 17:38:22.842: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-cf0196e0-0bdb-4d83-b306-70f45ca4402e container client-container: <nil>
STEP: delete the pod
Nov  5 17:38:22.887: INFO: Waiting for pod downwardapi-volume-cf0196e0-0bdb-4d83-b306-70f45ca4402e to disappear
Nov  5 17:38:22.894: INFO: Pod downwardapi-volume-cf0196e0-0bdb-4d83-b306-70f45ca4402e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:38:22.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8265" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":40,"skipped":783,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:38:22.926: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3532
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Nov  5 17:38:23.038: INFO: Found 0 stateful pods, waiting for 3
Nov  5 17:38:33.048: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:38:33.049: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:38:33.049: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov  5 17:38:43.049: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:38:43.049: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:38:43.050: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov  5 17:38:43.105: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov  5 17:38:53.173: INFO: Updating stateful set ss2
Nov  5 17:38:53.194: INFO: Waiting for Pod statefulset-3532/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Nov  5 17:39:03.282: INFO: Found 2 stateful pods, waiting for 3
Nov  5 17:39:13.293: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:39:13.293: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:39:13.293: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov  5 17:39:23.290: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:39:23.291: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:39:23.291: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov  5 17:39:23.331: INFO: Updating stateful set ss2
Nov  5 17:39:23.353: INFO: Waiting for Pod statefulset-3532/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov  5 17:39:33.400: INFO: Updating stateful set ss2
Nov  5 17:39:33.415: INFO: Waiting for StatefulSet statefulset-3532/ss2 to complete update
Nov  5 17:39:33.415: INFO: Waiting for Pod statefulset-3532/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov  5 17:39:43.437: INFO: Waiting for StatefulSet statefulset-3532/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov  5 17:39:53.437: INFO: Deleting all statefulset in ns statefulset-3532
Nov  5 17:39:53.445: INFO: Scaling statefulset ss2 to 0
Nov  5 17:40:13.480: INFO: Waiting for statefulset status.replicas updated to 0
Nov  5 17:40:13.488: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:40:13.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3532" for this suite.

• [SLOW TEST:110.628 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":41,"skipped":788,"failed":0}
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:40:13.554: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 17:40:13.645: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7473
I1105 17:40:13.669083      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7473, replica count: 1
I1105 17:40:14.722767      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:40:15.723337      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:40:16.723723      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:40:17.725690      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 17:40:17.853: INFO: Created: latency-svc-5jhqj
Nov  5 17:40:17.860: INFO: Got endpoints: latency-svc-5jhqj [34.494097ms]
Nov  5 17:40:17.887: INFO: Created: latency-svc-5qmhn
Nov  5 17:40:17.890: INFO: Got endpoints: latency-svc-5qmhn [29.453669ms]
Nov  5 17:40:17.893: INFO: Created: latency-svc-vfh8x
Nov  5 17:40:17.902: INFO: Got endpoints: latency-svc-vfh8x [39.260254ms]
Nov  5 17:40:17.905: INFO: Created: latency-svc-6dtnx
Nov  5 17:40:17.914: INFO: Got endpoints: latency-svc-6dtnx [52.093862ms]
Nov  5 17:40:17.925: INFO: Created: latency-svc-xq8v2
Nov  5 17:40:17.925: INFO: Got endpoints: latency-svc-xq8v2 [62.58633ms]
Nov  5 17:40:17.927: INFO: Created: latency-svc-dfrrp
Nov  5 17:40:17.933: INFO: Got endpoints: latency-svc-dfrrp [71.165737ms]
Nov  5 17:40:17.939: INFO: Created: latency-svc-wc5p9
Nov  5 17:40:17.946: INFO: Got endpoints: latency-svc-wc5p9 [84.123825ms]
Nov  5 17:40:17.952: INFO: Created: latency-svc-4wp4v
Nov  5 17:40:17.959: INFO: Got endpoints: latency-svc-4wp4v [97.241634ms]
Nov  5 17:40:17.964: INFO: Created: latency-svc-gz72m
Nov  5 17:40:17.973: INFO: Got endpoints: latency-svc-gz72m [112.6754ms]
Nov  5 17:40:17.976: INFO: Created: latency-svc-wztln
Nov  5 17:40:17.983: INFO: Got endpoints: latency-svc-wztln [120.599895ms]
Nov  5 17:40:17.990: INFO: Created: latency-svc-vl6zh
Nov  5 17:40:17.997: INFO: Got endpoints: latency-svc-vl6zh [136.143142ms]
Nov  5 17:40:18.000: INFO: Created: latency-svc-vkpmk
Nov  5 17:40:18.004: INFO: Got endpoints: latency-svc-vkpmk [143.285361ms]
Nov  5 17:40:18.010: INFO: Created: latency-svc-kmcv2
Nov  5 17:40:18.018: INFO: Got endpoints: latency-svc-kmcv2 [156.624813ms]
Nov  5 17:40:18.021: INFO: Created: latency-svc-77hrv
Nov  5 17:40:18.031: INFO: Created: latency-svc-xgp7l
Nov  5 17:40:18.031: INFO: Got endpoints: latency-svc-77hrv [169.236696ms]
Nov  5 17:40:18.038: INFO: Got endpoints: latency-svc-xgp7l [175.762407ms]
Nov  5 17:40:18.045: INFO: Created: latency-svc-n8k5r
Nov  5 17:40:18.050: INFO: Got endpoints: latency-svc-n8k5r [188.344826ms]
Nov  5 17:40:18.056: INFO: Created: latency-svc-5vrts
Nov  5 17:40:18.064: INFO: Got endpoints: latency-svc-5vrts [174.36396ms]
Nov  5 17:40:18.066: INFO: Created: latency-svc-m9sbq
Nov  5 17:40:18.077: INFO: Got endpoints: latency-svc-m9sbq [174.698807ms]
Nov  5 17:40:18.087: INFO: Created: latency-svc-d6lrb
Nov  5 17:40:18.094: INFO: Got endpoints: latency-svc-d6lrb [180.209572ms]
Nov  5 17:40:18.098: INFO: Created: latency-svc-xmpzt
Nov  5 17:40:18.109: INFO: Got endpoints: latency-svc-xmpzt [184.562793ms]
Nov  5 17:40:18.110: INFO: Created: latency-svc-qp67b
Nov  5 17:40:18.115: INFO: Got endpoints: latency-svc-qp67b [182.168581ms]
Nov  5 17:40:18.119: INFO: Created: latency-svc-lrcjk
Nov  5 17:40:18.130: INFO: Got endpoints: latency-svc-lrcjk [183.461616ms]
Nov  5 17:40:18.133: INFO: Created: latency-svc-ddgbs
Nov  5 17:40:18.142: INFO: Created: latency-svc-sthlt
Nov  5 17:40:18.142: INFO: Got endpoints: latency-svc-ddgbs [183.480872ms]
Nov  5 17:40:18.151: INFO: Got endpoints: latency-svc-sthlt [178.306349ms]
Nov  5 17:40:18.159: INFO: Created: latency-svc-gx24g
Nov  5 17:40:18.168: INFO: Got endpoints: latency-svc-gx24g [184.439121ms]
Nov  5 17:40:18.173: INFO: Created: latency-svc-gz2db
Nov  5 17:40:18.180: INFO: Got endpoints: latency-svc-gz2db [183.228875ms]
Nov  5 17:40:18.182: INFO: Created: latency-svc-bscmn
Nov  5 17:40:18.188: INFO: Got endpoints: latency-svc-bscmn [183.730378ms]
Nov  5 17:40:18.193: INFO: Created: latency-svc-4kx2r
Nov  5 17:40:18.198: INFO: Got endpoints: latency-svc-4kx2r [179.921688ms]
Nov  5 17:40:18.202: INFO: Created: latency-svc-87wj6
Nov  5 17:40:18.212: INFO: Got endpoints: latency-svc-87wj6 [180.982769ms]
Nov  5 17:40:18.214: INFO: Created: latency-svc-tdfb4
Nov  5 17:40:18.219: INFO: Got endpoints: latency-svc-tdfb4 [181.04018ms]
Nov  5 17:40:18.226: INFO: Created: latency-svc-nrgb8
Nov  5 17:40:18.234: INFO: Got endpoints: latency-svc-nrgb8 [183.761119ms]
Nov  5 17:40:18.240: INFO: Created: latency-svc-lzn22
Nov  5 17:40:18.245: INFO: Got endpoints: latency-svc-lzn22 [181.349334ms]
Nov  5 17:40:18.246: INFO: Created: latency-svc-cdh7q
Nov  5 17:40:18.249: INFO: Got endpoints: latency-svc-cdh7q [172.671413ms]
Nov  5 17:40:18.277: INFO: Created: latency-svc-t7xqr
Nov  5 17:40:18.284: INFO: Got endpoints: latency-svc-t7xqr [188.650168ms]
Nov  5 17:40:18.296: INFO: Created: latency-svc-86jlf
Nov  5 17:40:18.319: INFO: Created: latency-svc-hh5nh
Nov  5 17:40:18.320: INFO: Created: latency-svc-cgbls
Nov  5 17:40:18.320: INFO: Got endpoints: latency-svc-cgbls [204.25304ms]
Nov  5 17:40:18.320: INFO: Got endpoints: latency-svc-86jlf [210.701687ms]
Nov  5 17:40:18.321: INFO: Got endpoints: latency-svc-hh5nh [191.213804ms]
Nov  5 17:40:18.323: INFO: Created: latency-svc-xs5hw
Nov  5 17:40:18.329: INFO: Got endpoints: latency-svc-xs5hw [186.880904ms]
Nov  5 17:40:18.332: INFO: Created: latency-svc-xkcpk
Nov  5 17:40:18.341: INFO: Got endpoints: latency-svc-xkcpk [189.250308ms]
Nov  5 17:40:18.343: INFO: Created: latency-svc-xgmlr
Nov  5 17:40:18.351: INFO: Created: latency-svc-s578c
Nov  5 17:40:18.356: INFO: Got endpoints: latency-svc-xgmlr [188.388127ms]
Nov  5 17:40:18.364: INFO: Created: latency-svc-mthj2
Nov  5 17:40:18.370: INFO: Created: latency-svc-ggb4d
Nov  5 17:40:18.382: INFO: Created: latency-svc-fks6w
Nov  5 17:40:18.394: INFO: Created: latency-svc-75tn7
Nov  5 17:40:18.402: INFO: Created: latency-svc-5w7zk
Nov  5 17:40:18.409: INFO: Got endpoints: latency-svc-s578c [228.222056ms]
Nov  5 17:40:18.413: INFO: Created: latency-svc-2mggm
Nov  5 17:40:18.422: INFO: Created: latency-svc-hs7f6
Nov  5 17:40:18.432: INFO: Created: latency-svc-g97df
Nov  5 17:40:18.442: INFO: Created: latency-svc-lncp8
Nov  5 17:40:18.453: INFO: Created: latency-svc-f5h4g
Nov  5 17:40:18.458: INFO: Got endpoints: latency-svc-mthj2 [269.097429ms]
Nov  5 17:40:18.463: INFO: Created: latency-svc-qdgq6
Nov  5 17:40:18.474: INFO: Created: latency-svc-vk922
Nov  5 17:40:18.485: INFO: Created: latency-svc-km42x
Nov  5 17:40:18.497: INFO: Created: latency-svc-qbjb2
Nov  5 17:40:18.506: INFO: Got endpoints: latency-svc-ggb4d [307.536617ms]
Nov  5 17:40:18.509: INFO: Created: latency-svc-wv7c5
Nov  5 17:40:18.517: INFO: Created: latency-svc-bwv2j
Nov  5 17:40:18.528: INFO: Created: latency-svc-ncdzv
Nov  5 17:40:18.557: INFO: Got endpoints: latency-svc-fks6w [343.712794ms]
Nov  5 17:40:18.579: INFO: Created: latency-svc-9rbl4
Nov  5 17:40:18.608: INFO: Got endpoints: latency-svc-75tn7 [388.4785ms]
Nov  5 17:40:18.634: INFO: Created: latency-svc-7js99
Nov  5 17:40:18.662: INFO: Got endpoints: latency-svc-5w7zk [428.280445ms]
Nov  5 17:40:18.687: INFO: Created: latency-svc-9tdqd
Nov  5 17:40:18.710: INFO: Got endpoints: latency-svc-2mggm [463.82302ms]
Nov  5 17:40:18.732: INFO: Created: latency-svc-kf5f6
Nov  5 17:40:18.755: INFO: Got endpoints: latency-svc-hs7f6 [505.966508ms]
Nov  5 17:40:18.774: INFO: Created: latency-svc-flwd2
Nov  5 17:40:18.809: INFO: Got endpoints: latency-svc-g97df [525.476832ms]
Nov  5 17:40:18.831: INFO: Created: latency-svc-qtwnn
Nov  5 17:40:18.856: INFO: Got endpoints: latency-svc-lncp8 [535.61862ms]
Nov  5 17:40:18.874: INFO: Created: latency-svc-pt8t2
Nov  5 17:40:18.907: INFO: Got endpoints: latency-svc-f5h4g [585.938619ms]
Nov  5 17:40:18.926: INFO: Created: latency-svc-hlcnc
Nov  5 17:40:18.959: INFO: Got endpoints: latency-svc-qdgq6 [636.680844ms]
Nov  5 17:40:18.979: INFO: Created: latency-svc-brzpw
Nov  5 17:40:19.007: INFO: Got endpoints: latency-svc-vk922 [677.167128ms]
Nov  5 17:40:19.026: INFO: Created: latency-svc-5fjfr
Nov  5 17:40:19.057: INFO: Got endpoints: latency-svc-km42x [715.612973ms]
Nov  5 17:40:19.078: INFO: Created: latency-svc-xq7b8
Nov  5 17:40:19.105: INFO: Got endpoints: latency-svc-qbjb2 [748.451418ms]
Nov  5 17:40:19.124: INFO: Created: latency-svc-df2cc
Nov  5 17:40:19.159: INFO: Got endpoints: latency-svc-wv7c5 [749.438798ms]
Nov  5 17:40:19.179: INFO: Created: latency-svc-wqbmq
Nov  5 17:40:19.212: INFO: Got endpoints: latency-svc-bwv2j [753.914795ms]
Nov  5 17:40:19.237: INFO: Created: latency-svc-hsczb
Nov  5 17:40:19.255: INFO: Got endpoints: latency-svc-ncdzv [748.944535ms]
Nov  5 17:40:19.272: INFO: Created: latency-svc-4k4mm
Nov  5 17:40:19.307: INFO: Got endpoints: latency-svc-9rbl4 [749.487055ms]
Nov  5 17:40:19.323: INFO: Created: latency-svc-fdxhz
Nov  5 17:40:19.357: INFO: Got endpoints: latency-svc-7js99 [749.009931ms]
Nov  5 17:40:19.384: INFO: Created: latency-svc-svnnc
Nov  5 17:40:19.405: INFO: Got endpoints: latency-svc-9tdqd [742.243422ms]
Nov  5 17:40:19.423: INFO: Created: latency-svc-xpprh
Nov  5 17:40:19.458: INFO: Got endpoints: latency-svc-kf5f6 [748.081388ms]
Nov  5 17:40:19.475: INFO: Created: latency-svc-dcgdh
Nov  5 17:40:19.507: INFO: Got endpoints: latency-svc-flwd2 [751.999441ms]
Nov  5 17:40:19.529: INFO: Created: latency-svc-j6d8w
Nov  5 17:40:19.555: INFO: Got endpoints: latency-svc-qtwnn [745.931807ms]
Nov  5 17:40:19.576: INFO: Created: latency-svc-bxxf7
Nov  5 17:40:19.606: INFO: Got endpoints: latency-svc-pt8t2 [749.876254ms]
Nov  5 17:40:19.624: INFO: Created: latency-svc-2swdf
Nov  5 17:40:19.660: INFO: Got endpoints: latency-svc-hlcnc [753.319011ms]
Nov  5 17:40:19.682: INFO: Created: latency-svc-s6725
Nov  5 17:40:19.708: INFO: Got endpoints: latency-svc-brzpw [749.150811ms]
Nov  5 17:40:19.729: INFO: Created: latency-svc-xcbmv
Nov  5 17:40:19.770: INFO: Got endpoints: latency-svc-5fjfr [763.097585ms]
Nov  5 17:40:19.798: INFO: Created: latency-svc-7xphf
Nov  5 17:40:19.808: INFO: Got endpoints: latency-svc-xq7b8 [750.766694ms]
Nov  5 17:40:19.827: INFO: Created: latency-svc-wwgm4
Nov  5 17:40:19.856: INFO: Got endpoints: latency-svc-df2cc [750.953385ms]
Nov  5 17:40:19.875: INFO: Created: latency-svc-2tmxw
Nov  5 17:40:19.906: INFO: Got endpoints: latency-svc-wqbmq [747.397793ms]
Nov  5 17:40:19.937: INFO: Created: latency-svc-r8k2x
Nov  5 17:40:19.958: INFO: Got endpoints: latency-svc-hsczb [745.726181ms]
Nov  5 17:40:19.976: INFO: Created: latency-svc-drrpx
Nov  5 17:40:20.008: INFO: Got endpoints: latency-svc-4k4mm [751.959895ms]
Nov  5 17:40:20.029: INFO: Created: latency-svc-g5lnn
Nov  5 17:40:20.058: INFO: Got endpoints: latency-svc-fdxhz [750.85114ms]
Nov  5 17:40:20.079: INFO: Created: latency-svc-lqr2l
Nov  5 17:40:20.109: INFO: Got endpoints: latency-svc-svnnc [751.628937ms]
Nov  5 17:40:20.130: INFO: Created: latency-svc-psftv
Nov  5 17:40:20.159: INFO: Got endpoints: latency-svc-xpprh [754.029251ms]
Nov  5 17:40:20.177: INFO: Created: latency-svc-hgfmb
Nov  5 17:40:20.211: INFO: Got endpoints: latency-svc-dcgdh [752.056492ms]
Nov  5 17:40:20.231: INFO: Created: latency-svc-m87qx
Nov  5 17:40:20.310: INFO: Got endpoints: latency-svc-j6d8w [802.585539ms]
Nov  5 17:40:20.331: INFO: Created: latency-svc-89pwn
Nov  5 17:40:20.359: INFO: Got endpoints: latency-svc-bxxf7 [803.209798ms]
Nov  5 17:40:20.379: INFO: Created: latency-svc-twdrt
Nov  5 17:40:20.409: INFO: Got endpoints: latency-svc-2swdf [802.712179ms]
Nov  5 17:40:20.431: INFO: Created: latency-svc-md2bh
Nov  5 17:40:20.459: INFO: Got endpoints: latency-svc-s6725 [798.424203ms]
Nov  5 17:40:20.481: INFO: Created: latency-svc-mtc6w
Nov  5 17:40:20.509: INFO: Got endpoints: latency-svc-xcbmv [800.974515ms]
Nov  5 17:40:20.533: INFO: Created: latency-svc-bfc2p
Nov  5 17:40:20.560: INFO: Got endpoints: latency-svc-7xphf [789.959753ms]
Nov  5 17:40:20.581: INFO: Created: latency-svc-gl4xg
Nov  5 17:40:20.610: INFO: Got endpoints: latency-svc-wwgm4 [801.802488ms]
Nov  5 17:40:20.630: INFO: Created: latency-svc-c42q7
Nov  5 17:40:20.658: INFO: Got endpoints: latency-svc-2tmxw [801.340255ms]
Nov  5 17:40:20.682: INFO: Created: latency-svc-vjtmm
Nov  5 17:40:20.707: INFO: Got endpoints: latency-svc-r8k2x [801.269881ms]
Nov  5 17:40:20.729: INFO: Created: latency-svc-wdhxl
Nov  5 17:40:20.763: INFO: Got endpoints: latency-svc-drrpx [804.854297ms]
Nov  5 17:40:20.790: INFO: Created: latency-svc-gkw8x
Nov  5 17:40:20.808: INFO: Got endpoints: latency-svc-g5lnn [799.736291ms]
Nov  5 17:40:20.827: INFO: Created: latency-svc-krdlp
Nov  5 17:40:20.860: INFO: Got endpoints: latency-svc-lqr2l [802.604211ms]
Nov  5 17:40:20.884: INFO: Created: latency-svc-cbsbs
Nov  5 17:40:20.909: INFO: Got endpoints: latency-svc-psftv [800.156257ms]
Nov  5 17:40:20.926: INFO: Created: latency-svc-bgvsd
Nov  5 17:40:20.956: INFO: Got endpoints: latency-svc-hgfmb [797.137205ms]
Nov  5 17:40:20.976: INFO: Created: latency-svc-4d762
Nov  5 17:40:21.008: INFO: Got endpoints: latency-svc-m87qx [796.490478ms]
Nov  5 17:40:21.026: INFO: Created: latency-svc-xt4ms
Nov  5 17:40:21.059: INFO: Got endpoints: latency-svc-89pwn [748.865412ms]
Nov  5 17:40:21.079: INFO: Created: latency-svc-llqwn
Nov  5 17:40:21.108: INFO: Got endpoints: latency-svc-twdrt [748.737551ms]
Nov  5 17:40:21.129: INFO: Created: latency-svc-k72zv
Nov  5 17:40:21.157: INFO: Got endpoints: latency-svc-md2bh [747.659332ms]
Nov  5 17:40:21.179: INFO: Created: latency-svc-p9r4h
Nov  5 17:40:21.212: INFO: Got endpoints: latency-svc-mtc6w [752.908315ms]
Nov  5 17:40:21.232: INFO: Created: latency-svc-v7sfv
Nov  5 17:40:21.258: INFO: Got endpoints: latency-svc-bfc2p [748.647401ms]
Nov  5 17:40:21.277: INFO: Created: latency-svc-rgs4v
Nov  5 17:40:21.309: INFO: Got endpoints: latency-svc-gl4xg [748.756642ms]
Nov  5 17:40:21.330: INFO: Created: latency-svc-dg892
Nov  5 17:40:21.360: INFO: Got endpoints: latency-svc-c42q7 [750.099218ms]
Nov  5 17:40:21.381: INFO: Created: latency-svc-ngrdv
Nov  5 17:40:21.404: INFO: Got endpoints: latency-svc-vjtmm [746.322967ms]
Nov  5 17:40:21.426: INFO: Created: latency-svc-7xhxp
Nov  5 17:40:21.458: INFO: Got endpoints: latency-svc-wdhxl [750.005399ms]
Nov  5 17:40:21.479: INFO: Created: latency-svc-x5hp7
Nov  5 17:40:21.507: INFO: Got endpoints: latency-svc-gkw8x [744.135678ms]
Nov  5 17:40:21.531: INFO: Created: latency-svc-hqdxd
Nov  5 17:40:21.558: INFO: Got endpoints: latency-svc-krdlp [750.471404ms]
Nov  5 17:40:21.581: INFO: Created: latency-svc-9r8l2
Nov  5 17:40:21.606: INFO: Got endpoints: latency-svc-cbsbs [745.181534ms]
Nov  5 17:40:21.626: INFO: Created: latency-svc-jb6h2
Nov  5 17:40:21.656: INFO: Got endpoints: latency-svc-bgvsd [747.033982ms]
Nov  5 17:40:21.675: INFO: Created: latency-svc-zn9x5
Nov  5 17:40:21.707: INFO: Got endpoints: latency-svc-4d762 [751.180573ms]
Nov  5 17:40:21.733: INFO: Created: latency-svc-4jtdp
Nov  5 17:40:21.757: INFO: Got endpoints: latency-svc-xt4ms [749.227421ms]
Nov  5 17:40:21.777: INFO: Created: latency-svc-5rmkp
Nov  5 17:40:21.809: INFO: Got endpoints: latency-svc-llqwn [749.347738ms]
Nov  5 17:40:21.833: INFO: Created: latency-svc-g9lp8
Nov  5 17:40:21.860: INFO: Got endpoints: latency-svc-k72zv [752.3922ms]
Nov  5 17:40:21.883: INFO: Created: latency-svc-v6sqb
Nov  5 17:40:21.910: INFO: Got endpoints: latency-svc-p9r4h [753.700889ms]
Nov  5 17:40:21.935: INFO: Created: latency-svc-54d62
Nov  5 17:40:21.959: INFO: Got endpoints: latency-svc-v7sfv [746.645646ms]
Nov  5 17:40:22.007: INFO: Got endpoints: latency-svc-rgs4v [749.576605ms]
Nov  5 17:40:22.059: INFO: Got endpoints: latency-svc-dg892 [749.960463ms]
Nov  5 17:40:22.110: INFO: Got endpoints: latency-svc-ngrdv [749.554455ms]
Nov  5 17:40:22.159: INFO: Got endpoints: latency-svc-7xhxp [754.631399ms]
Nov  5 17:40:22.208: INFO: Got endpoints: latency-svc-x5hp7 [750.17744ms]
Nov  5 17:40:22.258: INFO: Got endpoints: latency-svc-hqdxd [750.817176ms]
Nov  5 17:40:22.307: INFO: Got endpoints: latency-svc-9r8l2 [748.800705ms]
Nov  5 17:40:22.357: INFO: Got endpoints: latency-svc-jb6h2 [750.525231ms]
Nov  5 17:40:22.409: INFO: Got endpoints: latency-svc-zn9x5 [752.491478ms]
Nov  5 17:40:22.450: INFO: Created: latency-svc-ftj4z
Nov  5 17:40:22.461: INFO: Got endpoints: latency-svc-4jtdp [753.760702ms]
Nov  5 17:40:22.471: INFO: Created: latency-svc-4ffgp
Nov  5 17:40:22.479: INFO: Created: latency-svc-7z8ps
Nov  5 17:40:22.489: INFO: Created: latency-svc-z2rkf
Nov  5 17:40:22.499: INFO: Created: latency-svc-z6f59
Nov  5 17:40:22.506: INFO: Got endpoints: latency-svc-5rmkp [749.345938ms]
Nov  5 17:40:22.509: INFO: Created: latency-svc-9w6pt
Nov  5 17:40:22.517: INFO: Created: latency-svc-r77lq
Nov  5 17:40:22.526: INFO: Created: latency-svc-xmnd9
Nov  5 17:40:22.541: INFO: Created: latency-svc-vb72n
Nov  5 17:40:22.553: INFO: Created: latency-svc-mfbjf
Nov  5 17:40:22.558: INFO: Got endpoints: latency-svc-g9lp8 [748.988349ms]
Nov  5 17:40:22.564: INFO: Created: latency-svc-gbrnk
Nov  5 17:40:22.575: INFO: Created: latency-svc-chm4j
Nov  5 17:40:22.587: INFO: Created: latency-svc-j79b6
Nov  5 17:40:22.612: INFO: Got endpoints: latency-svc-v6sqb [751.167525ms]
Nov  5 17:40:22.634: INFO: Created: latency-svc-kkm7k
Nov  5 17:40:22.659: INFO: Got endpoints: latency-svc-54d62 [748.346601ms]
Nov  5 17:40:22.694: INFO: Created: latency-svc-zh22d
Nov  5 17:40:22.707: INFO: Got endpoints: latency-svc-ftj4z [748.815387ms]
Nov  5 17:40:22.747: INFO: Created: latency-svc-rbght
Nov  5 17:40:22.758: INFO: Got endpoints: latency-svc-4ffgp [750.79759ms]
Nov  5 17:40:22.803: INFO: Created: latency-svc-k2bnx
Nov  5 17:40:22.808: INFO: Got endpoints: latency-svc-7z8ps [748.21122ms]
Nov  5 17:40:22.868: INFO: Created: latency-svc-nqf4p
Nov  5 17:40:22.880: INFO: Got endpoints: latency-svc-z2rkf [769.968252ms]
Nov  5 17:40:22.908: INFO: Got endpoints: latency-svc-z6f59 [748.734854ms]
Nov  5 17:40:22.929: INFO: Created: latency-svc-vvnr7
Nov  5 17:40:22.943: INFO: Created: latency-svc-nw2k4
Nov  5 17:40:22.968: INFO: Got endpoints: latency-svc-9w6pt [759.624678ms]
Nov  5 17:40:23.006: INFO: Created: latency-svc-ks29q
Nov  5 17:40:23.006: INFO: Got endpoints: latency-svc-r77lq [747.739842ms]
Nov  5 17:40:23.033: INFO: Created: latency-svc-ttzj7
Nov  5 17:40:23.586: INFO: Got endpoints: latency-svc-chm4j [1.079375453s]
Nov  5 17:40:23.587: INFO: Got endpoints: latency-svc-xmnd9 [1.278943334s]
Nov  5 17:40:23.588: INFO: Got endpoints: latency-svc-vb72n [1.230965354s]
Nov  5 17:40:23.588: INFO: Got endpoints: latency-svc-mfbjf [1.179114045s]
Nov  5 17:40:23.589: INFO: Got endpoints: latency-svc-kkm7k [977.275989ms]
Nov  5 17:40:23.589: INFO: Got endpoints: latency-svc-j79b6 [1.031271093s]
Nov  5 17:40:23.590: INFO: Got endpoints: latency-svc-gbrnk [1.12836858s]
Nov  5 17:40:23.597: INFO: Got endpoints: latency-svc-nqf4p [789.145393ms]
Nov  5 17:40:23.597: INFO: Got endpoints: latency-svc-rbght [889.5992ms]
Nov  5 17:40:23.597: INFO: Got endpoints: latency-svc-zh22d [938.483397ms]
Nov  5 17:40:23.598: INFO: Got endpoints: latency-svc-k2bnx [838.9478ms]
Nov  5 17:40:23.632: INFO: Created: latency-svc-bv6cv
Nov  5 17:40:23.632: INFO: Got endpoints: latency-svc-vvnr7 [752.450785ms]
Nov  5 17:40:23.641: INFO: Created: latency-svc-77dvl
Nov  5 17:40:23.654: INFO: Created: latency-svc-xsj7l
Nov  5 17:40:23.656: INFO: Got endpoints: latency-svc-nw2k4 [747.48857ms]
Nov  5 17:40:23.664: INFO: Created: latency-svc-8jqfl
Nov  5 17:40:23.685: INFO: Created: latency-svc-njcsw
Nov  5 17:40:23.687: INFO: Created: latency-svc-grmms
Nov  5 17:40:23.699: INFO: Created: latency-svc-q27vd
Nov  5 17:40:23.706: INFO: Got endpoints: latency-svc-ks29q [737.835657ms]
Nov  5 17:40:23.709: INFO: Created: latency-svc-dn4qr
Nov  5 17:40:23.719: INFO: Created: latency-svc-6tpzx
Nov  5 17:40:23.729: INFO: Created: latency-svc-m4mv6
Nov  5 17:40:23.738: INFO: Created: latency-svc-2xz55
Nov  5 17:40:23.752: INFO: Created: latency-svc-wx5zb
Nov  5 17:40:23.754: INFO: Got endpoints: latency-svc-ttzj7 [747.654913ms]
Nov  5 17:40:23.764: INFO: Created: latency-svc-2cr6h
Nov  5 17:40:23.769: INFO: Created: latency-svc-s8m5t
Nov  5 17:40:23.789: INFO: Created: latency-svc-nvkvq
Nov  5 17:40:23.808: INFO: Got endpoints: latency-svc-bv6cv [221.95656ms]
Nov  5 17:40:23.829: INFO: Created: latency-svc-7g2ck
Nov  5 17:40:23.856: INFO: Got endpoints: latency-svc-77dvl [269.751255ms]
Nov  5 17:40:23.888: INFO: Created: latency-svc-5bxgr
Nov  5 17:40:23.907: INFO: Got endpoints: latency-svc-xsj7l [319.412181ms]
Nov  5 17:40:23.930: INFO: Created: latency-svc-rjnsg
Nov  5 17:40:23.961: INFO: Got endpoints: latency-svc-8jqfl [371.965647ms]
Nov  5 17:40:23.981: INFO: Created: latency-svc-bnjd2
Nov  5 17:40:24.008: INFO: Got endpoints: latency-svc-njcsw [419.165104ms]
Nov  5 17:40:24.028: INFO: Created: latency-svc-bg6mp
Nov  5 17:40:24.058: INFO: Got endpoints: latency-svc-grmms [468.468783ms]
Nov  5 17:40:24.082: INFO: Created: latency-svc-wb4hd
Nov  5 17:40:24.111: INFO: Got endpoints: latency-svc-q27vd [521.742517ms]
Nov  5 17:40:24.133: INFO: Created: latency-svc-lhp7j
Nov  5 17:40:24.159: INFO: Got endpoints: latency-svc-dn4qr [562.045746ms]
Nov  5 17:40:24.178: INFO: Created: latency-svc-54qtm
Nov  5 17:40:24.209: INFO: Got endpoints: latency-svc-6tpzx [611.16619ms]
Nov  5 17:40:24.229: INFO: Created: latency-svc-zqq8w
Nov  5 17:40:24.260: INFO: Got endpoints: latency-svc-m4mv6 [662.760314ms]
Nov  5 17:40:24.281: INFO: Created: latency-svc-d2627
Nov  5 17:40:24.314: INFO: Got endpoints: latency-svc-2xz55 [716.714832ms]
Nov  5 17:40:24.334: INFO: Created: latency-svc-j8g9d
Nov  5 17:40:24.359: INFO: Got endpoints: latency-svc-wx5zb [726.17755ms]
Nov  5 17:40:24.383: INFO: Created: latency-svc-9l9nc
Nov  5 17:40:24.407: INFO: Got endpoints: latency-svc-2cr6h [751.530236ms]
Nov  5 17:40:24.429: INFO: Created: latency-svc-g2nh2
Nov  5 17:40:24.456: INFO: Got endpoints: latency-svc-s8m5t [749.815437ms]
Nov  5 17:40:24.476: INFO: Created: latency-svc-6755s
Nov  5 17:40:24.507: INFO: Got endpoints: latency-svc-nvkvq [753.784517ms]
Nov  5 17:40:24.527: INFO: Created: latency-svc-jbqhk
Nov  5 17:40:24.568: INFO: Got endpoints: latency-svc-7g2ck [760.076616ms]
Nov  5 17:40:24.599: INFO: Created: latency-svc-fvxrn
Nov  5 17:40:24.607: INFO: Got endpoints: latency-svc-5bxgr [750.467002ms]
Nov  5 17:40:24.636: INFO: Created: latency-svc-sqp4d
Nov  5 17:40:24.659: INFO: Got endpoints: latency-svc-rjnsg [751.773766ms]
Nov  5 17:40:24.692: INFO: Created: latency-svc-5dsfj
Nov  5 17:40:24.715: INFO: Got endpoints: latency-svc-bnjd2 [752.909059ms]
Nov  5 17:40:24.753: INFO: Created: latency-svc-2vm9d
Nov  5 17:40:24.757: INFO: Got endpoints: latency-svc-bg6mp [749.787933ms]
Nov  5 17:40:24.792: INFO: Created: latency-svc-8xrmx
Nov  5 17:40:24.828: INFO: Got endpoints: latency-svc-wb4hd [770.314681ms]
Nov  5 17:40:24.866: INFO: Created: latency-svc-pgxzr
Nov  5 17:40:24.868: INFO: Got endpoints: latency-svc-lhp7j [756.100211ms]
Nov  5 17:40:24.952: INFO: Got endpoints: latency-svc-54qtm [792.340182ms]
Nov  5 17:40:24.952: INFO: Created: latency-svc-99tsk
Nov  5 17:40:24.973: INFO: Got endpoints: latency-svc-zqq8w [764.390668ms]
Nov  5 17:40:25.006: INFO: Created: latency-svc-qpgk8
Nov  5 17:40:25.008: INFO: Got endpoints: latency-svc-d2627 [747.537761ms]
Nov  5 17:40:25.038: INFO: Created: latency-svc-b9mwm
Nov  5 17:40:25.060: INFO: Created: latency-svc-vg2tn
Nov  5 17:40:25.062: INFO: Got endpoints: latency-svc-j8g9d [747.830565ms]
Nov  5 17:40:25.086: INFO: Created: latency-svc-wfnwd
Nov  5 17:40:25.109: INFO: Got endpoints: latency-svc-9l9nc [750.595198ms]
Nov  5 17:40:25.133: INFO: Created: latency-svc-dk96f
Nov  5 17:40:25.158: INFO: Got endpoints: latency-svc-g2nh2 [750.449435ms]
Nov  5 17:40:25.180: INFO: Created: latency-svc-hjxfg
Nov  5 17:40:25.207: INFO: Got endpoints: latency-svc-6755s [750.930291ms]
Nov  5 17:40:25.230: INFO: Created: latency-svc-n4h4s
Nov  5 17:40:25.260: INFO: Got endpoints: latency-svc-jbqhk [752.113267ms]
Nov  5 17:40:25.283: INFO: Created: latency-svc-spbd7
Nov  5 17:40:25.310: INFO: Got endpoints: latency-svc-fvxrn [741.600646ms]
Nov  5 17:40:25.331: INFO: Created: latency-svc-9jgbd
Nov  5 17:40:25.359: INFO: Got endpoints: latency-svc-sqp4d [751.175838ms]
Nov  5 17:40:25.386: INFO: Created: latency-svc-zlhnm
Nov  5 17:40:25.409: INFO: Got endpoints: latency-svc-5dsfj [750.033863ms]
Nov  5 17:40:25.438: INFO: Created: latency-svc-gpcj9
Nov  5 17:40:25.459: INFO: Got endpoints: latency-svc-2vm9d [743.987453ms]
Nov  5 17:40:25.480: INFO: Created: latency-svc-x4752
Nov  5 17:40:25.510: INFO: Got endpoints: latency-svc-8xrmx [752.253794ms]
Nov  5 17:40:25.531: INFO: Created: latency-svc-csgqm
Nov  5 17:40:25.557: INFO: Got endpoints: latency-svc-pgxzr [728.580018ms]
Nov  5 17:40:25.579: INFO: Created: latency-svc-hxf4g
Nov  5 17:40:25.612: INFO: Got endpoints: latency-svc-99tsk [744.408185ms]
Nov  5 17:40:25.631: INFO: Created: latency-svc-cxfrd
Nov  5 17:40:25.658: INFO: Got endpoints: latency-svc-qpgk8 [706.091753ms]
Nov  5 17:40:25.683: INFO: Created: latency-svc-b6qrn
Nov  5 17:40:25.705: INFO: Got endpoints: latency-svc-b9mwm [731.441268ms]
Nov  5 17:40:25.738: INFO: Created: latency-svc-hgf2w
Nov  5 17:40:25.759: INFO: Got endpoints: latency-svc-vg2tn [750.871892ms]
Nov  5 17:40:25.808: INFO: Got endpoints: latency-svc-wfnwd [746.173775ms]
Nov  5 17:40:25.858: INFO: Got endpoints: latency-svc-dk96f [748.045956ms]
Nov  5 17:40:25.907: INFO: Got endpoints: latency-svc-hjxfg [749.041955ms]
Nov  5 17:40:25.958: INFO: Got endpoints: latency-svc-n4h4s [750.395591ms]
Nov  5 17:40:26.009: INFO: Got endpoints: latency-svc-spbd7 [748.607098ms]
Nov  5 17:40:26.060: INFO: Got endpoints: latency-svc-9jgbd [750.041031ms]
Nov  5 17:40:26.109: INFO: Got endpoints: latency-svc-zlhnm [749.911917ms]
Nov  5 17:40:26.160: INFO: Got endpoints: latency-svc-gpcj9 [750.437849ms]
Nov  5 17:40:26.209: INFO: Got endpoints: latency-svc-x4752 [750.172994ms]
Nov  5 17:40:26.258: INFO: Got endpoints: latency-svc-csgqm [747.539815ms]
Nov  5 17:40:26.308: INFO: Got endpoints: latency-svc-hxf4g [750.284488ms]
Nov  5 17:40:26.361: INFO: Got endpoints: latency-svc-cxfrd [748.174832ms]
Nov  5 17:40:26.409: INFO: Got endpoints: latency-svc-b6qrn [751.106711ms]
Nov  5 17:40:26.457: INFO: Got endpoints: latency-svc-hgf2w [752.447619ms]
Nov  5 17:40:26.457: INFO: Latencies: [29.453669ms 39.260254ms 52.093862ms 62.58633ms 71.165737ms 84.123825ms 97.241634ms 112.6754ms 120.599895ms 136.143142ms 143.285361ms 156.624813ms 169.236696ms 172.671413ms 174.36396ms 174.698807ms 175.762407ms 178.306349ms 179.921688ms 180.209572ms 180.982769ms 181.04018ms 181.349334ms 182.168581ms 183.228875ms 183.461616ms 183.480872ms 183.730378ms 183.761119ms 184.439121ms 184.562793ms 186.880904ms 188.344826ms 188.388127ms 188.650168ms 189.250308ms 191.213804ms 204.25304ms 210.701687ms 221.95656ms 228.222056ms 269.097429ms 269.751255ms 307.536617ms 319.412181ms 343.712794ms 371.965647ms 388.4785ms 419.165104ms 428.280445ms 463.82302ms 468.468783ms 505.966508ms 521.742517ms 525.476832ms 535.61862ms 562.045746ms 585.938619ms 611.16619ms 636.680844ms 662.760314ms 677.167128ms 706.091753ms 715.612973ms 716.714832ms 726.17755ms 728.580018ms 731.441268ms 737.835657ms 741.600646ms 742.243422ms 743.987453ms 744.135678ms 744.408185ms 745.181534ms 745.726181ms 745.931807ms 746.173775ms 746.322967ms 746.645646ms 747.033982ms 747.397793ms 747.48857ms 747.537761ms 747.539815ms 747.654913ms 747.659332ms 747.739842ms 747.830565ms 748.045956ms 748.081388ms 748.174832ms 748.21122ms 748.346601ms 748.451418ms 748.607098ms 748.647401ms 748.734854ms 748.737551ms 748.756642ms 748.800705ms 748.815387ms 748.865412ms 748.944535ms 748.988349ms 749.009931ms 749.041955ms 749.150811ms 749.227421ms 749.345938ms 749.347738ms 749.438798ms 749.487055ms 749.554455ms 749.576605ms 749.787933ms 749.815437ms 749.876254ms 749.911917ms 749.960463ms 750.005399ms 750.033863ms 750.041031ms 750.099218ms 750.172994ms 750.17744ms 750.284488ms 750.395591ms 750.437849ms 750.449435ms 750.467002ms 750.471404ms 750.525231ms 750.595198ms 750.766694ms 750.79759ms 750.817176ms 750.85114ms 750.871892ms 750.930291ms 750.953385ms 751.106711ms 751.167525ms 751.175838ms 751.180573ms 751.530236ms 751.628937ms 751.773766ms 751.959895ms 751.999441ms 752.056492ms 752.113267ms 752.253794ms 752.3922ms 752.447619ms 752.450785ms 752.491478ms 752.908315ms 752.909059ms 753.319011ms 753.700889ms 753.760702ms 753.784517ms 753.914795ms 754.029251ms 754.631399ms 756.100211ms 759.624678ms 760.076616ms 763.097585ms 764.390668ms 769.968252ms 770.314681ms 789.145393ms 789.959753ms 792.340182ms 796.490478ms 797.137205ms 798.424203ms 799.736291ms 800.156257ms 800.974515ms 801.269881ms 801.340255ms 801.802488ms 802.585539ms 802.604211ms 802.712179ms 803.209798ms 804.854297ms 838.9478ms 889.5992ms 938.483397ms 977.275989ms 1.031271093s 1.079375453s 1.12836858s 1.179114045s 1.230965354s 1.278943334s]
Nov  5 17:40:26.458: INFO: 50 %ile: 748.800705ms
Nov  5 17:40:26.458: INFO: 90 %ile: 800.156257ms
Nov  5 17:40:26.458: INFO: 99 %ile: 1.230965354s
Nov  5 17:40:26.458: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:40:26.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7473" for this suite.

• [SLOW TEST:12.932 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":42,"skipped":792,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:40:26.490: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:40:30.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7731" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":43,"skipped":794,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:40:30.728: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-445361a9-6842-4322-8395-c095a08f216e
STEP: Creating a pod to test consume secrets
Nov  5 17:40:30.855: INFO: Waiting up to 5m0s for pod "pod-secrets-ed4c6fff-ecf3-41a9-a2e2-2fa227df2ccc" in namespace "secrets-902" to be "Succeeded or Failed"
Nov  5 17:40:30.862: INFO: Pod "pod-secrets-ed4c6fff-ecf3-41a9-a2e2-2fa227df2ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.860572ms
Nov  5 17:40:32.873: INFO: Pod "pod-secrets-ed4c6fff-ecf3-41a9-a2e2-2fa227df2ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017550153s
Nov  5 17:40:34.880: INFO: Pod "pod-secrets-ed4c6fff-ecf3-41a9-a2e2-2fa227df2ccc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025069468s
STEP: Saw pod success
Nov  5 17:40:34.881: INFO: Pod "pod-secrets-ed4c6fff-ecf3-41a9-a2e2-2fa227df2ccc" satisfied condition "Succeeded or Failed"
Nov  5 17:40:34.891: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-secrets-ed4c6fff-ecf3-41a9-a2e2-2fa227df2ccc container secret-volume-test: <nil>
STEP: delete the pod
Nov  5 17:40:34.987: INFO: Waiting for pod pod-secrets-ed4c6fff-ecf3-41a9-a2e2-2fa227df2ccc to disappear
Nov  5 17:40:34.993: INFO: Pod pod-secrets-ed4c6fff-ecf3-41a9-a2e2-2fa227df2ccc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:40:34.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-902" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":44,"skipped":798,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:40:35.015: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-7257
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov  5 17:40:35.092: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov  5 17:40:35.147: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 17:40:37.160: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 17:40:39.156: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:40:41.156: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:40:43.155: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:40:45.161: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:40:47.158: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:40:49.157: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:40:51.157: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:40:53.159: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:40:55.157: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 17:40:57.160: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov  5 17:40:57.177: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov  5 17:41:01.228: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.52:8080/dial?request=hostname&protocol=http&host=10.2.0.21&port=8080&tries=1'] Namespace:pod-network-test-7257 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 17:41:01.228: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 17:41:01.439: INFO: Waiting for responses: map[]
Nov  5 17:41:01.447: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.52:8080/dial?request=hostname&protocol=http&host=10.2.1.51&port=8080&tries=1'] Namespace:pod-network-test-7257 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 17:41:01.447: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 17:41:01.649: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:41:01.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7257" for this suite.

• [SLOW TEST:26.660 seconds]
[sig-network] Networking
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":45,"skipped":800,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:41:01.677: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Nov  5 17:41:01.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 cluster-info'
Nov  5 17:41:01.894: INFO: stderr: ""
Nov  5 17:41:01.894: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:41:01.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2609" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":46,"skipped":813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:41:01.922: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2978.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2978.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2978.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2978.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2978.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2978.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  5 17:41:16.140: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2978/dns-test-0508e369-20c6-4cb7-9327-ba00d4d6071a: the server could not find the requested resource (get pods dns-test-0508e369-20c6-4cb7-9327-ba00d4d6071a)
Nov  5 17:41:16.140: INFO: Lookups using dns-2978/dns-test-0508e369-20c6-4cb7-9327-ba00d4d6071a failed for: [jessie_tcp@PodARecord]

Nov  5 17:41:21.220: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2978/dns-test-0508e369-20c6-4cb7-9327-ba00d4d6071a: the server could not find the requested resource (get pods dns-test-0508e369-20c6-4cb7-9327-ba00d4d6071a)
Nov  5 17:41:21.220: INFO: Lookups using dns-2978/dns-test-0508e369-20c6-4cb7-9327-ba00d4d6071a failed for: [jessie_tcp@PodARecord]

Nov  5 17:41:26.233: INFO: DNS probes using dns-2978/dns-test-0508e369-20c6-4cb7-9327-ba00d4d6071a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:41:26.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2978" for this suite.

• [SLOW TEST:24.451 seconds]
[sig-network] DNS
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":47,"skipped":876,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:41:26.374: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 17:41:26.453: INFO: Creating deployment "webserver-deployment"
Nov  5 17:41:26.463: INFO: Waiting for observed generation 1
Nov  5 17:41:28.490: INFO: Waiting for all required pods to come up
Nov  5 17:41:28.502: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov  5 17:41:40.522: INFO: Waiting for deployment "webserver-deployment" to complete
Nov  5 17:41:40.537: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov  5 17:41:40.562: INFO: Updating deployment webserver-deployment
Nov  5 17:41:40.562: INFO: Waiting for observed generation 2
Nov  5 17:41:42.579: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov  5 17:41:42.585: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov  5 17:41:42.593: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov  5 17:41:42.615: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov  5 17:41:42.616: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov  5 17:41:42.622: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov  5 17:41:42.636: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov  5 17:41:42.636: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov  5 17:41:42.655: INFO: Updating deployment webserver-deployment
Nov  5 17:41:42.655: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov  5 17:41:42.669: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov  5 17:41:42.679: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov  5 17:41:42.701: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1379 /apis/apps/v1/namespaces/deployment-1379/deployments/webserver-deployment 8ce620fe-f967-4c4b-86c7-89fc0eb46d0e 1646720579 3 2020-11-05 17:41:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005504bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-11-05 17:41:40 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-05 17:41:42 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov  5 17:41:42.744: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1379 /apis/apps/v1/namespaces/deployment-1379/replicasets/webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 1646720569 3 2020-11-05 17:41:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8ce620fe-f967-4c4b-86c7-89fc0eb46d0e 0xc005505087 0xc005505088}] []  [{kube-controller-manager Update apps/v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ce620fe-f967-4c4b-86c7-89fc0eb46d0e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005505108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov  5 17:41:42.744: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov  5 17:41:42.744: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-1379 /apis/apps/v1/namespaces/deployment-1379/replicasets/webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 1646720564 3 2020-11-05 17:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8ce620fe-f967-4c4b-86c7-89fc0eb46d0e 0xc005505167 0xc005505168}] []  [{kube-controller-manager Update apps/v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ce620fe-f967-4c4b-86c7-89fc0eb46d0e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055051d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov  5 17:41:42.810: INFO: Pod "webserver-deployment-795d758f88-4bd2t" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4bd2t webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-4bd2t d32a72f1-fe1f-406a-b2c1-6e3bb5837233 1646720359 0 2020-11-05 17:41:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.2.1.61/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc001cd9397 0xc001cd9398}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-05 17:41:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:,StartTime:2020-11-05 17:41:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.810: INFO: Pod "webserver-deployment-795d758f88-4g67j" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4g67j webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-4g67j 1e9ff17e-43d3-4fcb-b8d1-fa64e1cde687 1646720693 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc001cd9557 0xc001cd9558}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.810: INFO: Pod "webserver-deployment-795d758f88-4xb25" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4xb25 webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-4xb25 b287046d-86f4-4ad6-8ffc-dc5a0169ba95 1646720665 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc001cd9687 0xc001cd9688}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.811: INFO: Pod "webserver-deployment-795d758f88-5qmj8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5qmj8 webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-5qmj8 4d32ad09-92ae-498b-839b-a26734c46990 1646720679 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc001cd97b7 0xc001cd97b8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.83.35.110,PodIP:,StartTime:2020-11-05 17:41:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.811: INFO: Pod "webserver-deployment-795d758f88-7xbcn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7xbcn webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-7xbcn 06f91abe-f2be-4019-bf4d-ac9d900edd41 1646720671 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc001cd9947 0xc001cd9948}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.811: INFO: Pod "webserver-deployment-795d758f88-br9ww" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-br9ww webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-br9ww 7a34f169-1a5e-4e02-ac91-629edcaa8cc9 1646720256 0 2020-11-05 17:41:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.2.1.60/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc001cd9a87 0xc001cd9a88}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-05 17:41:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-11-05 17:41:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:,StartTime:2020-11-05 17:41:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.812: INFO: Pod "webserver-deployment-795d758f88-cwp2s" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cwp2s webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-cwp2s 186ec3b4-80ec-410c-b3ce-64ec577670ca 1646720651 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc001cd9c37 0xc001cd9c38}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.812: INFO: Pod "webserver-deployment-795d758f88-h5nws" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-h5nws webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-h5nws 5aeddc96-c316-436f-a2ef-df6a189a24ad 1646720294 0 2020-11-05 17:41:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.2.0.27/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc001cd9d77 0xc001cd9d78}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-05 17:41:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.83.35.110,PodIP:,StartTime:2020-11-05 17:41:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.812: INFO: Pod "webserver-deployment-795d758f88-hlcl4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hlcl4 webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-hlcl4 eabcef70-b808-4bca-a886-7ad6319be1c1 1646720678 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc001cd9f27 0xc001cd9f28}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.812: INFO: Pod "webserver-deployment-795d758f88-klgdf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-klgdf webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-klgdf 93ea6aec-c109-4664-b52a-3a7393aff2af 1646720670 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc005808057 0xc005808058}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.812: INFO: Pod "webserver-deployment-795d758f88-sll4f" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sll4f webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-sll4f 44cdb505-5772-400b-a7d1-db33a0816c65 1646720690 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc005808187 0xc005808188}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.813: INFO: Pod "webserver-deployment-795d758f88-x8v6w" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-x8v6w webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-x8v6w 603bfc6b-1543-4411-ad74-90097bc73a0a 1646720257 0 2020-11-05 17:41:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.2.0.26/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc0058082c7 0xc0058082c8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-05 17:41:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-11-05 17:41:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.83.35.110,PodIP:,StartTime:2020-11-05 17:41:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.813: INFO: Pod "webserver-deployment-795d758f88-zdjh4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zdjh4 webserver-deployment-795d758f88- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-795d758f88-zdjh4 9e2e2f6a-d505-4ab0-a9eb-a3019c8b328d 1646720437 0 2020-11-05 17:41:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.2.1.62/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 12508e6e-170a-4edd-a7f0-2ad0d695bf18 0xc005808487 0xc005808488}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12508e6e-170a-4edd-a7f0-2ad0d695bf18\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-05 17:41:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:,StartTime:2020-11-05 17:41:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.813: INFO: Pod "webserver-deployment-dd94f59b7-2d4p2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2d4p2 webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-2d4p2 e08241ff-4c24-44b3-a50b-4b37c618c053 1646717112 0 2020-11-05 17:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.2.0.23/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005808647 0xc005808648}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 17:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 17:41:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.83.35.110,PodIP:10.2.0.23,StartTime:2020-11-05 17:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 17:41:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://eead13c68a59474ce10e3ba4aa7eb8a176f06cd451704ea7cc043733c00d4fb1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.814: INFO: Pod "webserver-deployment-dd94f59b7-4qdkb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4qdkb webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-4qdkb 1cdd1e51-5e5f-4e87-8b27-8bfeb1e39495 1646720643 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc0058087f7 0xc0058087f8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.814: INFO: Pod "webserver-deployment-dd94f59b7-4xlt2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4xlt2 webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-4xlt2 17a5372b-3d11-4592-98a7-e2ccadec7fca 1646716425 0 2020-11-05 17:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.2.0.22/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005808927 0xc005808928}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 17:41:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 17:41:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.83.35.110,PodIP:10.2.0.22,StartTime:2020-11-05 17:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 17:41:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8d8b43783080ad3b175920bab6f522ba417f62d4bafb3752d85e756bad005368,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.815: INFO: Pod "webserver-deployment-dd94f59b7-6cmbn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6cmbn webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-6cmbn 0dd61e80-f874-4253-90a8-f01c323a8ab2 1646720676 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005808ad7 0xc005808ad8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:,StartTime:2020-11-05 17:41:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.815: INFO: Pod "webserver-deployment-dd94f59b7-6mdgp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6mdgp webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-6mdgp 75ba4d76-5c90-4fc0-8c15-39e1ed169f53 1646720668 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005808c47 0xc005808c48}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.815: INFO: Pod "webserver-deployment-dd94f59b7-6njkl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6njkl webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-6njkl 454272c6-0066-4a4e-a225-3c2d9dcd8e81 1646718485 0 2020-11-05 17:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.2.0.25/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005808d77 0xc005808d78}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 17:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 17:41:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.83.35.110,PodIP:10.2.0.25,StartTime:2020-11-05 17:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 17:41:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8df59c64a47d1a43a8e7c266e48f844a55258d1f3a03e5d87f9c5e5a97a397df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.816: INFO: Pod "webserver-deployment-dd94f59b7-6qwgb" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6qwgb webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-6qwgb 40497097-9d86-408d-a0ed-f8952fd7ec2d 1646717975 0 2020-11-05 17:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.2.1.56/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005808f37 0xc005808f38}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 17:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 17:41:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:10.2.1.56,StartTime:2020-11-05 17:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 17:41:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7518222f6afafb90fd7a71e31349861db61ec61e5c13d495ca0fb12f3b87e260,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.816: INFO: Pod "webserver-deployment-dd94f59b7-82494" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-82494 webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-82494 9c7fab22-8b74-4c88-a6ea-6e13fcd1866a 1646720675 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc0058090e7 0xc0058090e8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.816: INFO: Pod "webserver-deployment-dd94f59b7-8jn7q" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8jn7q webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-8jn7q 8db64c21-da43-4bd0-a3ae-34b7efc3b26f 1646720639 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005809207 0xc005809208}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:,StartTime:2020-11-05 17:41:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.816: INFO: Pod "webserver-deployment-dd94f59b7-9w6p6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9w6p6 webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-9w6p6 64f8b346-008c-4feb-af84-b568a92f49f0 1646720664 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005809377 0xc005809378}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.817: INFO: Pod "webserver-deployment-dd94f59b7-ccv7h" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ccv7h webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-ccv7h 5f24c91f-9051-4328-97d9-29ff804360a9 1646720691 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc0058094a7 0xc0058094a8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.817: INFO: Pod "webserver-deployment-dd94f59b7-gb8qh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gb8qh webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-gb8qh 3a977f63-58b3-40bd-be54-59d206d56b6a 1646720695 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc0058095c7 0xc0058095c8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.817: INFO: Pod "webserver-deployment-dd94f59b7-kvpnh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kvpnh webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-kvpnh 51cce0a4-5a36-40aa-b9f2-86c8589d7483 1646718245 0 2020-11-05 17:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.2.1.57/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc0058096f7 0xc0058096f8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 17:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 17:41:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:10.2.1.57,StartTime:2020-11-05 17:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 17:41:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3d12a975d6c209d0e95171fb86685f5ba479a989b3b9375ae48d637c1e6ee69c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.817: INFO: Pod "webserver-deployment-dd94f59b7-lv56d" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lv56d webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-lv56d 7d2019fc-e476-4e32-8214-222367b6f75e 1646720589 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc0058098a7 0xc0058098a8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.817: INFO: Pod "webserver-deployment-dd94f59b7-lvph4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lvph4 webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-lvph4 b42557aa-3164-4f1d-ba53-195509ff07e7 1646717262 0 2020-11-05 17:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.2.1.55/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc0058099d7 0xc0058099d8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 17:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 17:41:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:10.2.1.55,StartTime:2020-11-05 17:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 17:41:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://44e909a8c7abd0704261bd4d35bfc9f714df94ccfaef2cd8e56b1ac37d94902a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.818: INFO: Pod "webserver-deployment-dd94f59b7-nkdmz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nkdmz webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-nkdmz 7f3bb78f-d73f-48e8-9849-2ee423102db5 1646720694 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005809b87 0xc005809b88}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.818: INFO: Pod "webserver-deployment-dd94f59b7-vgk4d" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vgk4d webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-vgk4d 20be5d1a-1aa5-4882-85e0-a7f3bd5eb8f4 1646720607 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005809ca7 0xc005809ca8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.818: INFO: Pod "webserver-deployment-dd94f59b7-vvmbg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vvmbg webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-vvmbg 56a00388-0106-46ed-8432-bb1509049b16 1646720674 0 2020-11-05 17:41:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005809dc7 0xc005809dc8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.818: INFO: Pod "webserver-deployment-dd94f59b7-xp2qb" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xp2qb webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-xp2qb 0088913d-8168-486b-b26e-58e086f892a3 1646717801 0 2020-11-05 17:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.2.0.24/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc005809ef7 0xc005809ef8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 17:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 17:41:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-29f9289e-2d09-4bea-972e-88a5817d7ad8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.83.35.110,PodIP:10.2.0.24,StartTime:2020-11-05 17:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 17:41:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2177875dfeb5701d79b1375f7ce2adb8bf3b7b1effa3aa16b518a6ac0ba8e55a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 17:41:42.819: INFO: Pod "webserver-deployment-dd94f59b7-zf6f2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zf6f2 webserver-deployment-dd94f59b7- deployment-1379 /api/v1/namespaces/deployment-1379/pods/webserver-deployment-dd94f59b7-zf6f2 f151877e-7187-4287-ad9e-a396fa984152 1646716566 0 2020-11-05 17:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.2.1.54/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 93ff492e-6146-4c57-beae-13259d0b78a4 0xc0058340b7 0xc0058340b8}] []  [{kube-controller-manager Update v1 2020-11-05 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93ff492e-6146-4c57-beae-13259d0b78a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 17:41:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 17:41:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-78h2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-78h2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-78h2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:10.2.1.54,StartTime:2020-11-05 17:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 17:41:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2a71caf870e050179af59614e9f163b707ee731d4d2e62dc9bb02fcd0c0a1708,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:41:42.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1379" for this suite.

• [SLOW TEST:16.474 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":48,"skipped":881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:41:42.849: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-768
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-768
STEP: creating replication controller externalsvc in namespace services-768
I1105 17:41:43.062681      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-768, replica count: 2
I1105 17:41:46.122283      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:41:49.123081      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:41:52.123466      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:41:55.124208      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:41:58.124524      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:42:01.124946      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:42:04.125321      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov  5 17:42:04.178: INFO: Creating new exec pod
Nov  5 17:42:08.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-768 execpod8lr4s -- /bin/sh -x -c nslookup nodeport-service.services-768.svc.cluster.local'
Nov  5 17:42:08.553: INFO: stderr: "+ nslookup nodeport-service.services-768.svc.cluster.local\n"
Nov  5 17:42:08.553: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nnodeport-service.services-768.svc.cluster.local\tcanonical name = externalsvc.services-768.svc.cluster.local.\nName:\texternalsvc.services-768.svc.cluster.local\nAddress: 10.3.145.184\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-768, will wait for the garbage collector to delete the pods
Nov  5 17:42:08.631: INFO: Deleting ReplicationController externalsvc took: 18.64381ms
Nov  5 17:42:09.431: INFO: Terminating ReplicationController externalsvc pods took: 800.60614ms
Nov  5 17:42:20.374: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:42:20.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-768" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:37.574 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":49,"skipped":929,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:42:20.426: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 17:42:21.190: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 17:42:23.214: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194941, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194941, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194941, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740194941, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 17:42:26.241: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov  5 17:42:30.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 attach --namespace=webhook-1776 to-be-attached-pod -i -c=container1'
Nov  5 17:42:30.490: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:42:30.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1776" for this suite.
STEP: Destroying namespace "webhook-1776-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.220 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":50,"skipped":947,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:42:30.646: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 17:42:30.719: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:42:31.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5440" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":51,"skipped":951,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:42:31.349: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-c6fed4a9-e16d-4a68-82b2-21d52ac0f4e4 in namespace container-probe-5191
Nov  5 17:42:35.489: INFO: Started pod liveness-c6fed4a9-e16d-4a68-82b2-21d52ac0f4e4 in namespace container-probe-5191
STEP: checking the pod's current state and verifying that restartCount is present
Nov  5 17:42:35.497: INFO: Initial restart count of pod liveness-c6fed4a9-e16d-4a68-82b2-21d52ac0f4e4 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:46:36.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5191" for this suite.

• [SLOW TEST:245.603 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":52,"skipped":960,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:46:36.953: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8498
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Nov  5 17:46:37.061: INFO: Found 0 stateful pods, waiting for 3
Nov  5 17:46:47.072: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:46:47.072: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:46:47.072: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov  5 17:46:57.073: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:46:57.073: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:46:57.073: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 17:46:57.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-8498 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov  5 17:46:57.459: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov  5 17:46:57.459: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov  5 17:46:57.459: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov  5 17:47:07.527: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov  5 17:47:17.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-8498 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 17:47:17.915: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov  5 17:47:17.915: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov  5 17:47:17.915: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov  5 17:47:37.968: INFO: Waiting for StatefulSet statefulset-8498/ss2 to complete update
Nov  5 17:47:37.968: INFO: Waiting for Pod statefulset-8498/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Nov  5 17:47:47.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-8498 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov  5 17:47:48.325: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov  5 17:47:48.325: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov  5 17:47:48.325: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov  5 17:47:58.397: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov  5 17:48:08.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-8498 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 17:48:09.823: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov  5 17:48:09.823: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov  5 17:48:09.823: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov  5 17:48:19.927: INFO: Waiting for StatefulSet statefulset-8498/ss2 to complete update
Nov  5 17:48:19.927: INFO: Waiting for Pod statefulset-8498/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov  5 17:48:19.927: INFO: Waiting for Pod statefulset-8498/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov  5 17:48:19.927: INFO: Waiting for Pod statefulset-8498/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov  5 17:48:29.944: INFO: Waiting for StatefulSet statefulset-8498/ss2 to complete update
Nov  5 17:48:29.944: INFO: Waiting for Pod statefulset-8498/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov  5 17:48:39.945: INFO: Waiting for StatefulSet statefulset-8498/ss2 to complete update
Nov  5 17:48:39.946: INFO: Waiting for Pod statefulset-8498/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov  5 17:48:50.189: INFO: Deleting all statefulset in ns statefulset-8498
Nov  5 17:48:50.254: INFO: Scaling statefulset ss2 to 0
Nov  5 17:49:10.324: INFO: Waiting for statefulset status.replicas updated to 0
Nov  5 17:49:10.339: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:49:10.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8498" for this suite.

• [SLOW TEST:153.463 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":53,"skipped":966,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:49:10.423: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 17:49:10.519: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de1ae4d4-8a89-46f5-be1a-4cd4cb833768" in namespace "projected-1272" to be "Succeeded or Failed"
Nov  5 17:49:10.527: INFO: Pod "downwardapi-volume-de1ae4d4-8a89-46f5-be1a-4cd4cb833768": Phase="Pending", Reason="", readiness=false. Elapsed: 7.68883ms
Nov  5 17:49:12.537: INFO: Pod "downwardapi-volume-de1ae4d4-8a89-46f5-be1a-4cd4cb833768": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017101031s
Nov  5 17:49:14.549: INFO: Pod "downwardapi-volume-de1ae4d4-8a89-46f5-be1a-4cd4cb833768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029282301s
STEP: Saw pod success
Nov  5 17:49:14.549: INFO: Pod "downwardapi-volume-de1ae4d4-8a89-46f5-be1a-4cd4cb833768" satisfied condition "Succeeded or Failed"
Nov  5 17:49:14.557: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-de1ae4d4-8a89-46f5-be1a-4cd4cb833768 container client-container: <nil>
STEP: delete the pod
Nov  5 17:49:14.669: INFO: Waiting for pod downwardapi-volume-de1ae4d4-8a89-46f5-be1a-4cd4cb833768 to disappear
Nov  5 17:49:14.679: INFO: Pod downwardapi-volume-de1ae4d4-8a89-46f5-be1a-4cd4cb833768 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:49:14.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1272" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":54,"skipped":969,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:49:14.703: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 17:49:15.450: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 17:49:17.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195355, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195355, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195355, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195355, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 17:49:20.833: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:49:20.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2565" for this suite.
STEP: Destroying namespace "webhook-2565-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.381 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":55,"skipped":969,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:49:21.085: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:49:21.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9589" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":56,"skipped":970,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:49:21.195: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov  5 17:49:21.279: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov  5 17:49:21.300: INFO: Waiting for terminating namespaces to be deleted...
Nov  5 17:49:21.308: INFO: 
Logging pods the apiserver thinks is on node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 before test
Nov  5 17:49:21.322: INFO: canal-55ppt from kube-system started at 2020-11-05 17:15:19 +0000 UTC (2 container statuses recorded)
Nov  5 17:49:21.322: INFO: 	Container calico-node ready: true, restart count 0
Nov  5 17:49:21.322: INFO: 	Container kube-flannel ready: true, restart count 2
Nov  5 17:49:21.322: INFO: coredns-6ccf64844b-zjcgs from kube-system started at 2020-11-05 17:23:56 +0000 UTC (1 container statuses recorded)
Nov  5 17:49:21.323: INFO: 	Container coredns ready: true, restart count 0
Nov  5 17:49:21.323: INFO: kube-proxy-j2wfk from kube-system started at 2020-11-05 17:15:40 +0000 UTC (1 container statuses recorded)
Nov  5 17:49:21.323: INFO: 	Container kube-proxy ready: true, restart count 0
Nov  5 17:49:21.323: INFO: metrics-server-664567f776-8wvsk from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 17:49:21.324: INFO: 	Container metrics-server ready: true, restart count 0
Nov  5 17:49:21.324: INFO: wormhole-2r8fc from kube-system started at 2020-10-19 12:36:30 +0000 UTC (1 container statuses recorded)
Nov  5 17:49:21.324: INFO: 	Container wormhole ready: true, restart count 0
Nov  5 17:49:21.324: INFO: sonobuoy from sonobuoy started at 2020-11-05 17:25:05 +0000 UTC (1 container statuses recorded)
Nov  5 17:49:21.324: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov  5 17:49:21.325: INFO: sonobuoy-e2e-job-6a656b1abb77479a from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 17:49:21.325: INFO: 	Container e2e ready: true, restart count 0
Nov  5 17:49:21.325: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 17:49:21.325: INFO: sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-vv8nw from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 17:49:21.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 17:49:21.326: INFO: 	Container systemd-logs ready: true, restart count 0
Nov  5 17:49:21.326: INFO: 
Logging pods the apiserver thinks is on node node-a7de9392-4cc3-4830-84ff-49e7615b291e before test
Nov  5 17:49:21.340: INFO: canal-sqp5g from kube-system started at 2020-11-05 17:15:35 +0000 UTC (2 container statuses recorded)
Nov  5 17:49:21.340: INFO: 	Container calico-node ready: true, restart count 0
Nov  5 17:49:21.340: INFO: 	Container kube-flannel ready: true, restart count 0
Nov  5 17:49:21.340: INFO: coredns-6ccf64844b-qrpm5 from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 17:49:21.340: INFO: 	Container coredns ready: true, restart count 0
Nov  5 17:49:21.340: INFO: kube-dns-autoscaler-7944596f47-whbv8 from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 17:49:21.341: INFO: 	Container autoscaler ready: true, restart count 0
Nov  5 17:49:21.341: INFO: kube-proxy-bhxd4 from kube-system started at 2020-11-05 17:15:16 +0000 UTC (1 container statuses recorded)
Nov  5 17:49:21.341: INFO: 	Container kube-proxy ready: true, restart count 0
Nov  5 17:49:21.341: INFO: wormhole-4m5c5 from kube-system started at 2020-10-19 12:36:45 +0000 UTC (1 container statuses recorded)
Nov  5 17:49:21.341: INFO: 	Container wormhole ready: true, restart count 0
Nov  5 17:49:21.341: INFO: sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-nf5fs from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 17:49:21.341: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 17:49:21.341: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-582dd2e8-5679-458f-8b9b-7da07a354eb0 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-582dd2e8-5679-458f-8b9b-7da07a354eb0 off the node node-a7de9392-4cc3-4830-84ff-49e7615b291e
STEP: verifying the node doesn't have the label kubernetes.io/e2e-582dd2e8-5679-458f-8b9b-7da07a354eb0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:54:29.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5761" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.373 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":57,"skipped":970,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:54:29.570: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov  5 17:54:29.666: INFO: Waiting up to 5m0s for pod "pod-ac2428ca-49a7-484e-a5c3-5da2014325ed" in namespace "emptydir-6657" to be "Succeeded or Failed"
Nov  5 17:54:29.676: INFO: Pod "pod-ac2428ca-49a7-484e-a5c3-5da2014325ed": Phase="Pending", Reason="", readiness=false. Elapsed: 9.341004ms
Nov  5 17:54:31.684: INFO: Pod "pod-ac2428ca-49a7-484e-a5c3-5da2014325ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018035796s
Nov  5 17:54:33.697: INFO: Pod "pod-ac2428ca-49a7-484e-a5c3-5da2014325ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03058261s
STEP: Saw pod success
Nov  5 17:54:33.697: INFO: Pod "pod-ac2428ca-49a7-484e-a5c3-5da2014325ed" satisfied condition "Succeeded or Failed"
Nov  5 17:54:33.705: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-ac2428ca-49a7-484e-a5c3-5da2014325ed container test-container: <nil>
STEP: delete the pod
Nov  5 17:54:33.805: INFO: Waiting for pod pod-ac2428ca-49a7-484e-a5c3-5da2014325ed to disappear
Nov  5 17:54:33.812: INFO: Pod pod-ac2428ca-49a7-484e-a5c3-5da2014325ed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:54:33.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6657" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":58,"skipped":1009,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:54:33.839: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Nov  5 17:54:33.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-6368'
Nov  5 17:54:34.330: INFO: stderr: ""
Nov  5 17:54:34.330: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov  5 17:54:34.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6368'
Nov  5 17:54:34.924: INFO: stderr: ""
Nov  5 17:54:34.924: INFO: stdout: "update-demo-nautilus-nq5qp update-demo-nautilus-p2mrq "
Nov  5 17:54:34.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-nq5qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:54:35.096: INFO: stderr: ""
Nov  5 17:54:35.096: INFO: stdout: ""
Nov  5 17:54:35.096: INFO: update-demo-nautilus-nq5qp is created but not running
Nov  5 17:54:40.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6368'
Nov  5 17:54:40.241: INFO: stderr: ""
Nov  5 17:54:40.241: INFO: stdout: "update-demo-nautilus-nq5qp update-demo-nautilus-p2mrq "
Nov  5 17:54:40.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-nq5qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:54:40.747: INFO: stderr: ""
Nov  5 17:54:40.747: INFO: stdout: "true"
Nov  5 17:54:40.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-nq5qp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:54:40.895: INFO: stderr: ""
Nov  5 17:54:40.895: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  5 17:54:40.895: INFO: validating pod update-demo-nautilus-nq5qp
Nov  5 17:54:40.905: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  5 17:54:40.905: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  5 17:54:40.905: INFO: update-demo-nautilus-nq5qp is verified up and running
Nov  5 17:54:40.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-p2mrq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:54:41.048: INFO: stderr: ""
Nov  5 17:54:41.048: INFO: stdout: "true"
Nov  5 17:54:41.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-p2mrq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:54:41.178: INFO: stderr: ""
Nov  5 17:54:41.178: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  5 17:54:41.178: INFO: validating pod update-demo-nautilus-p2mrq
Nov  5 17:54:41.209: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  5 17:54:41.209: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  5 17:54:41.209: INFO: update-demo-nautilus-p2mrq is verified up and running
STEP: scaling down the replication controller
Nov  5 17:54:41.213: INFO: scanned /root for discovery docs: <nil>
Nov  5 17:54:41.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6368'
Nov  5 17:54:42.385: INFO: stderr: ""
Nov  5 17:54:42.385: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov  5 17:54:42.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6368'
Nov  5 17:54:42.516: INFO: stderr: ""
Nov  5 17:54:42.516: INFO: stdout: "update-demo-nautilus-nq5qp update-demo-nautilus-p2mrq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov  5 17:54:47.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6368'
Nov  5 17:54:47.656: INFO: stderr: ""
Nov  5 17:54:47.656: INFO: stdout: "update-demo-nautilus-nq5qp update-demo-nautilus-p2mrq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov  5 17:54:52.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6368'
Nov  5 17:54:52.792: INFO: stderr: ""
Nov  5 17:54:52.792: INFO: stdout: "update-demo-nautilus-nq5qp "
Nov  5 17:54:52.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-nq5qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:54:52.917: INFO: stderr: ""
Nov  5 17:54:52.917: INFO: stdout: "true"
Nov  5 17:54:52.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-nq5qp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:54:53.043: INFO: stderr: ""
Nov  5 17:54:53.043: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  5 17:54:53.043: INFO: validating pod update-demo-nautilus-nq5qp
Nov  5 17:54:53.060: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  5 17:54:53.060: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  5 17:54:53.060: INFO: update-demo-nautilus-nq5qp is verified up and running
STEP: scaling up the replication controller
Nov  5 17:54:53.063: INFO: scanned /root for discovery docs: <nil>
Nov  5 17:54:53.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6368'
Nov  5 17:54:54.227: INFO: stderr: ""
Nov  5 17:54:54.227: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov  5 17:54:54.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6368'
Nov  5 17:54:54.363: INFO: stderr: ""
Nov  5 17:54:54.363: INFO: stdout: "update-demo-nautilus-bw9jw update-demo-nautilus-nq5qp "
Nov  5 17:54:54.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-bw9jw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:54:54.478: INFO: stderr: ""
Nov  5 17:54:54.478: INFO: stdout: ""
Nov  5 17:54:54.478: INFO: update-demo-nautilus-bw9jw is created but not running
Nov  5 17:54:59.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6368'
Nov  5 17:54:59.623: INFO: stderr: ""
Nov  5 17:54:59.623: INFO: stdout: "update-demo-nautilus-bw9jw update-demo-nautilus-nq5qp "
Nov  5 17:54:59.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-bw9jw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:54:59.757: INFO: stderr: ""
Nov  5 17:54:59.757: INFO: stdout: "true"
Nov  5 17:54:59.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-bw9jw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:54:59.885: INFO: stderr: ""
Nov  5 17:54:59.885: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  5 17:54:59.885: INFO: validating pod update-demo-nautilus-bw9jw
Nov  5 17:54:59.905: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  5 17:54:59.905: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  5 17:54:59.905: INFO: update-demo-nautilus-bw9jw is verified up and running
Nov  5 17:54:59.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-nq5qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:55:00.029: INFO: stderr: ""
Nov  5 17:55:00.029: INFO: stdout: "true"
Nov  5 17:55:00.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-nq5qp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6368'
Nov  5 17:55:00.145: INFO: stderr: ""
Nov  5 17:55:00.145: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  5 17:55:00.145: INFO: validating pod update-demo-nautilus-nq5qp
Nov  5 17:55:00.156: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  5 17:55:00.156: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  5 17:55:00.156: INFO: update-demo-nautilus-nq5qp is verified up and running
STEP: using delete to clean up resources
Nov  5 17:55:00.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete --grace-period=0 --force -f - --namespace=kubectl-6368'
Nov  5 17:55:00.322: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  5 17:55:00.322: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov  5 17:55:00.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6368'
Nov  5 17:55:00.464: INFO: stderr: "No resources found in kubectl-6368 namespace.\n"
Nov  5 17:55:00.465: INFO: stdout: ""
Nov  5 17:55:00.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -l name=update-demo --namespace=kubectl-6368 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov  5 17:55:00.602: INFO: stderr: ""
Nov  5 17:55:00.603: INFO: stdout: "update-demo-nautilus-bw9jw\nupdate-demo-nautilus-nq5qp\n"
Nov  5 17:55:01.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6368'
Nov  5 17:55:01.260: INFO: stderr: "No resources found in kubectl-6368 namespace.\n"
Nov  5 17:55:01.260: INFO: stdout: ""
Nov  5 17:55:01.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -l name=update-demo --namespace=kubectl-6368 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov  5 17:55:01.412: INFO: stderr: ""
Nov  5 17:55:01.412: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:55:01.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6368" for this suite.

• [SLOW TEST:27.599 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":59,"skipped":1012,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:55:01.438: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 17:55:02.095: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 17:55:04.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195702, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195702, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195702, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195702, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 17:55:07.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:55:07.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5317" for this suite.
STEP: Destroying namespace "webhook-5317-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.710 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":60,"skipped":1031,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:55:08.150: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov  5 17:55:13.300: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:55:13.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9911" for this suite.

• [SLOW TEST:5.201 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":61,"skipped":1072,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:55:13.357: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7818
STEP: creating service affinity-clusterip-transition in namespace services-7818
STEP: creating replication controller affinity-clusterip-transition in namespace services-7818
I1105 17:55:13.491638      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-7818, replica count: 3
I1105 17:55:16.542060      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:55:19.542309      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 17:55:19.557: INFO: Creating new exec pod
Nov  5 17:55:24.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-7818 execpod-affinityj25vr -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Nov  5 17:55:24.962: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov  5 17:55:24.962: INFO: stdout: ""
Nov  5 17:55:24.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-7818 execpod-affinityj25vr -- /bin/sh -x -c nc -zv -t -w 2 10.3.25.250 80'
Nov  5 17:55:25.323: INFO: stderr: "+ nc -zv -t -w 2 10.3.25.250 80\nConnection to 10.3.25.250 80 port [tcp/http] succeeded!\n"
Nov  5 17:55:25.323: INFO: stdout: ""
Nov  5 17:55:25.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-7818 execpod-affinityj25vr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.25.250:80/ ; done'
Nov  5 17:55:26.042: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n"
Nov  5 17:55:26.043: INFO: stdout: "\naffinity-clusterip-transition-fr5vh\naffinity-clusterip-transition-fr5vh\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-fr5vh\naffinity-clusterip-transition-fr5vh\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-fr5vh\naffinity-clusterip-transition-fr5vh\naffinity-clusterip-transition-fr5vh\naffinity-clusterip-transition-98rnb\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-fr5vh\naffinity-clusterip-transition-fr5vh\naffinity-clusterip-transition-98rnb"
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-fr5vh
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-fr5vh
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-fr5vh
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-fr5vh
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-fr5vh
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-fr5vh
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-fr5vh
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-98rnb
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-fr5vh
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-fr5vh
Nov  5 17:55:26.043: INFO: Received response from host: affinity-clusterip-transition-98rnb
Nov  5 17:55:26.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-7818 execpod-affinityj25vr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.25.250:80/ ; done'
Nov  5 17:55:26.545: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.25.250:80/\n"
Nov  5 17:55:26.545: INFO: stdout: "\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw\naffinity-clusterip-transition-76ldw"
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Received response from host: affinity-clusterip-transition-76ldw
Nov  5 17:55:26.546: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7818, will wait for the garbage collector to delete the pods
Nov  5 17:55:26.658: INFO: Deleting ReplicationController affinity-clusterip-transition took: 17.583345ms
Nov  5 17:55:26.758: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.275614ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:55:40.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7818" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:26.873 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":62,"skipped":1086,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:55:40.233: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov  5 17:55:40.390: INFO: Number of nodes with available pods: 0
Nov  5 17:55:40.390: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 17:55:41.409: INFO: Number of nodes with available pods: 0
Nov  5 17:55:41.409: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 17:55:42.668: INFO: Number of nodes with available pods: 0
Nov  5 17:55:42.668: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 17:55:43.415: INFO: Number of nodes with available pods: 0
Nov  5 17:55:43.415: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 17:55:44.410: INFO: Number of nodes with available pods: 0
Nov  5 17:55:44.410: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 17:55:45.406: INFO: Number of nodes with available pods: 2
Nov  5 17:55:45.406: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov  5 17:55:45.463: INFO: Number of nodes with available pods: 1
Nov  5 17:55:45.463: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 17:55:46.486: INFO: Number of nodes with available pods: 1
Nov  5 17:55:46.486: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 17:55:47.487: INFO: Number of nodes with available pods: 1
Nov  5 17:55:47.487: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 17:55:48.487: INFO: Number of nodes with available pods: 1
Nov  5 17:55:48.487: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 17:55:49.485: INFO: Number of nodes with available pods: 2
Nov  5 17:55:49.485: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6713, will wait for the garbage collector to delete the pods
Nov  5 17:55:49.579: INFO: Deleting DaemonSet.extensions daemon-set took: 18.377103ms
Nov  5 17:55:49.680: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.236419ms
Nov  5 17:55:59.790: INFO: Number of nodes with available pods: 0
Nov  5 17:55:59.790: INFO: Number of running nodes: 0, number of available pods: 0
Nov  5 17:55:59.796: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6713/daemonsets","resourceVersion":"1646997036"},"items":null}

Nov  5 17:55:59.801: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6713/pods","resourceVersion":"1646997037"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:55:59.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6713" for this suite.

• [SLOW TEST:19.616 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":63,"skipped":1102,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:55:59.855: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-0cc5d0cd-705a-4ead-9aa6-2e6d10f00a22 in namespace container-probe-4969
Nov  5 17:56:03.959: INFO: Started pod liveness-0cc5d0cd-705a-4ead-9aa6-2e6d10f00a22 in namespace container-probe-4969
STEP: checking the pod's current state and verifying that restartCount is present
Nov  5 17:56:03.966: INFO: Initial restart count of pod liveness-0cc5d0cd-705a-4ead-9aa6-2e6d10f00a22 is 0
Nov  5 17:56:24.068: INFO: Restart count of pod container-probe-4969/liveness-0cc5d0cd-705a-4ead-9aa6-2e6d10f00a22 is now 1 (20.102124522s elapsed)
Nov  5 17:56:44.175: INFO: Restart count of pod container-probe-4969/liveness-0cc5d0cd-705a-4ead-9aa6-2e6d10f00a22 is now 2 (40.209407669s elapsed)
Nov  5 17:57:04.308: INFO: Restart count of pod container-probe-4969/liveness-0cc5d0cd-705a-4ead-9aa6-2e6d10f00a22 is now 3 (1m0.342506564s elapsed)
Nov  5 17:57:24.416: INFO: Restart count of pod container-probe-4969/liveness-0cc5d0cd-705a-4ead-9aa6-2e6d10f00a22 is now 4 (1m20.450264658s elapsed)
Nov  5 17:58:36.804: INFO: Restart count of pod container-probe-4969/liveness-0cc5d0cd-705a-4ead-9aa6-2e6d10f00a22 is now 5 (2m32.83848066s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:58:36.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4969" for this suite.

• [SLOW TEST:156.991 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":1114,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:58:36.846: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-5064
STEP: creating replication controller nodeport-test in namespace services-5064
I1105 17:58:36.964745      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-5064, replica count: 2
I1105 17:58:40.019012      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 17:58:43.019553      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 17:58:43.020: INFO: Creating new exec pod
Nov  5 17:58:48.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-5064 execpodcflcq -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Nov  5 17:58:48.931: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov  5 17:58:48.931: INFO: stdout: ""
Nov  5 17:58:48.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-5064 execpodcflcq -- /bin/sh -x -c nc -zv -t -w 2 10.3.198.208 80'
Nov  5 17:58:49.284: INFO: stderr: "+ nc -zv -t -w 2 10.3.198.208 80\nConnection to 10.3.198.208 80 port [tcp/http] succeeded!\n"
Nov  5 17:58:49.284: INFO: stdout: ""
Nov  5 17:58:49.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-5064 execpodcflcq -- /bin/sh -x -c nc -zv -t -w 2 51.83.35.110 31088'
Nov  5 17:58:49.628: INFO: stderr: "+ nc -zv -t -w 2 51.83.35.110 31088\nConnection to 51.83.35.110 31088 port [tcp/31088] succeeded!\n"
Nov  5 17:58:49.628: INFO: stdout: ""
Nov  5 17:58:49.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-5064 execpodcflcq -- /bin/sh -x -c nc -zv -t -w 2 51.91.151.116 31088'
Nov  5 17:58:49.982: INFO: stderr: "+ nc -zv -t -w 2 51.91.151.116 31088\nConnection to 51.91.151.116 31088 port [tcp/31088] succeeded!\n"
Nov  5 17:58:49.982: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:58:49.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5064" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:13.165 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":65,"skipped":1129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:58:50.017: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-9a7cb31c-aa7a-44cb-acc7-e566ee732a74
STEP: Creating a pod to test consume configMaps
Nov  5 17:58:50.116: INFO: Waiting up to 5m0s for pod "pod-configmaps-cad505f5-93d1-4388-b7d1-8a2fc8de848d" in namespace "configmap-2285" to be "Succeeded or Failed"
Nov  5 17:58:50.127: INFO: Pod "pod-configmaps-cad505f5-93d1-4388-b7d1-8a2fc8de848d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.839965ms
Nov  5 17:58:52.136: INFO: Pod "pod-configmaps-cad505f5-93d1-4388-b7d1-8a2fc8de848d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020515715s
Nov  5 17:58:54.146: INFO: Pod "pod-configmaps-cad505f5-93d1-4388-b7d1-8a2fc8de848d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030151332s
STEP: Saw pod success
Nov  5 17:58:54.146: INFO: Pod "pod-configmaps-cad505f5-93d1-4388-b7d1-8a2fc8de848d" satisfied condition "Succeeded or Failed"
Nov  5 17:58:54.157: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-configmaps-cad505f5-93d1-4388-b7d1-8a2fc8de848d container configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 17:58:54.260: INFO: Waiting for pod pod-configmaps-cad505f5-93d1-4388-b7d1-8a2fc8de848d to disappear
Nov  5 17:58:54.268: INFO: Pod pod-configmaps-cad505f5-93d1-4388-b7d1-8a2fc8de848d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:58:54.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2285" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":1151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:58:54.300: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 17:58:54.382: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov  5 17:58:54.400: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov  5 17:58:59.422: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov  5 17:58:59.422: INFO: Creating deployment "test-rolling-update-deployment"
Nov  5 17:58:59.432: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov  5 17:58:59.449: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov  5 17:59:01.472: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov  5 17:59:01.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195939, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195939, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195939, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740195939, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 17:59:03.491: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov  5 17:59:03.514: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1120 /apis/apps/v1/namespaces/deployment-1120/deployments/test-rolling-update-deployment e2e1cf4b-02fa-4bcb-b61a-5b6ab8c21449 1647056338 1 2020-11-05 17:58:59 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-11-05 17:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-05 17:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040477d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-05 17:58:59 +0000 UTC,LastTransitionTime:2020-11-05 17:58:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-11-05 17:59:01 +0000 UTC,LastTransitionTime:2020-11-05 17:58:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov  5 17:59:03.524: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-1120 /apis/apps/v1/namespaces/deployment-1120/replicasets/test-rolling-update-deployment-c4cb8d6d9 780df745-b9d7-4289-b1fb-a4a3724b9551 1647056320 1 2020-11-05 17:58:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e2e1cf4b-02fa-4bcb-b61a-5b6ab8c21449 0xc006db62e0 0xc006db62e1}] []  [{kube-controller-manager Update apps/v1 2020-11-05 17:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2e1cf4b-02fa-4bcb-b61a-5b6ab8c21449\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006db6358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov  5 17:59:03.524: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov  5 17:59:03.525: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1120 /apis/apps/v1/namespaces/deployment-1120/replicasets/test-rolling-update-controller 35655e4f-3b9d-445c-87cb-97b0fa8e45c0 1647056334 2 2020-11-05 17:58:54 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e2e1cf4b-02fa-4bcb-b61a-5b6ab8c21449 0xc006db61d7 0xc006db61d8}] []  [{e2e.test Update apps/v1 2020-11-05 17:58:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-05 17:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2e1cf4b-02fa-4bcb-b61a-5b6ab8c21449\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006db6278 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov  5 17:59:03.532: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-t9qc2" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-t9qc2 test-rolling-update-deployment-c4cb8d6d9- deployment-1120 /api/v1/namespaces/deployment-1120/pods/test-rolling-update-deployment-c4cb8d6d9-t9qc2 0a38bf03-649b-4f1a-898a-f6a3f65efc27 1647056317 0 2020-11-05 17:58:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:10.2.1.104/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 780df745-b9d7-4289-b1fb-a4a3724b9551 0xc006db6850 0xc006db6851}] []  [{kube-controller-manager Update v1 2020-11-05 17:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"780df745-b9d7-4289-b1fb-a4a3724b9551\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 17:59:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 17:59:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4q29l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4q29l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4q29l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:58:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 17:58:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:10.2.1.104,StartTime:2020-11-05 17:58:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 17:59:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://3879f319170137199fb140d366996b31f2f2a70d04dc4cdd7590c11a4efee150,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:59:03.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1120" for this suite.

• [SLOW TEST:9.252 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":67,"skipped":1182,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:59:03.552: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 17:59:03.641: INFO: Waiting up to 5m0s for pod "downwardapi-volume-792ea251-d0b7-4fa4-b07c-45d2e684c4e5" in namespace "downward-api-5794" to be "Succeeded or Failed"
Nov  5 17:59:03.647: INFO: Pod "downwardapi-volume-792ea251-d0b7-4fa4-b07c-45d2e684c4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00372ms
Nov  5 17:59:05.657: INFO: Pod "downwardapi-volume-792ea251-d0b7-4fa4-b07c-45d2e684c4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015733165s
Nov  5 17:59:07.667: INFO: Pod "downwardapi-volume-792ea251-d0b7-4fa4-b07c-45d2e684c4e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025239083s
STEP: Saw pod success
Nov  5 17:59:07.667: INFO: Pod "downwardapi-volume-792ea251-d0b7-4fa4-b07c-45d2e684c4e5" satisfied condition "Succeeded or Failed"
Nov  5 17:59:07.675: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-792ea251-d0b7-4fa4-b07c-45d2e684c4e5 container client-container: <nil>
STEP: delete the pod
Nov  5 17:59:07.735: INFO: Waiting for pod downwardapi-volume-792ea251-d0b7-4fa4-b07c-45d2e684c4e5 to disappear
Nov  5 17:59:07.742: INFO: Pod downwardapi-volume-792ea251-d0b7-4fa4-b07c-45d2e684c4e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:59:07.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5794" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":68,"skipped":1185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:59:07.766: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 17:59:07.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88106009-a755-4f3d-9e79-3364f3da815a" in namespace "downward-api-1225" to be "Succeeded or Failed"
Nov  5 17:59:07.873: INFO: Pod "downwardapi-volume-88106009-a755-4f3d-9e79-3364f3da815a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.684518ms
Nov  5 17:59:09.883: INFO: Pod "downwardapi-volume-88106009-a755-4f3d-9e79-3364f3da815a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018611084s
Nov  5 17:59:11.891: INFO: Pod "downwardapi-volume-88106009-a755-4f3d-9e79-3364f3da815a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027167499s
STEP: Saw pod success
Nov  5 17:59:11.892: INFO: Pod "downwardapi-volume-88106009-a755-4f3d-9e79-3364f3da815a" satisfied condition "Succeeded or Failed"
Nov  5 17:59:11.899: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-88106009-a755-4f3d-9e79-3364f3da815a container client-container: <nil>
STEP: delete the pod
Nov  5 17:59:11.947: INFO: Waiting for pod downwardapi-volume-88106009-a755-4f3d-9e79-3364f3da815a to disappear
Nov  5 17:59:11.955: INFO: Pod downwardapi-volume-88106009-a755-4f3d-9e79-3364f3da815a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:59:11.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1225" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":69,"skipped":1209,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:59:11.975: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Nov  5 17:59:12.059: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-230475851 proxy --unix-socket=/tmp/kubectl-proxy-unix658473746/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:59:12.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3006" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":70,"skipped":1213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:59:12.166: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:59:20.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2195" for this suite.

• [SLOW TEST:8.159 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":71,"skipped":1239,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:59:20.324: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 17:59:20.462: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a95866c5-e40a-42b9-a5d9-ae88825566b2", Controller:(*bool)(0xc004010f72), BlockOwnerDeletion:(*bool)(0xc004010f73)}}
Nov  5 17:59:20.480: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f02991c8-e9ca-4f42-bd8d-990d998bba7d", Controller:(*bool)(0xc0040111a6), BlockOwnerDeletion:(*bool)(0xc0040111a7)}}
Nov  5 17:59:20.499: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2f18d220-9d8c-461d-b592-7d1a2f16e154", Controller:(*bool)(0xc004011496), BlockOwnerDeletion:(*bool)(0xc004011497)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:59:25.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2407" for this suite.

• [SLOW TEST:5.255 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":72,"skipped":1256,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:59:25.580: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Nov  5 17:59:25.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-136'
Nov  5 17:59:25.987: INFO: stderr: ""
Nov  5 17:59:25.987: INFO: stdout: "pod/pause created\n"
Nov  5 17:59:25.987: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov  5 17:59:25.988: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-136" to be "running and ready"
Nov  5 17:59:26.000: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.626034ms
Nov  5 17:59:28.010: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022513832s
Nov  5 17:59:30.020: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.032237871s
Nov  5 17:59:30.020: INFO: Pod "pause" satisfied condition "running and ready"
Nov  5 17:59:30.020: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Nov  5 17:59:30.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 label pods pause testing-label=testing-label-value --namespace=kubectl-136'
Nov  5 17:59:30.162: INFO: stderr: ""
Nov  5 17:59:30.162: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov  5 17:59:30.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pod pause -L testing-label --namespace=kubectl-136'
Nov  5 17:59:30.281: INFO: stderr: ""
Nov  5 17:59:30.281: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov  5 17:59:30.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 label pods pause testing-label- --namespace=kubectl-136'
Nov  5 17:59:30.411: INFO: stderr: ""
Nov  5 17:59:30.411: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov  5 17:59:30.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pod pause -L testing-label --namespace=kubectl-136'
Nov  5 17:59:30.549: INFO: stderr: ""
Nov  5 17:59:30.549: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Nov  5 17:59:30.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete --grace-period=0 --force -f - --namespace=kubectl-136'
Nov  5 17:59:30.688: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  5 17:59:30.688: INFO: stdout: "pod \"pause\" force deleted\n"
Nov  5 17:59:30.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get rc,svc -l name=pause --no-headers --namespace=kubectl-136'
Nov  5 17:59:30.824: INFO: stderr: "No resources found in kubectl-136 namespace.\n"
Nov  5 17:59:30.824: INFO: stdout: ""
Nov  5 17:59:30.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -l name=pause --namespace=kubectl-136 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov  5 17:59:30.961: INFO: stderr: ""
Nov  5 17:59:30.961: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:59:30.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-136" for this suite.

• [SLOW TEST:5.406 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1330
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":73,"skipped":1268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:59:30.989: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 17:59:31.070: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov  5 17:59:34.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-4069 create -f -'
Nov  5 17:59:36.243: INFO: stderr: ""
Nov  5 17:59:36.243: INFO: stdout: "e2e-test-crd-publish-openapi-2233-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov  5 17:59:36.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-4069 delete e2e-test-crd-publish-openapi-2233-crds test-cr'
Nov  5 17:59:36.446: INFO: stderr: ""
Nov  5 17:59:36.446: INFO: stdout: "e2e-test-crd-publish-openapi-2233-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov  5 17:59:36.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-4069 apply -f -'
Nov  5 17:59:36.753: INFO: stderr: ""
Nov  5 17:59:36.753: INFO: stdout: "e2e-test-crd-publish-openapi-2233-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov  5 17:59:36.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-4069 delete e2e-test-crd-publish-openapi-2233-crds test-cr'
Nov  5 17:59:36.900: INFO: stderr: ""
Nov  5 17:59:36.900: INFO: stdout: "e2e-test-crd-publish-openapi-2233-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov  5 17:59:36.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 explain e2e-test-crd-publish-openapi-2233-crds'
Nov  5 17:59:37.183: INFO: stderr: ""
Nov  5 17:59:37.183: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2233-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:59:41.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4069" for this suite.

• [SLOW TEST:10.124 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":74,"skipped":1323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:59:41.117: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4405.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4405.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4405.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4405.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4405.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4405.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  5 17:59:47.288: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4405/dns-test-cc72b86b-a3d5-45a0-a225-a402e0dd008c: the server could not find the requested resource (get pods dns-test-cc72b86b-a3d5-45a0-a225-a402e0dd008c)
Nov  5 17:59:47.334: INFO: Lookups using dns-4405/dns-test-cc72b86b-a3d5-45a0-a225-a402e0dd008c failed for: [wheezy_tcp@PodARecord]

Nov  5 17:59:52.385: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4405/dns-test-cc72b86b-a3d5-45a0-a225-a402e0dd008c: the server could not find the requested resource (get pods dns-test-cc72b86b-a3d5-45a0-a225-a402e0dd008c)
Nov  5 17:59:52.431: INFO: Lookups using dns-4405/dns-test-cc72b86b-a3d5-45a0-a225-a402e0dd008c failed for: [wheezy_tcp@PodARecord]

Nov  5 17:59:57.420: INFO: DNS probes using dns-4405/dns-test-cc72b86b-a3d5-45a0-a225-a402e0dd008c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 17:59:57.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4405" for this suite.

• [SLOW TEST:16.353 seconds]
[sig-network] DNS
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":75,"skipped":1351,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 17:59:57.470: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov  5 18:00:04.129: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8050 pod-service-account-bc79a0e2-acbf-45fd-98c7-975693a52587 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov  5 18:00:04.466: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8050 pod-service-account-bc79a0e2-acbf-45fd-98c7-975693a52587 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov  5 18:00:04.796: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8050 pod-service-account-bc79a0e2-acbf-45fd-98c7-975693a52587 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:00:05.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8050" for this suite.

• [SLOW TEST:7.682 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":76,"skipped":1360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:00:05.153: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:161
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:00:05.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9001" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":77,"skipped":1397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:00:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:00:16.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7731" for this suite.

• [SLOW TEST:11.189 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":78,"skipped":1419,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:00:16.496: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:00:16.578: INFO: Creating deployment "test-recreate-deployment"
Nov  5 18:00:16.587: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov  5 18:00:16.606: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov  5 18:00:18.623: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov  5 18:00:18.630: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196016, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196016, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196016, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196016, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:00:20.639: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov  5 18:00:20.660: INFO: Updating deployment test-recreate-deployment
Nov  5 18:00:20.660: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov  5 18:00:20.773: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-661 /apis/apps/v1/namespaces/deployment-661/deployments/test-recreate-deployment 04b68aa5-d989-43ca-8b8f-5d406506a021 1647082412 2 2020-11-05 18:00:16 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-05 18:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-05 18:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006c68d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-05 18:00:20 +0000 UTC,LastTransitionTime:2020-11-05 18:00:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-11-05 18:00:20 +0000 UTC,LastTransitionTime:2020-11-05 18:00:16 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov  5 18:00:20.783: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-661 /apis/apps/v1/namespaces/deployment-661/replicasets/test-recreate-deployment-f79dd4667 db090d73-85ae-45f3-8a74-bdf17b2e7996 1647082410 1 2020-11-05 18:00:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 04b68aa5-d989-43ca-8b8f-5d406506a021 0xc006c69320 0xc006c69321}] []  [{kube-controller-manager Update apps/v1 2020-11-05 18:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04b68aa5-d989-43ca-8b8f-5d406506a021\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006c693a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov  5 18:00:20.783: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov  5 18:00:20.783: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-661 /apis/apps/v1/namespaces/deployment-661/replicasets/test-recreate-deployment-c96cf48f e8ce5048-4296-413e-abe9-7697aea6c8a7 1647082399 2 2020-11-05 18:00:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 04b68aa5-d989-43ca-8b8f-5d406506a021 0xc006c6920f 0xc006c69220}] []  [{kube-controller-manager Update apps/v1 2020-11-05 18:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04b68aa5-d989-43ca-8b8f-5d406506a021\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006c692a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov  5 18:00:20.791: INFO: Pod "test-recreate-deployment-f79dd4667-4mvjr" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-4mvjr test-recreate-deployment-f79dd4667- deployment-661 /api/v1/namespaces/deployment-661/pods/test-recreate-deployment-f79dd4667-4mvjr 2d2c55aa-dc8b-4c14-8181-40a2785b0642 1647082404 0 2020-11-05 18:00:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 db090d73-85ae-45f3-8a74-bdf17b2e7996 0xc006c699c0 0xc006c699c1}] []  [{kube-controller-manager Update v1 2020-11-05 18:00:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db090d73-85ae-45f3-8a74-bdf17b2e7996\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tvv7n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tvv7n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tvv7n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:00:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:00:20.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-661" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":79,"skipped":1434,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:00:20.824: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Nov  5 18:00:20.910: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-230475851 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:00:21.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2928" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":80,"skipped":1446,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:00:21.053: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov  5 18:00:21.612: INFO: Pod name wrapped-volume-race-1d8322ff-d863-4d07-af5b-624bb4a7f8ff: Found 0 pods out of 5
Nov  5 18:00:26.630: INFO: Pod name wrapped-volume-race-1d8322ff-d863-4d07-af5b-624bb4a7f8ff: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1d8322ff-d863-4d07-af5b-624bb4a7f8ff in namespace emptydir-wrapper-4874, will wait for the garbage collector to delete the pods
Nov  5 18:00:40.772: INFO: Deleting ReplicationController wrapped-volume-race-1d8322ff-d863-4d07-af5b-624bb4a7f8ff took: 18.820379ms
Nov  5 18:00:41.572: INFO: Terminating ReplicationController wrapped-volume-race-1d8322ff-d863-4d07-af5b-624bb4a7f8ff pods took: 800.189981ms
STEP: Creating RC which spawns configmap-volume pods
Nov  5 18:00:50.505: INFO: Pod name wrapped-volume-race-9abcff7c-4194-4d82-9f0d-f2ddb614da6a: Found 0 pods out of 5
Nov  5 18:00:55.520: INFO: Pod name wrapped-volume-race-9abcff7c-4194-4d82-9f0d-f2ddb614da6a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9abcff7c-4194-4d82-9f0d-f2ddb614da6a in namespace emptydir-wrapper-4874, will wait for the garbage collector to delete the pods
Nov  5 18:01:09.665: INFO: Deleting ReplicationController wrapped-volume-race-9abcff7c-4194-4d82-9f0d-f2ddb614da6a took: 21.188721ms
Nov  5 18:01:10.465: INFO: Terminating ReplicationController wrapped-volume-race-9abcff7c-4194-4d82-9f0d-f2ddb614da6a pods took: 800.231559ms
STEP: Creating RC which spawns configmap-volume pods
Nov  5 18:01:20.407: INFO: Pod name wrapped-volume-race-12a6a278-6daf-4827-a9bb-0cb98b87ee82: Found 0 pods out of 5
Nov  5 18:01:25.427: INFO: Pod name wrapped-volume-race-12a6a278-6daf-4827-a9bb-0cb98b87ee82: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-12a6a278-6daf-4827-a9bb-0cb98b87ee82 in namespace emptydir-wrapper-4874, will wait for the garbage collector to delete the pods
Nov  5 18:01:39.589: INFO: Deleting ReplicationController wrapped-volume-race-12a6a278-6daf-4827-a9bb-0cb98b87ee82 took: 22.331466ms
Nov  5 18:01:40.389: INFO: Terminating ReplicationController wrapped-volume-race-12a6a278-6daf-4827-a9bb-0cb98b87ee82 pods took: 800.237302ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:01:51.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4874" for this suite.

• [SLOW TEST:90.237 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":81,"skipped":1460,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:01:51.293: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov  5 18:01:56.468: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:01:56.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3281" for this suite.

• [SLOW TEST:5.236 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":82,"skipped":1472,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:01:56.529: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:01:56.607: INFO: Creating ReplicaSet my-hostname-basic-fdc236db-d8bd-4b7d-8d75-3fa32a99e364
Nov  5 18:01:56.624: INFO: Pod name my-hostname-basic-fdc236db-d8bd-4b7d-8d75-3fa32a99e364: Found 0 pods out of 1
Nov  5 18:02:01.631: INFO: Pod name my-hostname-basic-fdc236db-d8bd-4b7d-8d75-3fa32a99e364: Found 1 pods out of 1
Nov  5 18:02:01.631: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-fdc236db-d8bd-4b7d-8d75-3fa32a99e364" is running
Nov  5 18:02:01.638: INFO: Pod "my-hostname-basic-fdc236db-d8bd-4b7d-8d75-3fa32a99e364-7dmpc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-05 18:01:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-05 18:01:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-05 18:01:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-05 18:01:56 +0000 UTC Reason: Message:}])
Nov  5 18:02:01.639: INFO: Trying to dial the pod
Nov  5 18:02:06.669: INFO: Controller my-hostname-basic-fdc236db-d8bd-4b7d-8d75-3fa32a99e364: Got expected result from replica 1 [my-hostname-basic-fdc236db-d8bd-4b7d-8d75-3fa32a99e364-7dmpc]: "my-hostname-basic-fdc236db-d8bd-4b7d-8d75-3fa32a99e364-7dmpc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:02:06.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7126" for this suite.

• [SLOW TEST:10.163 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":83,"skipped":1474,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:02:06.692: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Nov  5 18:02:06.798: INFO: Created pod &Pod{ObjectMeta:{dns-9417  dns-9417 /api/v1/namespaces/dns-9417/pods/dns-9417 04df82fb-6bbd-45c2-aedf-6e91299edcf1 1647117657 0 2020-11-05 18:02:06 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-11-05 18:02:06 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xljxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xljxl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xljxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov  5 18:02:06.809: INFO: The status of Pod dns-9417 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 18:02:08.818: INFO: The status of Pod dns-9417 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 18:02:10.819: INFO: The status of Pod dns-9417 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Nov  5 18:02:10.819: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9417 PodName:dns-9417 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 18:02:10.819: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Verifying customized DNS server is configured on pod...
Nov  5 18:02:11.022: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9417 PodName:dns-9417 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 18:02:11.022: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 18:02:11.240: INFO: Deleting pod dns-9417...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:02:11.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9417" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":84,"skipped":1478,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:02:11.305: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Nov  5 18:02:11.396: INFO: created test-event-1
Nov  5 18:02:11.404: INFO: created test-event-2
Nov  5 18:02:11.412: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Nov  5 18:02:11.419: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Nov  5 18:02:11.466: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:02:11.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5217" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":85,"skipped":1502,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:02:11.499: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-4135
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4135 to expose endpoints map[]
Nov  5 18:02:11.612: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Nov  5 18:02:12.629: INFO: successfully validated that service multi-endpoint-test in namespace services-4135 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4135
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4135 to expose endpoints map[pod1:[100]]
Nov  5 18:02:15.706: INFO: successfully validated that service multi-endpoint-test in namespace services-4135 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4135
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4135 to expose endpoints map[pod1:[100] pod2:[101]]
Nov  5 18:02:18.769: INFO: successfully validated that service multi-endpoint-test in namespace services-4135 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-4135
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4135 to expose endpoints map[pod2:[101]]
Nov  5 18:02:19.864: INFO: successfully validated that service multi-endpoint-test in namespace services-4135 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4135
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4135 to expose endpoints map[]
Nov  5 18:02:20.089: INFO: successfully validated that service multi-endpoint-test in namespace services-4135 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:02:20.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4135" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:8.688 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":86,"skipped":1506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:02:20.188: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-a97ad09c-3025-49d8-b4bb-e5ccf63873df
STEP: Creating a pod to test consume secrets
Nov  5 18:02:20.301: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c6794350-d2e2-4e2c-995f-730099d3b969" in namespace "projected-6425" to be "Succeeded or Failed"
Nov  5 18:02:20.312: INFO: Pod "pod-projected-secrets-c6794350-d2e2-4e2c-995f-730099d3b969": Phase="Pending", Reason="", readiness=false. Elapsed: 11.704624ms
Nov  5 18:02:22.323: INFO: Pod "pod-projected-secrets-c6794350-d2e2-4e2c-995f-730099d3b969": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022615693s
Nov  5 18:02:24.334: INFO: Pod "pod-projected-secrets-c6794350-d2e2-4e2c-995f-730099d3b969": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033650123s
STEP: Saw pod success
Nov  5 18:02:24.335: INFO: Pod "pod-projected-secrets-c6794350-d2e2-4e2c-995f-730099d3b969" satisfied condition "Succeeded or Failed"
Nov  5 18:02:24.342: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-secrets-c6794350-d2e2-4e2c-995f-730099d3b969 container secret-volume-test: <nil>
STEP: delete the pod
Nov  5 18:02:24.454: INFO: Waiting for pod pod-projected-secrets-c6794350-d2e2-4e2c-995f-730099d3b969 to disappear
Nov  5 18:02:24.461: INFO: Pod pod-projected-secrets-c6794350-d2e2-4e2c-995f-730099d3b969 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:02:24.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6425" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1549,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:02:24.490: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov  5 18:02:24.652: INFO: Number of nodes with available pods: 0
Nov  5 18:02:24.652: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 18:02:25.675: INFO: Number of nodes with available pods: 0
Nov  5 18:02:25.675: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 18:02:26.673: INFO: Number of nodes with available pods: 0
Nov  5 18:02:26.673: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 18:02:27.674: INFO: Number of nodes with available pods: 0
Nov  5 18:02:27.674: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 18:02:28.671: INFO: Number of nodes with available pods: 1
Nov  5 18:02:28.671: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:02:29.672: INFO: Number of nodes with available pods: 2
Nov  5 18:02:29.672: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov  5 18:02:29.716: INFO: Number of nodes with available pods: 1
Nov  5 18:02:29.716: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:02:30.732: INFO: Number of nodes with available pods: 1
Nov  5 18:02:30.732: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:02:31.747: INFO: Number of nodes with available pods: 1
Nov  5 18:02:31.747: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:02:32.737: INFO: Number of nodes with available pods: 1
Nov  5 18:02:32.737: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:02:33.735: INFO: Number of nodes with available pods: 1
Nov  5 18:02:33.735: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:02:34.739: INFO: Number of nodes with available pods: 1
Nov  5 18:02:34.739: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:02:35.739: INFO: Number of nodes with available pods: 1
Nov  5 18:02:35.739: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:02:36.740: INFO: Number of nodes with available pods: 1
Nov  5 18:02:36.740: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:02:37.739: INFO: Number of nodes with available pods: 2
Nov  5 18:02:37.739: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6527, will wait for the garbage collector to delete the pods
Nov  5 18:02:37.824: INFO: Deleting DaemonSet.extensions daemon-set took: 20.418258ms
Nov  5 18:02:38.624: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.197737ms
Nov  5 18:02:49.832: INFO: Number of nodes with available pods: 0
Nov  5 18:02:49.832: INFO: Number of running nodes: 0, number of available pods: 0
Nov  5 18:02:49.840: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6527/daemonsets","resourceVersion":"1647131842"},"items":null}

Nov  5 18:02:49.848: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6527/pods","resourceVersion":"1647131843"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:02:49.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6527" for this suite.

• [SLOW TEST:25.407 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":88,"skipped":1563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:02:49.897: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1105 18:02:51.127708      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1105 18:02:51.127737      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1105 18:02:51.127746      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov  5 18:02:51.127: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:02:51.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7866" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":89,"skipped":1604,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:02:51.163: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Nov  5 18:02:51.245: INFO: Waiting up to 5m0s for pod "client-containers-076c4a06-af81-453e-b055-3adaee1b9c37" in namespace "containers-6942" to be "Succeeded or Failed"
Nov  5 18:02:51.255: INFO: Pod "client-containers-076c4a06-af81-453e-b055-3adaee1b9c37": Phase="Pending", Reason="", readiness=false. Elapsed: 10.035321ms
Nov  5 18:02:53.264: INFO: Pod "client-containers-076c4a06-af81-453e-b055-3adaee1b9c37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01844059s
Nov  5 18:02:55.271: INFO: Pod "client-containers-076c4a06-af81-453e-b055-3adaee1b9c37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026093405s
STEP: Saw pod success
Nov  5 18:02:55.271: INFO: Pod "client-containers-076c4a06-af81-453e-b055-3adaee1b9c37" satisfied condition "Succeeded or Failed"
Nov  5 18:02:55.278: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod client-containers-076c4a06-af81-453e-b055-3adaee1b9c37 container test-container: <nil>
STEP: delete the pod
Nov  5 18:02:55.317: INFO: Waiting for pod client-containers-076c4a06-af81-453e-b055-3adaee1b9c37 to disappear
Nov  5 18:02:55.324: INFO: Pod client-containers-076c4a06-af81-453e-b055-3adaee1b9c37 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:02:55.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6942" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":90,"skipped":1614,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:02:55.345: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:02:55.436: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:03:01.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8560" for this suite.

• [SLOW TEST:6.208 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":91,"skipped":1619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:03:01.553: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-de4c89d5-c1c8-4d4a-b4c9-a631cbbc1923
STEP: Creating configMap with name cm-test-opt-upd-ff59aa11-1dea-4b5a-a688-1b4e9c9e12c2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-de4c89d5-c1c8-4d4a-b4c9-a631cbbc1923
STEP: Updating configmap cm-test-opt-upd-ff59aa11-1dea-4b5a-a688-1b4e9c9e12c2
STEP: Creating configMap with name cm-test-opt-create-d2b44148-4ad6-4a6b-9de6-a2207e84f1c4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:04:32.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9771" for this suite.

• [SLOW TEST:91.406 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":92,"skipped":1652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:04:32.960: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov  5 18:04:37.633: INFO: Successfully updated pod "labelsupdate74c1446d-0e09-45b8-b831-5a2bdcdc05e6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:04:39.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-748" for this suite.

• [SLOW TEST:6.747 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1682,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:04:39.708: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:04:39.794: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:04:46.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-178" for this suite.

• [SLOW TEST:6.377 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":94,"skipped":1691,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:04:46.086: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4859
Nov  5 18:04:48.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4859 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov  5 18:04:48.551: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov  5 18:04:48.551: INFO: stdout: "iptables"
Nov  5 18:04:48.551: INFO: proxyMode: iptables
Nov  5 18:04:48.566: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov  5 18:04:48.574: INFO: Pod kube-proxy-mode-detector still exists
Nov  5 18:04:50.574: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov  5 18:04:50.584: INFO: Pod kube-proxy-mode-detector still exists
Nov  5 18:04:52.574: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov  5 18:04:52.584: INFO: Pod kube-proxy-mode-detector still exists
Nov  5 18:04:54.574: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov  5 18:04:54.586: INFO: Pod kube-proxy-mode-detector still exists
Nov  5 18:04:56.574: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov  5 18:04:56.588: INFO: Pod kube-proxy-mode-detector still exists
Nov  5 18:04:58.574: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov  5 18:04:58.584: INFO: Pod kube-proxy-mode-detector still exists
Nov  5 18:05:00.574: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov  5 18:05:00.586: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4859
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4859
I1105 18:05:00.622532      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4859, replica count: 3
I1105 18:05:03.673526      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 18:05:06.674142      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 18:05:06.690: INFO: Creating new exec pod
Nov  5 18:05:11.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4859 execpod-affinityzdvzd -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Nov  5 18:05:12.077: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Nov  5 18:05:12.077: INFO: stdout: ""
Nov  5 18:05:12.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4859 execpod-affinityzdvzd -- /bin/sh -x -c nc -zv -t -w 2 10.3.174.1 80'
Nov  5 18:05:12.407: INFO: stderr: "+ nc -zv -t -w 2 10.3.174.1 80\nConnection to 10.3.174.1 80 port [tcp/http] succeeded!\n"
Nov  5 18:05:12.407: INFO: stdout: ""
Nov  5 18:05:12.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4859 execpod-affinityzdvzd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.174.1:80/ ; done'
Nov  5 18:05:12.857: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n"
Nov  5 18:05:12.858: INFO: stdout: "\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w\naffinity-clusterip-timeout-7j96w"
Nov  5 18:05:12.858: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.858: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.859: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.859: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.859: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.859: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.860: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.860: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.860: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.860: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.861: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.861: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.861: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.861: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.861: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.861: INFO: Received response from host: affinity-clusterip-timeout-7j96w
Nov  5 18:05:12.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4859 execpod-affinityzdvzd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.3.174.1:80/'
Nov  5 18:05:13.218: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n"
Nov  5 18:05:13.218: INFO: stdout: "affinity-clusterip-timeout-7j96w"
Nov  5 18:05:28.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4859 execpod-affinityzdvzd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.3.174.1:80/'
Nov  5 18:05:28.553: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n"
Nov  5 18:05:28.553: INFO: stdout: "affinity-clusterip-timeout-7j96w"
Nov  5 18:05:43.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4859 execpod-affinityzdvzd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.3.174.1:80/'
Nov  5 18:05:43.908: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n"
Nov  5 18:05:43.908: INFO: stdout: "affinity-clusterip-timeout-7j96w"
Nov  5 18:05:58.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4859 execpod-affinityzdvzd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.3.174.1:80/'
Nov  5 18:05:59.251: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n"
Nov  5 18:05:59.251: INFO: stdout: "affinity-clusterip-timeout-7j96w"
Nov  5 18:06:14.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4859 execpod-affinityzdvzd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.3.174.1:80/'
Nov  5 18:06:14.617: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.3.174.1:80/\n"
Nov  5 18:06:14.617: INFO: stdout: "affinity-clusterip-timeout-jftz4"
Nov  5 18:06:14.617: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4859, will wait for the garbage collector to delete the pods
Nov  5 18:06:14.719: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 21.8243ms
Nov  5 18:06:15.919: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 1.200210478s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:06:30.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4859" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:104.201 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":95,"skipped":1723,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:06:30.293: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:06:30.392: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3220284-2faf-4ecd-84ed-348af6881b6f" in namespace "downward-api-681" to be "Succeeded or Failed"
Nov  5 18:06:30.400: INFO: Pod "downwardapi-volume-a3220284-2faf-4ecd-84ed-348af6881b6f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.55522ms
Nov  5 18:06:32.409: INFO: Pod "downwardapi-volume-a3220284-2faf-4ecd-84ed-348af6881b6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017096585s
Nov  5 18:06:34.420: INFO: Pod "downwardapi-volume-a3220284-2faf-4ecd-84ed-348af6881b6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027734824s
STEP: Saw pod success
Nov  5 18:06:34.420: INFO: Pod "downwardapi-volume-a3220284-2faf-4ecd-84ed-348af6881b6f" satisfied condition "Succeeded or Failed"
Nov  5 18:06:34.429: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-a3220284-2faf-4ecd-84ed-348af6881b6f container client-container: <nil>
STEP: delete the pod
Nov  5 18:06:34.534: INFO: Waiting for pod downwardapi-volume-a3220284-2faf-4ecd-84ed-348af6881b6f to disappear
Nov  5 18:06:34.542: INFO: Pod downwardapi-volume-a3220284-2faf-4ecd-84ed-348af6881b6f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:06:34.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-681" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":96,"skipped":1736,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:06:34.569: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-7fe69a08-b598-4fc7-89f9-a7c9092fb2d0
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-7fe69a08-b598-4fc7-89f9-a7c9092fb2d0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:06:40.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6181" for this suite.

• [SLOW TEST:6.237 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":97,"skipped":1753,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:06:40.810: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov  5 18:06:40.894: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov  5 18:06:40.910: INFO: Waiting for terminating namespaces to be deleted...
Nov  5 18:06:40.917: INFO: 
Logging pods the apiserver thinks is on node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 before test
Nov  5 18:06:40.930: INFO: canal-55ppt from kube-system started at 2020-11-05 17:15:19 +0000 UTC (2 container statuses recorded)
Nov  5 18:06:40.930: INFO: 	Container calico-node ready: true, restart count 0
Nov  5 18:06:40.930: INFO: 	Container kube-flannel ready: true, restart count 2
Nov  5 18:06:40.930: INFO: coredns-6ccf64844b-zjcgs from kube-system started at 2020-11-05 17:23:56 +0000 UTC (1 container statuses recorded)
Nov  5 18:06:40.931: INFO: 	Container coredns ready: true, restart count 0
Nov  5 18:06:40.931: INFO: kube-proxy-j2wfk from kube-system started at 2020-11-05 17:15:40 +0000 UTC (1 container statuses recorded)
Nov  5 18:06:40.931: INFO: 	Container kube-proxy ready: true, restart count 0
Nov  5 18:06:40.931: INFO: metrics-server-664567f776-8wvsk from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 18:06:40.931: INFO: 	Container metrics-server ready: true, restart count 0
Nov  5 18:06:40.931: INFO: wormhole-2r8fc from kube-system started at 2020-10-19 12:36:30 +0000 UTC (1 container statuses recorded)
Nov  5 18:06:40.931: INFO: 	Container wormhole ready: true, restart count 0
Nov  5 18:06:40.932: INFO: sonobuoy from sonobuoy started at 2020-11-05 17:25:05 +0000 UTC (1 container statuses recorded)
Nov  5 18:06:40.932: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov  5 18:06:40.932: INFO: sonobuoy-e2e-job-6a656b1abb77479a from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:06:40.932: INFO: 	Container e2e ready: true, restart count 0
Nov  5 18:06:40.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 18:06:40.933: INFO: sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-vv8nw from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:06:40.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 18:06:40.933: INFO: 	Container systemd-logs ready: true, restart count 0
Nov  5 18:06:40.933: INFO: 
Logging pods the apiserver thinks is on node node-a7de9392-4cc3-4830-84ff-49e7615b291e before test
Nov  5 18:06:40.945: INFO: pod-configmaps-c10b871d-35a9-4188-85e6-b9b962c1e592 from configmap-6181 started at 2020-11-05 18:06:34 +0000 UTC (1 container statuses recorded)
Nov  5 18:06:40.945: INFO: 	Container configmap-volume-test ready: true, restart count 0
Nov  5 18:06:40.945: INFO: canal-sqp5g from kube-system started at 2020-11-05 17:15:35 +0000 UTC (2 container statuses recorded)
Nov  5 18:06:40.945: INFO: 	Container calico-node ready: true, restart count 0
Nov  5 18:06:40.946: INFO: 	Container kube-flannel ready: true, restart count 0
Nov  5 18:06:40.946: INFO: coredns-6ccf64844b-qrpm5 from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 18:06:40.946: INFO: 	Container coredns ready: true, restart count 0
Nov  5 18:06:40.946: INFO: kube-dns-autoscaler-7944596f47-whbv8 from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 18:06:40.946: INFO: 	Container autoscaler ready: true, restart count 0
Nov  5 18:06:40.946: INFO: kube-proxy-bhxd4 from kube-system started at 2020-11-05 17:15:16 +0000 UTC (1 container statuses recorded)
Nov  5 18:06:40.946: INFO: 	Container kube-proxy ready: true, restart count 0
Nov  5 18:06:40.946: INFO: wormhole-4m5c5 from kube-system started at 2020-10-19 12:36:45 +0000 UTC (1 container statuses recorded)
Nov  5 18:06:40.946: INFO: 	Container wormhole ready: true, restart count 0
Nov  5 18:06:40.946: INFO: sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-nf5fs from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:06:40.946: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 18:06:40.946: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5a920f74-317b-4225-949f-0259b5036220 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5a920f74-317b-4225-949f-0259b5036220 off the node node-a7de9392-4cc3-4830-84ff-49e7615b291e
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5a920f74-317b-4225-949f-0259b5036220
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:06:49.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9157" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.332 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":98,"skipped":1756,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:06:49.155: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:06:50.267: INFO: Checking APIGroup: apiregistration.k8s.io
Nov  5 18:06:50.270: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov  5 18:06:50.270: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.270: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov  5 18:06:50.270: INFO: Checking APIGroup: extensions
Nov  5 18:06:50.273: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Nov  5 18:06:50.273: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Nov  5 18:06:50.273: INFO: extensions/v1beta1 matches extensions/v1beta1
Nov  5 18:06:50.273: INFO: Checking APIGroup: apps
Nov  5 18:06:50.276: INFO: PreferredVersion.GroupVersion: apps/v1
Nov  5 18:06:50.276: INFO: Versions found [{apps/v1 v1}]
Nov  5 18:06:50.276: INFO: apps/v1 matches apps/v1
Nov  5 18:06:50.276: INFO: Checking APIGroup: events.k8s.io
Nov  5 18:06:50.279: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov  5 18:06:50.279: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.279: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov  5 18:06:50.279: INFO: Checking APIGroup: authentication.k8s.io
Nov  5 18:06:50.282: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov  5 18:06:50.282: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.282: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov  5 18:06:50.282: INFO: Checking APIGroup: authorization.k8s.io
Nov  5 18:06:50.285: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov  5 18:06:50.285: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.285: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov  5 18:06:50.285: INFO: Checking APIGroup: autoscaling
Nov  5 18:06:50.288: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Nov  5 18:06:50.288: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Nov  5 18:06:50.288: INFO: autoscaling/v1 matches autoscaling/v1
Nov  5 18:06:50.288: INFO: Checking APIGroup: batch
Nov  5 18:06:50.291: INFO: PreferredVersion.GroupVersion: batch/v1
Nov  5 18:06:50.291: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Nov  5 18:06:50.291: INFO: batch/v1 matches batch/v1
Nov  5 18:06:50.291: INFO: Checking APIGroup: certificates.k8s.io
Nov  5 18:06:50.294: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov  5 18:06:50.294: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.294: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov  5 18:06:50.294: INFO: Checking APIGroup: networking.k8s.io
Nov  5 18:06:50.297: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov  5 18:06:50.297: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.297: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov  5 18:06:50.297: INFO: Checking APIGroup: policy
Nov  5 18:06:50.299: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Nov  5 18:06:50.300: INFO: Versions found [{policy/v1beta1 v1beta1}]
Nov  5 18:06:50.300: INFO: policy/v1beta1 matches policy/v1beta1
Nov  5 18:06:50.300: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov  5 18:06:50.302: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov  5 18:06:50.302: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.302: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov  5 18:06:50.302: INFO: Checking APIGroup: storage.k8s.io
Nov  5 18:06:50.305: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov  5 18:06:50.305: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.305: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov  5 18:06:50.305: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov  5 18:06:50.308: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov  5 18:06:50.308: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.308: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov  5 18:06:50.308: INFO: Checking APIGroup: apiextensions.k8s.io
Nov  5 18:06:50.311: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov  5 18:06:50.311: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.311: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov  5 18:06:50.311: INFO: Checking APIGroup: scheduling.k8s.io
Nov  5 18:06:50.314: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov  5 18:06:50.314: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.314: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov  5 18:06:50.314: INFO: Checking APIGroup: coordination.k8s.io
Nov  5 18:06:50.317: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov  5 18:06:50.317: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.317: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov  5 18:06:50.317: INFO: Checking APIGroup: node.k8s.io
Nov  5 18:06:50.320: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Nov  5 18:06:50.320: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.320: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Nov  5 18:06:50.320: INFO: Checking APIGroup: discovery.k8s.io
Nov  5 18:06:50.322: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Nov  5 18:06:50.322: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.322: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Nov  5 18:06:50.322: INFO: Checking APIGroup: crd.projectcalico.org
Nov  5 18:06:50.325: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Nov  5 18:06:50.325: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Nov  5 18:06:50.325: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Nov  5 18:06:50.325: INFO: Checking APIGroup: k8s.nginx.org
Nov  5 18:06:50.327: INFO: PreferredVersion.GroupVersion: k8s.nginx.org/v1
Nov  5 18:06:50.327: INFO: Versions found [{k8s.nginx.org/v1 v1} {k8s.nginx.org/v1alpha1 v1alpha1}]
Nov  5 18:06:50.327: INFO: k8s.nginx.org/v1 matches k8s.nginx.org/v1
Nov  5 18:06:50.327: INFO: Checking APIGroup: kube.cloud.ovh.com
Nov  5 18:06:50.330: INFO: PreferredVersion.GroupVersion: kube.cloud.ovh.com/v1alpha1
Nov  5 18:06:50.331: INFO: Versions found [{kube.cloud.ovh.com/v1alpha1 v1alpha1}]
Nov  5 18:06:50.331: INFO: kube.cloud.ovh.com/v1alpha1 matches kube.cloud.ovh.com/v1alpha1
Nov  5 18:06:50.331: INFO: Checking APIGroup: appprotect.f5.com
Nov  5 18:06:50.334: INFO: PreferredVersion.GroupVersion: appprotect.f5.com/v1beta1
Nov  5 18:06:50.334: INFO: Versions found [{appprotect.f5.com/v1beta1 v1beta1}]
Nov  5 18:06:50.334: INFO: appprotect.f5.com/v1beta1 matches appprotect.f5.com/v1beta1
Nov  5 18:06:50.335: INFO: Checking APIGroup: snapshot.storage.k8s.io
Nov  5 18:06:50.337: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Nov  5 18:06:50.338: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.338: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Nov  5 18:06:50.338: INFO: Checking APIGroup: metrics.k8s.io
Nov  5 18:06:50.341: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Nov  5 18:06:50.341: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Nov  5 18:06:50.341: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:06:50.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2669" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":99,"skipped":1843,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:06:50.375: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-ee81b099-e2a8-42b8-8ea4-655591e5bf3f
STEP: Creating a pod to test consume configMaps
Nov  5 18:06:50.477: INFO: Waiting up to 5m0s for pod "pod-configmaps-f958fa5c-1250-4f2f-8a3c-eaedfa805f0e" in namespace "configmap-2375" to be "Succeeded or Failed"
Nov  5 18:06:50.487: INFO: Pod "pod-configmaps-f958fa5c-1250-4f2f-8a3c-eaedfa805f0e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.294475ms
Nov  5 18:06:52.495: INFO: Pod "pod-configmaps-f958fa5c-1250-4f2f-8a3c-eaedfa805f0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017391196s
Nov  5 18:06:54.502: INFO: Pod "pod-configmaps-f958fa5c-1250-4f2f-8a3c-eaedfa805f0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024821728s
STEP: Saw pod success
Nov  5 18:06:54.502: INFO: Pod "pod-configmaps-f958fa5c-1250-4f2f-8a3c-eaedfa805f0e" satisfied condition "Succeeded or Failed"
Nov  5 18:06:54.510: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-configmaps-f958fa5c-1250-4f2f-8a3c-eaedfa805f0e container configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:06:54.556: INFO: Waiting for pod pod-configmaps-f958fa5c-1250-4f2f-8a3c-eaedfa805f0e to disappear
Nov  5 18:06:54.570: INFO: Pod pod-configmaps-f958fa5c-1250-4f2f-8a3c-eaedfa805f0e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:06:54.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2375" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":100,"skipped":1844,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:06:54.601: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-aed687b5-ba14-4a12-85f4-50d360bfe4fe
STEP: Creating a pod to test consume configMaps
Nov  5 18:06:54.709: INFO: Waiting up to 5m0s for pod "pod-configmaps-d42ff3aa-8a1e-4aaf-8c7c-a8e3a86095da" in namespace "configmap-1448" to be "Succeeded or Failed"
Nov  5 18:06:54.717: INFO: Pod "pod-configmaps-d42ff3aa-8a1e-4aaf-8c7c-a8e3a86095da": Phase="Pending", Reason="", readiness=false. Elapsed: 8.210456ms
Nov  5 18:06:56.732: INFO: Pod "pod-configmaps-d42ff3aa-8a1e-4aaf-8c7c-a8e3a86095da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022939346s
Nov  5 18:06:58.741: INFO: Pod "pod-configmaps-d42ff3aa-8a1e-4aaf-8c7c-a8e3a86095da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032098189s
STEP: Saw pod success
Nov  5 18:06:58.741: INFO: Pod "pod-configmaps-d42ff3aa-8a1e-4aaf-8c7c-a8e3a86095da" satisfied condition "Succeeded or Failed"
Nov  5 18:06:58.749: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-configmaps-d42ff3aa-8a1e-4aaf-8c7c-a8e3a86095da container configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:06:58.789: INFO: Waiting for pod pod-configmaps-d42ff3aa-8a1e-4aaf-8c7c-a8e3a86095da to disappear
Nov  5 18:06:58.799: INFO: Pod pod-configmaps-d42ff3aa-8a1e-4aaf-8c7c-a8e3a86095da no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:06:58.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1448" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":101,"skipped":1852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:06:58.826: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:07:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6978" for this suite.

• [SLOW TEST:34.729 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":102,"skipped":1877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:07:33.556: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov  5 18:07:41.849: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  5 18:07:41.860: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  5 18:07:43.861: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  5 18:07:43.869: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  5 18:07:45.861: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  5 18:07:45.871: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  5 18:07:47.861: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  5 18:07:47.873: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  5 18:07:49.861: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  5 18:07:49.872: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  5 18:07:51.861: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  5 18:07:51.870: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:07:51.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6181" for this suite.

• [SLOW TEST:18.341 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":103,"skipped":1905,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:07:51.907: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-25483e1f-979b-4f37-afe2-2031959718f3 in namespace container-probe-7269
Nov  5 18:07:56.029: INFO: Started pod test-webserver-25483e1f-979b-4f37-afe2-2031959718f3 in namespace container-probe-7269
STEP: checking the pod's current state and verifying that restartCount is present
Nov  5 18:07:56.042: INFO: Initial restart count of pod test-webserver-25483e1f-979b-4f37-afe2-2031959718f3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:11:58.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7269" for this suite.

• [SLOW TEST:246.131 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":104,"skipped":1926,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:11:58.042: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:12:14.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1065" for this suite.

• [SLOW TEST:16.385 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":105,"skipped":1927,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:12:14.430: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov  5 18:12:22.683: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov  5 18:12:22.696: INFO: Pod pod-with-poststart-http-hook still exists
Nov  5 18:12:24.697: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov  5 18:12:24.706: INFO: Pod pod-with-poststart-http-hook still exists
Nov  5 18:12:26.697: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov  5 18:12:26.704: INFO: Pod pod-with-poststart-http-hook still exists
Nov  5 18:12:28.697: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov  5 18:12:28.706: INFO: Pod pod-with-poststart-http-hook still exists
Nov  5 18:12:30.697: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov  5 18:12:30.755: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:12:30.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5450" for this suite.

• [SLOW TEST:16.419 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":106,"skipped":1981,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:12:30.856: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:12:31.023: INFO: Waiting up to 5m0s for pod "busybox-user-65534-95468d06-27ed-4fd5-b434-7e1f81f80f86" in namespace "security-context-test-3332" to be "Succeeded or Failed"
Nov  5 18:12:31.052: INFO: Pod "busybox-user-65534-95468d06-27ed-4fd5-b434-7e1f81f80f86": Phase="Pending", Reason="", readiness=false. Elapsed: 28.700531ms
Nov  5 18:12:33.129: INFO: Pod "busybox-user-65534-95468d06-27ed-4fd5-b434-7e1f81f80f86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.106203092s
Nov  5 18:12:35.412: INFO: Pod "busybox-user-65534-95468d06-27ed-4fd5-b434-7e1f81f80f86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.388669031s
Nov  5 18:12:35.412: INFO: Pod "busybox-user-65534-95468d06-27ed-4fd5-b434-7e1f81f80f86" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:12:35.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3332" for this suite.

• [SLOW TEST:5.413 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a container with runAsUser
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1991,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:12:36.269: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-b46124d8-fd1d-4cd3-8b56-0bd59de5778c
STEP: Creating secret with name s-test-opt-upd-842ee15b-a6f5-4de0-b6c1-6ed660c37159
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b46124d8-fd1d-4cd3-8b56-0bd59de5778c
STEP: Updating secret s-test-opt-upd-842ee15b-a6f5-4de0-b6c1-6ed660c37159
STEP: Creating secret with name s-test-opt-create-75e485de-6bda-4e80-af62-92b7c0845869
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:14:02.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3263" for this suite.

• [SLOW TEST:86.310 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1994,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:14:02.579: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:14:08.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7251" for this suite.

• [SLOW TEST:6.164 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":109,"skipped":1995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:14:08.756: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov  5 18:14:08.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1806'
Nov  5 18:14:10.184: INFO: stderr: ""
Nov  5 18:14:10.184: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Nov  5 18:14:10.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete pods e2e-test-httpd-pod --namespace=kubectl-1806'
Nov  5 18:14:19.862: INFO: stderr: ""
Nov  5 18:14:19.863: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:14:19.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1806" for this suite.

• [SLOW TEST:11.171 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":110,"skipped":2048,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:14:19.928: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov  5 18:14:25.169: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:14:25.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5281" for this suite.

• [SLOW TEST:5.291 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":111,"skipped":2062,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:14:25.224: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov  5 18:14:25.330: INFO: Waiting up to 5m0s for pod "pod-9fffdbf4-2a14-4e19-9f32-67963531fe14" in namespace "emptydir-8991" to be "Succeeded or Failed"
Nov  5 18:14:25.337: INFO: Pod "pod-9fffdbf4-2a14-4e19-9f32-67963531fe14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.498777ms
Nov  5 18:14:27.345: INFO: Pod "pod-9fffdbf4-2a14-4e19-9f32-67963531fe14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01525094s
Nov  5 18:14:29.356: INFO: Pod "pod-9fffdbf4-2a14-4e19-9f32-67963531fe14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025743301s
STEP: Saw pod success
Nov  5 18:14:29.356: INFO: Pod "pod-9fffdbf4-2a14-4e19-9f32-67963531fe14" satisfied condition "Succeeded or Failed"
Nov  5 18:14:29.363: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-9fffdbf4-2a14-4e19-9f32-67963531fe14 container test-container: <nil>
STEP: delete the pod
Nov  5 18:14:29.406: INFO: Waiting for pod pod-9fffdbf4-2a14-4e19-9f32-67963531fe14 to disappear
Nov  5 18:14:29.416: INFO: Pod pod-9fffdbf4-2a14-4e19-9f32-67963531fe14 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:14:29.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8991" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":112,"skipped":2069,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:14:29.440: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:14:29.541: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov  5 18:14:34.549: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov  5 18:14:34.549: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov  5 18:14:36.556: INFO: Creating deployment "test-rollover-deployment"
Nov  5 18:14:36.577: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov  5 18:14:38.594: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov  5 18:14:38.610: INFO: Ensure that both replica sets have 1 created replica
Nov  5 18:14:38.627: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov  5 18:14:38.648: INFO: Updating deployment test-rollover-deployment
Nov  5 18:14:38.648: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov  5 18:14:40.667: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov  5 18:14:40.687: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov  5 18:14:40.703: INFO: all replica sets need to contain the pod-template-hash label
Nov  5 18:14:40.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196878, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:14:42.722: INFO: all replica sets need to contain the pod-template-hash label
Nov  5 18:14:42.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196882, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:14:44.726: INFO: all replica sets need to contain the pod-template-hash label
Nov  5 18:14:44.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196882, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:14:47.470: INFO: all replica sets need to contain the pod-template-hash label
Nov  5 18:14:47.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196882, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:14:48.733: INFO: all replica sets need to contain the pod-template-hash label
Nov  5 18:14:48.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196882, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:14:50.722: INFO: all replica sets need to contain the pod-template-hash label
Nov  5 18:14:50.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196882, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196876, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:14:52.722: INFO: 
Nov  5 18:14:52.722: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov  5 18:14:52.745: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6445 /apis/apps/v1/namespaces/deployment-6445/deployments/test-rollover-deployment 74727754-fd6e-4b86-b873-93654687fa28 1647364331 2 2020-11-05 18:14:36 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-05 18:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-05 18:14:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00088bd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-05 18:14:36 +0000 UTC,LastTransitionTime:2020-11-05 18:14:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-11-05 18:14:52 +0000 UTC,LastTransitionTime:2020-11-05 18:14:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov  5 18:14:52.755: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-6445 /apis/apps/v1/namespaces/deployment-6445/replicasets/test-rollover-deployment-5797c7764 08837874-4bab-4639-a46f-8c13e4448988 1647364301 2 2020-11-05 18:14:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 74727754-fd6e-4b86-b873-93654687fa28 0xc001ff45a0 0xc001ff45a1}] []  [{kube-controller-manager Update apps/v1 2020-11-05 18:14:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74727754-fd6e-4b86-b873-93654687fa28\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ff4618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov  5 18:14:52.755: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov  5 18:14:52.756: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6445 /apis/apps/v1/namespaces/deployment-6445/replicasets/test-rollover-controller 9e72041d-5789-4c5a-bc64-3986b45a2ea2 1647364327 2 2020-11-05 18:14:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 74727754-fd6e-4b86-b873-93654687fa28 0xc001ff4497 0xc001ff4498}] []  [{e2e.test Update apps/v1 2020-11-05 18:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-05 18:14:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74727754-fd6e-4b86-b873-93654687fa28\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001ff4538 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov  5 18:14:52.756: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-6445 /apis/apps/v1/namespaces/deployment-6445/replicasets/test-rollover-deployment-78bc8b888c 2aecb258-d7b7-41d6-9213-81dfb899f4d0 1647359802 2 2020-11-05 18:14:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 74727754-fd6e-4b86-b873-93654687fa28 0xc001ff4687 0xc001ff4688}] []  [{kube-controller-manager Update apps/v1 2020-11-05 18:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74727754-fd6e-4b86-b873-93654687fa28\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ff4718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov  5 18:14:52.766: INFO: Pod "test-rollover-deployment-5797c7764-9t9xd" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-9t9xd test-rollover-deployment-5797c7764- deployment-6445 /api/v1/namespaces/deployment-6445/pods/test-rollover-deployment-5797c7764-9t9xd f7f66c29-9075-4d72-ba66-504143149f0b 1647361054 0 2020-11-05 18:14:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:10.2.1.165/32] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 08837874-4bab-4639-a46f-8c13e4448988 0xc001ff4e00 0xc001ff4e01}] []  [{kube-controller-manager Update v1 2020-11-05 18:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08837874-4bab-4639-a46f-8c13e4448988\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 18:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 18:14:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mkhpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mkhpx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mkhpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:14:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:14:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:14:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:14:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:10.2.1.165,StartTime:2020-11-05 18:14:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 18:14:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://4533ca7902d067a9cc1c9b2dd6b8d525591edd64ca5405376dd71e6f8b3e4b88,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.165,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:14:52.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6445" for this suite.

• [SLOW TEST:23.350 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":113,"skipped":2078,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:14:52.790: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov  5 18:14:57.850: INFO: Successfully updated pod "annotationupdated0b3ecee-d698-4729-84b4-6d306d292f84"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:14:59.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6467" for this suite.

• [SLOW TEST:7.129 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":2084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:14:59.923: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:15:00.517: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov  5 18:15:02.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196900, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196900, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196900, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740196900, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:15:05.657: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:15:05.665: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:07.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8325" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.909 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":115,"skipped":2113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:07.835: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov  5 18:15:07.951: INFO: Waiting up to 5m0s for pod "downward-api-a55de861-1d08-474e-a9da-e2ebd4442a0e" in namespace "downward-api-9660" to be "Succeeded or Failed"
Nov  5 18:15:07.966: INFO: Pod "downward-api-a55de861-1d08-474e-a9da-e2ebd4442a0e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.559223ms
Nov  5 18:15:09.976: INFO: Pod "downward-api-a55de861-1d08-474e-a9da-e2ebd4442a0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025060374s
Nov  5 18:15:11.993: INFO: Pod "downward-api-a55de861-1d08-474e-a9da-e2ebd4442a0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042453784s
STEP: Saw pod success
Nov  5 18:15:11.994: INFO: Pod "downward-api-a55de861-1d08-474e-a9da-e2ebd4442a0e" satisfied condition "Succeeded or Failed"
Nov  5 18:15:12.004: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downward-api-a55de861-1d08-474e-a9da-e2ebd4442a0e container dapi-container: <nil>
STEP: delete the pod
Nov  5 18:15:12.059: INFO: Waiting for pod downward-api-a55de861-1d08-474e-a9da-e2ebd4442a0e to disappear
Nov  5 18:15:12.071: INFO: Pod downward-api-a55de861-1d08-474e-a9da-e2ebd4442a0e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:12.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9660" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":116,"skipped":2166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:12.095: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Nov  5 18:15:18.251: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1635 PodName:pod-sharedvolume-cd20ea4d-6922-414c-b027-a4b1ef0dac33 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 18:15:18.251: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 18:15:18.441: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:18.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1635" for this suite.

• [SLOW TEST:6.370 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":117,"skipped":2196,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:18.469: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:15:18.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92098874-d90f-4d10-a676-7d12517184d1" in namespace "projected-5648" to be "Succeeded or Failed"
Nov  5 18:15:18.572: INFO: Pod "downwardapi-volume-92098874-d90f-4d10-a676-7d12517184d1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.397516ms
Nov  5 18:15:20.582: INFO: Pod "downwardapi-volume-92098874-d90f-4d10-a676-7d12517184d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018577443s
Nov  5 18:15:22.592: INFO: Pod "downwardapi-volume-92098874-d90f-4d10-a676-7d12517184d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028180581s
STEP: Saw pod success
Nov  5 18:15:22.592: INFO: Pod "downwardapi-volume-92098874-d90f-4d10-a676-7d12517184d1" satisfied condition "Succeeded or Failed"
Nov  5 18:15:22.601: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-92098874-d90f-4d10-a676-7d12517184d1 container client-container: <nil>
STEP: delete the pod
Nov  5 18:15:22.644: INFO: Waiting for pod downwardapi-volume-92098874-d90f-4d10-a676-7d12517184d1 to disappear
Nov  5 18:15:22.651: INFO: Pod downwardapi-volume-92098874-d90f-4d10-a676-7d12517184d1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:22.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5648" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":2203,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:22.675: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:29.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6063" for this suite.

• [SLOW TEST:7.134 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":119,"skipped":2232,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:29.810: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-62db826a-22c2-43f4-b774-aa574ebbdd64
STEP: Creating a pod to test consume secrets
Nov  5 18:15:29.918: INFO: Waiting up to 5m0s for pod "pod-secrets-07ff85d0-cc18-404c-80e2-8fed0125b527" in namespace "secrets-1582" to be "Succeeded or Failed"
Nov  5 18:15:29.928: INFO: Pod "pod-secrets-07ff85d0-cc18-404c-80e2-8fed0125b527": Phase="Pending", Reason="", readiness=false. Elapsed: 10.036047ms
Nov  5 18:15:31.940: INFO: Pod "pod-secrets-07ff85d0-cc18-404c-80e2-8fed0125b527": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022089325s
Nov  5 18:15:33.949: INFO: Pod "pod-secrets-07ff85d0-cc18-404c-80e2-8fed0125b527": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031155774s
STEP: Saw pod success
Nov  5 18:15:33.949: INFO: Pod "pod-secrets-07ff85d0-cc18-404c-80e2-8fed0125b527" satisfied condition "Succeeded or Failed"
Nov  5 18:15:33.956: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-secrets-07ff85d0-cc18-404c-80e2-8fed0125b527 container secret-volume-test: <nil>
STEP: delete the pod
Nov  5 18:15:34.001: INFO: Waiting for pod pod-secrets-07ff85d0-cc18-404c-80e2-8fed0125b527 to disappear
Nov  5 18:15:34.009: INFO: Pod pod-secrets-07ff85d0-cc18-404c-80e2-8fed0125b527 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:34.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1582" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":120,"skipped":2250,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:34.034: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:15:38.185: INFO: Waiting up to 5m0s for pod "client-envvars-4b45d233-50ce-440a-a992-b682068f6c74" in namespace "pods-8270" to be "Succeeded or Failed"
Nov  5 18:15:38.194: INFO: Pod "client-envvars-4b45d233-50ce-440a-a992-b682068f6c74": Phase="Pending", Reason="", readiness=false. Elapsed: 8.792804ms
Nov  5 18:15:40.202: INFO: Pod "client-envvars-4b45d233-50ce-440a-a992-b682068f6c74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017384516s
Nov  5 18:15:42.211: INFO: Pod "client-envvars-4b45d233-50ce-440a-a992-b682068f6c74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025828221s
STEP: Saw pod success
Nov  5 18:15:42.211: INFO: Pod "client-envvars-4b45d233-50ce-440a-a992-b682068f6c74" satisfied condition "Succeeded or Failed"
Nov  5 18:15:42.217: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod client-envvars-4b45d233-50ce-440a-a992-b682068f6c74 container env3cont: <nil>
STEP: delete the pod
Nov  5 18:15:42.257: INFO: Waiting for pod client-envvars-4b45d233-50ce-440a-a992-b682068f6c74 to disappear
Nov  5 18:15:42.269: INFO: Pod client-envvars-4b45d233-50ce-440a-a992-b682068f6c74 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:42.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8270" for this suite.

• [SLOW TEST:8.254 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":2261,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:42.291: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Nov  5 18:15:42.393: INFO: Waiting up to 5m0s for pod "var-expansion-21785b4c-0086-4701-88fe-b43182edaadd" in namespace "var-expansion-8377" to be "Succeeded or Failed"
Nov  5 18:15:42.405: INFO: Pod "var-expansion-21785b4c-0086-4701-88fe-b43182edaadd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.06961ms
Nov  5 18:15:44.415: INFO: Pod "var-expansion-21785b4c-0086-4701-88fe-b43182edaadd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021343712s
Nov  5 18:15:46.428: INFO: Pod "var-expansion-21785b4c-0086-4701-88fe-b43182edaadd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034347967s
STEP: Saw pod success
Nov  5 18:15:46.428: INFO: Pod "var-expansion-21785b4c-0086-4701-88fe-b43182edaadd" satisfied condition "Succeeded or Failed"
Nov  5 18:15:46.435: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod var-expansion-21785b4c-0086-4701-88fe-b43182edaadd container dapi-container: <nil>
STEP: delete the pod
Nov  5 18:15:46.483: INFO: Waiting for pod var-expansion-21785b4c-0086-4701-88fe-b43182edaadd to disappear
Nov  5 18:15:46.490: INFO: Pod var-expansion-21785b4c-0086-4701-88fe-b43182edaadd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:46.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8377" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":122,"skipped":2265,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:46.516: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-904b5221-46db-4d99-aba8-63e3597167e3
STEP: Creating a pod to test consume configMaps
Nov  5 18:15:46.622: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-751053c2-beb3-4fff-81a6-f6d4f5df6343" in namespace "projected-8332" to be "Succeeded or Failed"
Nov  5 18:15:46.631: INFO: Pod "pod-projected-configmaps-751053c2-beb3-4fff-81a6-f6d4f5df6343": Phase="Pending", Reason="", readiness=false. Elapsed: 9.326002ms
Nov  5 18:15:48.643: INFO: Pod "pod-projected-configmaps-751053c2-beb3-4fff-81a6-f6d4f5df6343": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020706113s
Nov  5 18:15:50.652: INFO: Pod "pod-projected-configmaps-751053c2-beb3-4fff-81a6-f6d4f5df6343": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030453111s
STEP: Saw pod success
Nov  5 18:15:50.652: INFO: Pod "pod-projected-configmaps-751053c2-beb3-4fff-81a6-f6d4f5df6343" satisfied condition "Succeeded or Failed"
Nov  5 18:15:50.669: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-configmaps-751053c2-beb3-4fff-81a6-f6d4f5df6343 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:15:50.719: INFO: Waiting for pod pod-projected-configmaps-751053c2-beb3-4fff-81a6-f6d4f5df6343 to disappear
Nov  5 18:15:50.733: INFO: Pod pod-projected-configmaps-751053c2-beb3-4fff-81a6-f6d4f5df6343 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:50.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8332" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":2271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:50.756: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:15:50.883: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b646a826-53f6-4625-8b51-095828468cb6" in namespace "projected-8092" to be "Succeeded or Failed"
Nov  5 18:15:50.896: INFO: Pod "downwardapi-volume-b646a826-53f6-4625-8b51-095828468cb6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.730222ms
Nov  5 18:15:52.907: INFO: Pod "downwardapi-volume-b646a826-53f6-4625-8b51-095828468cb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023429931s
Nov  5 18:15:54.918: INFO: Pod "downwardapi-volume-b646a826-53f6-4625-8b51-095828468cb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034303894s
STEP: Saw pod success
Nov  5 18:15:54.918: INFO: Pod "downwardapi-volume-b646a826-53f6-4625-8b51-095828468cb6" satisfied condition "Succeeded or Failed"
Nov  5 18:15:54.925: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-b646a826-53f6-4625-8b51-095828468cb6 container client-container: <nil>
STEP: delete the pod
Nov  5 18:15:54.971: INFO: Waiting for pod downwardapi-volume-b646a826-53f6-4625-8b51-095828468cb6 to disappear
Nov  5 18:15:54.980: INFO: Pod downwardapi-volume-b646a826-53f6-4625-8b51-095828468cb6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:54.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8092" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":124,"skipped":2308,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:55.006: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov  5 18:15:55.097: INFO: Waiting up to 5m0s for pod "pod-d1548c33-5ce8-4607-bb5a-66613ffb7c07" in namespace "emptydir-9232" to be "Succeeded or Failed"
Nov  5 18:15:55.108: INFO: Pod "pod-d1548c33-5ce8-4607-bb5a-66613ffb7c07": Phase="Pending", Reason="", readiness=false. Elapsed: 11.197699ms
Nov  5 18:15:57.119: INFO: Pod "pod-d1548c33-5ce8-4607-bb5a-66613ffb7c07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022324646s
Nov  5 18:15:59.126: INFO: Pod "pod-d1548c33-5ce8-4607-bb5a-66613ffb7c07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029389649s
STEP: Saw pod success
Nov  5 18:15:59.127: INFO: Pod "pod-d1548c33-5ce8-4607-bb5a-66613ffb7c07" satisfied condition "Succeeded or Failed"
Nov  5 18:15:59.134: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-d1548c33-5ce8-4607-bb5a-66613ffb7c07 container test-container: <nil>
STEP: delete the pod
Nov  5 18:15:59.174: INFO: Waiting for pod pod-d1548c33-5ce8-4607-bb5a-66613ffb7c07 to disappear
Nov  5 18:15:59.179: INFO: Pod pod-d1548c33-5ce8-4607-bb5a-66613ffb7c07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:15:59.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9232" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":2321,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:15:59.210: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:15:59.303: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9bdfb5b-186c-422f-9f7d-f7dd9e748011" in namespace "projected-5173" to be "Succeeded or Failed"
Nov  5 18:15:59.313: INFO: Pod "downwardapi-volume-c9bdfb5b-186c-422f-9f7d-f7dd9e748011": Phase="Pending", Reason="", readiness=false. Elapsed: 9.212863ms
Nov  5 18:16:01.530: INFO: Pod "downwardapi-volume-c9bdfb5b-186c-422f-9f7d-f7dd9e748011": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225996838s
Nov  5 18:16:03.537: INFO: Pod "downwardapi-volume-c9bdfb5b-186c-422f-9f7d-f7dd9e748011": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.233740587s
STEP: Saw pod success
Nov  5 18:16:03.537: INFO: Pod "downwardapi-volume-c9bdfb5b-186c-422f-9f7d-f7dd9e748011" satisfied condition "Succeeded or Failed"
Nov  5 18:16:03.685: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-c9bdfb5b-186c-422f-9f7d-f7dd9e748011 container client-container: <nil>
STEP: delete the pod
Nov  5 18:16:03.751: INFO: Waiting for pod downwardapi-volume-c9bdfb5b-186c-422f-9f7d-f7dd9e748011 to disappear
Nov  5 18:16:03.758: INFO: Pod downwardapi-volume-c9bdfb5b-186c-422f-9f7d-f7dd9e748011 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:16:03.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5173" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":126,"skipped":2346,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:16:03.787: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3024.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3024.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3024.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3024.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3024.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3024.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 234.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.234_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3024.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3024.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3024.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3024.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3024.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3024.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3024.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 234.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.234_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  5 18:16:09.967: INFO: Unable to read wheezy_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:09.981: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:09.991: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:10.003: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:10.066: INFO: Unable to read jessie_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:10.074: INFO: Unable to read jessie_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:10.082: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:10.092: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:10.148: INFO: Lookups using dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093 failed for: [wheezy_udp@dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_udp@dns-test-service.dns-3024.svc.cluster.local jessie_tcp@dns-test-service.dns-3024.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local]

Nov  5 18:16:15.159: INFO: Unable to read wheezy_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:15.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:15.180: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:15.203: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:15.284: INFO: Unable to read jessie_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:15.295: INFO: Unable to read jessie_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:15.303: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:15.314: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:15.374: INFO: Lookups using dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093 failed for: [wheezy_udp@dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_udp@dns-test-service.dns-3024.svc.cluster.local jessie_tcp@dns-test-service.dns-3024.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local]

Nov  5 18:16:20.160: INFO: Unable to read wheezy_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:20.169: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:20.179: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:20.189: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:20.267: INFO: Unable to read jessie_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:20.280: INFO: Unable to read jessie_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:20.289: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:20.297: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:20.369: INFO: Lookups using dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093 failed for: [wheezy_udp@dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_udp@dns-test-service.dns-3024.svc.cluster.local jessie_tcp@dns-test-service.dns-3024.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local]

Nov  5 18:16:25.161: INFO: Unable to read wheezy_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:25.171: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:25.183: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:25.200: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:25.277: INFO: Unable to read jessie_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:25.287: INFO: Unable to read jessie_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:25.298: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:25.308: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:25.369: INFO: Lookups using dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093 failed for: [wheezy_udp@dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_udp@dns-test-service.dns-3024.svc.cluster.local jessie_tcp@dns-test-service.dns-3024.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local]

Nov  5 18:16:30.160: INFO: Unable to read wheezy_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:30.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:30.181: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:30.189: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:30.270: INFO: Unable to read jessie_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:30.282: INFO: Unable to read jessie_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:30.291: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:30.305: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:30.368: INFO: Lookups using dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093 failed for: [wheezy_udp@dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_udp@dns-test-service.dns-3024.svc.cluster.local jessie_tcp@dns-test-service.dns-3024.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local]

Nov  5 18:16:35.159: INFO: Unable to read wheezy_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:35.168: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:35.179: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:35.187: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:35.345: INFO: Unable to read jessie_udp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:35.355: INFO: Unable to read jessie_tcp@dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:35.366: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:35.377: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local from pod dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093: the server could not find the requested resource (get pods dns-test-3a379d9c-5349-4a29-af24-84bf97c32093)
Nov  5 18:16:35.442: INFO: Lookups using dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093 failed for: [wheezy_udp@dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@dns-test-service.dns-3024.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_udp@dns-test-service.dns-3024.svc.cluster.local jessie_tcp@dns-test-service.dns-3024.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3024.svc.cluster.local]

Nov  5 18:16:40.374: INFO: DNS probes using dns-3024/dns-test-3a379d9c-5349-4a29-af24-84bf97c32093 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:16:40.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3024" for this suite.

• [SLOW TEST:36.719 seconds]
[sig-network] DNS
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":127,"skipped":2354,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:16:40.507: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:16:44.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9386" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":128,"skipped":2360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:16:44.669: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Nov  5 18:16:45.311: INFO: created pod pod-service-account-defaultsa
Nov  5 18:16:45.311: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov  5 18:16:45.321: INFO: created pod pod-service-account-mountsa
Nov  5 18:16:45.322: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov  5 18:16:45.334: INFO: created pod pod-service-account-nomountsa
Nov  5 18:16:45.334: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov  5 18:16:45.344: INFO: created pod pod-service-account-defaultsa-mountspec
Nov  5 18:16:45.344: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov  5 18:16:45.362: INFO: created pod pod-service-account-mountsa-mountspec
Nov  5 18:16:45.362: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov  5 18:16:45.371: INFO: created pod pod-service-account-nomountsa-mountspec
Nov  5 18:16:45.371: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov  5 18:16:45.379: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov  5 18:16:45.379: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov  5 18:16:45.386: INFO: created pod pod-service-account-mountsa-nomountspec
Nov  5 18:16:45.386: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov  5 18:16:45.395: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov  5 18:16:45.395: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:16:45.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9571" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":129,"skipped":2382,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:16:45.421: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Nov  5 18:16:45.491: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:17:07.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1121" for this suite.

• [SLOW TEST:21.753 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":130,"skipped":2384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:17:07.174: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Nov  5 18:17:07.271: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov  5 18:17:07.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-8598'
Nov  5 18:17:07.664: INFO: stderr: ""
Nov  5 18:17:07.664: INFO: stdout: "service/agnhost-replica created\n"
Nov  5 18:17:07.664: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov  5 18:17:07.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-8598'
Nov  5 18:17:07.997: INFO: stderr: ""
Nov  5 18:17:07.998: INFO: stdout: "service/agnhost-primary created\n"
Nov  5 18:17:07.998: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov  5 18:17:07.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-8598'
Nov  5 18:17:08.283: INFO: stderr: ""
Nov  5 18:17:08.283: INFO: stdout: "service/frontend created\n"
Nov  5 18:17:08.284: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov  5 18:17:08.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-8598'
Nov  5 18:17:08.632: INFO: stderr: ""
Nov  5 18:17:08.632: INFO: stdout: "deployment.apps/frontend created\n"
Nov  5 18:17:08.633: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov  5 18:17:08.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-8598'
Nov  5 18:17:08.923: INFO: stderr: ""
Nov  5 18:17:08.923: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov  5 18:17:08.923: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov  5 18:17:08.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-8598'
Nov  5 18:17:09.245: INFO: stderr: ""
Nov  5 18:17:09.245: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Nov  5 18:17:09.245: INFO: Waiting for all frontend pods to be Running.
Nov  5 18:17:14.295: INFO: Waiting for frontend to serve content.
Nov  5 18:17:14.320: INFO: Trying to add a new entry to the guestbook.
Nov  5 18:17:14.338: INFO: Verifying that added entry can be retrieved.
Nov  5 18:17:14.362: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Nov  5 18:17:19.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete --grace-period=0 --force -f - --namespace=kubectl-8598'
Nov  5 18:17:19.554: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  5 18:17:19.554: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Nov  5 18:17:19.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete --grace-period=0 --force -f - --namespace=kubectl-8598'
Nov  5 18:17:19.739: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  5 18:17:19.739: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov  5 18:17:19.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete --grace-period=0 --force -f - --namespace=kubectl-8598'
Nov  5 18:17:19.924: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  5 18:17:19.924: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov  5 18:17:19.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete --grace-period=0 --force -f - --namespace=kubectl-8598'
Nov  5 18:17:20.075: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  5 18:17:20.075: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov  5 18:17:20.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete --grace-period=0 --force -f - --namespace=kubectl-8598'
Nov  5 18:17:20.210: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  5 18:17:20.211: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov  5 18:17:20.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete --grace-period=0 --force -f - --namespace=kubectl-8598'
Nov  5 18:17:20.365: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  5 18:17:20.365: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:17:20.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8598" for this suite.

• [SLOW TEST:13.213 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":131,"skipped":2407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:17:20.387: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:17:27.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5721" for this suite.
STEP: Destroying namespace "nsdeletetest-3187" for this suite.
Nov  5 18:17:27.202: INFO: Namespace nsdeletetest-3187 was already deleted
STEP: Destroying namespace "nsdeletetest-4185" for this suite.

• [SLOW TEST:6.834 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":132,"skipped":2443,"failed":0}
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:17:27.221: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-2127/configmap-test-7727addf-2279-498f-9c76-779b9a8abd1f
STEP: Creating a pod to test consume configMaps
Nov  5 18:17:27.313: INFO: Waiting up to 5m0s for pod "pod-configmaps-dad22b0c-bee1-468f-a296-e316b84df2ed" in namespace "configmap-2127" to be "Succeeded or Failed"
Nov  5 18:17:27.321: INFO: Pod "pod-configmaps-dad22b0c-bee1-468f-a296-e316b84df2ed": Phase="Pending", Reason="", readiness=false. Elapsed: 7.226228ms
Nov  5 18:17:29.334: INFO: Pod "pod-configmaps-dad22b0c-bee1-468f-a296-e316b84df2ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020263505s
Nov  5 18:17:31.348: INFO: Pod "pod-configmaps-dad22b0c-bee1-468f-a296-e316b84df2ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034052568s
Nov  5 18:17:33.359: INFO: Pod "pod-configmaps-dad22b0c-bee1-468f-a296-e316b84df2ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045196113s
STEP: Saw pod success
Nov  5 18:17:33.359: INFO: Pod "pod-configmaps-dad22b0c-bee1-468f-a296-e316b84df2ed" satisfied condition "Succeeded or Failed"
Nov  5 18:17:33.368: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-configmaps-dad22b0c-bee1-468f-a296-e316b84df2ed container env-test: <nil>
STEP: delete the pod
Nov  5 18:17:33.427: INFO: Waiting for pod pod-configmaps-dad22b0c-bee1-468f-a296-e316b84df2ed to disappear
Nov  5 18:17:33.437: INFO: Pod pod-configmaps-dad22b0c-bee1-468f-a296-e316b84df2ed no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:17:33.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2127" for this suite.

• [SLOW TEST:6.236 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2443,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:17:33.463: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Nov  5 18:17:33.556: INFO: Waiting up to 5m0s for pod "client-containers-e4dafb64-2569-49b6-9d00-51ee58499e63" in namespace "containers-361" to be "Succeeded or Failed"
Nov  5 18:17:33.564: INFO: Pod "client-containers-e4dafb64-2569-49b6-9d00-51ee58499e63": Phase="Pending", Reason="", readiness=false. Elapsed: 7.391597ms
Nov  5 18:17:35.574: INFO: Pod "client-containers-e4dafb64-2569-49b6-9d00-51ee58499e63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017335066s
Nov  5 18:17:37.584: INFO: Pod "client-containers-e4dafb64-2569-49b6-9d00-51ee58499e63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027255893s
STEP: Saw pod success
Nov  5 18:17:37.584: INFO: Pod "client-containers-e4dafb64-2569-49b6-9d00-51ee58499e63" satisfied condition "Succeeded or Failed"
Nov  5 18:17:37.593: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod client-containers-e4dafb64-2569-49b6-9d00-51ee58499e63 container test-container: <nil>
STEP: delete the pod
Nov  5 18:17:37.632: INFO: Waiting for pod client-containers-e4dafb64-2569-49b6-9d00-51ee58499e63 to disappear
Nov  5 18:17:37.639: INFO: Pod client-containers-e4dafb64-2569-49b6-9d00-51ee58499e63 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:17:37.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-361" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":134,"skipped":2446,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:17:37.666: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-47bc14b6-c945-4e04-9a6e-708a73d473c3
STEP: Creating a pod to test consume configMaps
Nov  5 18:17:37.771: INFO: Waiting up to 5m0s for pod "pod-configmaps-bbf9aa87-51bf-42db-92c4-4f409258b096" in namespace "configmap-8443" to be "Succeeded or Failed"
Nov  5 18:17:37.780: INFO: Pod "pod-configmaps-bbf9aa87-51bf-42db-92c4-4f409258b096": Phase="Pending", Reason="", readiness=false. Elapsed: 9.404626ms
Nov  5 18:17:39.788: INFO: Pod "pod-configmaps-bbf9aa87-51bf-42db-92c4-4f409258b096": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017244224s
Nov  5 18:17:41.802: INFO: Pod "pod-configmaps-bbf9aa87-51bf-42db-92c4-4f409258b096": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030608833s
STEP: Saw pod success
Nov  5 18:17:41.802: INFO: Pod "pod-configmaps-bbf9aa87-51bf-42db-92c4-4f409258b096" satisfied condition "Succeeded or Failed"
Nov  5 18:17:41.809: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-configmaps-bbf9aa87-51bf-42db-92c4-4f409258b096 container configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:17:41.873: INFO: Waiting for pod pod-configmaps-bbf9aa87-51bf-42db-92c4-4f409258b096 to disappear
Nov  5 18:17:41.880: INFO: Pod pod-configmaps-bbf9aa87-51bf-42db-92c4-4f409258b096 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:17:41.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8443" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2453,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:17:41.907: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1105 18:17:48.063615      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1105 18:17:48.063713      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1105 18:17:48.063734      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov  5 18:17:48.063: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:17:48.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3716" for this suite.

• [SLOW TEST:6.182 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":136,"skipped":2473,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:17:48.089: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov  5 18:17:48.221: INFO: starting watch
STEP: patching
STEP: updating
Nov  5 18:17:48.247: INFO: waiting for watch events with expected annotations
Nov  5 18:17:48.247: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:17:48.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5460" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":137,"skipped":2493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:17:48.386: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-40fc435c-eb78-4042-88c2-dc6ff75f53fc
STEP: Creating a pod to test consume configMaps
Nov  5 18:17:48.488: INFO: Waiting up to 5m0s for pod "pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca" in namespace "configmap-6019" to be "Succeeded or Failed"
Nov  5 18:17:48.497: INFO: Pod "pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.348622ms
Nov  5 18:17:50.507: INFO: Pod "pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019154519s
Nov  5 18:17:52.515: INFO: Pod "pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027147776s
Nov  5 18:17:54.527: INFO: Pod "pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039130546s
Nov  5 18:17:56.539: INFO: Pod "pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050260337s
Nov  5 18:17:58.547: INFO: Pod "pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.058968003s
STEP: Saw pod success
Nov  5 18:17:58.548: INFO: Pod "pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca" satisfied condition "Succeeded or Failed"
Nov  5 18:17:58.556: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca container configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:17:58.610: INFO: Waiting for pod pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca to disappear
Nov  5 18:17:58.617: INFO: Pod pod-configmaps-06d6c4f9-3426-4c76-b9c0-02aab3a521ca no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:17:58.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6019" for this suite.

• [SLOW TEST:10.252 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":138,"skipped":2520,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:17:58.641: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-3fb587d3-7416-42d9-9e17-cd899a8651fd
STEP: Creating a pod to test consume secrets
Nov  5 18:17:58.745: INFO: Waiting up to 5m0s for pod "pod-secrets-b5f8b232-1215-40a2-9252-d8277de19967" in namespace "secrets-1999" to be "Succeeded or Failed"
Nov  5 18:17:58.751: INFO: Pod "pod-secrets-b5f8b232-1215-40a2-9252-d8277de19967": Phase="Pending", Reason="", readiness=false. Elapsed: 6.30455ms
Nov  5 18:18:00.760: INFO: Pod "pod-secrets-b5f8b232-1215-40a2-9252-d8277de19967": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014965588s
Nov  5 18:18:02.771: INFO: Pod "pod-secrets-b5f8b232-1215-40a2-9252-d8277de19967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025659064s
STEP: Saw pod success
Nov  5 18:18:02.771: INFO: Pod "pod-secrets-b5f8b232-1215-40a2-9252-d8277de19967" satisfied condition "Succeeded or Failed"
Nov  5 18:18:02.780: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-secrets-b5f8b232-1215-40a2-9252-d8277de19967 container secret-volume-test: <nil>
STEP: delete the pod
Nov  5 18:18:02.896: INFO: Waiting for pod pod-secrets-b5f8b232-1215-40a2-9252-d8277de19967 to disappear
Nov  5 18:18:02.903: INFO: Pod pod-secrets-b5f8b232-1215-40a2-9252-d8277de19967 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:18:02.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1999" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":139,"skipped":2536,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:18:02.973: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:18:03.071: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3fea1bb7-d326-4d38-83b8-df88a5196feb" in namespace "downward-api-4347" to be "Succeeded or Failed"
Nov  5 18:18:03.077: INFO: Pod "downwardapi-volume-3fea1bb7-d326-4d38-83b8-df88a5196feb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.199488ms
Nov  5 18:18:05.083: INFO: Pod "downwardapi-volume-3fea1bb7-d326-4d38-83b8-df88a5196feb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012311477s
Nov  5 18:18:07.092: INFO: Pod "downwardapi-volume-3fea1bb7-d326-4d38-83b8-df88a5196feb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021170835s
STEP: Saw pod success
Nov  5 18:18:07.092: INFO: Pod "downwardapi-volume-3fea1bb7-d326-4d38-83b8-df88a5196feb" satisfied condition "Succeeded or Failed"
Nov  5 18:18:07.101: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-3fea1bb7-d326-4d38-83b8-df88a5196feb container client-container: <nil>
STEP: delete the pod
Nov  5 18:18:07.146: INFO: Waiting for pod downwardapi-volume-3fea1bb7-d326-4d38-83b8-df88a5196feb to disappear
Nov  5 18:18:07.152: INFO: Pod downwardapi-volume-3fea1bb7-d326-4d38-83b8-df88a5196feb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:18:07.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4347" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":140,"skipped":2553,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:18:07.174: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Nov  5 18:18:07.285: INFO: Waiting up to 5m0s for pod "var-expansion-295a5db7-b0a2-4779-a3ac-ea79ca796888" in namespace "var-expansion-125" to be "Succeeded or Failed"
Nov  5 18:18:07.294: INFO: Pod "var-expansion-295a5db7-b0a2-4779-a3ac-ea79ca796888": Phase="Pending", Reason="", readiness=false. Elapsed: 8.968134ms
Nov  5 18:18:09.304: INFO: Pod "var-expansion-295a5db7-b0a2-4779-a3ac-ea79ca796888": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01909501s
Nov  5 18:18:11.312: INFO: Pod "var-expansion-295a5db7-b0a2-4779-a3ac-ea79ca796888": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026715205s
Nov  5 18:18:13.320: INFO: Pod "var-expansion-295a5db7-b0a2-4779-a3ac-ea79ca796888": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034796657s
STEP: Saw pod success
Nov  5 18:18:13.320: INFO: Pod "var-expansion-295a5db7-b0a2-4779-a3ac-ea79ca796888" satisfied condition "Succeeded or Failed"
Nov  5 18:18:13.329: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod var-expansion-295a5db7-b0a2-4779-a3ac-ea79ca796888 container dapi-container: <nil>
STEP: delete the pod
Nov  5 18:18:13.386: INFO: Waiting for pod var-expansion-295a5db7-b0a2-4779-a3ac-ea79ca796888 to disappear
Nov  5 18:18:13.392: INFO: Pod var-expansion-295a5db7-b0a2-4779-a3ac-ea79ca796888 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:18:13.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-125" for this suite.

• [SLOW TEST:6.238 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":141,"skipped":2562,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:18:13.412: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-6805dabc-8629-450c-9c58-35583ecdd679
STEP: Creating a pod to test consume configMaps
Nov  5 18:18:13.509: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e8bc2c05-0d18-4952-8a9d-cb653c5c14ce" in namespace "projected-7572" to be "Succeeded or Failed"
Nov  5 18:18:13.519: INFO: Pod "pod-projected-configmaps-e8bc2c05-0d18-4952-8a9d-cb653c5c14ce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.465848ms
Nov  5 18:18:15.528: INFO: Pod "pod-projected-configmaps-e8bc2c05-0d18-4952-8a9d-cb653c5c14ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018327851s
Nov  5 18:18:17.539: INFO: Pod "pod-projected-configmaps-e8bc2c05-0d18-4952-8a9d-cb653c5c14ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030063389s
STEP: Saw pod success
Nov  5 18:18:17.539: INFO: Pod "pod-projected-configmaps-e8bc2c05-0d18-4952-8a9d-cb653c5c14ce" satisfied condition "Succeeded or Failed"
Nov  5 18:18:17.549: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-configmaps-e8bc2c05-0d18-4952-8a9d-cb653c5c14ce container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:18:17.602: INFO: Waiting for pod pod-projected-configmaps-e8bc2c05-0d18-4952-8a9d-cb653c5c14ce to disappear
Nov  5 18:18:17.610: INFO: Pod pod-projected-configmaps-e8bc2c05-0d18-4952-8a9d-cb653c5c14ce no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:18:17.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7572" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":142,"skipped":2562,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:18:17.632: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov  5 18:18:17.713: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov  5 18:18:17.742: INFO: Waiting for terminating namespaces to be deleted...
Nov  5 18:18:17.749: INFO: 
Logging pods the apiserver thinks is on node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 before test
Nov  5 18:18:17.763: INFO: canal-55ppt from kube-system started at 2020-11-05 17:15:19 +0000 UTC (2 container statuses recorded)
Nov  5 18:18:17.763: INFO: 	Container calico-node ready: true, restart count 0
Nov  5 18:18:17.763: INFO: 	Container kube-flannel ready: true, restart count 2
Nov  5 18:18:17.763: INFO: coredns-6ccf64844b-zjcgs from kube-system started at 2020-11-05 17:23:56 +0000 UTC (1 container statuses recorded)
Nov  5 18:18:17.763: INFO: 	Container coredns ready: true, restart count 0
Nov  5 18:18:17.763: INFO: kube-proxy-j2wfk from kube-system started at 2020-11-05 17:15:40 +0000 UTC (1 container statuses recorded)
Nov  5 18:18:17.764: INFO: 	Container kube-proxy ready: true, restart count 0
Nov  5 18:18:17.764: INFO: metrics-server-664567f776-8wvsk from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 18:18:17.764: INFO: 	Container metrics-server ready: true, restart count 0
Nov  5 18:18:17.764: INFO: wormhole-2r8fc from kube-system started at 2020-10-19 12:36:30 +0000 UTC (1 container statuses recorded)
Nov  5 18:18:17.764: INFO: 	Container wormhole ready: true, restart count 0
Nov  5 18:18:17.764: INFO: sonobuoy from sonobuoy started at 2020-11-05 17:25:05 +0000 UTC (1 container statuses recorded)
Nov  5 18:18:17.764: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov  5 18:18:17.764: INFO: sonobuoy-e2e-job-6a656b1abb77479a from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:18:17.764: INFO: 	Container e2e ready: true, restart count 0
Nov  5 18:18:17.764: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 18:18:17.764: INFO: sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-vv8nw from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:18:17.764: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 18:18:17.765: INFO: 	Container systemd-logs ready: true, restart count 0
Nov  5 18:18:17.765: INFO: 
Logging pods the apiserver thinks is on node node-a7de9392-4cc3-4830-84ff-49e7615b291e before test
Nov  5 18:18:17.786: INFO: canal-sqp5g from kube-system started at 2020-11-05 17:15:35 +0000 UTC (2 container statuses recorded)
Nov  5 18:18:17.786: INFO: 	Container calico-node ready: true, restart count 0
Nov  5 18:18:17.786: INFO: 	Container kube-flannel ready: true, restart count 0
Nov  5 18:18:17.786: INFO: coredns-6ccf64844b-qrpm5 from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 18:18:17.786: INFO: 	Container coredns ready: true, restart count 0
Nov  5 18:18:17.786: INFO: kube-dns-autoscaler-7944596f47-whbv8 from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 18:18:17.786: INFO: 	Container autoscaler ready: true, restart count 0
Nov  5 18:18:17.786: INFO: kube-proxy-bhxd4 from kube-system started at 2020-11-05 17:15:16 +0000 UTC (1 container statuses recorded)
Nov  5 18:18:17.786: INFO: 	Container kube-proxy ready: true, restart count 0
Nov  5 18:18:17.786: INFO: wormhole-4m5c5 from kube-system started at 2020-10-19 12:36:45 +0000 UTC (1 container statuses recorded)
Nov  5 18:18:17.786: INFO: 	Container wormhole ready: true, restart count 0
Nov  5 18:18:17.786: INFO: sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-nf5fs from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:18:17.786: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 18:18:17.786: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8ebd66ec-fb43-4013-ba3a-60d0f70d43d0 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-8ebd66ec-fb43-4013-ba3a-60d0f70d43d0 off the node node-a7de9392-4cc3-4830-84ff-49e7615b291e
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8ebd66ec-fb43-4013-ba3a-60d0f70d43d0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:18:36.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4753" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:18.401 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":143,"skipped":2564,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:18:36.038: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-d64f1860-f2cd-4554-8349-cf686ed1bfed
STEP: Creating a pod to test consume secrets
Nov  5 18:18:36.256: INFO: Waiting up to 5m0s for pod "pod-secrets-bc505367-fce1-4afa-aa5d-d2536bb7fad3" in namespace "secrets-999" to be "Succeeded or Failed"
Nov  5 18:18:36.266: INFO: Pod "pod-secrets-bc505367-fce1-4afa-aa5d-d2536bb7fad3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.762359ms
Nov  5 18:18:38.280: INFO: Pod "pod-secrets-bc505367-fce1-4afa-aa5d-d2536bb7fad3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023007747s
Nov  5 18:18:40.290: INFO: Pod "pod-secrets-bc505367-fce1-4afa-aa5d-d2536bb7fad3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033608566s
STEP: Saw pod success
Nov  5 18:18:40.290: INFO: Pod "pod-secrets-bc505367-fce1-4afa-aa5d-d2536bb7fad3" satisfied condition "Succeeded or Failed"
Nov  5 18:18:40.301: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-secrets-bc505367-fce1-4afa-aa5d-d2536bb7fad3 container secret-volume-test: <nil>
STEP: delete the pod
Nov  5 18:18:40.352: INFO: Waiting for pod pod-secrets-bc505367-fce1-4afa-aa5d-d2536bb7fad3 to disappear
Nov  5 18:18:40.359: INFO: Pod pod-secrets-bc505367-fce1-4afa-aa5d-d2536bb7fad3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:18:40.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-999" for this suite.
STEP: Destroying namespace "secret-namespace-2561" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":144,"skipped":2584,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:18:40.401: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-44e3d06d-f522-445e-92aa-9be0b70405dd
STEP: Creating a pod to test consume configMaps
Nov  5 18:18:40.497: INFO: Waiting up to 5m0s for pod "pod-configmaps-926288f4-3fd8-423b-a8ed-e7c4b0e06c20" in namespace "configmap-7570" to be "Succeeded or Failed"
Nov  5 18:18:40.505: INFO: Pod "pod-configmaps-926288f4-3fd8-423b-a8ed-e7c4b0e06c20": Phase="Pending", Reason="", readiness=false. Elapsed: 8.312515ms
Nov  5 18:18:42.512: INFO: Pod "pod-configmaps-926288f4-3fd8-423b-a8ed-e7c4b0e06c20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015088565s
Nov  5 18:18:44.523: INFO: Pod "pod-configmaps-926288f4-3fd8-423b-a8ed-e7c4b0e06c20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026319112s
STEP: Saw pod success
Nov  5 18:18:44.523: INFO: Pod "pod-configmaps-926288f4-3fd8-423b-a8ed-e7c4b0e06c20" satisfied condition "Succeeded or Failed"
Nov  5 18:18:44.529: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-configmaps-926288f4-3fd8-423b-a8ed-e7c4b0e06c20 container configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:18:44.575: INFO: Waiting for pod pod-configmaps-926288f4-3fd8-423b-a8ed-e7c4b0e06c20 to disappear
Nov  5 18:18:44.584: INFO: Pod pod-configmaps-926288f4-3fd8-423b-a8ed-e7c4b0e06c20 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:18:44.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7570" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2592,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:18:44.624: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:18:55.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7002" for this suite.

• [SLOW TEST:11.495 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":146,"skipped":2592,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:18:56.124: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3751
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov  5 18:18:56.209: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov  5 18:18:56.268: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 18:18:58.276: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov  5 18:19:00.278: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 18:19:02.276: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 18:19:04.279: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 18:19:06.289: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov  5 18:19:08.277: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov  5 18:19:08.292: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov  5 18:19:10.302: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov  5 18:19:12.301: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov  5 18:19:16.381: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.0.57 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3751 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 18:19:16.381: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 18:19:17.593: INFO: Found all expected endpoints: [netserver-0]
Nov  5 18:19:17.603: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.1.211 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3751 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 18:19:17.603: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 18:19:18.795: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:19:18.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3751" for this suite.

• [SLOW TEST:22.699 seconds]
[sig-network] Networking
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":147,"skipped":2599,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:19:18.825: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov  5 18:19:18.948: INFO: Waiting up to 5m0s for pod "pod-e2b19367-3142-4502-b57c-19d9bcc44c17" in namespace "emptydir-9963" to be "Succeeded or Failed"
Nov  5 18:19:18.958: INFO: Pod "pod-e2b19367-3142-4502-b57c-19d9bcc44c17": Phase="Pending", Reason="", readiness=false. Elapsed: 9.585284ms
Nov  5 18:19:20.966: INFO: Pod "pod-e2b19367-3142-4502-b57c-19d9bcc44c17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018062928s
Nov  5 18:19:22.977: INFO: Pod "pod-e2b19367-3142-4502-b57c-19d9bcc44c17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028877742s
STEP: Saw pod success
Nov  5 18:19:22.977: INFO: Pod "pod-e2b19367-3142-4502-b57c-19d9bcc44c17" satisfied condition "Succeeded or Failed"
Nov  5 18:19:22.983: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-e2b19367-3142-4502-b57c-19d9bcc44c17 container test-container: <nil>
STEP: delete the pod
Nov  5 18:19:23.028: INFO: Waiting for pod pod-e2b19367-3142-4502-b57c-19d9bcc44c17 to disappear
Nov  5 18:19:23.034: INFO: Pod pod-e2b19367-3142-4502-b57c-19d9bcc44c17 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:19:23.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9963" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2611,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:19:23.061: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:19:23.150: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Creating first CR 
Nov  5 18:19:24.580: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-05T18:19:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-05T18:19:24Z]] name:name1 resourceVersion:1647453544 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d88f0a04-6fe0-42c9-9aac-9793c28c09e5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov  5 18:19:34.595: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-05T18:19:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-05T18:19:34Z]] name:name2 resourceVersion:1647456855 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:384462bc-7a2d-44f3-a09e-94a6863a658b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov  5 18:19:44.622: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-05T18:19:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-05T18:19:44Z]] name:name1 resourceVersion:1647460109 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d88f0a04-6fe0-42c9-9aac-9793c28c09e5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov  5 18:19:54.635: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-05T18:19:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-05T18:19:54Z]] name:name2 resourceVersion:1647463301 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:384462bc-7a2d-44f3-a09e-94a6863a658b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov  5 18:20:04.657: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-05T18:19:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-05T18:19:44Z]] name:name1 resourceVersion:1647466702 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d88f0a04-6fe0-42c9-9aac-9793c28c09e5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov  5 18:20:14.694: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-05T18:19:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-05T18:19:54Z]] name:name2 resourceVersion:1647470133 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:384462bc-7a2d-44f3-a09e-94a6863a658b] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:20:25.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6428" for this suite.

• [SLOW TEST:62.374 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":149,"skipped":2619,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:20:25.438: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1165
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1165
STEP: Creating statefulset with conflicting port in namespace statefulset-1165
STEP: Waiting until pod test-pod will start running in namespace statefulset-1165
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1165
Nov  5 18:20:31.697: INFO: Observed stateful pod in namespace: statefulset-1165, name: ss-0, uid: 33b01ac3-de23-4a8d-b82e-c3169e3400ca, status phase: Pending. Waiting for statefulset controller to delete.
Nov  5 18:20:31.972: INFO: Observed stateful pod in namespace: statefulset-1165, name: ss-0, uid: 33b01ac3-de23-4a8d-b82e-c3169e3400ca, status phase: Failed. Waiting for statefulset controller to delete.
Nov  5 18:20:31.987: INFO: Observed stateful pod in namespace: statefulset-1165, name: ss-0, uid: 33b01ac3-de23-4a8d-b82e-c3169e3400ca, status phase: Failed. Waiting for statefulset controller to delete.
Nov  5 18:20:32.013: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1165
STEP: Removing pod with conflicting port in namespace statefulset-1165
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1165 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov  5 18:20:40.111: INFO: Deleting all statefulset in ns statefulset-1165
Nov  5 18:20:40.118: INFO: Scaling statefulset ss to 0
Nov  5 18:20:50.158: INFO: Waiting for statefulset status.replicas updated to 0
Nov  5 18:20:50.167: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:20:50.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1165" for this suite.

• [SLOW TEST:24.794 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":150,"skipped":2622,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:20:50.241: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov  5 18:20:50.341: INFO: Waiting up to 5m0s for pod "pod-3c0b4739-935a-4688-bc15-3ce27d26d516" in namespace "emptydir-2882" to be "Succeeded or Failed"
Nov  5 18:20:50.349: INFO: Pod "pod-3c0b4739-935a-4688-bc15-3ce27d26d516": Phase="Pending", Reason="", readiness=false. Elapsed: 7.275325ms
Nov  5 18:20:52.359: INFO: Pod "pod-3c0b4739-935a-4688-bc15-3ce27d26d516": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017496889s
Nov  5 18:20:54.369: INFO: Pod "pod-3c0b4739-935a-4688-bc15-3ce27d26d516": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027327337s
STEP: Saw pod success
Nov  5 18:20:54.369: INFO: Pod "pod-3c0b4739-935a-4688-bc15-3ce27d26d516" satisfied condition "Succeeded or Failed"
Nov  5 18:20:54.376: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-3c0b4739-935a-4688-bc15-3ce27d26d516 container test-container: <nil>
STEP: delete the pod
Nov  5 18:20:54.479: INFO: Waiting for pod pod-3c0b4739-935a-4688-bc15-3ce27d26d516 to disappear
Nov  5 18:20:54.484: INFO: Pod pod-3c0b4739-935a-4688-bc15-3ce27d26d516 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:20:54.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2882" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":151,"skipped":2650,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:20:54.518: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:20:54.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ac4c5fe-b07f-4ad9-af22-479d4a7d07aa" in namespace "downward-api-4544" to be "Succeeded or Failed"
Nov  5 18:20:54.628: INFO: Pod "downwardapi-volume-4ac4c5fe-b07f-4ad9-af22-479d4a7d07aa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.843327ms
Nov  5 18:20:56.637: INFO: Pod "downwardapi-volume-4ac4c5fe-b07f-4ad9-af22-479d4a7d07aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016646914s
Nov  5 18:20:58.650: INFO: Pod "downwardapi-volume-4ac4c5fe-b07f-4ad9-af22-479d4a7d07aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029161373s
STEP: Saw pod success
Nov  5 18:20:58.650: INFO: Pod "downwardapi-volume-4ac4c5fe-b07f-4ad9-af22-479d4a7d07aa" satisfied condition "Succeeded or Failed"
Nov  5 18:20:58.658: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-4ac4c5fe-b07f-4ad9-af22-479d4a7d07aa container client-container: <nil>
STEP: delete the pod
Nov  5 18:20:58.702: INFO: Waiting for pod downwardapi-volume-4ac4c5fe-b07f-4ad9-af22-479d4a7d07aa to disappear
Nov  5 18:20:58.709: INFO: Pod downwardapi-volume-4ac4c5fe-b07f-4ad9-af22-479d4a7d07aa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:20:58.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4544" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":152,"skipped":2681,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:20:58.734: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov  5 18:20:58.813: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 18:21:02.733: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:21:18.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9155" for this suite.

• [SLOW TEST:19.936 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":153,"skipped":2683,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:21:18.670: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:21:19.966: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 18:21:21.993: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197279, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197279, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197280, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197279, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:21:25.022: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:21:35.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4824" for this suite.
STEP: Destroying namespace "webhook-4824-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.753 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":154,"skipped":2685,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:21:35.425: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:21:36.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197296, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197296, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197296, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197295, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:21:38.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197296, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197296, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197296, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197295, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:21:41.243: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:21:41.251: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1526-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:21:42.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4029" for this suite.
STEP: Destroying namespace "webhook-4029-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.172 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":155,"skipped":2686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:21:42.598: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-a209deda-781e-41b3-97e3-7eb6d7ae927d in namespace container-probe-5220
Nov  5 18:21:46.733: INFO: Started pod busybox-a209deda-781e-41b3-97e3-7eb6d7ae927d in namespace container-probe-5220
STEP: checking the pod's current state and verifying that restartCount is present
Nov  5 18:21:46.739: INFO: Initial restart count of pod busybox-a209deda-781e-41b3-97e3-7eb6d7ae927d is 0
Nov  5 18:22:41.592: INFO: Restart count of pod container-probe-5220/busybox-a209deda-781e-41b3-97e3-7eb6d7ae927d is now 1 (54.853022823s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:22:41.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5220" for this suite.

• [SLOW TEST:59.046 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":156,"skipped":2741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:22:41.646: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov  5 18:22:41.735: INFO: Waiting up to 5m0s for pod "pod-ba08a697-21d9-4a56-a111-b201fc5a2522" in namespace "emptydir-9351" to be "Succeeded or Failed"
Nov  5 18:22:41.744: INFO: Pod "pod-ba08a697-21d9-4a56-a111-b201fc5a2522": Phase="Pending", Reason="", readiness=false. Elapsed: 8.845085ms
Nov  5 18:22:43.753: INFO: Pod "pod-ba08a697-21d9-4a56-a111-b201fc5a2522": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017902652s
Nov  5 18:22:45.763: INFO: Pod "pod-ba08a697-21d9-4a56-a111-b201fc5a2522": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027424115s
STEP: Saw pod success
Nov  5 18:22:45.763: INFO: Pod "pod-ba08a697-21d9-4a56-a111-b201fc5a2522" satisfied condition "Succeeded or Failed"
Nov  5 18:22:45.770: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-ba08a697-21d9-4a56-a111-b201fc5a2522 container test-container: <nil>
STEP: delete the pod
Nov  5 18:22:45.874: INFO: Waiting for pod pod-ba08a697-21d9-4a56-a111-b201fc5a2522 to disappear
Nov  5 18:22:45.880: INFO: Pod pod-ba08a697-21d9-4a56-a111-b201fc5a2522 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:22:45.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9351" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2775,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:22:45.902: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-e536f580-8841-4e0c-bbaa-1083921e84fc
STEP: Creating a pod to test consume configMaps
Nov  5 18:22:46.022: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-11730e15-13f4-43b1-89cf-3c9b95f61ae1" in namespace "projected-4034" to be "Succeeded or Failed"
Nov  5 18:22:46.035: INFO: Pod "pod-projected-configmaps-11730e15-13f4-43b1-89cf-3c9b95f61ae1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.695353ms
Nov  5 18:22:48.046: INFO: Pod "pod-projected-configmaps-11730e15-13f4-43b1-89cf-3c9b95f61ae1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024353781s
Nov  5 18:22:50.058: INFO: Pod "pod-projected-configmaps-11730e15-13f4-43b1-89cf-3c9b95f61ae1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036599184s
STEP: Saw pod success
Nov  5 18:22:50.058: INFO: Pod "pod-projected-configmaps-11730e15-13f4-43b1-89cf-3c9b95f61ae1" satisfied condition "Succeeded or Failed"
Nov  5 18:22:50.068: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-configmaps-11730e15-13f4-43b1-89cf-3c9b95f61ae1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:22:50.114: INFO: Waiting for pod pod-projected-configmaps-11730e15-13f4-43b1-89cf-3c9b95f61ae1 to disappear
Nov  5 18:22:50.121: INFO: Pod pod-projected-configmaps-11730e15-13f4-43b1-89cf-3c9b95f61ae1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:22:50.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4034" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":158,"skipped":2779,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:22:50.141: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-57c1a866-b1c7-45f1-bc03-dc19f2c17d08
STEP: Creating a pod to test consume configMaps
Nov  5 18:22:50.252: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bde09dd9-56f9-4eea-bad9-1c028c079ae3" in namespace "projected-6687" to be "Succeeded or Failed"
Nov  5 18:22:50.265: INFO: Pod "pod-projected-configmaps-bde09dd9-56f9-4eea-bad9-1c028c079ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.946126ms
Nov  5 18:22:52.273: INFO: Pod "pod-projected-configmaps-bde09dd9-56f9-4eea-bad9-1c028c079ae3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020913804s
STEP: Saw pod success
Nov  5 18:22:52.273: INFO: Pod "pod-projected-configmaps-bde09dd9-56f9-4eea-bad9-1c028c079ae3" satisfied condition "Succeeded or Failed"
Nov  5 18:22:52.280: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-configmaps-bde09dd9-56f9-4eea-bad9-1c028c079ae3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:22:52.320: INFO: Waiting for pod pod-projected-configmaps-bde09dd9-56f9-4eea-bad9-1c028c079ae3 to disappear
Nov  5 18:22:52.327: INFO: Pod pod-projected-configmaps-bde09dd9-56f9-4eea-bad9-1c028c079ae3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:22:52.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6687" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":159,"skipped":2789,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:22:52.348: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:22:52.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-5556'
Nov  5 18:22:53.235: INFO: stderr: ""
Nov  5 18:22:53.236: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov  5 18:22:53.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-5556'
Nov  5 18:22:53.553: INFO: stderr: ""
Nov  5 18:22:53.553: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov  5 18:22:54.562: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 18:22:54.562: INFO: Found 0 / 1
Nov  5 18:22:55.562: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 18:22:55.562: INFO: Found 0 / 1
Nov  5 18:22:56.563: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 18:22:56.563: INFO: Found 1 / 1
Nov  5 18:22:56.563: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov  5 18:22:56.573: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 18:22:56.573: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov  5 18:22:56.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 describe pod agnhost-primary-hmzx7 --namespace=kubectl-5556'
Nov  5 18:22:56.746: INFO: stderr: ""
Nov  5 18:22:56.746: INFO: stdout: "Name:         agnhost-primary-hmzx7\nNamespace:    kubectl-5556\nPriority:     0\nNode:         node-a7de9392-4cc3-4830-84ff-49e7615b291e/51.91.151.116\nStart Time:   Thu, 05 Nov 2020 18:22:54 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.2.1.224/32\nStatus:       Running\nIP:           10.2.1.224\nIPs:\n  IP:           10.2.1.224\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://e6580e331619caa8df624c2b1010f396d54dace20cac29f6c987219c9acaccc1\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Nov 2020 18:22:56 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9wcjn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-9wcjn:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-9wcjn\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-5556/agnhost-primary-hmzx7 to node-a7de9392-4cc3-4830-84ff-49e7615b291e\n  Normal  Pulling    1s    kubelet            Pulling image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\"\n  Normal  Pulled     1s    kubelet            Successfully pulled image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" in 730.935563ms\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Nov  5 18:22:56.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 describe rc agnhost-primary --namespace=kubectl-5556'
Nov  5 18:22:56.926: INFO: stderr: ""
Nov  5 18:22:56.926: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5556\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-hmzx7\n"
Nov  5 18:22:56.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 describe service agnhost-primary --namespace=kubectl-5556'
Nov  5 18:22:57.097: INFO: stderr: ""
Nov  5 18:22:57.098: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5556\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.3.253.29\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.2.1.224:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov  5 18:22:57.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 describe node node-29f9289e-2d09-4bea-972e-88a5817d7ad8'
Nov  5 18:22:57.314: INFO: stderr: ""
Nov  5 18:22:57.314: INFO: stdout: "Name:               node-29f9289e-2d09-4bea-972e-88a5817d7ad8\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=9b312f04-e6ff-493f-bbd1-134510f49258\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=SBG5\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=node-29f9289e-2d09-4bea-972e-88a5817d7ad8\n                    kubernetes.io/os=linux\n                    node.k8s.ovh/type=standard\n                    node.kubernetes.io/instance-type=9b312f04-e6ff-493f-bbd1-134510f49258\n                    nodepool=nodepool-5257d3bf-a801-4298-8041-70a6b9e20136\n                    topology.cinder.csi.openstack.org/zone=nova\n                    topology.kubernetes.io/region=SBG5\n                    topology.kubernetes.io/zone=nova\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 51.83.35.110\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"cb00c8e8-2b2a-4f6d-9d97-aa280ac959c8\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"ea:5d:e4:29:3c:6c\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 51.83.35.110\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.2.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 19 Oct 2020 12:36:30 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  node-29f9289e-2d09-4bea-972e-88a5817d7ad8\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 05 Nov 2020 18:22:56 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 05 Nov 2020 18:19:53 +0000   Thu, 05 Nov 2020 17:23:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 05 Nov 2020 18:19:53 +0000   Thu, 05 Nov 2020 17:23:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 05 Nov 2020 18:19:53 +0000   Thu, 05 Nov 2020 17:23:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 05 Nov 2020 18:19:53 +0000   Thu, 05 Nov 2020 17:23:29 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  51.83.35.110\n  Hostname:    node-29f9289e-2d09-4bea-972e-88a5817d7ad8\nCapacity:\n  cpu:                2\n  ephemeral-storage:  50633164Ki\n  hugepages-2Mi:      0\n  memory:             6965740Ki\n  pods:               110\nAllocatable:\n  cpu:                1900m\n  ephemeral-storage:  50633164Ki\n  hugepages-2Mi:      0\n  memory:             5405164Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 cb00c8e82b2a4f6d9d97aa280ac959c8\n  System UUID:                CB00C8E8-2B2A-4F6D-9D97-AA280AC959C8\n  Boot ID:                    fde87f83-afa9-447f-830d-95ae10fed34c\n  Kernel Version:             4.15.0-122-generic\n  OS Image:                   Ubuntu 18.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.6.3\n  Kubelet Version:            v1.19.2\n  Kube-Proxy Version:         v1.19.2\nPodCIDR:                      10.2.0.0/24\nPodCIDRs:                     10.2.0.0/24\nProviderID:                   openstack:///cb00c8e8-2b2a-4f6d-9d97-aa280ac959c8\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-55ppt                                                250m (13%)    0 (0%)      0 (0%)           0 (0%)         67m\n  kube-system                 coredns-6ccf64844b-zjcgs                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (3%)     59m\n  kube-system                 kube-proxy-j2wfk                                           100m (5%)     0 (0%)      200Mi (3%)       200Mi (3%)     67m\n  kube-system                 metrics-server-664567f776-8wvsk                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  kube-system                 wormhole-2r8fc                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         17d\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         57m\n  sonobuoy                    sonobuoy-e2e-job-6a656b1abb77479a                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         57m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-vv8nw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         57m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                450m (23%)  0 (0%)\n  memory             270Mi (5%)  370Mi (7%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                From        Message\n  ----     ------                   ----               ----        -------\n  Normal   Starting                 59m                kubelet     Starting kubelet.\n  Normal   NodeHasSufficientMemory  59m (x2 over 59m)  kubelet     Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    59m (x2 over 59m)  kubelet     Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     59m (x2 over 59m)  kubelet     Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  59m                kubelet     Updated limits on kube reserved cgroup /system.slice\n  Normal   NodeAllocatableEnforced  59m                kubelet     Updated Node Allocatable limit across pods\n  Warning  Rebooted                 59m                kubelet     Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 has been rebooted, boot id: fde87f83-afa9-447f-830d-95ae10fed34c\n  Normal   NodeNotReady             59m                kubelet     Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 status is now: NodeNotReady\n  Normal   NodeNotSchedulable       59m                kubelet     Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 status is now: NodeNotSchedulable\n  Normal   Starting                 59m                kube-proxy  Starting kube-proxy.\n  Normal   NodeReady                59m                kubelet     Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 status is now: NodeReady\n  Normal   NodeSchedulable          58m                kubelet     Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 status is now: NodeSchedulable\n"
Nov  5 18:22:57.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 describe namespace kubectl-5556'
Nov  5 18:22:57.484: INFO: stderr: ""
Nov  5 18:22:57.484: INFO: stdout: "Name:         kubectl-5556\nLabels:       e2e-framework=kubectl\n              e2e-run=d4797c05-1fba-4ca2-bb90-f0b59d3c3acd\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:22:57.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5556" for this suite.

• [SLOW TEST:5.173 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":160,"skipped":2789,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:22:57.521: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-594c9787-4481-4466-9623-dc2bfbc79313
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:22:57.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1939" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":161,"skipped":2801,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:22:57.643: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:22:57.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6a80298-2bde-4d9d-a31e-6ca9244d02e6" in namespace "downward-api-1205" to be "Succeeded or Failed"
Nov  5 18:22:57.757: INFO: Pod "downwardapi-volume-a6a80298-2bde-4d9d-a31e-6ca9244d02e6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.128203ms
Nov  5 18:22:59.765: INFO: Pod "downwardapi-volume-a6a80298-2bde-4d9d-a31e-6ca9244d02e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018702206s
Nov  5 18:23:01.777: INFO: Pod "downwardapi-volume-a6a80298-2bde-4d9d-a31e-6ca9244d02e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030080663s
STEP: Saw pod success
Nov  5 18:23:01.777: INFO: Pod "downwardapi-volume-a6a80298-2bde-4d9d-a31e-6ca9244d02e6" satisfied condition "Succeeded or Failed"
Nov  5 18:23:01.785: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-a6a80298-2bde-4d9d-a31e-6ca9244d02e6 container client-container: <nil>
STEP: delete the pod
Nov  5 18:23:01.841: INFO: Waiting for pod downwardapi-volume-a6a80298-2bde-4d9d-a31e-6ca9244d02e6 to disappear
Nov  5 18:23:01.849: INFO: Pod downwardapi-volume-a6a80298-2bde-4d9d-a31e-6ca9244d02e6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:23:01.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1205" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":162,"skipped":2806,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:23:01.868: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov  5 18:23:01.944: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov  5 18:23:01.962: INFO: Waiting for terminating namespaces to be deleted...
Nov  5 18:23:01.970: INFO: 
Logging pods the apiserver thinks is on node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 before test
Nov  5 18:23:01.982: INFO: canal-55ppt from kube-system started at 2020-11-05 17:15:19 +0000 UTC (2 container statuses recorded)
Nov  5 18:23:01.982: INFO: 	Container calico-node ready: true, restart count 0
Nov  5 18:23:01.982: INFO: 	Container kube-flannel ready: true, restart count 2
Nov  5 18:23:01.982: INFO: coredns-6ccf64844b-zjcgs from kube-system started at 2020-11-05 17:23:56 +0000 UTC (1 container statuses recorded)
Nov  5 18:23:01.982: INFO: 	Container coredns ready: true, restart count 0
Nov  5 18:23:01.982: INFO: kube-proxy-j2wfk from kube-system started at 2020-11-05 17:15:40 +0000 UTC (1 container statuses recorded)
Nov  5 18:23:01.982: INFO: 	Container kube-proxy ready: true, restart count 0
Nov  5 18:23:01.982: INFO: metrics-server-664567f776-8wvsk from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 18:23:01.982: INFO: 	Container metrics-server ready: true, restart count 0
Nov  5 18:23:01.982: INFO: wormhole-2r8fc from kube-system started at 2020-10-19 12:36:30 +0000 UTC (1 container statuses recorded)
Nov  5 18:23:01.982: INFO: 	Container wormhole ready: true, restart count 0
Nov  5 18:23:01.982: INFO: sonobuoy from sonobuoy started at 2020-11-05 17:25:05 +0000 UTC (1 container statuses recorded)
Nov  5 18:23:01.982: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov  5 18:23:01.982: INFO: sonobuoy-e2e-job-6a656b1abb77479a from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:23:01.982: INFO: 	Container e2e ready: true, restart count 0
Nov  5 18:23:01.982: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 18:23:01.982: INFO: sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-vv8nw from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:23:01.982: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 18:23:01.982: INFO: 	Container systemd-logs ready: true, restart count 0
Nov  5 18:23:01.982: INFO: 
Logging pods the apiserver thinks is on node node-a7de9392-4cc3-4830-84ff-49e7615b291e before test
Nov  5 18:23:02.002: INFO: canal-sqp5g from kube-system started at 2020-11-05 17:15:35 +0000 UTC (2 container statuses recorded)
Nov  5 18:23:02.002: INFO: 	Container calico-node ready: true, restart count 0
Nov  5 18:23:02.002: INFO: 	Container kube-flannel ready: true, restart count 0
Nov  5 18:23:02.002: INFO: coredns-6ccf64844b-qrpm5 from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 18:23:02.002: INFO: 	Container coredns ready: true, restart count 0
Nov  5 18:23:02.002: INFO: kube-dns-autoscaler-7944596f47-whbv8 from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 18:23:02.002: INFO: 	Container autoscaler ready: true, restart count 0
Nov  5 18:23:02.002: INFO: kube-proxy-bhxd4 from kube-system started at 2020-11-05 17:15:16 +0000 UTC (1 container statuses recorded)
Nov  5 18:23:02.002: INFO: 	Container kube-proxy ready: true, restart count 0
Nov  5 18:23:02.002: INFO: wormhole-4m5c5 from kube-system started at 2020-10-19 12:36:45 +0000 UTC (1 container statuses recorded)
Nov  5 18:23:02.002: INFO: 	Container wormhole ready: true, restart count 0
Nov  5 18:23:02.002: INFO: agnhost-primary-hmzx7 from kubectl-5556 started at 2020-11-05 18:22:54 +0000 UTC (1 container statuses recorded)
Nov  5 18:23:02.002: INFO: 	Container agnhost-primary ready: true, restart count 0
Nov  5 18:23:02.002: INFO: sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-nf5fs from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:23:02.002: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 18:23:02.002: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1644afba53a8476f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:23:03.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-463" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":163,"skipped":2809,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:23:03.089: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:23:03.216: INFO: Create a RollingUpdate DaemonSet
Nov  5 18:23:03.229: INFO: Check that daemon pods launch on every node of the cluster
Nov  5 18:23:03.246: INFO: Number of nodes with available pods: 0
Nov  5 18:23:03.246: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 18:23:04.267: INFO: Number of nodes with available pods: 0
Nov  5 18:23:04.267: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 18:23:05.265: INFO: Number of nodes with available pods: 0
Nov  5 18:23:05.265: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 18:23:06.267: INFO: Number of nodes with available pods: 0
Nov  5 18:23:06.267: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 18:23:07.261: INFO: Number of nodes with available pods: 1
Nov  5 18:23:07.261: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 18:23:08.261: INFO: Number of nodes with available pods: 2
Nov  5 18:23:08.261: INFO: Number of running nodes: 2, number of available pods: 2
Nov  5 18:23:08.261: INFO: Update the DaemonSet to trigger a rollout
Nov  5 18:23:08.281: INFO: Updating DaemonSet daemon-set
Nov  5 18:23:20.321: INFO: Roll back the DaemonSet before rollout is complete
Nov  5 18:23:20.345: INFO: Updating DaemonSet daemon-set
Nov  5 18:23:20.345: INFO: Make sure DaemonSet rollback is complete
Nov  5 18:23:20.355: INFO: Wrong image for pod: daemon-set-wcm5l. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov  5 18:23:20.356: INFO: Pod daemon-set-wcm5l is not available
Nov  5 18:23:21.372: INFO: Wrong image for pod: daemon-set-wcm5l. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov  5 18:23:21.373: INFO: Pod daemon-set-wcm5l is not available
Nov  5 18:23:22.376: INFO: Pod daemon-set-j7f25 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2257, will wait for the garbage collector to delete the pods
Nov  5 18:23:22.484: INFO: Deleting DaemonSet.extensions daemon-set took: 21.223538ms
Nov  5 18:23:22.584: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.139338ms
Nov  5 18:23:29.792: INFO: Number of nodes with available pods: 0
Nov  5 18:23:29.792: INFO: Number of running nodes: 0, number of available pods: 0
Nov  5 18:23:29.799: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2257/daemonsets","resourceVersion":"1647533566"},"items":null}

Nov  5 18:23:29.807: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2257/pods","resourceVersion":"1647533568"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:23:29.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2257" for this suite.

• [SLOW TEST:26.765 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":164,"skipped":2829,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:23:29.855: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:23:29.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8155" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":165,"skipped":2834,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:23:29.970: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:23:30.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92931319-7a83-411b-b166-177dd0eb4f6c" in namespace "projected-1418" to be "Succeeded or Failed"
Nov  5 18:23:30.082: INFO: Pod "downwardapi-volume-92931319-7a83-411b-b166-177dd0eb4f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.592448ms
Nov  5 18:23:32.100: INFO: Pod "downwardapi-volume-92931319-7a83-411b-b166-177dd0eb4f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030023773s
Nov  5 18:23:34.110: INFO: Pod "downwardapi-volume-92931319-7a83-411b-b166-177dd0eb4f6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039606103s
STEP: Saw pod success
Nov  5 18:23:34.110: INFO: Pod "downwardapi-volume-92931319-7a83-411b-b166-177dd0eb4f6c" satisfied condition "Succeeded or Failed"
Nov  5 18:23:34.119: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-92931319-7a83-411b-b166-177dd0eb4f6c container client-container: <nil>
STEP: delete the pod
Nov  5 18:23:34.168: INFO: Waiting for pod downwardapi-volume-92931319-7a83-411b-b166-177dd0eb4f6c to disappear
Nov  5 18:23:34.176: INFO: Pod downwardapi-volume-92931319-7a83-411b-b166-177dd0eb4f6c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:23:34.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1418" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":166,"skipped":2869,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:23:34.198: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov  5 18:23:34.301: INFO: Waiting up to 5m0s for pod "pod-48494823-2f7a-4220-a266-c6c2903f8992" in namespace "emptydir-7177" to be "Succeeded or Failed"
Nov  5 18:23:34.324: INFO: Pod "pod-48494823-2f7a-4220-a266-c6c2903f8992": Phase="Pending", Reason="", readiness=false. Elapsed: 23.356857ms
Nov  5 18:23:36.332: INFO: Pod "pod-48494823-2f7a-4220-a266-c6c2903f8992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031391417s
Nov  5 18:23:38.344: INFO: Pod "pod-48494823-2f7a-4220-a266-c6c2903f8992": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04342075s
STEP: Saw pod success
Nov  5 18:23:38.344: INFO: Pod "pod-48494823-2f7a-4220-a266-c6c2903f8992" satisfied condition "Succeeded or Failed"
Nov  5 18:23:38.357: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-48494823-2f7a-4220-a266-c6c2903f8992 container test-container: <nil>
STEP: delete the pod
Nov  5 18:23:38.406: INFO: Waiting for pod pod-48494823-2f7a-4220-a266-c6c2903f8992 to disappear
Nov  5 18:23:38.413: INFO: Pod pod-48494823-2f7a-4220-a266-c6c2903f8992 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:23:38.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7177" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":167,"skipped":2884,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:23:38.447: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Nov  5 18:23:38.608: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:23:38.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3201" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":168,"skipped":2889,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:23:38.686: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6560.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6560.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  5 18:23:44.887: INFO: DNS probes using dns-6560/dns-test-27fcacbb-5e5f-44c6-918e-3ce660bafa3d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:23:44.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6560" for this suite.

• [SLOW TEST:6.252 seconds]
[sig-network] DNS
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":169,"skipped":2909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:23:44.950: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:23:45.585: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 18:23:47.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197425, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197425, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197425, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197425, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:23:50.640: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Nov  5 18:23:50.683: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:23:50.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3224" for this suite.
STEP: Destroying namespace "webhook-3224-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.945 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":170,"skipped":2931,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:23:50.895: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-jkft
STEP: Creating a pod to test atomic-volume-subpath
Nov  5 18:23:51.017: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-jkft" in namespace "subpath-2867" to be "Succeeded or Failed"
Nov  5 18:23:51.025: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Pending", Reason="", readiness=false. Elapsed: 7.380828ms
Nov  5 18:23:53.034: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016488957s
Nov  5 18:23:55.044: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Running", Reason="", readiness=true. Elapsed: 4.026864854s
Nov  5 18:23:57.055: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Running", Reason="", readiness=true. Elapsed: 6.037173563s
Nov  5 18:23:59.068: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Running", Reason="", readiness=true. Elapsed: 8.050264407s
Nov  5 18:24:01.084: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Running", Reason="", readiness=true. Elapsed: 10.066926576s
Nov  5 18:24:03.093: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Running", Reason="", readiness=true. Elapsed: 12.075500618s
Nov  5 18:24:05.103: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Running", Reason="", readiness=true. Elapsed: 14.085756979s
Nov  5 18:24:07.114: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Running", Reason="", readiness=true. Elapsed: 16.096269662s
Nov  5 18:24:09.124: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Running", Reason="", readiness=true. Elapsed: 18.106602948s
Nov  5 18:24:11.134: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Running", Reason="", readiness=true. Elapsed: 20.116290407s
Nov  5 18:24:13.143: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Running", Reason="", readiness=true. Elapsed: 22.125758926s
Nov  5 18:24:15.153: INFO: Pod "pod-subpath-test-downwardapi-jkft": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.135958283s
STEP: Saw pod success
Nov  5 18:24:15.154: INFO: Pod "pod-subpath-test-downwardapi-jkft" satisfied condition "Succeeded or Failed"
Nov  5 18:24:15.160: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-subpath-test-downwardapi-jkft container test-container-subpath-downwardapi-jkft: <nil>
STEP: delete the pod
Nov  5 18:24:15.210: INFO: Waiting for pod pod-subpath-test-downwardapi-jkft to disappear
Nov  5 18:24:15.220: INFO: Pod pod-subpath-test-downwardapi-jkft no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-jkft
Nov  5 18:24:15.220: INFO: Deleting pod "pod-subpath-test-downwardapi-jkft" in namespace "subpath-2867"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:24:15.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2867" for this suite.

• [SLOW TEST:24.354 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":171,"skipped":2946,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:24:15.251: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:24:16.153: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 18:24:18.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197456, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197456, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197456, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197456, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:24:21.376: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:24:21.385: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5342-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:24:22.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1404" for this suite.
STEP: Destroying namespace "webhook-1404-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.761 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":172,"skipped":2946,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:24:23.017: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:24:23.925: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 18:24:25.954: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197463, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197463, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197463, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197463, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:24:28.983: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:24:41.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1656" for this suite.
STEP: Destroying namespace "webhook-1656-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.459 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":173,"skipped":2957,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:24:41.477: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Nov  5 18:24:41.567: INFO: Waiting up to 1m0s for all nodes to be ready
Nov  5 18:25:41.603: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:25:41.610: INFO: Starting informer...
STEP: Starting pods...
Nov  5 18:25:41.854: INFO: Pod1 is running on node-a7de9392-4cc3-4830-84ff-49e7615b291e. Tainting Node
Nov  5 18:25:46.110: INFO: Pod2 is running on node-a7de9392-4cc3-4830-84ff-49e7615b291e. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Nov  5 18:25:59.655: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov  5 18:26:19.630: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:26:19.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-733" for this suite.

• [SLOW TEST:98.201 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":174,"skipped":3001,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:26:19.682: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3257
STEP: creating service affinity-nodeport-transition in namespace services-3257
STEP: creating replication controller affinity-nodeport-transition in namespace services-3257
I1105 18:26:19.816817      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-3257, replica count: 3
I1105 18:26:22.870622      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 18:26:25.870892      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 18:26:25.899: INFO: Creating new exec pod
Nov  5 18:26:30.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3257 execpod-affinityblbzx -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Nov  5 18:26:32.176: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov  5 18:26:32.176: INFO: stdout: ""
Nov  5 18:26:32.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3257 execpod-affinityblbzx -- /bin/sh -x -c nc -zv -t -w 2 10.3.85.244 80'
Nov  5 18:26:32.528: INFO: stderr: "+ nc -zv -t -w 2 10.3.85.244 80\nConnection to 10.3.85.244 80 port [tcp/http] succeeded!\n"
Nov  5 18:26:32.528: INFO: stdout: ""
Nov  5 18:26:32.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3257 execpod-affinityblbzx -- /bin/sh -x -c nc -zv -t -w 2 51.83.35.110 31680'
Nov  5 18:26:32.859: INFO: stderr: "+ nc -zv -t -w 2 51.83.35.110 31680\nConnection to 51.83.35.110 31680 port [tcp/31680] succeeded!\n"
Nov  5 18:26:32.859: INFO: stdout: ""
Nov  5 18:26:32.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3257 execpod-affinityblbzx -- /bin/sh -x -c nc -zv -t -w 2 51.91.151.116 31680'
Nov  5 18:26:33.191: INFO: stderr: "+ nc -zv -t -w 2 51.91.151.116 31680\nConnection to 51.91.151.116 31680 port [tcp/31680] succeeded!\n"
Nov  5 18:26:33.191: INFO: stdout: ""
Nov  5 18:26:33.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3257 execpod-affinityblbzx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://51.83.35.110:31680/ ; done'
Nov  5 18:26:33.669: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n"
Nov  5 18:26:33.669: INFO: stdout: "\naffinity-nodeport-transition-vjbkf\naffinity-nodeport-transition-vjbkf\naffinity-nodeport-transition-x2zqk\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-vjbkf\naffinity-nodeport-transition-x2zqk\naffinity-nodeport-transition-vjbkf\naffinity-nodeport-transition-vjbkf\naffinity-nodeport-transition-vjbkf\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-x2zqk\naffinity-nodeport-transition-vjbkf\naffinity-nodeport-transition-vjbkf\naffinity-nodeport-transition-vjbkf\naffinity-nodeport-transition-vjbkf"
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-vjbkf
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-vjbkf
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-x2zqk
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-vjbkf
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-x2zqk
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-vjbkf
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-vjbkf
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-vjbkf
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-x2zqk
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-vjbkf
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-vjbkf
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-vjbkf
Nov  5 18:26:33.670: INFO: Received response from host: affinity-nodeport-transition-vjbkf
Nov  5 18:26:33.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3257 execpod-affinityblbzx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://51.83.35.110:31680/ ; done'
Nov  5 18:26:34.153: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31680/\n"
Nov  5 18:26:34.153: INFO: stdout: "\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx\naffinity-nodeport-transition-qjnbx"
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Received response from host: affinity-nodeport-transition-qjnbx
Nov  5 18:26:34.153: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3257, will wait for the garbage collector to delete the pods
Nov  5 18:26:34.259: INFO: Deleting ReplicationController affinity-nodeport-transition took: 19.815996ms
Nov  5 18:26:35.059: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 800.223108ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:26:50.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3257" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:30.685 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":175,"skipped":3041,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:26:50.370: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov  5 18:26:50.447: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov  5 18:26:50.461: INFO: Waiting for terminating namespaces to be deleted...
Nov  5 18:26:50.469: INFO: 
Logging pods the apiserver thinks is on node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 before test
Nov  5 18:26:50.480: INFO: canal-55ppt from kube-system started at 2020-11-05 17:15:19 +0000 UTC (2 container statuses recorded)
Nov  5 18:26:50.480: INFO: 	Container calico-node ready: true, restart count 0
Nov  5 18:26:50.481: INFO: 	Container kube-flannel ready: true, restart count 2
Nov  5 18:26:50.481: INFO: coredns-6ccf64844b-j6r5k from kube-system started at 2020-11-05 18:25:46 +0000 UTC (1 container statuses recorded)
Nov  5 18:26:50.481: INFO: 	Container coredns ready: true, restart count 0
Nov  5 18:26:50.481: INFO: coredns-6ccf64844b-zjcgs from kube-system started at 2020-11-05 17:23:56 +0000 UTC (1 container statuses recorded)
Nov  5 18:26:50.481: INFO: 	Container coredns ready: true, restart count 0
Nov  5 18:26:50.481: INFO: kube-dns-autoscaler-7944596f47-fdjl2 from kube-system started at 2020-11-05 18:25:46 +0000 UTC (1 container statuses recorded)
Nov  5 18:26:50.482: INFO: 	Container autoscaler ready: true, restart count 0
Nov  5 18:26:50.482: INFO: kube-proxy-j2wfk from kube-system started at 2020-11-05 17:15:40 +0000 UTC (1 container statuses recorded)
Nov  5 18:26:50.482: INFO: 	Container kube-proxy ready: true, restart count 0
Nov  5 18:26:50.482: INFO: metrics-server-664567f776-8wvsk from kube-system started at 2020-11-05 17:36:35 +0000 UTC (1 container statuses recorded)
Nov  5 18:26:50.483: INFO: 	Container metrics-server ready: true, restart count 0
Nov  5 18:26:50.483: INFO: wormhole-2r8fc from kube-system started at 2020-10-19 12:36:30 +0000 UTC (1 container statuses recorded)
Nov  5 18:26:50.483: INFO: 	Container wormhole ready: true, restart count 0
Nov  5 18:26:50.483: INFO: sonobuoy from sonobuoy started at 2020-11-05 17:25:05 +0000 UTC (1 container statuses recorded)
Nov  5 18:26:50.484: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov  5 18:26:50.484: INFO: sonobuoy-e2e-job-6a656b1abb77479a from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:26:50.484: INFO: 	Container e2e ready: true, restart count 0
Nov  5 18:26:50.484: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov  5 18:26:50.484: INFO: sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-vv8nw from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:26:50.484: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov  5 18:26:50.484: INFO: 	Container systemd-logs ready: true, restart count 0
Nov  5 18:26:50.484: INFO: 
Logging pods the apiserver thinks is on node node-a7de9392-4cc3-4830-84ff-49e7615b291e before test
Nov  5 18:26:50.496: INFO: canal-sqp5g from kube-system started at 2020-11-05 17:15:35 +0000 UTC (2 container statuses recorded)
Nov  5 18:26:50.496: INFO: 	Container calico-node ready: true, restart count 0
Nov  5 18:26:50.496: INFO: 	Container kube-flannel ready: true, restart count 0
Nov  5 18:26:50.496: INFO: kube-proxy-bhxd4 from kube-system started at 2020-11-05 17:15:16 +0000 UTC (1 container statuses recorded)
Nov  5 18:26:50.496: INFO: 	Container kube-proxy ready: true, restart count 0
Nov  5 18:26:50.496: INFO: wormhole-4m5c5 from kube-system started at 2020-10-19 12:36:45 +0000 UTC (1 container statuses recorded)
Nov  5 18:26:50.496: INFO: 	Container wormhole ready: true, restart count 0
Nov  5 18:26:50.496: INFO: sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-nf5fs from sonobuoy started at 2020-11-05 17:25:12 +0000 UTC (2 container statuses recorded)
Nov  5 18:26:50.496: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov  5 18:26:50.496: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
STEP: verifying the node has the label node node-a7de9392-4cc3-4830-84ff-49e7615b291e
Nov  5 18:26:50.573: INFO: Pod canal-55ppt requesting resource cpu=250m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
Nov  5 18:26:50.573: INFO: Pod canal-sqp5g requesting resource cpu=250m on Node node-a7de9392-4cc3-4830-84ff-49e7615b291e
Nov  5 18:26:50.573: INFO: Pod coredns-6ccf64844b-j6r5k requesting resource cpu=100m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
Nov  5 18:26:50.573: INFO: Pod coredns-6ccf64844b-zjcgs requesting resource cpu=100m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
Nov  5 18:26:50.574: INFO: Pod kube-dns-autoscaler-7944596f47-fdjl2 requesting resource cpu=20m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
Nov  5 18:26:50.574: INFO: Pod kube-proxy-bhxd4 requesting resource cpu=100m on Node node-a7de9392-4cc3-4830-84ff-49e7615b291e
Nov  5 18:26:50.574: INFO: Pod kube-proxy-j2wfk requesting resource cpu=100m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
Nov  5 18:26:50.574: INFO: Pod metrics-server-664567f776-8wvsk requesting resource cpu=0m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
Nov  5 18:26:50.574: INFO: Pod wormhole-2r8fc requesting resource cpu=0m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
Nov  5 18:26:50.574: INFO: Pod wormhole-4m5c5 requesting resource cpu=0m on Node node-a7de9392-4cc3-4830-84ff-49e7615b291e
Nov  5 18:26:50.574: INFO: Pod sonobuoy requesting resource cpu=0m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
Nov  5 18:26:50.574: INFO: Pod sonobuoy-e2e-job-6a656b1abb77479a requesting resource cpu=0m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
Nov  5 18:26:50.574: INFO: Pod sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-nf5fs requesting resource cpu=0m on Node node-a7de9392-4cc3-4830-84ff-49e7615b291e
Nov  5 18:26:50.574: INFO: Pod sonobuoy-systemd-logs-daemon-set-f68e247b9395491a-vv8nw requesting resource cpu=0m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
STEP: Starting Pods to consume most of the cluster CPU.
Nov  5 18:26:50.574: INFO: Creating a pod which consumes cpu=931m on Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
Nov  5 18:26:50.588: INFO: Creating a pod which consumes cpu=1085m on Node node-a7de9392-4cc3-4830-84ff-49e7615b291e
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3fda736c-48c5-4621-915c-e7cf658e6539.1644afef8ac263b7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2809/filler-pod-3fda736c-48c5-4621-915c-e7cf658e6539 to node-a7de9392-4cc3-4830-84ff-49e7615b291e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f63312d9-0475-474c-8f88-89eb068bd565.1644afef8a3fcdf6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2809/filler-pod-f63312d9-0475-474c-8f88-89eb068bd565 to node-29f9289e-2d09-4bea-972e-88a5817d7ad8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3fda736c-48c5-4621-915c-e7cf658e6539.1644aff02afa5c25], Reason = [Started], Message = [Started container filler-pod-3fda736c-48c5-4621-915c-e7cf658e6539]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f63312d9-0475-474c-8f88-89eb068bd565.1644aff002f90b5d], Reason = [Started], Message = [Started container filler-pod-f63312d9-0475-474c-8f88-89eb068bd565]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f63312d9-0475-474c-8f88-89eb068bd565.1644afeff31de4fe], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 660.493121ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3fda736c-48c5-4621-915c-e7cf658e6539.1644afeff62d272a], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3fda736c-48c5-4621-915c-e7cf658e6539.1644aff021c45528], Reason = [Created], Message = [Created container filler-pod-3fda736c-48c5-4621-915c-e7cf658e6539]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f63312d9-0475-474c-8f88-89eb068bd565.1644afeff7b85ca8], Reason = [Created], Message = [Created container filler-pod-f63312d9-0475-474c-8f88-89eb068bd565]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f63312d9-0475-474c-8f88-89eb068bd565.1644afefcbbf2be2], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3fda736c-48c5-4621-915c-e7cf658e6539.1644aff01d3444b9], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 654.758793ms]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1644aff07c36f832], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node node-29f9289e-2d09-4bea-972e-88a5817d7ad8
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node-a7de9392-4cc3-4830-84ff-49e7615b291e
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:26:55.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2809" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.376 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":176,"skipped":3045,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:26:55.746: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:26:55.822: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:26:56.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6270" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":177,"skipped":3046,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:26:56.893: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov  5 18:26:56.998: INFO: Waiting up to 1m0s for all nodes to be ready
Nov  5 18:27:57.042: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Nov  5 18:27:57.101: INFO: Created pod: pod0-sched-preemption-low-priority
Nov  5 18:27:57.136: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:28:25.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2008" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:89.074 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":178,"skipped":3052,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:28:25.969: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:28:44.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6329" for this suite.

• [SLOW TEST:18.132 seconds]
[sig-apps] Job
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":179,"skipped":3067,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:28:44.101: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:29:01.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4426" for this suite.

• [SLOW TEST:17.210 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":180,"skipped":3078,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:29:01.311: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov  5 18:29:07.940: INFO: Successfully updated pod "adopt-release-5vdnb"
STEP: Checking that the Job readopts the Pod
Nov  5 18:29:07.940: INFO: Waiting up to 15m0s for pod "adopt-release-5vdnb" in namespace "job-6767" to be "adopted"
Nov  5 18:29:07.963: INFO: Pod "adopt-release-5vdnb": Phase="Running", Reason="", readiness=true. Elapsed: 23.314957ms
Nov  5 18:29:09.973: INFO: Pod "adopt-release-5vdnb": Phase="Running", Reason="", readiness=true. Elapsed: 2.033043831s
Nov  5 18:29:09.973: INFO: Pod "adopt-release-5vdnb" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov  5 18:29:10.499: INFO: Successfully updated pod "adopt-release-5vdnb"
STEP: Checking that the Job releases the Pod
Nov  5 18:29:10.499: INFO: Waiting up to 15m0s for pod "adopt-release-5vdnb" in namespace "job-6767" to be "released"
Nov  5 18:29:10.508: INFO: Pod "adopt-release-5vdnb": Phase="Running", Reason="", readiness=true. Elapsed: 8.291289ms
Nov  5 18:29:12.519: INFO: Pod "adopt-release-5vdnb": Phase="Running", Reason="", readiness=true. Elapsed: 2.019086623s
Nov  5 18:29:12.519: INFO: Pod "adopt-release-5vdnb" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:29:12.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6767" for this suite.

• [SLOW TEST:11.234 seconds]
[sig-apps] Job
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":181,"skipped":3081,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:29:12.546: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:29:12.645: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov  5 18:29:17.672: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov  5 18:29:17.672: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov  5 18:29:21.809: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7458 /apis/apps/v1/namespaces/deployment-7458/deployments/test-cleanup-deployment 8e044bee-fdd3-4856-ae5d-824606d73213 1647646880 1 2020-11-05 18:29:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2020-11-05 18:29:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-05 18:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049c2a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-05 18:29:17 +0000 UTC,LastTransitionTime:2020-11-05 18:29:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5d446bdd47" has successfully progressed.,LastUpdateTime:2020-11-05 18:29:19 +0000 UTC,LastTransitionTime:2020-11-05 18:29:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov  5 18:29:21.822: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-7458 /apis/apps/v1/namespaces/deployment-7458/replicasets/test-cleanup-deployment-5d446bdd47 27769d5b-186a-4e8a-bb8d-88ac9c7de34e 1647646856 1 2020-11-05 18:29:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8e044bee-fdd3-4856-ae5d-824606d73213 0xc0049c3017 0xc0049c3018}] []  [{kube-controller-manager Update apps/v1 2020-11-05 18:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e044bee-fdd3-4856-ae5d-824606d73213\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049c30a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov  5 18:29:21.829: INFO: Pod "test-cleanup-deployment-5d446bdd47-znpgz" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-znpgz test-cleanup-deployment-5d446bdd47- deployment-7458 /api/v1/namespaces/deployment-7458/pods/test-cleanup-deployment-5d446bdd47-znpgz b2557d02-dd4b-4fc0-b8ea-3f513ee92902 1647646845 0 2020-11-05 18:29:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[cni.projectcalico.org/podIP:10.2.1.251/32] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 27769d5b-186a-4e8a-bb8d-88ac9c7de34e 0xc0049c34e7 0xc0049c34e8}] []  [{kube-controller-manager Update v1 2020-11-05 18:29:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27769d5b-186a-4e8a-bb8d-88ac9c7de34e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 18:29:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 18:29:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x5gq7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x5gq7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x5gq7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:29:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:29:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:10.2.1.251,StartTime:2020-11-05 18:29:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 18:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://8569ce7e0a3798521c1bf308081823e22960fd5076ff7d0049b80d47128ae005,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:29:21.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7458" for this suite.

• [SLOW TEST:9.312 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":182,"skipped":3090,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:29:21.858: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:29:21.952: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51dbfe70-f1aa-4a19-8a9d-c00f1a2146a1" in namespace "downward-api-1722" to be "Succeeded or Failed"
Nov  5 18:29:21.962: INFO: Pod "downwardapi-volume-51dbfe70-f1aa-4a19-8a9d-c00f1a2146a1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.264491ms
Nov  5 18:29:23.971: INFO: Pod "downwardapi-volume-51dbfe70-f1aa-4a19-8a9d-c00f1a2146a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018666677s
Nov  5 18:29:25.981: INFO: Pod "downwardapi-volume-51dbfe70-f1aa-4a19-8a9d-c00f1a2146a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028464052s
STEP: Saw pod success
Nov  5 18:29:25.981: INFO: Pod "downwardapi-volume-51dbfe70-f1aa-4a19-8a9d-c00f1a2146a1" satisfied condition "Succeeded or Failed"
Nov  5 18:29:25.990: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-51dbfe70-f1aa-4a19-8a9d-c00f1a2146a1 container client-container: <nil>
STEP: delete the pod
Nov  5 18:29:26.101: INFO: Waiting for pod downwardapi-volume-51dbfe70-f1aa-4a19-8a9d-c00f1a2146a1 to disappear
Nov  5 18:29:26.108: INFO: Pod downwardapi-volume-51dbfe70-f1aa-4a19-8a9d-c00f1a2146a1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:29:26.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1722" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":183,"skipped":3097,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:29:26.135: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Nov  5 18:29:26.249: INFO: created test-pod-1
Nov  5 18:29:26.261: INFO: created test-pod-2
Nov  5 18:29:26.273: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:29:26.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-309" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":184,"skipped":3111,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:29:26.413: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-4zn4
STEP: Creating a pod to test atomic-volume-subpath
Nov  5 18:29:26.546: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4zn4" in namespace "subpath-1050" to be "Succeeded or Failed"
Nov  5 18:29:26.553: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.449225ms
Nov  5 18:29:28.564: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017100859s
Nov  5 18:29:30.580: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Running", Reason="", readiness=true. Elapsed: 4.033724495s
Nov  5 18:29:32.589: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Running", Reason="", readiness=true. Elapsed: 6.042469261s
Nov  5 18:29:34.602: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Running", Reason="", readiness=true. Elapsed: 8.055975463s
Nov  5 18:29:36.614: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Running", Reason="", readiness=true. Elapsed: 10.067486256s
Nov  5 18:29:38.623: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Running", Reason="", readiness=true. Elapsed: 12.076236629s
Nov  5 18:29:40.646: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Running", Reason="", readiness=true. Elapsed: 14.099606976s
Nov  5 18:29:42.656: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Running", Reason="", readiness=true. Elapsed: 16.109630679s
Nov  5 18:29:44.665: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Running", Reason="", readiness=true. Elapsed: 18.118944772s
Nov  5 18:29:46.675: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Running", Reason="", readiness=true. Elapsed: 20.128190405s
Nov  5 18:29:48.685: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Running", Reason="", readiness=true. Elapsed: 22.138479198s
Nov  5 18:29:50.703: INFO: Pod "pod-subpath-test-configmap-4zn4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.156463743s
STEP: Saw pod success
Nov  5 18:29:50.703: INFO: Pod "pod-subpath-test-configmap-4zn4" satisfied condition "Succeeded or Failed"
Nov  5 18:29:50.710: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-subpath-test-configmap-4zn4 container test-container-subpath-configmap-4zn4: <nil>
STEP: delete the pod
Nov  5 18:29:50.768: INFO: Waiting for pod pod-subpath-test-configmap-4zn4 to disappear
Nov  5 18:29:50.775: INFO: Pod pod-subpath-test-configmap-4zn4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4zn4
Nov  5 18:29:50.775: INFO: Deleting pod "pod-subpath-test-configmap-4zn4" in namespace "subpath-1050"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:29:50.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1050" for this suite.

• [SLOW TEST:24.391 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":185,"skipped":3111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:29:50.806: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:29:50.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9267" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":186,"skipped":3176,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:29:50.988: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov  5 18:29:59.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  5 18:29:59.205: INFO: Pod pod-with-prestop-http-hook still exists
Nov  5 18:30:01.206: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  5 18:30:01.219: INFO: Pod pod-with-prestop-http-hook still exists
Nov  5 18:30:03.206: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  5 18:30:03.214: INFO: Pod pod-with-prestop-http-hook still exists
Nov  5 18:30:05.206: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  5 18:30:05.215: INFO: Pod pod-with-prestop-http-hook still exists
Nov  5 18:30:07.206: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  5 18:30:07.215: INFO: Pod pod-with-prestop-http-hook still exists
Nov  5 18:30:09.206: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  5 18:30:09.222: INFO: Pod pod-with-prestop-http-hook still exists
Nov  5 18:30:11.206: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  5 18:30:11.215: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:30:11.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9188" for this suite.

• [SLOW TEST:20.270 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":187,"skipped":3187,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:30:11.258: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:30:15.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2960" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":188,"skipped":3191,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:30:15.421: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3095
Nov  5 18:30:19.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3095 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov  5 18:30:19.883: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov  5 18:30:19.883: INFO: stdout: "iptables"
Nov  5 18:30:19.883: INFO: proxyMode: iptables
Nov  5 18:30:19.904: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov  5 18:30:19.917: INFO: Pod kube-proxy-mode-detector still exists
Nov  5 18:30:21.917: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov  5 18:30:21.932: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3095
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3095
I1105 18:30:22.000373      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3095, replica count: 3
I1105 18:30:25.058244      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 18:30:28.059731      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 18:30:28.616: INFO: Creating new exec pod
Nov  5 18:30:33.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3095 execpod-affinitykvdp5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Nov  5 18:30:34.767: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Nov  5 18:30:34.767: INFO: stdout: ""
Nov  5 18:30:34.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3095 execpod-affinitykvdp5 -- /bin/sh -x -c nc -zv -t -w 2 10.3.209.59 80'
Nov  5 18:30:35.151: INFO: stderr: "+ nc -zv -t -w 2 10.3.209.59 80\nConnection to 10.3.209.59 80 port [tcp/http] succeeded!\n"
Nov  5 18:30:35.152: INFO: stdout: ""
Nov  5 18:30:35.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3095 execpod-affinitykvdp5 -- /bin/sh -x -c nc -zv -t -w 2 51.83.35.110 31406'
Nov  5 18:30:35.522: INFO: stderr: "+ nc -zv -t -w 2 51.83.35.110 31406\nConnection to 51.83.35.110 31406 port [tcp/31406] succeeded!\n"
Nov  5 18:30:35.522: INFO: stdout: ""
Nov  5 18:30:35.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3095 execpod-affinitykvdp5 -- /bin/sh -x -c nc -zv -t -w 2 51.91.151.116 31406'
Nov  5 18:30:35.856: INFO: stderr: "+ nc -zv -t -w 2 51.91.151.116 31406\nConnection to 51.91.151.116 31406 port [tcp/31406] succeeded!\n"
Nov  5 18:30:35.856: INFO: stdout: ""
Nov  5 18:30:35.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3095 execpod-affinitykvdp5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://51.83.35.110:31406/ ; done'
Nov  5 18:30:36.295: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n"
Nov  5 18:30:36.295: INFO: stdout: "\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574\naffinity-nodeport-timeout-gg574"
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Received response from host: affinity-nodeport-timeout-gg574
Nov  5 18:30:36.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3095 execpod-affinitykvdp5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://51.83.35.110:31406/'
Nov  5 18:30:36.690: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n"
Nov  5 18:30:36.690: INFO: stdout: "affinity-nodeport-timeout-gg574"
Nov  5 18:30:51.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3095 execpod-affinitykvdp5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://51.83.35.110:31406/'
Nov  5 18:30:52.085: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n"
Nov  5 18:30:52.085: INFO: stdout: "affinity-nodeport-timeout-gg574"
Nov  5 18:31:07.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3095 execpod-affinitykvdp5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://51.83.35.110:31406/'
Nov  5 18:31:07.443: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://51.83.35.110:31406/\n"
Nov  5 18:31:07.443: INFO: stdout: "affinity-nodeport-timeout-9x5f7"
Nov  5 18:31:07.443: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3095, will wait for the garbage collector to delete the pods
Nov  5 18:31:07.548: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 19.936331ms
Nov  5 18:31:08.348: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 800.201557ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:31:19.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3095" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:64.206 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":189,"skipped":3205,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:31:19.634: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov  5 18:31:19.745: INFO: Waiting up to 5m0s for pod "pod-7e28eb16-d666-41f3-9350-6b2d1f232483" in namespace "emptydir-6154" to be "Succeeded or Failed"
Nov  5 18:31:19.753: INFO: Pod "pod-7e28eb16-d666-41f3-9350-6b2d1f232483": Phase="Pending", Reason="", readiness=false. Elapsed: 7.827277ms
Nov  5 18:31:21.764: INFO: Pod "pod-7e28eb16-d666-41f3-9350-6b2d1f232483": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018342914s
Nov  5 18:31:23.775: INFO: Pod "pod-7e28eb16-d666-41f3-9350-6b2d1f232483": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029390428s
STEP: Saw pod success
Nov  5 18:31:23.775: INFO: Pod "pod-7e28eb16-d666-41f3-9350-6b2d1f232483" satisfied condition "Succeeded or Failed"
Nov  5 18:31:23.782: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-7e28eb16-d666-41f3-9350-6b2d1f232483 container test-container: <nil>
STEP: delete the pod
Nov  5 18:31:23.831: INFO: Waiting for pod pod-7e28eb16-d666-41f3-9350-6b2d1f232483 to disappear
Nov  5 18:31:23.837: INFO: Pod pod-7e28eb16-d666-41f3-9350-6b2d1f232483 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:31:23.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6154" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":190,"skipped":3211,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:31:23.866: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7454.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7454.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7454.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7454.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7454.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7454.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  5 18:31:30.024: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:30.035: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:30.045: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:30.058: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:30.070: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:30.080: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:30.097: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:30.105: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:30.115: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:30.124: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:30.145: INFO: Lookups using dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7454.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7454.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_udp@dns-test-service-2.dns-7454.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7454.svc.cluster.local]

Nov  5 18:31:35.158: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:35.170: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:35.180: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:35.191: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:35.200: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:35.215: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:35.229: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:35.238: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:35.256: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:35.276: INFO: Lookups using dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7454.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7454.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7454.svc.cluster.local]

Nov  5 18:31:40.157: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:40.167: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:40.219: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:40.229: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:40.251: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:40.270: INFO: Lookups using dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7454.svc.cluster.local]

Nov  5 18:31:45.159: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:45.169: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:45.237: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:45.247: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:45.289: INFO: Lookups using dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local]

Nov  5 18:31:50.164: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:50.175: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:50.231: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:50.241: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:50.283: INFO: Lookups using dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local]

Nov  5 18:31:55.157: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:55.167: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:55.229: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:55.239: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local from pod dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37: the server could not find the requested resource (get pods dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37)
Nov  5 18:31:55.291: INFO: Lookups using dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7454.svc.cluster.local]

Nov  5 18:32:00.273: INFO: DNS probes using dns-7454/dns-test-e90f3fbc-6d7a-468d-a878-6cd075e9fd37 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:32:00.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7454" for this suite.

• [SLOW TEST:36.502 seconds]
[sig-network] DNS
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":191,"skipped":3225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:32:00.383: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:32:00.905: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 18:32:02.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197920, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197920, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197920, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740197920, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:32:05.966: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:32:06.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9328" for this suite.
STEP: Destroying namespace "webhook-9328-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.802 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":192,"skipped":3312,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:32:06.185: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3001
STEP: creating service affinity-nodeport in namespace services-3001
STEP: creating replication controller affinity-nodeport in namespace services-3001
I1105 18:32:06.307450      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-3001, replica count: 3
I1105 18:32:09.357913      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 18:32:12.358210      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 18:32:12.659: INFO: Creating new exec pod
Nov  5 18:32:17.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3001 execpod-affinitywlhbk -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Nov  5 18:32:18.082: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov  5 18:32:18.083: INFO: stdout: ""
Nov  5 18:32:18.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3001 execpod-affinitywlhbk -- /bin/sh -x -c nc -zv -t -w 2 10.3.38.216 80'
Nov  5 18:32:18.421: INFO: stderr: "+ nc -zv -t -w 2 10.3.38.216 80\nConnection to 10.3.38.216 80 port [tcp/http] succeeded!\n"
Nov  5 18:32:18.422: INFO: stdout: ""
Nov  5 18:32:18.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3001 execpod-affinitywlhbk -- /bin/sh -x -c nc -zv -t -w 2 51.83.35.110 32425'
Nov  5 18:32:18.764: INFO: stderr: "+ nc -zv -t -w 2 51.83.35.110 32425\nConnection to 51.83.35.110 32425 port [tcp/32425] succeeded!\n"
Nov  5 18:32:18.764: INFO: stdout: ""
Nov  5 18:32:18.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3001 execpod-affinitywlhbk -- /bin/sh -x -c nc -zv -t -w 2 51.91.151.116 32425'
Nov  5 18:32:19.109: INFO: stderr: "+ nc -zv -t -w 2 51.91.151.116 32425\nConnection to 51.91.151.116 32425 port [tcp/32425] succeeded!\n"
Nov  5 18:32:19.110: INFO: stdout: ""
Nov  5 18:32:19.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-3001 execpod-affinitywlhbk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://51.83.35.110:32425/ ; done'
Nov  5 18:32:19.584: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.83.35.110:32425/\n"
Nov  5 18:32:19.584: INFO: stdout: "\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj\naffinity-nodeport-5vcnj"
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Received response from host: affinity-nodeport-5vcnj
Nov  5 18:32:19.584: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3001, will wait for the garbage collector to delete the pods
Nov  5 18:32:19.695: INFO: Deleting ReplicationController affinity-nodeport took: 24.058798ms
Nov  5 18:32:19.795: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.799899ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:32:29.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3001" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:23.668 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":193,"skipped":3320,"failed":0}
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:32:29.857: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-374, will wait for the garbage collector to delete the pods
Nov  5 18:32:36.105: INFO: Deleting Job.batch foo took: 21.44628ms
Nov  5 18:32:36.206: INFO: Terminating Job.batch foo pods took: 100.360948ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:33:19.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-374" for this suite.

• [SLOW TEST:49.678 seconds]
[sig-apps] Job
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":194,"skipped":3320,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:33:19.536: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Nov  5 18:33:25.655: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2320 PodName:var-expansion-540b0932-49c1-499c-9ca8-862d2bbebe82 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 18:33:25.655: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: test for file in mounted path
Nov  5 18:33:25.850: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2320 PodName:var-expansion-540b0932-49c1-499c-9ca8-862d2bbebe82 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 18:33:25.850: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: updating the annotation value
Nov  5 18:33:26.569: INFO: Successfully updated pod "var-expansion-540b0932-49c1-499c-9ca8-862d2bbebe82"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Nov  5 18:33:26.577: INFO: Deleting pod "var-expansion-540b0932-49c1-499c-9ca8-862d2bbebe82" in namespace "var-expansion-2320"
Nov  5 18:33:26.593: INFO: Wait up to 5m0s for pod "var-expansion-540b0932-49c1-499c-9ca8-862d2bbebe82" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:34:10.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2320" for this suite.

• [SLOW TEST:51.104 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":195,"skipped":3329,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:34:10.644: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:34:11.606: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 18:34:13.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198051, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198051, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198051, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198051, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:34:16.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:34:18.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2730" for this suite.
STEP: Destroying namespace "webhook-2730-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.548 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":196,"skipped":3343,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:34:18.196: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-bd795ef2-5d01-4d35-bade-dd6ca401a97d
STEP: Creating a pod to test consume secrets
Nov  5 18:34:18.307: INFO: Waiting up to 5m0s for pod "pod-secrets-c7366aa2-a429-457d-b302-b550af3964a6" in namespace "secrets-4295" to be "Succeeded or Failed"
Nov  5 18:34:18.313: INFO: Pod "pod-secrets-c7366aa2-a429-457d-b302-b550af3964a6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.797418ms
Nov  5 18:34:20.326: INFO: Pod "pod-secrets-c7366aa2-a429-457d-b302-b550af3964a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018505211s
Nov  5 18:34:22.335: INFO: Pod "pod-secrets-c7366aa2-a429-457d-b302-b550af3964a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027054689s
STEP: Saw pod success
Nov  5 18:34:22.335: INFO: Pod "pod-secrets-c7366aa2-a429-457d-b302-b550af3964a6" satisfied condition "Succeeded or Failed"
Nov  5 18:34:22.342: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-secrets-c7366aa2-a429-457d-b302-b550af3964a6 container secret-volume-test: <nil>
STEP: delete the pod
Nov  5 18:34:22.442: INFO: Waiting for pod pod-secrets-c7366aa2-a429-457d-b302-b550af3964a6 to disappear
Nov  5 18:34:22.448: INFO: Pod pod-secrets-c7366aa2-a429-457d-b302-b550af3964a6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:34:22.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4295" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":197,"skipped":3346,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:34:22.475: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:34:23.071: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 18:34:25.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198063, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198063, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198063, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198063, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:34:28.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:34:28.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8760" for this suite.
STEP: Destroying namespace "webhook-8760-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.863 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":198,"skipped":3358,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:34:28.344: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-fda3a27b-2abd-4ffb-b967-fa67f37c1aa0
STEP: Creating secret with name secret-projected-all-test-volume-4645c40a-8929-416a-83ec-921d99143680
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov  5 18:34:28.453: INFO: Waiting up to 5m0s for pod "projected-volume-c3ddebe6-42f1-4156-b25f-e8ea59995d25" in namespace "projected-3968" to be "Succeeded or Failed"
Nov  5 18:34:28.471: INFO: Pod "projected-volume-c3ddebe6-42f1-4156-b25f-e8ea59995d25": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00128ms
Nov  5 18:34:30.482: INFO: Pod "projected-volume-c3ddebe6-42f1-4156-b25f-e8ea59995d25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029174696s
Nov  5 18:34:32.492: INFO: Pod "projected-volume-c3ddebe6-42f1-4156-b25f-e8ea59995d25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038285589s
Nov  5 18:34:34.502: INFO: Pod "projected-volume-c3ddebe6-42f1-4156-b25f-e8ea59995d25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048756971s
STEP: Saw pod success
Nov  5 18:34:34.502: INFO: Pod "projected-volume-c3ddebe6-42f1-4156-b25f-e8ea59995d25" satisfied condition "Succeeded or Failed"
Nov  5 18:34:34.510: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod projected-volume-c3ddebe6-42f1-4156-b25f-e8ea59995d25 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov  5 18:34:34.562: INFO: Waiting for pod projected-volume-c3ddebe6-42f1-4156-b25f-e8ea59995d25 to disappear
Nov  5 18:34:34.571: INFO: Pod projected-volume-c3ddebe6-42f1-4156-b25f-e8ea59995d25 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:34:34.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3968" for this suite.

• [SLOW TEST:6.249 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":199,"skipped":3374,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:34:34.596: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:34:34.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9494" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":200,"skipped":3379,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:34:34.708: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Nov  5 18:34:34.794: INFO: Major version: 1
STEP: Confirm minor version
Nov  5 18:34:34.794: INFO: cleanMinorVersion: 19
Nov  5 18:34:34.794: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:34:34.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7538" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":201,"skipped":3384,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:34:34.815: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:34:34.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85cdc6a9-b340-4d24-8435-a6049ff00f2a" in namespace "downward-api-2793" to be "Succeeded or Failed"
Nov  5 18:34:34.916: INFO: Pod "downwardapi-volume-85cdc6a9-b340-4d24-8435-a6049ff00f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.115298ms
Nov  5 18:34:36.927: INFO: Pod "downwardapi-volume-85cdc6a9-b340-4d24-8435-a6049ff00f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017378524s
Nov  5 18:34:38.935: INFO: Pod "downwardapi-volume-85cdc6a9-b340-4d24-8435-a6049ff00f2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025972135s
STEP: Saw pod success
Nov  5 18:34:38.935: INFO: Pod "downwardapi-volume-85cdc6a9-b340-4d24-8435-a6049ff00f2a" satisfied condition "Succeeded or Failed"
Nov  5 18:34:38.942: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-85cdc6a9-b340-4d24-8435-a6049ff00f2a container client-container: <nil>
STEP: delete the pod
Nov  5 18:34:38.984: INFO: Waiting for pod downwardapi-volume-85cdc6a9-b340-4d24-8435-a6049ff00f2a to disappear
Nov  5 18:34:38.990: INFO: Pod downwardapi-volume-85cdc6a9-b340-4d24-8435-a6049ff00f2a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:34:38.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2793" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":202,"skipped":3388,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:34:39.013: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov  5 18:34:39.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6395'
Nov  5 18:34:39.243: INFO: stderr: ""
Nov  5 18:34:39.243: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Nov  5 18:34:44.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pod e2e-test-httpd-pod --namespace=kubectl-6395 -o json'
Nov  5 18:34:44.427: INFO: stderr: ""
Nov  5 18:34:44.427: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.2.1.22/32\"\n        },\n        \"creationTimestamp\": \"2020-11-05T18:34:39Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-05T18:34:39Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-05T18:34:40Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.2.1.22\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-05T18:34:43Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6395\",\n        \"resourceVersion\": \"1647751017\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6395/pods/e2e-test-httpd-pod\",\n        \"uid\": \"e5a2bfa9-5dc0-4d42-adf5-cf1fa484da36\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-w4vdx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node-a7de9392-4cc3-4830-84ff-49e7615b291e\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-w4vdx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-w4vdx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-05T18:34:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-05T18:34:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-05T18:34:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-05T18:34:39Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2ddb50ad2a9c0089a648746a4365c040df409bb8bfffda1fdd70bcf28d83e0f8\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-11-05T18:34:43Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"51.91.151.116\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.2.1.22\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.2.1.22\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-05T18:34:40Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov  5 18:34:44.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 replace -f - --namespace=kubectl-6395'
Nov  5 18:34:44.838: INFO: stderr: ""
Nov  5 18:34:44.838: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Nov  5 18:34:44.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete pods e2e-test-httpd-pod --namespace=kubectl-6395'
Nov  5 18:34:49.653: INFO: stderr: ""
Nov  5 18:34:49.653: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:34:49.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6395" for this suite.

• [SLOW TEST:10.662 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":203,"skipped":3392,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:34:49.676: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1105 18:34:59.850046      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1105 18:34:59.850081      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1105 18:34:59.850092      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov  5 18:34:59.850: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:34:59.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5369" for this suite.

• [SLOW TEST:10.198 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":204,"skipped":3405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:34:59.882: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-2071
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2071 to expose endpoints map[]
Nov  5 18:35:00.109: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Nov  5 18:35:01.131: INFO: successfully validated that service endpoint-test2 in namespace services-2071 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2071
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2071 to expose endpoints map[pod1:[80]]
Nov  5 18:35:05.181: INFO: successfully validated that service endpoint-test2 in namespace services-2071 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-2071
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2071 to expose endpoints map[pod1:[80] pod2:[80]]
Nov  5 18:35:09.235: INFO: successfully validated that service endpoint-test2 in namespace services-2071 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-2071
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2071 to expose endpoints map[pod2:[80]]
Nov  5 18:35:09.280: INFO: successfully validated that service endpoint-test2 in namespace services-2071 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-2071
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2071 to expose endpoints map[]
Nov  5 18:35:09.313: INFO: successfully validated that service endpoint-test2 in namespace services-2071 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:35:09.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2071" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.494 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":205,"skipped":3433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:35:09.376: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov  5 18:35:09.457: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:35:19.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8808" for this suite.

• [SLOW TEST:10.052 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3455,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:35:19.431: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-h8sk
STEP: Creating a pod to test atomic-volume-subpath
Nov  5 18:35:19.572: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-h8sk" in namespace "subpath-2689" to be "Succeeded or Failed"
Nov  5 18:35:19.581: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.35964ms
Nov  5 18:35:21.591: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017965452s
Nov  5 18:35:23.604: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Running", Reason="", readiness=true. Elapsed: 4.031515267s
Nov  5 18:35:25.616: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Running", Reason="", readiness=true. Elapsed: 6.042817651s
Nov  5 18:35:27.625: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Running", Reason="", readiness=true. Elapsed: 8.051873402s
Nov  5 18:35:29.643: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Running", Reason="", readiness=true. Elapsed: 10.069982454s
Nov  5 18:35:31.655: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Running", Reason="", readiness=true. Elapsed: 12.082354491s
Nov  5 18:35:33.663: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Running", Reason="", readiness=true. Elapsed: 14.090331773s
Nov  5 18:35:35.675: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Running", Reason="", readiness=true. Elapsed: 16.10265847s
Nov  5 18:35:37.686: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Running", Reason="", readiness=true. Elapsed: 18.113466791s
Nov  5 18:35:39.702: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Running", Reason="", readiness=true. Elapsed: 20.129245491s
Nov  5 18:35:41.710: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Running", Reason="", readiness=true. Elapsed: 22.137506457s
Nov  5 18:35:43.721: INFO: Pod "pod-subpath-test-secret-h8sk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.148526119s
STEP: Saw pod success
Nov  5 18:35:43.721: INFO: Pod "pod-subpath-test-secret-h8sk" satisfied condition "Succeeded or Failed"
Nov  5 18:35:43.733: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-subpath-test-secret-h8sk container test-container-subpath-secret-h8sk: <nil>
STEP: delete the pod
Nov  5 18:35:43.788: INFO: Waiting for pod pod-subpath-test-secret-h8sk to disappear
Nov  5 18:35:43.795: INFO: Pod pod-subpath-test-secret-h8sk no longer exists
STEP: Deleting pod pod-subpath-test-secret-h8sk
Nov  5 18:35:43.795: INFO: Deleting pod "pod-subpath-test-secret-h8sk" in namespace "subpath-2689"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:35:43.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2689" for this suite.

• [SLOW TEST:24.396 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":207,"skipped":3463,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:35:43.828: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:35:50.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7751" for this suite.

• [SLOW TEST:7.183 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":208,"skipped":3477,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:35:51.013: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:36:02.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4934" for this suite.

• [SLOW TEST:11.229 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":209,"skipped":3490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:36:02.248: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 18:36:02.352: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99d2a7d0-e36c-4a25-8485-800399a1d32a" in namespace "projected-8376" to be "Succeeded or Failed"
Nov  5 18:36:02.358: INFO: Pod "downwardapi-volume-99d2a7d0-e36c-4a25-8485-800399a1d32a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.58381ms
Nov  5 18:36:04.367: INFO: Pod "downwardapi-volume-99d2a7d0-e36c-4a25-8485-800399a1d32a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014460856s
Nov  5 18:36:06.377: INFO: Pod "downwardapi-volume-99d2a7d0-e36c-4a25-8485-800399a1d32a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024959685s
STEP: Saw pod success
Nov  5 18:36:06.377: INFO: Pod "downwardapi-volume-99d2a7d0-e36c-4a25-8485-800399a1d32a" satisfied condition "Succeeded or Failed"
Nov  5 18:36:06.387: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-99d2a7d0-e36c-4a25-8485-800399a1d32a container client-container: <nil>
STEP: delete the pod
Nov  5 18:36:06.433: INFO: Waiting for pod downwardapi-volume-99d2a7d0-e36c-4a25-8485-800399a1d32a to disappear
Nov  5 18:36:06.441: INFO: Pod downwardapi-volume-99d2a7d0-e36c-4a25-8485-800399a1d32a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:36:06.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8376" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":210,"skipped":3518,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:36:06.472: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:38:06.607: INFO: Deleting pod "var-expansion-e9084c94-f633-4d60-bd10-97867fc66104" in namespace "var-expansion-9388"
Nov  5 18:38:06.623: INFO: Wait up to 5m0s for pod "var-expansion-e9084c94-f633-4d60-bd10-97867fc66104" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:38:08.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9388" for this suite.

• [SLOW TEST:122.196 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":211,"skipped":3523,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:38:08.669: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov  5 18:38:08.770: INFO: Waiting up to 5m0s for pod "pod-a5dd2d63-1f8d-4711-ae81-5a93b593010a" in namespace "emptydir-688" to be "Succeeded or Failed"
Nov  5 18:38:08.780: INFO: Pod "pod-a5dd2d63-1f8d-4711-ae81-5a93b593010a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.971277ms
Nov  5 18:38:10.787: INFO: Pod "pod-a5dd2d63-1f8d-4711-ae81-5a93b593010a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01629435s
Nov  5 18:38:12.799: INFO: Pod "pod-a5dd2d63-1f8d-4711-ae81-5a93b593010a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028082456s
STEP: Saw pod success
Nov  5 18:38:12.799: INFO: Pod "pod-a5dd2d63-1f8d-4711-ae81-5a93b593010a" satisfied condition "Succeeded or Failed"
Nov  5 18:38:12.807: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-a5dd2d63-1f8d-4711-ae81-5a93b593010a container test-container: <nil>
STEP: delete the pod
Nov  5 18:38:12.902: INFO: Waiting for pod pod-a5dd2d63-1f8d-4711-ae81-5a93b593010a to disappear
Nov  5 18:38:12.908: INFO: Pod pod-a5dd2d63-1f8d-4711-ae81-5a93b593010a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:38:12.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-688" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":212,"skipped":3539,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:38:12.928: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov  5 18:38:19.617: INFO: Successfully updated pod "pod-update-activedeadlineseconds-764912e7-cda1-479a-ac58-4428e3e4cd89"
Nov  5 18:38:19.618: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-764912e7-cda1-479a-ac58-4428e3e4cd89" in namespace "pods-5039" to be "terminated due to deadline exceeded"
Nov  5 18:38:19.625: INFO: Pod "pod-update-activedeadlineseconds-764912e7-cda1-479a-ac58-4428e3e4cd89": Phase="Running", Reason="", readiness=true. Elapsed: 7.424035ms
Nov  5 18:38:21.637: INFO: Pod "pod-update-activedeadlineseconds-764912e7-cda1-479a-ac58-4428e3e4cd89": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.019092741s
Nov  5 18:38:21.637: INFO: Pod "pod-update-activedeadlineseconds-764912e7-cda1-479a-ac58-4428e3e4cd89" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:38:21.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5039" for this suite.

• [SLOW TEST:8.733 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":213,"skipped":3542,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:38:21.663: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov  5 18:38:21.772: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5970 /api/v1/namespaces/watch-5970/configmaps/e2e-watch-test-watch-closed 922d6fc9-14eb-419a-8938-a3e2ac2fd4ae 1647821573 0 2020-11-05 18:38:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-05 18:38:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 18:38:21.773: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5970 /api/v1/namespaces/watch-5970/configmaps/e2e-watch-test-watch-closed 922d6fc9-14eb-419a-8938-a3e2ac2fd4ae 1647821576 0 2020-11-05 18:38:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-05 18:38:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov  5 18:38:21.813: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5970 /api/v1/namespaces/watch-5970/configmaps/e2e-watch-test-watch-closed 922d6fc9-14eb-419a-8938-a3e2ac2fd4ae 1647821578 0 2020-11-05 18:38:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-05 18:38:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 18:38:21.813: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5970 /api/v1/namespaces/watch-5970/configmaps/e2e-watch-test-watch-closed 922d6fc9-14eb-419a-8938-a3e2ac2fd4ae 1647821589 0 2020-11-05 18:38:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-05 18:38:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:38:21.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5970" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":214,"skipped":3548,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:38:21.840: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-e326dd88-4a58-4210-b0c1-b1f38ad49910
STEP: Creating a pod to test consume secrets
Nov  5 18:38:21.985: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-deece78a-56c5-4bb5-adf3-ee183fe4b148" in namespace "projected-3573" to be "Succeeded or Failed"
Nov  5 18:38:21.991: INFO: Pod "pod-projected-secrets-deece78a-56c5-4bb5-adf3-ee183fe4b148": Phase="Pending", Reason="", readiness=false. Elapsed: 6.315586ms
Nov  5 18:38:24.003: INFO: Pod "pod-projected-secrets-deece78a-56c5-4bb5-adf3-ee183fe4b148": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0182014s
Nov  5 18:38:26.024: INFO: Pod "pod-projected-secrets-deece78a-56c5-4bb5-adf3-ee183fe4b148": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038928476s
STEP: Saw pod success
Nov  5 18:38:26.024: INFO: Pod "pod-projected-secrets-deece78a-56c5-4bb5-adf3-ee183fe4b148" satisfied condition "Succeeded or Failed"
Nov  5 18:38:26.031: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-secrets-deece78a-56c5-4bb5-adf3-ee183fe4b148 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov  5 18:38:26.075: INFO: Waiting for pod pod-projected-secrets-deece78a-56c5-4bb5-adf3-ee183fe4b148 to disappear
Nov  5 18:38:26.083: INFO: Pod pod-projected-secrets-deece78a-56c5-4bb5-adf3-ee183fe4b148 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:38:26.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3573" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":215,"skipped":3556,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:38:26.109: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov  5 18:38:26.190: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:38:33.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9373" for this suite.

• [SLOW TEST:7.865 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":216,"skipped":3567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:38:33.989: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:38:34.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9987" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":217,"skipped":3623,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:38:34.181: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3071
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-3071
Nov  5 18:38:34.296: INFO: Found 0 stateful pods, waiting for 1
Nov  5 18:38:44.316: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov  5 18:38:44.355: INFO: Deleting all statefulset in ns statefulset-3071
Nov  5 18:38:44.362: INFO: Scaling statefulset ss to 0
Nov  5 18:38:54.414: INFO: Waiting for statefulset status.replicas updated to 0
Nov  5 18:38:54.422: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:38:54.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3071" for this suite.

• [SLOW TEST:20.307 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":218,"skipped":3623,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:38:54.489: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6669
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6669
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6669
Nov  5 18:38:54.629: INFO: Found 0 stateful pods, waiting for 1
Nov  5 18:39:04.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov  5 18:39:04.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-6669 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov  5 18:39:05.384: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov  5 18:39:05.384: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov  5 18:39:05.384: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov  5 18:39:05.393: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov  5 18:39:15.403: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov  5 18:39:15.403: INFO: Waiting for statefulset status.replicas updated to 0
Nov  5 18:39:15.436: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999323s
Nov  5 18:39:16.446: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990457136s
Nov  5 18:39:17.459: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.980338826s
Nov  5 18:39:18.470: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.966754279s
Nov  5 18:39:19.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.956660799s
Nov  5 18:39:20.493: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.943741368s
Nov  5 18:39:21.503: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.932549591s
Nov  5 18:39:22.514: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.922992816s
Nov  5 18:39:23.525: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.911985024s
Nov  5 18:39:24.537: INFO: Verifying statefulset ss doesn't scale past 1 for another 900.78172ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6669
Nov  5 18:39:25.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-6669 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:39:25.921: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov  5 18:39:25.921: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov  5 18:39:25.921: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov  5 18:39:25.932: INFO: Found 1 stateful pods, waiting for 3
Nov  5 18:39:35.945: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 18:39:35.945: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 18:39:35.945: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov  5 18:39:35.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-6669 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov  5 18:39:36.313: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov  5 18:39:36.313: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov  5 18:39:36.313: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov  5 18:39:36.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-6669 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov  5 18:39:36.668: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov  5 18:39:36.668: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov  5 18:39:36.668: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov  5 18:39:36.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-6669 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov  5 18:39:37.009: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov  5 18:39:37.009: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov  5 18:39:37.009: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov  5 18:39:37.009: INFO: Waiting for statefulset status.replicas updated to 0
Nov  5 18:39:37.018: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov  5 18:39:47.049: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov  5 18:39:47.049: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov  5 18:39:47.049: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov  5 18:39:47.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999274s
Nov  5 18:39:48.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990644482s
Nov  5 18:39:49.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980451584s
Nov  5 18:39:50.109: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.969544759s
Nov  5 18:39:51.118: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.955447201s
Nov  5 18:39:52.130: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.947004574s
Nov  5 18:39:53.137: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.935261164s
Nov  5 18:39:54.148: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.927526473s
Nov  5 18:39:55.161: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.916446907s
Nov  5 18:39:56.172: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.220312ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6669
Nov  5 18:39:57.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-6669 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:39:57.513: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov  5 18:39:57.514: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov  5 18:39:57.514: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov  5 18:39:57.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-6669 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:39:57.882: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov  5 18:39:57.882: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov  5 18:39:57.882: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov  5 18:39:57.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-6669 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:39:58.247: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov  5 18:39:58.247: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov  5 18:39:58.247: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov  5 18:39:58.247: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov  5 18:40:18.287: INFO: Deleting all statefulset in ns statefulset-6669
Nov  5 18:40:18.295: INFO: Scaling statefulset ss to 0
Nov  5 18:40:18.322: INFO: Waiting for statefulset status.replicas updated to 0
Nov  5 18:40:18.329: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:40:18.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6669" for this suite.

• [SLOW TEST:83.906 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":219,"skipped":3630,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:40:18.396: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:40:18.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6937" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":220,"skipped":3639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:40:18.669: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov  5 18:40:22.813: INFO: &Pod{ObjectMeta:{send-events-0b1c9643-bae1-4dac-b9b1-0422ef089f11  events-6077 /api/v1/namespaces/events-6077/pods/send-events-0b1c9643-bae1-4dac-b9b1-0422ef089f11 84d551d3-c680-49f7-92bc-86560d95e36c 1647861166 0 2020-11-05 18:40:18 +0000 UTC <nil> <nil> map[name:foo time:749005186] map[cni.projectcalico.org/podIP:10.2.1.41/32] [] []  [{e2e.test Update v1 2020-11-05 18:40:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-05 18:40:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2020-11-05 18:40:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zkf8f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zkf8f,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zkf8f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-a7de9392-4cc3-4830-84ff-49e7615b291e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:40:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:40:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:40:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-05 18:40:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.91.151.116,PodIP:10.2.1.41,StartTime:2020-11-05 18:40:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-05 18:40:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://256fdd295a2b5f18ac7ee3d86945b0bb04dd0c6ee2eda5e376281250e106c153,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Nov  5 18:40:24.831: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov  5 18:40:26.843: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:40:26.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6077" for this suite.

• [SLOW TEST:8.217 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":221,"skipped":3672,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:40:26.887: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-031353b6-700c-4b3c-96c5-43efa298b7d2
STEP: Creating a pod to test consume secrets
Nov  5 18:40:27.007: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ae187a41-64bc-4a9a-bc8a-c3cfa1b7e03e" in namespace "projected-8098" to be "Succeeded or Failed"
Nov  5 18:40:27.016: INFO: Pod "pod-projected-secrets-ae187a41-64bc-4a9a-bc8a-c3cfa1b7e03e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.494767ms
Nov  5 18:40:29.026: INFO: Pod "pod-projected-secrets-ae187a41-64bc-4a9a-bc8a-c3cfa1b7e03e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019559078s
Nov  5 18:40:31.037: INFO: Pod "pod-projected-secrets-ae187a41-64bc-4a9a-bc8a-c3cfa1b7e03e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029810007s
STEP: Saw pod success
Nov  5 18:40:31.037: INFO: Pod "pod-projected-secrets-ae187a41-64bc-4a9a-bc8a-c3cfa1b7e03e" satisfied condition "Succeeded or Failed"
Nov  5 18:40:31.045: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-secrets-ae187a41-64bc-4a9a-bc8a-c3cfa1b7e03e container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov  5 18:40:31.173: INFO: Waiting for pod pod-projected-secrets-ae187a41-64bc-4a9a-bc8a-c3cfa1b7e03e to disappear
Nov  5 18:40:31.180: INFO: Pod pod-projected-secrets-ae187a41-64bc-4a9a-bc8a-c3cfa1b7e03e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:40:31.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8098" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":222,"skipped":3679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:40:31.206: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov  5 18:40:35.895: INFO: Successfully updated pod "annotationupdate4e548b6a-9ad3-44dd-97f4-ba8c1c3c70ea"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:40:37.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9231" for this suite.

• [SLOW TEST:6.758 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":223,"skipped":3707,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:40:37.968: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-9131
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9131
STEP: Deleting pre-stop pod
Nov  5 18:40:53.201: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:40:53.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9131" for this suite.

• [SLOW TEST:15.276 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":224,"skipped":3713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:40:53.248: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov  5 18:40:57.917: INFO: Successfully updated pod "labelsupdate88a4ef37-ef4c-485f-a137-87320121fcb9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:40:59.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-905" for this suite.

• [SLOW TEST:6.745 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":225,"skipped":3741,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:40:59.998: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov  5 18:41:00.096: INFO: Waiting up to 5m0s for pod "downward-api-92bbd354-ca91-4cdf-b985-48dd6e6784f9" in namespace "downward-api-7879" to be "Succeeded or Failed"
Nov  5 18:41:00.101: INFO: Pod "downward-api-92bbd354-ca91-4cdf-b985-48dd6e6784f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877501ms
Nov  5 18:41:02.111: INFO: Pod "downward-api-92bbd354-ca91-4cdf-b985-48dd6e6784f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015213147s
Nov  5 18:41:04.123: INFO: Pod "downward-api-92bbd354-ca91-4cdf-b985-48dd6e6784f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027677222s
STEP: Saw pod success
Nov  5 18:41:04.124: INFO: Pod "downward-api-92bbd354-ca91-4cdf-b985-48dd6e6784f9" satisfied condition "Succeeded or Failed"
Nov  5 18:41:04.131: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downward-api-92bbd354-ca91-4cdf-b985-48dd6e6784f9 container dapi-container: <nil>
STEP: delete the pod
Nov  5 18:41:04.175: INFO: Waiting for pod downward-api-92bbd354-ca91-4cdf-b985-48dd6e6784f9 to disappear
Nov  5 18:41:04.182: INFO: Pod downward-api-92bbd354-ca91-4cdf-b985-48dd6e6784f9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:41:04.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7879" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":226,"skipped":3743,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:41:04.205: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov  5 18:41:04.293: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov  5 18:41:19.990: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 18:41:23.921: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:41:39.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6021" for this suite.

• [SLOW TEST:35.225 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":227,"skipped":3751,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:41:39.435: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:41:39.563: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-3a11861d-7bdb-4fe4-8958-06180137e742" in namespace "security-context-test-6970" to be "Succeeded or Failed"
Nov  5 18:41:39.576: INFO: Pod "alpine-nnp-false-3a11861d-7bdb-4fe4-8958-06180137e742": Phase="Pending", Reason="", readiness=false. Elapsed: 12.044393ms
Nov  5 18:41:41.587: INFO: Pod "alpine-nnp-false-3a11861d-7bdb-4fe4-8958-06180137e742": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023504174s
Nov  5 18:41:43.594: INFO: Pod "alpine-nnp-false-3a11861d-7bdb-4fe4-8958-06180137e742": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030953406s
Nov  5 18:41:45.608: INFO: Pod "alpine-nnp-false-3a11861d-7bdb-4fe4-8958-06180137e742": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044009864s
Nov  5 18:41:45.608: INFO: Pod "alpine-nnp-false-3a11861d-7bdb-4fe4-8958-06180137e742" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:41:45.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6970" for this suite.

• [SLOW TEST:6.217 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":228,"skipped":3767,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:41:45.652: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Nov  5 18:41:45.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 api-versions'
Nov  5 18:41:45.874: INFO: stderr: ""
Nov  5 18:41:45.874: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\nappprotect.f5.com/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nk8s.nginx.org/v1\nk8s.nginx.org/v1alpha1\nkube.cloud.ovh.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:41:45.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-986" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":229,"skipped":3785,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:41:45.924: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:42:46.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-924" for this suite.

• [SLOW TEST:60.156 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":230,"skipped":3814,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:42:46.085: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:42:46.867: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov  5 18:42:48.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198566, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198566, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198566, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198566, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:42:52.334: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:42:52.347: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:42:53.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5882" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.702 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":231,"skipped":3820,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:42:53.793: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-769eedaf-c28f-4cc7-8693-9eda6add5a9a
STEP: Creating configMap with name cm-test-opt-upd-12ae1085-e9a6-440b-bdb8-cc823cfd8330
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-769eedaf-c28f-4cc7-8693-9eda6add5a9a
STEP: Updating configmap cm-test-opt-upd-12ae1085-e9a6-440b-bdb8-cc823cfd8330
STEP: Creating configMap with name cm-test-opt-create-f0dd2be0-ccd4-4a90-9a26-f0b0dbd7aee3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:44:24.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2541" for this suite.

• [SLOW TEST:90.509 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":232,"skipped":3827,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:44:24.303: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-34f21c0c-50b9-4901-a3ec-a2170f0b8baa
STEP: Creating a pod to test consume secrets
Nov  5 18:44:24.427: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-656480e6-c12d-4198-a205-78ba8af15378" in namespace "projected-6792" to be "Succeeded or Failed"
Nov  5 18:44:24.441: INFO: Pod "pod-projected-secrets-656480e6-c12d-4198-a205-78ba8af15378": Phase="Pending", Reason="", readiness=false. Elapsed: 13.936692ms
Nov  5 18:44:26.449: INFO: Pod "pod-projected-secrets-656480e6-c12d-4198-a205-78ba8af15378": Phase="Running", Reason="", readiness=true. Elapsed: 2.021579155s
Nov  5 18:44:28.461: INFO: Pod "pod-projected-secrets-656480e6-c12d-4198-a205-78ba8af15378": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033600238s
STEP: Saw pod success
Nov  5 18:44:28.461: INFO: Pod "pod-projected-secrets-656480e6-c12d-4198-a205-78ba8af15378" satisfied condition "Succeeded or Failed"
Nov  5 18:44:28.469: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-secrets-656480e6-c12d-4198-a205-78ba8af15378 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov  5 18:44:28.514: INFO: Waiting for pod pod-projected-secrets-656480e6-c12d-4198-a205-78ba8af15378 to disappear
Nov  5 18:44:28.520: INFO: Pod pod-projected-secrets-656480e6-c12d-4198-a205-78ba8af15378 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:44:28.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6792" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":233,"skipped":3832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:44:28.541: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:44:28.658: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov  5 18:44:28.677: INFO: Number of nodes with available pods: 0
Nov  5 18:44:28.677: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov  5 18:44:28.721: INFO: Number of nodes with available pods: 0
Nov  5 18:44:28.721: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:29.728: INFO: Number of nodes with available pods: 0
Nov  5 18:44:29.728: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:30.732: INFO: Number of nodes with available pods: 0
Nov  5 18:44:30.732: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:31.729: INFO: Number of nodes with available pods: 0
Nov  5 18:44:31.729: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:32.733: INFO: Number of nodes with available pods: 1
Nov  5 18:44:32.733: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov  5 18:44:32.776: INFO: Number of nodes with available pods: 1
Nov  5 18:44:32.776: INFO: Number of running nodes: 0, number of available pods: 1
Nov  5 18:44:33.784: INFO: Number of nodes with available pods: 0
Nov  5 18:44:33.784: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov  5 18:44:33.809: INFO: Number of nodes with available pods: 0
Nov  5 18:44:33.810: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:34.821: INFO: Number of nodes with available pods: 0
Nov  5 18:44:34.821: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:35.818: INFO: Number of nodes with available pods: 0
Nov  5 18:44:35.818: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:36.820: INFO: Number of nodes with available pods: 0
Nov  5 18:44:36.820: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:37.821: INFO: Number of nodes with available pods: 0
Nov  5 18:44:37.821: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:38.821: INFO: Number of nodes with available pods: 0
Nov  5 18:44:38.821: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:39.819: INFO: Number of nodes with available pods: 0
Nov  5 18:44:39.819: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:40.819: INFO: Number of nodes with available pods: 0
Nov  5 18:44:40.819: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:41.821: INFO: Number of nodes with available pods: 0
Nov  5 18:44:41.821: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:42.819: INFO: Number of nodes with available pods: 0
Nov  5 18:44:42.819: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 18:44:43.834: INFO: Number of nodes with available pods: 1
Nov  5 18:44:43.834: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-925, will wait for the garbage collector to delete the pods
Nov  5 18:44:43.939: INFO: Deleting DaemonSet.extensions daemon-set took: 18.052346ms
Nov  5 18:44:44.740: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.389731ms
Nov  5 18:44:47.449: INFO: Number of nodes with available pods: 0
Nov  5 18:44:47.449: INFO: Number of running nodes: 0, number of available pods: 0
Nov  5 18:44:47.457: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-925/daemonsets","resourceVersion":"1647947430"},"items":null}

Nov  5 18:44:47.464: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-925/pods","resourceVersion":"1647947431"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:44:47.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-925" for this suite.

• [SLOW TEST:18.988 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":234,"skipped":3865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:44:47.537: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov  5 18:44:47.672: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov  5 18:44:47.685: INFO: starting watch
STEP: patching
STEP: updating
Nov  5 18:44:47.707: INFO: waiting for watch events with expected annotations
Nov  5 18:44:47.707: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:44:47.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8710" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":235,"skipped":3896,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:44:47.842: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:45:04.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-827" for this suite.

• [SLOW TEST:16.226 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":236,"skipped":3901,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:45:04.072: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:45:04.155: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov  5 18:45:08.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-288 create -f -'
Nov  5 18:45:09.756: INFO: stderr: ""
Nov  5 18:45:09.756: INFO: stdout: "e2e-test-crd-publish-openapi-5751-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov  5 18:45:09.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-288 delete e2e-test-crd-publish-openapi-5751-crds test-cr'
Nov  5 18:45:09.891: INFO: stderr: ""
Nov  5 18:45:09.891: INFO: stdout: "e2e-test-crd-publish-openapi-5751-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov  5 18:45:09.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-288 apply -f -'
Nov  5 18:45:10.202: INFO: stderr: ""
Nov  5 18:45:10.202: INFO: stdout: "e2e-test-crd-publish-openapi-5751-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov  5 18:45:10.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-288 delete e2e-test-crd-publish-openapi-5751-crds test-cr'
Nov  5 18:45:10.351: INFO: stderr: ""
Nov  5 18:45:10.351: INFO: stdout: "e2e-test-crd-publish-openapi-5751-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov  5 18:45:10.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 explain e2e-test-crd-publish-openapi-5751-crds'
Nov  5 18:45:10.637: INFO: stderr: ""
Nov  5 18:45:10.637: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5751-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:45:14.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-288" for this suite.

• [SLOW TEST:10.484 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":237,"skipped":3901,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:45:14.556: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-ce429402-e7cc-47b4-ae9e-4e506b55ff20
STEP: Creating a pod to test consume secrets
Nov  5 18:45:14.673: INFO: Waiting up to 5m0s for pod "pod-secrets-3e889507-0ef4-49f1-953e-47b4f41729c3" in namespace "secrets-4803" to be "Succeeded or Failed"
Nov  5 18:45:14.682: INFO: Pod "pod-secrets-3e889507-0ef4-49f1-953e-47b4f41729c3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.327708ms
Nov  5 18:45:16.692: INFO: Pod "pod-secrets-3e889507-0ef4-49f1-953e-47b4f41729c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019578019s
Nov  5 18:45:18.703: INFO: Pod "pod-secrets-3e889507-0ef4-49f1-953e-47b4f41729c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030587794s
Nov  5 18:45:20.712: INFO: Pod "pod-secrets-3e889507-0ef4-49f1-953e-47b4f41729c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039113975s
STEP: Saw pod success
Nov  5 18:45:20.712: INFO: Pod "pod-secrets-3e889507-0ef4-49f1-953e-47b4f41729c3" satisfied condition "Succeeded or Failed"
Nov  5 18:45:20.719: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-secrets-3e889507-0ef4-49f1-953e-47b4f41729c3 container secret-env-test: <nil>
STEP: delete the pod
Nov  5 18:45:20.764: INFO: Waiting for pod pod-secrets-3e889507-0ef4-49f1-953e-47b4f41729c3 to disappear
Nov  5 18:45:20.771: INFO: Pod pod-secrets-3e889507-0ef4-49f1-953e-47b4f41729c3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:45:20.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4803" for this suite.

• [SLOW TEST:6.239 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":238,"skipped":3904,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:45:20.796: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov  5 18:45:20.878: INFO: PodSpec: initContainers in spec.initContainers
Nov  5 18:46:16.297: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-65ce190d-813e-46e5-a75e-794c69de151d", GenerateName:"", Namespace:"init-container-1615", SelfLink:"/api/v1/namespaces/init-container-1615/pods/pod-init-65ce190d-813e-46e5-a75e-794c69de151d", UID:"f3ff6179-53a4-4825-8c7b-676ca5ffe0a5", ResourceVersion:"1647976165", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63740198720, loc:(*time.Location)(0x7701840)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"878729438"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.2.1.56/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0017815c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0017815e0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001781620), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001781680)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0017816a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0017816e0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-kbj2b", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc006fbe400), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kbj2b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kbj2b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kbj2b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00506f5a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node-a7de9392-4cc3-4830-84ff-49e7615b291e", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0031ff420), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00506f6b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00506f6d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00506f6d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00506f6dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004c9df70), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198721, loc:(*time.Location)(0x7701840)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198721, loc:(*time.Location)(0x7701840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198721, loc:(*time.Location)(0x7701840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198720, loc:(*time.Location)(0x7701840)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"51.91.151.116", PodIP:"10.2.1.56", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.2.1.56"}}, StartTime:(*v1.Time)(0xc001781700), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0031ff500)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0031ff570)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://617673c95f6a958e85e8a0d75019f5f4b1b683b29ffb29fe9110e715cbec48b7", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001781760), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001781740), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00506f79f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:46:16.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1615" for this suite.

• [SLOW TEST:55.523 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":239,"skipped":3906,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:46:16.321: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov  5 18:46:16.426: INFO: Waiting up to 1m0s for all nodes to be ready
Nov  5 18:47:16.466: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Nov  5 18:47:16.510: INFO: Created pod: pod0-sched-preemption-low-priority
Nov  5 18:47:16.534: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:47:28.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7024" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:72.408 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":240,"skipped":3913,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:47:28.729: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Nov  5 18:47:29.017: INFO: Waiting up to 5m0s for pod "client-containers-beccfc61-c39a-4f64-8f5c-cfb6ef367234" in namespace "containers-8989" to be "Succeeded or Failed"
Nov  5 18:47:29.026: INFO: Pod "client-containers-beccfc61-c39a-4f64-8f5c-cfb6ef367234": Phase="Pending", Reason="", readiness=false. Elapsed: 8.562199ms
Nov  5 18:47:31.039: INFO: Pod "client-containers-beccfc61-c39a-4f64-8f5c-cfb6ef367234": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021769299s
Nov  5 18:47:33.050: INFO: Pod "client-containers-beccfc61-c39a-4f64-8f5c-cfb6ef367234": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03294447s
STEP: Saw pod success
Nov  5 18:47:33.050: INFO: Pod "client-containers-beccfc61-c39a-4f64-8f5c-cfb6ef367234" satisfied condition "Succeeded or Failed"
Nov  5 18:47:33.057: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod client-containers-beccfc61-c39a-4f64-8f5c-cfb6ef367234 container test-container: <nil>
STEP: delete the pod
Nov  5 18:47:33.180: INFO: Waiting for pod client-containers-beccfc61-c39a-4f64-8f5c-cfb6ef367234 to disappear
Nov  5 18:47:33.194: INFO: Pod client-containers-beccfc61-c39a-4f64-8f5c-cfb6ef367234 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:47:33.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8989" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":3924,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:47:33.230: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:47:33.930: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 18:47:35.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198853, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198853, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198853, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198853, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:47:38.985: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:47:38.994: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3625-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:47:40.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2039" for this suite.
STEP: Destroying namespace "webhook-2039-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.294 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":242,"skipped":3932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:47:40.531: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:47:40.615: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:47:47.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3866" for this suite.

• [SLOW TEST:6.688 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":243,"skipped":4003,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:47:47.220: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:47:48.093: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 18:47:50.116: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198868, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198868, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198868, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740198868, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:47:53.153: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:47:53.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2455" for this suite.
STEP: Destroying namespace "webhook-2455-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.202 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":244,"skipped":4021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:47:53.441: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:48:09.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-37" for this suite.
STEP: Destroying namespace "nsdeletetest-3003" for this suite.
Nov  5 18:48:09.772: INFO: Namespace nsdeletetest-3003 was already deleted
STEP: Destroying namespace "nsdeletetest-4766" for this suite.

• [SLOW TEST:16.347 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":245,"skipped":4043,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:48:09.797: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:48:23.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4807" for this suite.

• [SLOW TEST:13.256 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":246,"skipped":4088,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:48:23.054: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Nov  5 18:50:23.720: INFO: Successfully updated pod "var-expansion-a9994ad8-5e42-471a-869b-ebc6588b7dd9"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Nov  5 18:50:27.745: INFO: Deleting pod "var-expansion-a9994ad8-5e42-471a-869b-ebc6588b7dd9" in namespace "var-expansion-5431"
Nov  5 18:50:27.765: INFO: Wait up to 5m0s for pod "var-expansion-a9994ad8-5e42-471a-869b-ebc6588b7dd9" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:51:09.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5431" for this suite.

• [SLOW TEST:166.769 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":247,"skipped":4107,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:51:09.825: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-zjtnc in namespace proxy-8060
I1105 18:51:09.955950      20 runners.go:190] Created replication controller with name: proxy-service-zjtnc, namespace: proxy-8060, replica count: 1
I1105 18:51:11.006457      20 runners.go:190] proxy-service-zjtnc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 18:51:12.006731      20 runners.go:190] proxy-service-zjtnc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 18:51:13.007368      20 runners.go:190] proxy-service-zjtnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1105 18:51:14.008136      20 runners.go:190] proxy-service-zjtnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1105 18:51:15.008689      20 runners.go:190] proxy-service-zjtnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1105 18:51:16.009973      20 runners.go:190] proxy-service-zjtnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1105 18:51:17.010201      20 runners.go:190] proxy-service-zjtnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1105 18:51:18.010458      20 runners.go:190] proxy-service-zjtnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1105 18:51:19.010738      20 runners.go:190] proxy-service-zjtnc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 18:51:19.020: INFO: setup took 9.111265305s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov  5 18:51:19.046: INFO: (0) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 26.068217ms)
Nov  5 18:51:19.050: INFO: (0) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 29.112876ms)
Nov  5 18:51:19.050: INFO: (0) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 29.254705ms)
Nov  5 18:51:19.050: INFO: (0) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 29.109061ms)
Nov  5 18:51:19.054: INFO: (0) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 34.025782ms)
Nov  5 18:51:19.054: INFO: (0) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 34.128896ms)
Nov  5 18:51:19.055: INFO: (0) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 34.450411ms)
Nov  5 18:51:19.055: INFO: (0) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 34.574862ms)
Nov  5 18:51:19.055: INFO: (0) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 35.093182ms)
Nov  5 18:51:19.061: INFO: (0) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 40.646691ms)
Nov  5 18:51:19.061: INFO: (0) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 40.541716ms)
Nov  5 18:51:19.061: INFO: (0) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 41.238282ms)
Nov  5 18:51:19.061: INFO: (0) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 40.872991ms)
Nov  5 18:51:19.062: INFO: (0) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 41.446673ms)
Nov  5 18:51:19.062: INFO: (0) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 42.040584ms)
Nov  5 18:51:19.062: INFO: (0) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 41.301371ms)
Nov  5 18:51:19.079: INFO: (1) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 16.462185ms)
Nov  5 18:51:19.079: INFO: (1) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 16.237285ms)
Nov  5 18:51:19.081: INFO: (1) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 18.337379ms)
Nov  5 18:51:19.081: INFO: (1) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 18.443641ms)
Nov  5 18:51:19.081: INFO: (1) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 18.936657ms)
Nov  5 18:51:19.081: INFO: (1) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 18.830121ms)
Nov  5 18:51:19.082: INFO: (1) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 19.610558ms)
Nov  5 18:51:19.082: INFO: (1) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 19.158408ms)
Nov  5 18:51:19.082: INFO: (1) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 19.534623ms)
Nov  5 18:51:19.083: INFO: (1) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 20.300234ms)
Nov  5 18:51:19.083: INFO: (1) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 20.636433ms)
Nov  5 18:51:19.083: INFO: (1) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 20.560089ms)
Nov  5 18:51:19.083: INFO: (1) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 20.69896ms)
Nov  5 18:51:19.083: INFO: (1) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 20.505351ms)
Nov  5 18:51:19.083: INFO: (1) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 20.773245ms)
Nov  5 18:51:19.084: INFO: (1) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 21.830182ms)
Nov  5 18:51:19.100: INFO: (2) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 14.829717ms)
Nov  5 18:51:19.100: INFO: (2) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 14.087001ms)
Nov  5 18:51:19.100: INFO: (2) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 14.82529ms)
Nov  5 18:51:19.100: INFO: (2) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 14.541448ms)
Nov  5 18:51:19.103: INFO: (2) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 18.078934ms)
Nov  5 18:51:19.103: INFO: (2) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 17.949379ms)
Nov  5 18:51:19.104: INFO: (2) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 17.919154ms)
Nov  5 18:51:19.104: INFO: (2) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 19.204576ms)
Nov  5 18:51:19.104: INFO: (2) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 18.685334ms)
Nov  5 18:51:19.105: INFO: (2) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 20.204827ms)
Nov  5 18:51:19.105: INFO: (2) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 20.094863ms)
Nov  5 18:51:19.105: INFO: (2) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 19.572931ms)
Nov  5 18:51:19.105: INFO: (2) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 20.297307ms)
Nov  5 18:51:19.105: INFO: (2) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 21.031662ms)
Nov  5 18:51:19.106: INFO: (2) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 20.5098ms)
Nov  5 18:51:19.106: INFO: (2) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 20.682049ms)
Nov  5 18:51:19.115: INFO: (3) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 7.143361ms)
Nov  5 18:51:19.116: INFO: (3) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 7.75967ms)
Nov  5 18:51:19.116: INFO: (3) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 8.321708ms)
Nov  5 18:51:19.118: INFO: (3) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 9.057609ms)
Nov  5 18:51:19.119: INFO: (3) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 10.87291ms)
Nov  5 18:51:19.120: INFO: (3) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 12.449705ms)
Nov  5 18:51:19.120: INFO: (3) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 12.65558ms)
Nov  5 18:51:19.121: INFO: (3) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 11.429794ms)
Nov  5 18:51:19.121: INFO: (3) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 11.593353ms)
Nov  5 18:51:19.121: INFO: (3) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 12.71382ms)
Nov  5 18:51:19.122: INFO: (3) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 12.725389ms)
Nov  5 18:51:19.123: INFO: (3) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 14.060346ms)
Nov  5 18:51:19.123: INFO: (3) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 13.631689ms)
Nov  5 18:51:19.123: INFO: (3) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 14.457103ms)
Nov  5 18:51:19.123: INFO: (3) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 14.670243ms)
Nov  5 18:51:19.124: INFO: (3) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 14.847675ms)
Nov  5 18:51:19.136: INFO: (4) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 11.007979ms)
Nov  5 18:51:19.136: INFO: (4) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 10.948717ms)
Nov  5 18:51:19.136: INFO: (4) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 11.382724ms)
Nov  5 18:51:19.138: INFO: (4) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 13.164595ms)
Nov  5 18:51:19.138: INFO: (4) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 13.298585ms)
Nov  5 18:51:19.138: INFO: (4) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 13.795031ms)
Nov  5 18:51:19.138: INFO: (4) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 13.546941ms)
Nov  5 18:51:19.139: INFO: (4) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 14.444267ms)
Nov  5 18:51:19.139: INFO: (4) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 14.203484ms)
Nov  5 18:51:19.139: INFO: (4) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 14.338161ms)
Nov  5 18:51:19.139: INFO: (4) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 13.970983ms)
Nov  5 18:51:19.139: INFO: (4) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 15.015421ms)
Nov  5 18:51:19.143: INFO: (4) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 18.021958ms)
Nov  5 18:51:19.143: INFO: (4) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 18.071024ms)
Nov  5 18:51:19.143: INFO: (4) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 18.557167ms)
Nov  5 18:51:19.143: INFO: (4) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 18.163769ms)
Nov  5 18:51:19.165: INFO: (5) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 20.366137ms)
Nov  5 18:51:19.165: INFO: (5) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 20.757096ms)
Nov  5 18:51:19.165: INFO: (5) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 21.563636ms)
Nov  5 18:51:19.165: INFO: (5) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 21.267573ms)
Nov  5 18:51:19.166: INFO: (5) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 21.256341ms)
Nov  5 18:51:19.166: INFO: (5) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 21.197428ms)
Nov  5 18:51:19.166: INFO: (5) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 21.293658ms)
Nov  5 18:51:19.166: INFO: (5) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 22.18114ms)
Nov  5 18:51:19.166: INFO: (5) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 21.821862ms)
Nov  5 18:51:19.166: INFO: (5) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 21.882747ms)
Nov  5 18:51:19.166: INFO: (5) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 22.398076ms)
Nov  5 18:51:19.167: INFO: (5) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 22.565796ms)
Nov  5 18:51:19.167: INFO: (5) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 22.915559ms)
Nov  5 18:51:19.167: INFO: (5) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 23.064521ms)
Nov  5 18:51:19.167: INFO: (5) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 23.760003ms)
Nov  5 18:51:19.167: INFO: (5) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 23.102858ms)
Nov  5 18:51:19.181: INFO: (6) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 12.824184ms)
Nov  5 18:51:19.188: INFO: (6) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 19.213544ms)
Nov  5 18:51:19.188: INFO: (6) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 19.371591ms)
Nov  5 18:51:19.188: INFO: (6) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 19.793117ms)
Nov  5 18:51:19.189: INFO: (6) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 20.062807ms)
Nov  5 18:51:19.189: INFO: (6) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 20.270255ms)
Nov  5 18:51:19.189: INFO: (6) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 21.083074ms)
Nov  5 18:51:19.189: INFO: (6) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 21.152557ms)
Nov  5 18:51:19.189: INFO: (6) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 20.849129ms)
Nov  5 18:51:19.190: INFO: (6) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 21.679834ms)
Nov  5 18:51:19.190: INFO: (6) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 21.063887ms)
Nov  5 18:51:19.190: INFO: (6) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 21.382693ms)
Nov  5 18:51:19.190: INFO: (6) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 21.711678ms)
Nov  5 18:51:19.190: INFO: (6) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 21.887968ms)
Nov  5 18:51:19.191: INFO: (6) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 22.015529ms)
Nov  5 18:51:19.190: INFO: (6) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 22.106855ms)
Nov  5 18:51:19.201: INFO: (7) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 9.400358ms)
Nov  5 18:51:19.201: INFO: (7) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 9.792859ms)
Nov  5 18:51:19.203: INFO: (7) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 12.409085ms)
Nov  5 18:51:19.211: INFO: (7) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 19.55811ms)
Nov  5 18:51:19.221: INFO: (7) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 29.635042ms)
Nov  5 18:51:19.222: INFO: (7) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 30.166059ms)
Nov  5 18:51:19.222: INFO: (7) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 30.055796ms)
Nov  5 18:51:19.222: INFO: (7) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 30.439821ms)
Nov  5 18:51:19.222: INFO: (7) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 30.62775ms)
Nov  5 18:51:19.222: INFO: (7) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 31.007887ms)
Nov  5 18:51:19.222: INFO: (7) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 30.214541ms)
Nov  5 18:51:19.222: INFO: (7) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 30.501841ms)
Nov  5 18:51:19.222: INFO: (7) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 30.290477ms)
Nov  5 18:51:19.222: INFO: (7) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 30.335349ms)
Nov  5 18:51:19.222: INFO: (7) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 30.819244ms)
Nov  5 18:51:19.223: INFO: (7) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 31.426726ms)
Nov  5 18:51:19.239: INFO: (8) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 15.94681ms)
Nov  5 18:51:19.242: INFO: (8) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 18.817942ms)
Nov  5 18:51:19.242: INFO: (8) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 18.431697ms)
Nov  5 18:51:19.248: INFO: (8) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 24.771492ms)
Nov  5 18:51:19.248: INFO: (8) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 24.408044ms)
Nov  5 18:51:19.250: INFO: (8) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 27.520392ms)
Nov  5 18:51:19.250: INFO: (8) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 27.318973ms)
Nov  5 18:51:19.251: INFO: (8) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 27.106758ms)
Nov  5 18:51:19.251: INFO: (8) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 27.32656ms)
Nov  5 18:51:19.251: INFO: (8) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 27.513529ms)
Nov  5 18:51:19.251: INFO: (8) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 27.884465ms)
Nov  5 18:51:19.251: INFO: (8) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 28.195058ms)
Nov  5 18:51:19.251: INFO: (8) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 28.278878ms)
Nov  5 18:51:19.252: INFO: (8) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 28.13091ms)
Nov  5 18:51:19.252: INFO: (8) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 28.594958ms)
Nov  5 18:51:19.252: INFO: (8) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 29.158248ms)
Nov  5 18:51:19.266: INFO: (9) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 12.755984ms)
Nov  5 18:51:19.266: INFO: (9) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 12.7272ms)
Nov  5 18:51:19.266: INFO: (9) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 13.074453ms)
Nov  5 18:51:19.266: INFO: (9) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 13.340139ms)
Nov  5 18:51:19.267: INFO: (9) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 13.857391ms)
Nov  5 18:51:19.271: INFO: (9) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 17.181336ms)
Nov  5 18:51:19.271: INFO: (9) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 18.652533ms)
Nov  5 18:51:19.271: INFO: (9) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 18.494717ms)
Nov  5 18:51:19.272: INFO: (9) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 18.427902ms)
Nov  5 18:51:19.272: INFO: (9) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 18.418235ms)
Nov  5 18:51:19.272: INFO: (9) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 18.282454ms)
Nov  5 18:51:19.272: INFO: (9) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 19.453679ms)
Nov  5 18:51:19.272: INFO: (9) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 19.40631ms)
Nov  5 18:51:19.272: INFO: (9) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 19.278437ms)
Nov  5 18:51:19.272: INFO: (9) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 18.5571ms)
Nov  5 18:51:19.272: INFO: (9) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 18.778958ms)
Nov  5 18:51:19.284: INFO: (10) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 11.697355ms)
Nov  5 18:51:19.286: INFO: (10) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 13.741232ms)
Nov  5 18:51:19.287: INFO: (10) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 13.231054ms)
Nov  5 18:51:19.287: INFO: (10) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 13.488839ms)
Nov  5 18:51:19.287: INFO: (10) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 13.89026ms)
Nov  5 18:51:19.288: INFO: (10) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 14.700369ms)
Nov  5 18:51:19.288: INFO: (10) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 14.457772ms)
Nov  5 18:51:19.288: INFO: (10) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 14.636195ms)
Nov  5 18:51:19.288: INFO: (10) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 14.981704ms)
Nov  5 18:51:19.289: INFO: (10) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 16.194206ms)
Nov  5 18:51:19.289: INFO: (10) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 15.518394ms)
Nov  5 18:51:19.291: INFO: (10) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 17.776084ms)
Nov  5 18:51:19.292: INFO: (10) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 19.131866ms)
Nov  5 18:51:19.292: INFO: (10) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 20.005326ms)
Nov  5 18:51:19.292: INFO: (10) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 19.058967ms)
Nov  5 18:51:19.295: INFO: (10) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 22.14651ms)
Nov  5 18:51:19.310: INFO: (11) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 13.694952ms)
Nov  5 18:51:19.310: INFO: (11) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 14.697198ms)
Nov  5 18:51:19.312: INFO: (11) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 15.744772ms)
Nov  5 18:51:19.312: INFO: (11) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 16.246755ms)
Nov  5 18:51:19.314: INFO: (11) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 18.303371ms)
Nov  5 18:51:19.317: INFO: (11) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 21.189031ms)
Nov  5 18:51:19.317: INFO: (11) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 21.233915ms)
Nov  5 18:51:19.317: INFO: (11) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 21.249535ms)
Nov  5 18:51:19.318: INFO: (11) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 22.372172ms)
Nov  5 18:51:19.319: INFO: (11) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 22.733269ms)
Nov  5 18:51:19.318: INFO: (11) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 22.576135ms)
Nov  5 18:51:19.319: INFO: (11) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 23.240951ms)
Nov  5 18:51:19.319: INFO: (11) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 23.653817ms)
Nov  5 18:51:19.322: INFO: (11) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 26.234776ms)
Nov  5 18:51:19.322: INFO: (11) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 26.870796ms)
Nov  5 18:51:19.323: INFO: (11) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 26.719395ms)
Nov  5 18:51:19.331: INFO: (12) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 7.574949ms)
Nov  5 18:51:19.332: INFO: (12) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 8.464453ms)
Nov  5 18:51:19.333: INFO: (12) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 10.083811ms)
Nov  5 18:51:19.333: INFO: (12) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 9.96192ms)
Nov  5 18:51:19.337: INFO: (12) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 13.995303ms)
Nov  5 18:51:19.337: INFO: (12) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 13.73407ms)
Nov  5 18:51:19.337: INFO: (12) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 14.138919ms)
Nov  5 18:51:19.338: INFO: (12) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 14.40698ms)
Nov  5 18:51:19.338: INFO: (12) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 15.028383ms)
Nov  5 18:51:19.338: INFO: (12) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 14.99873ms)
Nov  5 18:51:19.339: INFO: (12) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 15.131074ms)
Nov  5 18:51:19.339: INFO: (12) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 15.374199ms)
Nov  5 18:51:19.339: INFO: (12) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 15.381707ms)
Nov  5 18:51:19.340: INFO: (12) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 16.184999ms)
Nov  5 18:51:19.340: INFO: (12) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 16.292508ms)
Nov  5 18:51:19.340: INFO: (12) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 16.124945ms)
Nov  5 18:51:19.350: INFO: (13) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 8.620941ms)
Nov  5 18:51:19.350: INFO: (13) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 9.859693ms)
Nov  5 18:51:19.350: INFO: (13) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 9.711467ms)
Nov  5 18:51:19.350: INFO: (13) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 9.627756ms)
Nov  5 18:51:19.350: INFO: (13) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 10.498135ms)
Nov  5 18:51:19.351: INFO: (13) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 8.858847ms)
Nov  5 18:51:19.351: INFO: (13) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 8.929831ms)
Nov  5 18:51:19.351: INFO: (13) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 9.903577ms)
Nov  5 18:51:19.352: INFO: (13) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 10.570511ms)
Nov  5 18:51:19.352: INFO: (13) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 11.655176ms)
Nov  5 18:51:19.370: INFO: (13) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 28.643106ms)
Nov  5 18:51:19.370: INFO: (13) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 30.242745ms)
Nov  5 18:51:19.373: INFO: (13) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 31.613608ms)
Nov  5 18:51:19.373: INFO: (13) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 31.70433ms)
Nov  5 18:51:19.374: INFO: (13) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 32.424456ms)
Nov  5 18:51:19.374: INFO: (13) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 32.707678ms)
Nov  5 18:51:19.385: INFO: (14) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 10.80862ms)
Nov  5 18:51:19.385: INFO: (14) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 10.565105ms)
Nov  5 18:51:19.387: INFO: (14) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 12.548984ms)
Nov  5 18:51:19.387: INFO: (14) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 12.244984ms)
Nov  5 18:51:19.387: INFO: (14) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 12.744066ms)
Nov  5 18:51:19.387: INFO: (14) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 12.158046ms)
Nov  5 18:51:19.388: INFO: (14) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 13.486552ms)
Nov  5 18:51:19.388: INFO: (14) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 13.821467ms)
Nov  5 18:51:19.388: INFO: (14) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 13.737861ms)
Nov  5 18:51:19.389: INFO: (14) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 13.696029ms)
Nov  5 18:51:19.389: INFO: (14) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 13.85508ms)
Nov  5 18:51:19.390: INFO: (14) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 15.691471ms)
Nov  5 18:51:19.390: INFO: (14) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 15.730246ms)
Nov  5 18:51:19.390: INFO: (14) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 15.699952ms)
Nov  5 18:51:19.394: INFO: (14) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 19.578988ms)
Nov  5 18:51:19.394: INFO: (14) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 19.69724ms)
Nov  5 18:51:19.414: INFO: (15) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 18.199081ms)
Nov  5 18:51:19.414: INFO: (15) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 18.427143ms)
Nov  5 18:51:19.414: INFO: (15) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 18.863317ms)
Nov  5 18:51:19.414: INFO: (15) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 19.388722ms)
Nov  5 18:51:19.414: INFO: (15) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 19.581201ms)
Nov  5 18:51:19.414: INFO: (15) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 19.234743ms)
Nov  5 18:51:19.414: INFO: (15) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 19.133121ms)
Nov  5 18:51:19.414: INFO: (15) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 19.189702ms)
Nov  5 18:51:19.414: INFO: (15) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 18.900223ms)
Nov  5 18:51:19.414: INFO: (15) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 18.743666ms)
Nov  5 18:51:19.415: INFO: (15) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 19.830522ms)
Nov  5 18:51:19.415: INFO: (15) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 19.570362ms)
Nov  5 18:51:19.416: INFO: (15) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 20.838651ms)
Nov  5 18:51:19.416: INFO: (15) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 19.886212ms)
Nov  5 18:51:19.416: INFO: (15) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 20.804693ms)
Nov  5 18:51:19.416: INFO: (15) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 20.735562ms)
Nov  5 18:51:19.437: INFO: (16) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 20.261427ms)
Nov  5 18:51:19.437: INFO: (16) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 20.220138ms)
Nov  5 18:51:19.437: INFO: (16) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 18.869777ms)
Nov  5 18:51:19.437: INFO: (16) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 19.94547ms)
Nov  5 18:51:19.442: INFO: (16) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 24.606925ms)
Nov  5 18:51:19.442: INFO: (16) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 24.280131ms)
Nov  5 18:51:19.442: INFO: (16) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 24.096238ms)
Nov  5 18:51:19.443: INFO: (16) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 24.803001ms)
Nov  5 18:51:19.443: INFO: (16) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 26.202327ms)
Nov  5 18:51:19.443: INFO: (16) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 25.891251ms)
Nov  5 18:51:19.445: INFO: (16) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 27.213495ms)
Nov  5 18:51:19.445: INFO: (16) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 28.792018ms)
Nov  5 18:51:19.446: INFO: (16) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 28.966474ms)
Nov  5 18:51:19.446: INFO: (16) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 28.645878ms)
Nov  5 18:51:19.446: INFO: (16) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 29.362165ms)
Nov  5 18:51:19.448: INFO: (16) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 29.982779ms)
Nov  5 18:51:19.456: INFO: (17) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 8.227984ms)
Nov  5 18:51:19.457: INFO: (17) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 8.528781ms)
Nov  5 18:51:19.457: INFO: (17) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 9.047317ms)
Nov  5 18:51:19.458: INFO: (17) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 10.181326ms)
Nov  5 18:51:19.459: INFO: (17) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 9.905206ms)
Nov  5 18:51:19.459: INFO: (17) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 11.191751ms)
Nov  5 18:51:19.460: INFO: (17) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 11.243601ms)
Nov  5 18:51:19.460: INFO: (17) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 11.404364ms)
Nov  5 18:51:19.460: INFO: (17) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 11.365278ms)
Nov  5 18:51:19.460: INFO: (17) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 11.166442ms)
Nov  5 18:51:19.461: INFO: (17) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 12.209441ms)
Nov  5 18:51:19.461: INFO: (17) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 12.703109ms)
Nov  5 18:51:19.462: INFO: (17) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 14.04762ms)
Nov  5 18:51:19.463: INFO: (17) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 14.312403ms)
Nov  5 18:51:19.463: INFO: (17) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 14.436462ms)
Nov  5 18:51:19.463: INFO: (17) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 15.200979ms)
Nov  5 18:51:19.471: INFO: (18) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 7.311254ms)
Nov  5 18:51:19.475: INFO: (18) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 11.788492ms)
Nov  5 18:51:19.475: INFO: (18) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 11.710237ms)
Nov  5 18:51:19.476: INFO: (18) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 11.753039ms)
Nov  5 18:51:19.476: INFO: (18) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 12.029661ms)
Nov  5 18:51:19.476: INFO: (18) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 12.253423ms)
Nov  5 18:51:19.476: INFO: (18) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 12.311148ms)
Nov  5 18:51:19.478: INFO: (18) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 14.416299ms)
Nov  5 18:51:19.479: INFO: (18) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 14.671121ms)
Nov  5 18:51:19.479: INFO: (18) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 14.792787ms)
Nov  5 18:51:19.479: INFO: (18) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 14.639721ms)
Nov  5 18:51:19.479: INFO: (18) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 15.200768ms)
Nov  5 18:51:19.479: INFO: (18) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 15.722796ms)
Nov  5 18:51:19.480: INFO: (18) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 15.943667ms)
Nov  5 18:51:19.481: INFO: (18) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 16.825411ms)
Nov  5 18:51:19.482: INFO: (18) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 17.746777ms)
Nov  5 18:51:19.490: INFO: (19) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">test<... (200; 7.711932ms)
Nov  5 18:51:19.490: INFO: (19) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:460/proxy/: tls baz (200; 7.742847ms)
Nov  5 18:51:19.491: INFO: (19) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 8.431537ms)
Nov  5 18:51:19.492: INFO: (19) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 9.091744ms)
Nov  5 18:51:19.492: INFO: (19) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:443/proxy/tlsrewritem... (200; 9.090888ms)
Nov  5 18:51:19.493: INFO: (19) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr/proxy/rewriteme">test</a> (200; 10.232153ms)
Nov  5 18:51:19.493: INFO: (19) /api/v1/namespaces/proxy-8060/pods/https:proxy-service-zjtnc-rs5fr:462/proxy/: tls qux (200; 10.769606ms)
Nov  5 18:51:19.493: INFO: (19) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:1080/proxy/rewriteme">... (200; 10.208726ms)
Nov  5 18:51:19.493: INFO: (19) /api/v1/namespaces/proxy-8060/pods/proxy-service-zjtnc-rs5fr:160/proxy/: foo (200; 10.663919ms)
Nov  5 18:51:19.494: INFO: (19) /api/v1/namespaces/proxy-8060/pods/http:proxy-service-zjtnc-rs5fr:162/proxy/: bar (200; 11.060349ms)
Nov  5 18:51:19.495: INFO: (19) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname1/proxy/: tls baz (200; 13.20439ms)
Nov  5 18:51:19.495: INFO: (19) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname1/proxy/: foo (200; 13.117037ms)
Nov  5 18:51:19.496: INFO: (19) /api/v1/namespaces/proxy-8060/services/http:proxy-service-zjtnc:portname2/proxy/: bar (200; 13.361791ms)
Nov  5 18:51:19.497: INFO: (19) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname1/proxy/: foo (200; 14.674821ms)
Nov  5 18:51:19.497: INFO: (19) /api/v1/namespaces/proxy-8060/services/proxy-service-zjtnc:portname2/proxy/: bar (200; 15.4262ms)
Nov  5 18:51:19.498: INFO: (19) /api/v1/namespaces/proxy-8060/services/https:proxy-service-zjtnc:tlsportname2/proxy/: tls qux (200; 16.111969ms)
STEP: deleting ReplicationController proxy-service-zjtnc in namespace proxy-8060, will wait for the garbage collector to delete the pods
Nov  5 18:51:19.570: INFO: Deleting ReplicationController proxy-service-zjtnc took: 16.440794ms
Nov  5 18:51:20.371: INFO: Terminating ReplicationController proxy-service-zjtnc pods took: 800.155159ms
[AfterEach] version v1
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:51:29.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8060" for this suite.

• [SLOW TEST:19.270 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":248,"skipped":4108,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:51:29.100: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov  5 18:51:29.270: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9187 /api/v1/namespaces/watch-9187/configmaps/e2e-watch-test-resource-version 941d4533-8b2b-4adc-8a9c-67d903924c86 1648076533 0 2020-11-05 18:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-05 18:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 18:51:29.270: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9187 /api/v1/namespaces/watch-9187/configmaps/e2e-watch-test-resource-version 941d4533-8b2b-4adc-8a9c-67d903924c86 1648076534 0 2020-11-05 18:51:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-05 18:51:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:51:29.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9187" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":249,"skipped":4119,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:51:29.302: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 18:51:30.175: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 18:51:32.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199090, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199090, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199090, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199090, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 18:51:35.234: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:51:35.241: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:51:36.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8250" for this suite.
STEP: Destroying namespace "webhook-8250-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.407 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":250,"skipped":4134,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:51:36.710: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7818
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7818
I1105 18:51:36.860804      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7818, replica count: 2
I1105 18:51:39.914418      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 18:51:42.914838      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 18:51:42.915: INFO: Creating new exec pod
Nov  5 18:51:47.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-7818 execpod29qrh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov  5 18:51:48.366: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov  5 18:51:48.368: INFO: stdout: ""
Nov  5 18:51:48.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-7818 execpod29qrh -- /bin/sh -x -c nc -zv -t -w 2 10.3.111.161 80'
Nov  5 18:51:48.706: INFO: stderr: "+ nc -zv -t -w 2 10.3.111.161 80\nConnection to 10.3.111.161 80 port [tcp/http] succeeded!\n"
Nov  5 18:51:48.706: INFO: stdout: ""
Nov  5 18:51:48.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-7818 execpod29qrh -- /bin/sh -x -c nc -zv -t -w 2 51.83.35.110 31801'
Nov  5 18:51:49.038: INFO: stderr: "+ nc -zv -t -w 2 51.83.35.110 31801\nConnection to 51.83.35.110 31801 port [tcp/31801] succeeded!\n"
Nov  5 18:51:49.038: INFO: stdout: ""
Nov  5 18:51:49.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-7818 execpod29qrh -- /bin/sh -x -c nc -zv -t -w 2 51.91.151.116 31801'
Nov  5 18:51:49.395: INFO: stderr: "+ nc -zv -t -w 2 51.91.151.116 31801\nConnection to 51.91.151.116 31801 port [tcp/31801] succeeded!\n"
Nov  5 18:51:49.395: INFO: stdout: ""
Nov  5 18:51:49.395: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:51:49.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7818" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:12.767 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":251,"skipped":4134,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:51:49.477: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-00ecc118-bfb6-4cec-8f26-89be17bc40bc
STEP: Creating a pod to test consume configMaps
Nov  5 18:51:49.608: INFO: Waiting up to 5m0s for pod "pod-configmaps-101d664e-8a77-4df1-aa05-06cad38d3028" in namespace "configmap-3255" to be "Succeeded or Failed"
Nov  5 18:51:49.619: INFO: Pod "pod-configmaps-101d664e-8a77-4df1-aa05-06cad38d3028": Phase="Pending", Reason="", readiness=false. Elapsed: 10.767096ms
Nov  5 18:51:51.630: INFO: Pod "pod-configmaps-101d664e-8a77-4df1-aa05-06cad38d3028": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021189517s
Nov  5 18:51:53.641: INFO: Pod "pod-configmaps-101d664e-8a77-4df1-aa05-06cad38d3028": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032622801s
STEP: Saw pod success
Nov  5 18:51:53.642: INFO: Pod "pod-configmaps-101d664e-8a77-4df1-aa05-06cad38d3028" satisfied condition "Succeeded or Failed"
Nov  5 18:51:53.651: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-configmaps-101d664e-8a77-4df1-aa05-06cad38d3028 container configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:51:53.753: INFO: Waiting for pod pod-configmaps-101d664e-8a77-4df1-aa05-06cad38d3028 to disappear
Nov  5 18:51:53.761: INFO: Pod pod-configmaps-101d664e-8a77-4df1-aa05-06cad38d3028 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:51:53.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3255" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":252,"skipped":4137,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:51:53.783: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov  5 18:52:00.959: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:52:02.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4200" for this suite.

• [SLOW TEST:8.248 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":253,"skipped":4140,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:52:02.036: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-49bb281d-f348-4f67-8136-932206afecc1
STEP: Creating a pod to test consume configMaps
Nov  5 18:52:02.144: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-983322a2-3d43-4db0-97c5-6e6f7c66ce1b" in namespace "projected-1086" to be "Succeeded or Failed"
Nov  5 18:52:02.155: INFO: Pod "pod-projected-configmaps-983322a2-3d43-4db0-97c5-6e6f7c66ce1b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.138071ms
Nov  5 18:52:04.164: INFO: Pod "pod-projected-configmaps-983322a2-3d43-4db0-97c5-6e6f7c66ce1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019925837s
Nov  5 18:52:06.175: INFO: Pod "pod-projected-configmaps-983322a2-3d43-4db0-97c5-6e6f7c66ce1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030889797s
STEP: Saw pod success
Nov  5 18:52:06.175: INFO: Pod "pod-projected-configmaps-983322a2-3d43-4db0-97c5-6e6f7c66ce1b" satisfied condition "Succeeded or Failed"
Nov  5 18:52:06.184: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-configmaps-983322a2-3d43-4db0-97c5-6e6f7c66ce1b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:52:06.230: INFO: Waiting for pod pod-projected-configmaps-983322a2-3d43-4db0-97c5-6e6f7c66ce1b to disappear
Nov  5 18:52:06.240: INFO: Pod pod-projected-configmaps-983322a2-3d43-4db0-97c5-6e6f7c66ce1b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:52:06.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1086" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":254,"skipped":4176,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:52:06.265: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov  5 18:52:14.461: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  5 18:52:14.469: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  5 18:52:16.469: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  5 18:52:16.481: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  5 18:52:18.469: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  5 18:52:18.478: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  5 18:52:20.469: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  5 18:52:20.483: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:52:20.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5014" for this suite.

• [SLOW TEST:14.275 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":255,"skipped":4183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:52:20.541: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Nov  5 18:52:20.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-5507'
Nov  5 18:52:21.040: INFO: stderr: ""
Nov  5 18:52:21.040: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov  5 18:52:22.050: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 18:52:22.050: INFO: Found 0 / 1
Nov  5 18:52:23.054: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 18:52:23.054: INFO: Found 0 / 1
Nov  5 18:52:24.052: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 18:52:24.052: INFO: Found 1 / 1
Nov  5 18:52:24.052: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov  5 18:52:24.060: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 18:52:24.060: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov  5 18:52:24.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 patch pod agnhost-primary-gnctp --namespace=kubectl-5507 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov  5 18:52:24.214: INFO: stderr: ""
Nov  5 18:52:24.214: INFO: stdout: "pod/agnhost-primary-gnctp patched\n"
STEP: checking annotations
Nov  5 18:52:24.223: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 18:52:24.223: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:52:24.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5507" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":256,"skipped":4213,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:52:24.247: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov  5 18:52:29.403: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:52:29.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9473" for this suite.

• [SLOW TEST:5.211 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":257,"skipped":4217,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:52:29.460: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:52:46.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7810" for this suite.

• [SLOW TEST:17.033 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":258,"skipped":4224,"failed":0}
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:52:46.493: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Nov  5 18:52:46.577: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Nov  5 18:52:47.137: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov  5 18:52:49.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:52:51.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:52:53.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:52:55.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:52:57.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:52:59.264: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740199167, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  5 18:53:02.572: INFO: Waited 1.267247977s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:53:03.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1017" for this suite.

• [SLOW TEST:17.105 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":259,"skipped":4225,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:53:03.599: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov  5 18:53:03.689: INFO: Waiting up to 5m0s for pod "pod-ace8b694-f2e7-468e-86ff-b38f34bd74e9" in namespace "emptydir-2310" to be "Succeeded or Failed"
Nov  5 18:53:03.699: INFO: Pod "pod-ace8b694-f2e7-468e-86ff-b38f34bd74e9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.085972ms
Nov  5 18:53:05.954: INFO: Pod "pod-ace8b694-f2e7-468e-86ff-b38f34bd74e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.264331572s
Nov  5 18:53:07.962: INFO: Pod "pod-ace8b694-f2e7-468e-86ff-b38f34bd74e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.272209096s
STEP: Saw pod success
Nov  5 18:53:07.962: INFO: Pod "pod-ace8b694-f2e7-468e-86ff-b38f34bd74e9" satisfied condition "Succeeded or Failed"
Nov  5 18:53:07.973: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-ace8b694-f2e7-468e-86ff-b38f34bd74e9 container test-container: <nil>
STEP: delete the pod
Nov  5 18:53:08.017: INFO: Waiting for pod pod-ace8b694-f2e7-468e-86ff-b38f34bd74e9 to disappear
Nov  5 18:53:08.024: INFO: Pod pod-ace8b694-f2e7-468e-86ff-b38f34bd74e9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:53:08.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2310" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":260,"skipped":4227,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:53:08.049: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:53:13.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6951" for this suite.

• [SLOW TEST:5.187 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":261,"skipped":4234,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:53:13.237: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Nov  5 18:53:13.322: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Nov  5 18:53:13.336: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov  5 18:53:13.336: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Nov  5 18:53:13.354: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov  5 18:53:13.354: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Nov  5 18:53:13.377: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov  5 18:53:13.377: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Nov  5 18:53:20.710: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:53:20.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3440" for this suite.

• [SLOW TEST:7.545 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":262,"skipped":4242,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:53:20.784: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Nov  5 18:53:20.893: INFO: Waiting up to 5m0s for pod "var-expansion-53335aa2-6c19-48b8-aa08-8b4d42ec749f" in namespace "var-expansion-702" to be "Succeeded or Failed"
Nov  5 18:53:20.906: INFO: Pod "var-expansion-53335aa2-6c19-48b8-aa08-8b4d42ec749f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.853996ms
Nov  5 18:53:22.914: INFO: Pod "var-expansion-53335aa2-6c19-48b8-aa08-8b4d42ec749f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020028984s
Nov  5 18:53:24.929: INFO: Pod "var-expansion-53335aa2-6c19-48b8-aa08-8b4d42ec749f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034959337s
Nov  5 18:53:26.937: INFO: Pod "var-expansion-53335aa2-6c19-48b8-aa08-8b4d42ec749f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043783586s
STEP: Saw pod success
Nov  5 18:53:26.938: INFO: Pod "var-expansion-53335aa2-6c19-48b8-aa08-8b4d42ec749f" satisfied condition "Succeeded or Failed"
Nov  5 18:53:26.946: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod var-expansion-53335aa2-6c19-48b8-aa08-8b4d42ec749f container dapi-container: <nil>
STEP: delete the pod
Nov  5 18:53:26.997: INFO: Waiting for pod var-expansion-53335aa2-6c19-48b8-aa08-8b4d42ec749f to disappear
Nov  5 18:53:27.009: INFO: Pod var-expansion-53335aa2-6c19-48b8-aa08-8b4d42ec749f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:53:27.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-702" for this suite.

• [SLOW TEST:6.245 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":263,"skipped":4247,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:53:27.030: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov  5 18:53:27.123: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-a e1366b1e-3cb3-484f-907c-21c2b4c09a2d 1648115067 0 2020-11-05 18:53:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-05 18:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 18:53:27.123: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-a e1366b1e-3cb3-484f-907c-21c2b4c09a2d 1648115067 0 2020-11-05 18:53:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-05 18:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov  5 18:53:37.140: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-a e1366b1e-3cb3-484f-907c-21c2b4c09a2d 1648118218 0 2020-11-05 18:53:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-05 18:53:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 18:53:37.141: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-a e1366b1e-3cb3-484f-907c-21c2b4c09a2d 1648118218 0 2020-11-05 18:53:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-05 18:53:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov  5 18:53:47.163: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-a e1366b1e-3cb3-484f-907c-21c2b4c09a2d 1648121479 0 2020-11-05 18:53:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-05 18:53:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 18:53:47.163: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-a e1366b1e-3cb3-484f-907c-21c2b4c09a2d 1648121479 0 2020-11-05 18:53:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-05 18:53:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov  5 18:53:57.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-a e1366b1e-3cb3-484f-907c-21c2b4c09a2d 1648124561 0 2020-11-05 18:53:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-05 18:53:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 18:53:57.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-a e1366b1e-3cb3-484f-907c-21c2b4c09a2d 1648124561 0 2020-11-05 18:53:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-05 18:53:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov  5 18:54:07.213: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-b 9ba5a1f2-8628-4585-bf9b-729a1dba6548 1648127603 0 2020-11-05 18:54:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-05 18:54:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 18:54:07.214: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-b 9ba5a1f2-8628-4585-bf9b-729a1dba6548 1648127603 0 2020-11-05 18:54:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-05 18:54:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov  5 18:54:17.233: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-b 9ba5a1f2-8628-4585-bf9b-729a1dba6548 1648130907 0 2020-11-05 18:54:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-05 18:54:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov  5 18:54:17.233: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7934 /api/v1/namespaces/watch-7934/configmaps/e2e-watch-test-configmap-b 9ba5a1f2-8628-4585-bf9b-729a1dba6548 1648130907 0 2020-11-05 18:54:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-05 18:54:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:54:27.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7934" for this suite.

• [SLOW TEST:60.230 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":264,"skipped":4280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:54:27.266: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-282d5e3c-7780-4689-b683-36734343ef54
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:54:33.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7507" for this suite.

• [SLOW TEST:6.212 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":265,"skipped":4316,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:54:33.478: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9784 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9784;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9784 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9784;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9784.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9784.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9784.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9784.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9784.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9784.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9784.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9784.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9784.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9784.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9784.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 146.64.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.64.146_udp@PTR;check="$$(dig +tcp +noall +answer +search 146.64.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.64.146_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9784 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9784;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9784 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9784;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9784.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9784.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9784.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9784.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9784.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9784.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9784.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9784.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9784.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9784.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9784.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9784.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 146.64.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.64.146_udp@PTR;check="$$(dig +tcp +noall +answer +search 146.64.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.64.146_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  5 18:54:39.639: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:39.649: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:39.660: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:39.672: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:39.683: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:39.691: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:39.701: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:39.711: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:40.206: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:40.216: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:40.226: INFO: Unable to read jessie_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:40.237: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:40.246: INFO: Unable to read jessie_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:40.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:40.266: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:40.281: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:40.324: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:40.344: INFO: Lookups using dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9784 wheezy_tcp@dns-test-service.dns-9784 wheezy_udp@dns-test-service.dns-9784.svc wheezy_tcp@dns-test-service.dns-9784.svc wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9784 jessie_tcp@dns-test-service.dns-9784 jessie_udp@dns-test-service.dns-9784.svc jessie_tcp@dns-test-service.dns-9784.svc jessie_udp@_http._tcp.dns-test-service.dns-9784.svc jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc jessie_tcp@PodARecord]

Nov  5 18:54:45.357: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.367: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.376: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.386: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.395: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.404: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.416: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.426: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.490: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.510: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.521: INFO: Unable to read jessie_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.546: INFO: Unable to read jessie_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.555: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.563: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.573: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.611: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:45.630: INFO: Lookups using dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9784 wheezy_tcp@dns-test-service.dns-9784 wheezy_udp@dns-test-service.dns-9784.svc wheezy_tcp@dns-test-service.dns-9784.svc wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9784 jessie_tcp@dns-test-service.dns-9784 jessie_udp@dns-test-service.dns-9784.svc jessie_tcp@dns-test-service.dns-9784.svc jessie_udp@_http._tcp.dns-test-service.dns-9784.svc jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc jessie_tcp@PodARecord]

Nov  5 18:54:50.357: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.366: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.377: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.388: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.400: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.409: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.420: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.431: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.501: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.511: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.521: INFO: Unable to read jessie_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.544: INFO: Unable to read jessie_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.554: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.563: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.574: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:50.636: INFO: Lookups using dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9784 wheezy_tcp@dns-test-service.dns-9784 wheezy_udp@dns-test-service.dns-9784.svc wheezy_tcp@dns-test-service.dns-9784.svc wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9784 jessie_tcp@dns-test-service.dns-9784 jessie_udp@dns-test-service.dns-9784.svc jessie_tcp@dns-test-service.dns-9784.svc jessie_udp@_http._tcp.dns-test-service.dns-9784.svc jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc]

Nov  5 18:54:56.201: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.463: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.484: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.497: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.509: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.522: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.533: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.541: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.622: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.631: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.640: INFO: Unable to read jessie_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.650: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.659: INFO: Unable to read jessie_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.669: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.679: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.692: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:54:56.761: INFO: Lookups using dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9784 wheezy_tcp@dns-test-service.dns-9784 wheezy_udp@dns-test-service.dns-9784.svc wheezy_tcp@dns-test-service.dns-9784.svc wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9784 jessie_tcp@dns-test-service.dns-9784 jessie_udp@dns-test-service.dns-9784.svc jessie_tcp@dns-test-service.dns-9784.svc jessie_udp@_http._tcp.dns-test-service.dns-9784.svc jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc]

Nov  5 18:55:00.357: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.366: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.375: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.386: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.395: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.406: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.415: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.426: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.498: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.507: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.518: INFO: Unable to read jessie_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.530: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.540: INFO: Unable to read jessie_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.550: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.561: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.571: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:00.647: INFO: Lookups using dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9784 wheezy_tcp@dns-test-service.dns-9784 wheezy_udp@dns-test-service.dns-9784.svc wheezy_tcp@dns-test-service.dns-9784.svc wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9784 jessie_tcp@dns-test-service.dns-9784 jessie_udp@dns-test-service.dns-9784.svc jessie_tcp@dns-test-service.dns-9784.svc jessie_udp@_http._tcp.dns-test-service.dns-9784.svc jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc]

Nov  5 18:55:05.357: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.372: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.384: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.395: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.406: INFO: Unable to read wheezy_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.416: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.427: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.437: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.514: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.525: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.542: INFO: Unable to read jessie_udp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.555: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784 from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.566: INFO: Unable to read jessie_udp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.575: INFO: Unable to read jessie_tcp@dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.596: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.611: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc from pod dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639: the server could not find the requested resource (get pods dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639)
Nov  5 18:55:05.683: INFO: Lookups using dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9784 wheezy_tcp@dns-test-service.dns-9784 wheezy_udp@dns-test-service.dns-9784.svc wheezy_tcp@dns-test-service.dns-9784.svc wheezy_udp@_http._tcp.dns-test-service.dns-9784.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9784.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9784 jessie_tcp@dns-test-service.dns-9784 jessie_udp@dns-test-service.dns-9784.svc jessie_tcp@dns-test-service.dns-9784.svc jessie_udp@_http._tcp.dns-test-service.dns-9784.svc jessie_tcp@_http._tcp.dns-test-service.dns-9784.svc]

Nov  5 18:55:10.675: INFO: DNS probes using dns-9784/dns-test-16fccceb-a012-4ee8-9ce8-655fce40e639 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:55:10.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9784" for this suite.

• [SLOW TEST:37.349 seconds]
[sig-network] DNS
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":266,"skipped":4321,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:55:10.828: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov  5 18:55:10.932: INFO: Waiting up to 5m0s for pod "downward-api-5e95e48a-8912-4c9a-9937-a76e7927909b" in namespace "downward-api-2211" to be "Succeeded or Failed"
Nov  5 18:55:10.943: INFO: Pod "downward-api-5e95e48a-8912-4c9a-9937-a76e7927909b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.977928ms
Nov  5 18:55:12.954: INFO: Pod "downward-api-5e95e48a-8912-4c9a-9937-a76e7927909b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020913643s
Nov  5 18:55:14.966: INFO: Pod "downward-api-5e95e48a-8912-4c9a-9937-a76e7927909b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033756006s
Nov  5 18:55:16.977: INFO: Pod "downward-api-5e95e48a-8912-4c9a-9937-a76e7927909b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044172249s
STEP: Saw pod success
Nov  5 18:55:16.977: INFO: Pod "downward-api-5e95e48a-8912-4c9a-9937-a76e7927909b" satisfied condition "Succeeded or Failed"
Nov  5 18:55:16.983: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downward-api-5e95e48a-8912-4c9a-9937-a76e7927909b container dapi-container: <nil>
STEP: delete the pod
Nov  5 18:55:17.045: INFO: Waiting for pod downward-api-5e95e48a-8912-4c9a-9937-a76e7927909b to disappear
Nov  5 18:55:17.053: INFO: Pod downward-api-5e95e48a-8912-4c9a-9937-a76e7927909b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:55:17.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2211" for this suite.

• [SLOW TEST:6.247 seconds]
[sig-node] Downward API
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4336,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:55:17.075: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:55:17.168: INFO: The status of Pod test-webserver-62703270-4fe3-40aa-bd23-b836fb2901ea is Pending, waiting for it to be Running (with Ready = true)
Nov  5 18:55:19.176: INFO: The status of Pod test-webserver-62703270-4fe3-40aa-bd23-b836fb2901ea is Pending, waiting for it to be Running (with Ready = true)
Nov  5 18:55:21.179: INFO: The status of Pod test-webserver-62703270-4fe3-40aa-bd23-b836fb2901ea is Running (Ready = false)
Nov  5 18:55:23.181: INFO: The status of Pod test-webserver-62703270-4fe3-40aa-bd23-b836fb2901ea is Running (Ready = false)
Nov  5 18:55:25.180: INFO: The status of Pod test-webserver-62703270-4fe3-40aa-bd23-b836fb2901ea is Running (Ready = false)
Nov  5 18:55:27.176: INFO: The status of Pod test-webserver-62703270-4fe3-40aa-bd23-b836fb2901ea is Running (Ready = false)
Nov  5 18:55:29.177: INFO: The status of Pod test-webserver-62703270-4fe3-40aa-bd23-b836fb2901ea is Running (Ready = false)
Nov  5 18:55:31.177: INFO: The status of Pod test-webserver-62703270-4fe3-40aa-bd23-b836fb2901ea is Running (Ready = false)
Nov  5 18:55:33.178: INFO: The status of Pod test-webserver-62703270-4fe3-40aa-bd23-b836fb2901ea is Running (Ready = false)
Nov  5 18:55:35.177: INFO: The status of Pod test-webserver-62703270-4fe3-40aa-bd23-b836fb2901ea is Running (Ready = true)
Nov  5 18:55:35.184: INFO: Container started at 2020-11-05 18:55:19 +0000 UTC, pod became ready at 2020-11-05 18:55:35 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:55:35.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1978" for this suite.

• [SLOW TEST:18.132 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":268,"skipped":4338,"failed":0}
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:55:35.208: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Nov  5 18:55:39.344: INFO: Pod pod-hostip-e27db793-d428-4b12-b2ed-00920e15cbf8 has hostIP: 51.91.151.116
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:55:39.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4565" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":269,"skipped":4338,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:55:39.381: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-7409/configmap-test-e4172c52-9165-46e8-b1aa-0bee30b1404f
STEP: Creating a pod to test consume configMaps
Nov  5 18:55:39.486: INFO: Waiting up to 5m0s for pod "pod-configmaps-bfc46fd1-54a8-48c5-9663-7f1b08a69898" in namespace "configmap-7409" to be "Succeeded or Failed"
Nov  5 18:55:39.495: INFO: Pod "pod-configmaps-bfc46fd1-54a8-48c5-9663-7f1b08a69898": Phase="Pending", Reason="", readiness=false. Elapsed: 9.056414ms
Nov  5 18:55:41.504: INFO: Pod "pod-configmaps-bfc46fd1-54a8-48c5-9663-7f1b08a69898": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018365227s
Nov  5 18:55:43.520: INFO: Pod "pod-configmaps-bfc46fd1-54a8-48c5-9663-7f1b08a69898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034341912s
STEP: Saw pod success
Nov  5 18:55:43.520: INFO: Pod "pod-configmaps-bfc46fd1-54a8-48c5-9663-7f1b08a69898" satisfied condition "Succeeded or Failed"
Nov  5 18:55:43.529: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-configmaps-bfc46fd1-54a8-48c5-9663-7f1b08a69898 container env-test: <nil>
STEP: delete the pod
Nov  5 18:55:43.580: INFO: Waiting for pod pod-configmaps-bfc46fd1-54a8-48c5-9663-7f1b08a69898 to disappear
Nov  5 18:55:43.587: INFO: Pod pod-configmaps-bfc46fd1-54a8-48c5-9663-7f1b08a69898 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:55:43.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7409" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4356,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:55:43.621: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1105 18:55:53.874310      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1105 18:55:53.874370      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1105 18:55:53.874382      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov  5 18:55:53.874: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov  5 18:55:53.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-7cdnf" in namespace "gc-5410"
Nov  5 18:55:53.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qhq9" in namespace "gc-5410"
Nov  5 18:55:53.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6ss4" in namespace "gc-5410"
Nov  5 18:55:53.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-dq9hz" in namespace "gc-5410"
Nov  5 18:55:54.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9mjz" in namespace "gc-5410"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:55:54.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5410" for this suite.

• [SLOW TEST:10.461 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":271,"skipped":4402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:55:54.084: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-0bb942a7-fc22-4565-ab54-8944bf6a7982
STEP: Creating a pod to test consume configMaps
Nov  5 18:55:54.194: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a6a2b3f8-86e0-404e-8751-a89413f0614b" in namespace "projected-70" to be "Succeeded or Failed"
Nov  5 18:55:54.204: INFO: Pod "pod-projected-configmaps-a6a2b3f8-86e0-404e-8751-a89413f0614b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.54085ms
Nov  5 18:55:56.213: INFO: Pod "pod-projected-configmaps-a6a2b3f8-86e0-404e-8751-a89413f0614b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018921399s
Nov  5 18:55:58.222: INFO: Pod "pod-projected-configmaps-a6a2b3f8-86e0-404e-8751-a89413f0614b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027332989s
STEP: Saw pod success
Nov  5 18:55:58.222: INFO: Pod "pod-projected-configmaps-a6a2b3f8-86e0-404e-8751-a89413f0614b" satisfied condition "Succeeded or Failed"
Nov  5 18:55:58.230: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-configmaps-a6a2b3f8-86e0-404e-8751-a89413f0614b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  5 18:55:58.280: INFO: Waiting for pod pod-projected-configmaps-a6a2b3f8-86e0-404e-8751-a89413f0614b to disappear
Nov  5 18:55:58.287: INFO: Pod pod-projected-configmaps-a6a2b3f8-86e0-404e-8751-a89413f0614b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:55:58.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-70" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":272,"skipped":4460,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:55:58.313: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3739.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3739.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  5 18:56:04.703: INFO: DNS probes using dns-test-2c5cfafd-f9a5-4daa-97b5-fcf9fcdbf827 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3739.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3739.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  5 18:56:10.850: INFO: File wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local from pod  dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov  5 18:56:10.862: INFO: File jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local from pod  dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov  5 18:56:10.862: INFO: Lookups using dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb failed for: [wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local]

Nov  5 18:56:15.876: INFO: File wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local from pod  dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov  5 18:56:15.886: INFO: File jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local from pod  dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov  5 18:56:15.887: INFO: Lookups using dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb failed for: [wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local]

Nov  5 18:56:20.877: INFO: File wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local from pod  dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov  5 18:56:20.889: INFO: File jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local from pod  dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov  5 18:56:20.890: INFO: Lookups using dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb failed for: [wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local]

Nov  5 18:56:25.873: INFO: File wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local from pod  dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov  5 18:56:25.882: INFO: File jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local from pod  dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov  5 18:56:25.882: INFO: Lookups using dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb failed for: [wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local]

Nov  5 18:56:30.876: INFO: File wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local from pod  dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov  5 18:56:30.886: INFO: File jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local from pod  dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov  5 18:56:30.886: INFO: Lookups using dns-3739/dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb failed for: [wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local]

Nov  5 18:56:35.889: INFO: DNS probes using dns-test-a78acd30-363c-41e5-b90d-6bebfb553fdb succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3739.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3739.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3739.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3739.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  5 18:56:42.045: INFO: DNS probes using dns-test-bea28b90-bf6c-447a-ba16-8ace31c2354d succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:56:42.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3739" for this suite.

• [SLOW TEST:43.828 seconds]
[sig-network] DNS
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":273,"skipped":4460,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:56:42.142: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-d258a257-78b8-4b09-801b-77601ec9dd6d
STEP: Creating a pod to test consume secrets
Nov  5 18:56:42.251: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a9968c89-aee0-40aa-bd93-15a0a01c095a" in namespace "projected-9816" to be "Succeeded or Failed"
Nov  5 18:56:42.260: INFO: Pod "pod-projected-secrets-a9968c89-aee0-40aa-bd93-15a0a01c095a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.72748ms
Nov  5 18:56:44.269: INFO: Pod "pod-projected-secrets-a9968c89-aee0-40aa-bd93-15a0a01c095a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017363314s
Nov  5 18:56:46.293: INFO: Pod "pod-projected-secrets-a9968c89-aee0-40aa-bd93-15a0a01c095a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041464634s
STEP: Saw pod success
Nov  5 18:56:46.293: INFO: Pod "pod-projected-secrets-a9968c89-aee0-40aa-bd93-15a0a01c095a" satisfied condition "Succeeded or Failed"
Nov  5 18:56:46.322: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-projected-secrets-a9968c89-aee0-40aa-bd93-15a0a01c095a container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov  5 18:56:46.363: INFO: Waiting for pod pod-projected-secrets-a9968c89-aee0-40aa-bd93-15a0a01c095a to disappear
Nov  5 18:56:46.370: INFO: Pod pod-projected-secrets-a9968c89-aee0-40aa-bd93-15a0a01c095a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:56:46.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9816" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4474,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:56:46.401: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov  5 18:56:46.480: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:56:56.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7463" for this suite.

• [SLOW TEST:10.095 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":275,"skipped":4485,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:56:56.496: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 18:56:56.582: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov  5 18:57:00.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-7196 create -f -'
Nov  5 18:57:01.500: INFO: stderr: ""
Nov  5 18:57:01.501: INFO: stdout: "e2e-test-crd-publish-openapi-7061-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov  5 18:57:01.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-7196 delete e2e-test-crd-publish-openapi-7061-crds test-foo'
Nov  5 18:57:01.656: INFO: stderr: ""
Nov  5 18:57:01.656: INFO: stdout: "e2e-test-crd-publish-openapi-7061-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov  5 18:57:01.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-7196 apply -f -'
Nov  5 18:57:01.964: INFO: stderr: ""
Nov  5 18:57:01.964: INFO: stdout: "e2e-test-crd-publish-openapi-7061-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov  5 18:57:01.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-7196 delete e2e-test-crd-publish-openapi-7061-crds test-foo'
Nov  5 18:57:02.107: INFO: stderr: ""
Nov  5 18:57:02.107: INFO: stdout: "e2e-test-crd-publish-openapi-7061-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov  5 18:57:02.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-7196 create -f -'
Nov  5 18:57:02.385: INFO: rc: 1
Nov  5 18:57:02.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-7196 apply -f -'
Nov  5 18:57:02.644: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov  5 18:57:02.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-7196 create -f -'
Nov  5 18:57:02.911: INFO: rc: 1
Nov  5 18:57:02.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 --namespace=crd-publish-openapi-7196 apply -f -'
Nov  5 18:57:03.171: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov  5 18:57:03.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 explain e2e-test-crd-publish-openapi-7061-crds'
Nov  5 18:57:03.446: INFO: stderr: ""
Nov  5 18:57:03.446: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7061-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov  5 18:57:03.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 explain e2e-test-crd-publish-openapi-7061-crds.metadata'
Nov  5 18:57:03.726: INFO: stderr: ""
Nov  5 18:57:03.726: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7061-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov  5 18:57:03.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 explain e2e-test-crd-publish-openapi-7061-crds.spec'
Nov  5 18:57:04.011: INFO: stderr: ""
Nov  5 18:57:04.011: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7061-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov  5 18:57:04.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 explain e2e-test-crd-publish-openapi-7061-crds.spec.bars'
Nov  5 18:57:04.279: INFO: stderr: ""
Nov  5 18:57:04.279: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7061-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov  5 18:57:04.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 explain e2e-test-crd-publish-openapi-7061-crds.spec.bars2'
Nov  5 18:57:04.551: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:57:08.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7196" for this suite.

• [SLOW TEST:11.950 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":276,"skipped":4487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:57:08.450: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-70e4d931-220e-4c53-a606-c7873a7e07d9
Nov  5 18:57:08.551: INFO: Pod name my-hostname-basic-70e4d931-220e-4c53-a606-c7873a7e07d9: Found 0 pods out of 1
Nov  5 18:57:13.557: INFO: Pod name my-hostname-basic-70e4d931-220e-4c53-a606-c7873a7e07d9: Found 1 pods out of 1
Nov  5 18:57:13.557: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-70e4d931-220e-4c53-a606-c7873a7e07d9" are running
Nov  5 18:57:13.564: INFO: Pod "my-hostname-basic-70e4d931-220e-4c53-a606-c7873a7e07d9-jp4dm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-05 18:57:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-05 18:57:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-05 18:57:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-05 18:57:08 +0000 UTC Reason: Message:}])
Nov  5 18:57:13.564: INFO: Trying to dial the pod
Nov  5 18:57:18.591: INFO: Controller my-hostname-basic-70e4d931-220e-4c53-a606-c7873a7e07d9: Got expected result from replica 1 [my-hostname-basic-70e4d931-220e-4c53-a606-c7873a7e07d9-jp4dm]: "my-hostname-basic-70e4d931-220e-4c53-a606-c7873a7e07d9-jp4dm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:57:18.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1215" for this suite.

• [SLOW TEST:10.167 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":277,"skipped":4521,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:57:18.618: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Nov  5 18:57:18.713: INFO: Waiting up to 5m0s for pod "pod-87d473c6-8343-40bd-b6e1-227f8651be57" in namespace "emptydir-4279" to be "Succeeded or Failed"
Nov  5 18:57:18.723: INFO: Pod "pod-87d473c6-8343-40bd-b6e1-227f8651be57": Phase="Pending", Reason="", readiness=false. Elapsed: 9.962299ms
Nov  5 18:57:20.733: INFO: Pod "pod-87d473c6-8343-40bd-b6e1-227f8651be57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02084919s
Nov  5 18:57:22.745: INFO: Pod "pod-87d473c6-8343-40bd-b6e1-227f8651be57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03212283s
STEP: Saw pod success
Nov  5 18:57:22.745: INFO: Pod "pod-87d473c6-8343-40bd-b6e1-227f8651be57" satisfied condition "Succeeded or Failed"
Nov  5 18:57:22.752: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-87d473c6-8343-40bd-b6e1-227f8651be57 container test-container: <nil>
STEP: delete the pod
Nov  5 18:57:22.800: INFO: Waiting for pod pod-87d473c6-8343-40bd-b6e1-227f8651be57 to disappear
Nov  5 18:57:22.808: INFO: Pod pod-87d473c6-8343-40bd-b6e1-227f8651be57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:57:22.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4279" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":278,"skipped":4528,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:57:22.829: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-825d4682-d589-42d2-b0f2-69a3c018d541 in namespace container-probe-510
Nov  5 18:57:26.947: INFO: Started pod liveness-825d4682-d589-42d2-b0f2-69a3c018d541 in namespace container-probe-510
STEP: checking the pod's current state and verifying that restartCount is present
Nov  5 18:57:26.954: INFO: Initial restart count of pod liveness-825d4682-d589-42d2-b0f2-69a3c018d541 is 0
Nov  5 18:57:47.070: INFO: Restart count of pod container-probe-510/liveness-825d4682-d589-42d2-b0f2-69a3c018d541 is now 1 (20.116092836s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:57:47.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-510" for this suite.

• [SLOW TEST:24.288 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":279,"skipped":4535,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:57:47.117: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Nov  5 18:57:47.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-1840'
Nov  5 18:57:47.595: INFO: stderr: ""
Nov  5 18:57:47.595: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov  5 18:57:47.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1840'
Nov  5 18:57:47.740: INFO: stderr: ""
Nov  5 18:57:47.740: INFO: stdout: "update-demo-nautilus-rblrm update-demo-nautilus-v7x8g "
Nov  5 18:57:47.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-rblrm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1840'
Nov  5 18:57:47.874: INFO: stderr: ""
Nov  5 18:57:47.874: INFO: stdout: ""
Nov  5 18:57:47.874: INFO: update-demo-nautilus-rblrm is created but not running
Nov  5 18:57:52.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1840'
Nov  5 18:57:53.017: INFO: stderr: ""
Nov  5 18:57:53.017: INFO: stdout: "update-demo-nautilus-rblrm update-demo-nautilus-v7x8g "
Nov  5 18:57:53.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-rblrm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1840'
Nov  5 18:57:53.152: INFO: stderr: ""
Nov  5 18:57:53.152: INFO: stdout: "true"
Nov  5 18:57:53.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-rblrm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1840'
Nov  5 18:57:53.276: INFO: stderr: ""
Nov  5 18:57:53.276: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  5 18:57:53.276: INFO: validating pod update-demo-nautilus-rblrm
Nov  5 18:57:53.287: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  5 18:57:53.287: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  5 18:57:53.287: INFO: update-demo-nautilus-rblrm is verified up and running
Nov  5 18:57:53.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-v7x8g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1840'
Nov  5 18:57:53.425: INFO: stderr: ""
Nov  5 18:57:53.425: INFO: stdout: "true"
Nov  5 18:57:53.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods update-demo-nautilus-v7x8g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1840'
Nov  5 18:57:53.561: INFO: stderr: ""
Nov  5 18:57:53.561: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  5 18:57:53.561: INFO: validating pod update-demo-nautilus-v7x8g
Nov  5 18:57:53.574: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  5 18:57:53.574: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  5 18:57:53.574: INFO: update-demo-nautilus-v7x8g is verified up and running
STEP: using delete to clean up resources
Nov  5 18:57:53.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete --grace-period=0 --force -f - --namespace=kubectl-1840'
Nov  5 18:57:53.733: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  5 18:57:53.733: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov  5 18:57:53.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1840'
Nov  5 18:57:53.878: INFO: stderr: "No resources found in kubectl-1840 namespace.\n"
Nov  5 18:57:53.878: INFO: stdout: ""
Nov  5 18:57:53.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -l name=update-demo --namespace=kubectl-1840 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov  5 18:57:54.012: INFO: stderr: ""
Nov  5 18:57:54.012: INFO: stdout: "update-demo-nautilus-rblrm\nupdate-demo-nautilus-v7x8g\n"
Nov  5 18:57:54.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1840'
Nov  5 18:57:54.648: INFO: stderr: "No resources found in kubectl-1840 namespace.\n"
Nov  5 18:57:54.648: INFO: stdout: ""
Nov  5 18:57:54.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 get pods -l name=update-demo --namespace=kubectl-1840 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov  5 18:57:54.801: INFO: stderr: ""
Nov  5 18:57:54.801: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 18:57:54.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1840" for this suite.

• [SLOW TEST:7.707 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":280,"skipped":4543,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 18:57:54.825: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5894
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-5894
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5894
Nov  5 18:57:55.169: INFO: Found 0 stateful pods, waiting for 1
Nov  5 18:58:05.180: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov  5 18:58:05.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov  5 18:58:05.537: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov  5 18:58:05.537: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov  5 18:58:05.537: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov  5 18:58:05.546: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov  5 18:58:15.555: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov  5 18:58:15.555: INFO: Waiting for statefulset status.replicas updated to 0
Nov  5 18:58:15.590: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:15.590: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:15.590: INFO: 
Nov  5 18:58:15.590: INFO: StatefulSet ss has not reached scale 3, at 1
Nov  5 18:58:16.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992196658s
Nov  5 18:58:17.610: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983317652s
Nov  5 18:58:18.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972513391s
Nov  5 18:58:19.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958472701s
Nov  5 18:58:20.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.949709986s
Nov  5 18:58:21.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.936156275s
Nov  5 18:58:22.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.92608034s
Nov  5 18:58:23.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.916094686s
Nov  5 18:58:24.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 905.379567ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5894
Nov  5 18:58:25.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:58:26.083: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov  5 18:58:26.083: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov  5 18:58:26.083: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov  5 18:58:26.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:58:26.412: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov  5 18:58:26.412: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov  5 18:58:26.412: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov  5 18:58:26.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:58:26.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov  5 18:58:26.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov  5 18:58:26.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov  5 18:58:26.801: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 18:58:26.801: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  5 18:58:26.801: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov  5 18:58:26.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov  5 18:58:27.142: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov  5 18:58:27.142: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov  5 18:58:27.142: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov  5 18:58:27.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov  5 18:58:27.474: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov  5 18:58:27.474: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov  5 18:58:27.474: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov  5 18:58:27.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov  5 18:58:27.840: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov  5 18:58:27.840: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov  5 18:58:27.840: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov  5 18:58:27.840: INFO: Waiting for statefulset status.replicas updated to 0
Nov  5 18:58:28.132: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov  5 18:58:38.153: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov  5 18:58:38.153: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov  5 18:58:38.153: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov  5 18:58:38.180: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:38.180: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:38.180: INFO: ss-1  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:38.180: INFO: ss-2  node-29f9289e-2d09-4bea-972e-88a5817d7ad8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:38.180: INFO: 
Nov  5 18:58:38.180: INFO: StatefulSet ss has not reached scale 0, at 3
Nov  5 18:58:39.188: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:39.188: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:39.188: INFO: ss-1  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:39.188: INFO: ss-2  node-29f9289e-2d09-4bea-972e-88a5817d7ad8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:39.188: INFO: 
Nov  5 18:58:39.188: INFO: StatefulSet ss has not reached scale 0, at 3
Nov  5 18:58:40.198: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:40.198: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:40.198: INFO: ss-1  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:40.199: INFO: 
Nov  5 18:58:40.199: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  5 18:58:41.208: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:41.209: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:41.209: INFO: ss-1  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:41.209: INFO: 
Nov  5 18:58:41.209: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  5 18:58:42.219: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:42.219: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:42.219: INFO: ss-1  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:42.219: INFO: 
Nov  5 18:58:42.219: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  5 18:58:43.231: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:43.231: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:43.231: INFO: ss-1  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:43.231: INFO: 
Nov  5 18:58:43.231: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  5 18:58:44.241: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:44.241: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:44.241: INFO: ss-1  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:44.242: INFO: 
Nov  5 18:58:44.242: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  5 18:58:45.250: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:45.250: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:45.250: INFO: ss-1  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:45.250: INFO: 
Nov  5 18:58:45.250: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  5 18:58:46.294: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:46.294: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:46.294: INFO: ss-1  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:46.294: INFO: 
Nov  5 18:58:46.294: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  5 18:58:47.304: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov  5 18:58:47.304: INFO: ss-0  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:57:55 +0000 UTC  }]
Nov  5 18:58:47.305: INFO: ss-1  node-a7de9392-4cc3-4830-84ff-49e7615b291e  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-05 18:58:15 +0000 UTC  }]
Nov  5 18:58:47.305: INFO: 
Nov  5 18:58:47.305: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5894
Nov  5 18:58:48.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:58:48.571: INFO: rc: 1
Nov  5 18:58:48.571: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov  5 18:58:58.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:58:58.716: INFO: rc: 1
Nov  5 18:58:58.716: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 18:59:08.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:59:08.868: INFO: rc: 1
Nov  5 18:59:08.868: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 18:59:18.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:59:19.015: INFO: rc: 1
Nov  5 18:59:19.015: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 18:59:29.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:59:29.154: INFO: rc: 1
Nov  5 18:59:29.154: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 18:59:39.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:59:39.289: INFO: rc: 1
Nov  5 18:59:39.289: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 18:59:49.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:59:49.435: INFO: rc: 1
Nov  5 18:59:49.435: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 18:59:59.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 18:59:59.598: INFO: rc: 1
Nov  5 18:59:59.598: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:00:09.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:00:09.749: INFO: rc: 1
Nov  5 19:00:09.749: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:00:19.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:00:19.896: INFO: rc: 1
Nov  5 19:00:19.896: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:00:29.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:00:30.034: INFO: rc: 1
Nov  5 19:00:30.034: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:00:40.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:00:40.179: INFO: rc: 1
Nov  5 19:00:40.179: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:00:50.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:00:50.319: INFO: rc: 1
Nov  5 19:00:50.319: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:01:00.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:01:00.462: INFO: rc: 1
Nov  5 19:01:00.462: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:01:10.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:01:10.606: INFO: rc: 1
Nov  5 19:01:10.606: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:01:20.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:01:20.739: INFO: rc: 1
Nov  5 19:01:20.739: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:01:30.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:01:30.899: INFO: rc: 1
Nov  5 19:01:30.899: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:01:40.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:01:41.043: INFO: rc: 1
Nov  5 19:01:41.043: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:01:51.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:01:51.207: INFO: rc: 1
Nov  5 19:01:51.207: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:02:01.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:02:01.353: INFO: rc: 1
Nov  5 19:02:01.353: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:02:11.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:02:11.495: INFO: rc: 1
Nov  5 19:02:11.495: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:02:21.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:02:21.632: INFO: rc: 1
Nov  5 19:02:21.632: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:02:31.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:02:31.778: INFO: rc: 1
Nov  5 19:02:31.778: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:02:41.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:02:41.914: INFO: rc: 1
Nov  5 19:02:41.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:02:51.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:02:52.063: INFO: rc: 1
Nov  5 19:02:52.063: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:03:02.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:03:02.211: INFO: rc: 1
Nov  5 19:03:02.211: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:03:12.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:03:12.348: INFO: rc: 1
Nov  5 19:03:12.348: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:03:22.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:03:22.481: INFO: rc: 1
Nov  5 19:03:22.481: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:03:32.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:03:32.642: INFO: rc: 1
Nov  5 19:03:32.642: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:03:42.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:03:42.783: INFO: rc: 1
Nov  5 19:03:42.783: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov  5 19:03:52.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=statefulset-5894 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov  5 19:03:52.925: INFO: rc: 1
Nov  5 19:03:52.925: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Nov  5 19:03:52.925: INFO: Scaling statefulset ss to 0
Nov  5 19:03:52.951: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov  5 19:03:52.959: INFO: Deleting all statefulset in ns statefulset-5894
Nov  5 19:03:52.967: INFO: Scaling statefulset ss to 0
Nov  5 19:03:52.991: INFO: Waiting for statefulset status.replicas updated to 0
Nov  5 19:03:52.998: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:03:53.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5894" for this suite.

• [SLOW TEST:358.233 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":281,"skipped":4543,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:03:53.064: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov  5 19:03:53.154: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:04:02.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4115" for this suite.

• [SLOW TEST:9.400 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":282,"skipped":4562,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:04:02.469: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Nov  5 19:04:02.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-2838 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov  5 19:04:02.698: INFO: stderr: ""
Nov  5 19:04:02.698: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Nov  5 19:04:02.698: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov  5 19:04:02.699: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2838" to be "running and ready, or succeeded"
Nov  5 19:04:02.708: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.313531ms
Nov  5 19:04:04.718: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01906667s
Nov  5 19:04:06.726: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.027607837s
Nov  5 19:04:06.727: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov  5 19:04:06.727: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov  5 19:04:06.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 logs logs-generator logs-generator --namespace=kubectl-2838'
Nov  5 19:04:06.962: INFO: stderr: ""
Nov  5 19:04:06.962: INFO: stdout: "I1105 19:04:05.523229       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/fgqr 416\nI1105 19:04:05.723374       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/845 512\nI1105 19:04:05.923337       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/vvnv 299\nI1105 19:04:06.123384       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/gk6 460\nI1105 19:04:06.323369       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ztkd 442\nI1105 19:04:06.523392       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/qz6 316\nI1105 19:04:06.723516       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/lwrp 362\nI1105 19:04:06.923482       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/rmdz 302\nI1105 19:04:07.123367       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/lbw 201\nI1105 19:04:07.323297       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/w24 487\nI1105 19:04:07.523405       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/lqqn 593\n"
STEP: limiting log lines
Nov  5 19:04:06.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 logs logs-generator logs-generator --namespace=kubectl-2838 --tail=1'
Nov  5 19:04:07.126: INFO: stderr: ""
Nov  5 19:04:07.126: INFO: stdout: "I1105 19:04:07.523405       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/lqqn 593\n"
Nov  5 19:04:07.126: INFO: got output "I1105 19:04:07.523405       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/lqqn 593\n"
STEP: limiting log bytes
Nov  5 19:04:07.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 logs logs-generator logs-generator --namespace=kubectl-2838 --limit-bytes=1'
Nov  5 19:04:07.280: INFO: stderr: ""
Nov  5 19:04:07.280: INFO: stdout: "I"
Nov  5 19:04:07.280: INFO: got output "I"
STEP: exposing timestamps
Nov  5 19:04:07.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 logs logs-generator logs-generator --namespace=kubectl-2838 --tail=1 --timestamps'
Nov  5 19:04:07.453: INFO: stderr: ""
Nov  5 19:04:07.453: INFO: stdout: "2020-11-05T19:04:07.923653431Z I1105 19:04:07.923362       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/h7dx 246\n"
Nov  5 19:04:07.453: INFO: got output "2020-11-05T19:04:07.923653431Z I1105 19:04:07.923362       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/h7dx 246\n"
STEP: restricting to a time range
Nov  5 19:04:09.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 logs logs-generator logs-generator --namespace=kubectl-2838 --since=1s'
Nov  5 19:04:10.117: INFO: stderr: ""
Nov  5 19:04:10.117: INFO: stdout: "I1105 19:04:09.723446       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/clr 550\nI1105 19:04:09.923391       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/vzf 325\nI1105 19:04:10.123299       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/msmb 267\nI1105 19:04:10.323439       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/wp6 537\nI1105 19:04:10.523384       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/vzn 369\n"
Nov  5 19:04:10.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 logs logs-generator logs-generator --namespace=kubectl-2838 --since=24h'
Nov  5 19:04:10.266: INFO: stderr: ""
Nov  5 19:04:10.267: INFO: stdout: "I1105 19:04:05.523229       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/fgqr 416\nI1105 19:04:05.723374       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/845 512\nI1105 19:04:05.923337       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/vvnv 299\nI1105 19:04:06.123384       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/gk6 460\nI1105 19:04:06.323369       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ztkd 442\nI1105 19:04:06.523392       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/qz6 316\nI1105 19:04:06.723516       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/lwrp 362\nI1105 19:04:06.923482       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/rmdz 302\nI1105 19:04:07.123367       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/lbw 201\nI1105 19:04:07.323297       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/w24 487\nI1105 19:04:07.523405       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/lqqn 593\nI1105 19:04:07.723359       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/pjq2 232\nI1105 19:04:07.923362       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/h7dx 246\nI1105 19:04:08.123360       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/k8wx 518\nI1105 19:04:08.323346       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/fqxq 351\nI1105 19:04:08.523491       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/lkf 513\nI1105 19:04:08.723462       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/vjl9 201\nI1105 19:04:08.923341       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/rb2 472\nI1105 19:04:09.123407       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/zcg8 508\nI1105 19:04:09.323348       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/mm4s 262\nI1105 19:04:09.523338       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/zhnc 583\nI1105 19:04:09.723446       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/clr 550\nI1105 19:04:09.923391       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/vzf 325\nI1105 19:04:10.123299       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/msmb 267\nI1105 19:04:10.323439       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/wp6 537\nI1105 19:04:10.523384       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/vzn 369\nI1105 19:04:10.723505       1 logs_generator.go:76] 26 POST /api/v1/namespaces/default/pods/lb89 371\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Nov  5 19:04:10.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 delete pod logs-generator --namespace=kubectl-2838'
Nov  5 19:04:13.012: INFO: stderr: ""
Nov  5 19:04:13.012: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:04:13.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2838" for this suite.

• [SLOW TEST:10.563 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":283,"skipped":4570,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:04:13.032: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov  5 19:04:19.705: INFO: Successfully updated pod "pod-update-7460bef2-9ada-4c12-a99e-63f9e857212d"
STEP: verifying the updated pod is in kubernetes
Nov  5 19:04:19.728: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:04:19.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6923" for this suite.

• [SLOW TEST:6.719 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":284,"skipped":4591,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:04:19.753: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Nov  5 19:04:19.823: INFO: namespace kubectl-6099
Nov  5 19:04:19.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 create -f - --namespace=kubectl-6099'
Nov  5 19:04:20.098: INFO: stderr: ""
Nov  5 19:04:20.098: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov  5 19:04:21.108: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 19:04:21.108: INFO: Found 0 / 1
Nov  5 19:04:22.109: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 19:04:22.109: INFO: Found 0 / 1
Nov  5 19:04:23.121: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 19:04:23.121: INFO: Found 0 / 1
Nov  5 19:04:24.106: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 19:04:24.106: INFO: Found 1 / 1
Nov  5 19:04:24.106: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov  5 19:04:24.114: INFO: Selector matched 1 pods for map[app:agnhost]
Nov  5 19:04:24.114: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov  5 19:04:24.114: INFO: wait on agnhost-primary startup in kubectl-6099 
Nov  5 19:04:24.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 logs agnhost-primary-pxxkd agnhost-primary --namespace=kubectl-6099'
Nov  5 19:04:24.273: INFO: stderr: ""
Nov  5 19:04:24.273: INFO: stdout: "Paused\n"
STEP: exposing RC
Nov  5 19:04:24.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6099'
Nov  5 19:04:24.455: INFO: stderr: ""
Nov  5 19:04:24.455: INFO: stdout: "service/rm2 exposed\n"
Nov  5 19:04:24.473: INFO: Service rm2 in namespace kubectl-6099 found.
STEP: exposing service
Nov  5 19:04:26.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6099'
Nov  5 19:04:26.657: INFO: stderr: ""
Nov  5 19:04:26.657: INFO: stdout: "service/rm3 exposed\n"
Nov  5 19:04:26.665: INFO: Service rm3 in namespace kubectl-6099 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:04:28.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6099" for this suite.

• [SLOW TEST:9.081 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":285,"skipped":4605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:04:28.835: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-8009/secret-test-a66a11f4-aea9-44b1-b8a3-1ed281e87bff
STEP: Creating a pod to test consume secrets
Nov  5 19:04:28.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-1b07d3ff-e5fa-4940-9be0-fe288909d3e9" in namespace "secrets-8009" to be "Succeeded or Failed"
Nov  5 19:04:28.955: INFO: Pod "pod-configmaps-1b07d3ff-e5fa-4940-9be0-fe288909d3e9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.989166ms
Nov  5 19:04:30.965: INFO: Pod "pod-configmaps-1b07d3ff-e5fa-4940-9be0-fe288909d3e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019534915s
Nov  5 19:04:32.996: INFO: Pod "pod-configmaps-1b07d3ff-e5fa-4940-9be0-fe288909d3e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05028771s
Nov  5 19:04:35.004: INFO: Pod "pod-configmaps-1b07d3ff-e5fa-4940-9be0-fe288909d3e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058581723s
STEP: Saw pod success
Nov  5 19:04:35.004: INFO: Pod "pod-configmaps-1b07d3ff-e5fa-4940-9be0-fe288909d3e9" satisfied condition "Succeeded or Failed"
Nov  5 19:04:35.012: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-configmaps-1b07d3ff-e5fa-4940-9be0-fe288909d3e9 container env-test: <nil>
STEP: delete the pod
Nov  5 19:04:35.060: INFO: Waiting for pod pod-configmaps-1b07d3ff-e5fa-4940-9be0-fe288909d3e9 to disappear
Nov  5 19:04:35.066: INFO: Pod pod-configmaps-1b07d3ff-e5fa-4940-9be0-fe288909d3e9 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:04:35.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8009" for this suite.

• [SLOW TEST:6.259 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":286,"skipped":4630,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:04:35.099: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov  5 19:04:45.284: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7665 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 19:04:45.284: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 19:04:45.510: INFO: Exec stderr: ""
Nov  5 19:04:45.511: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7665 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 19:04:45.511: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 19:04:45.721: INFO: Exec stderr: ""
Nov  5 19:04:45.721: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7665 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 19:04:45.721: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 19:04:45.919: INFO: Exec stderr: ""
Nov  5 19:04:45.919: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7665 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 19:04:45.920: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 19:04:46.122: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov  5 19:04:46.123: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7665 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 19:04:46.123: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 19:04:46.426: INFO: Exec stderr: ""
Nov  5 19:04:46.426: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7665 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 19:04:46.427: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 19:04:46.621: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov  5 19:04:46.622: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7665 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 19:04:46.622: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 19:04:46.820: INFO: Exec stderr: ""
Nov  5 19:04:46.820: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7665 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 19:04:46.820: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 19:04:47.011: INFO: Exec stderr: ""
Nov  5 19:04:47.011: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7665 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 19:04:47.011: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 19:04:47.224: INFO: Exec stderr: ""
Nov  5 19:04:47.224: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7665 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  5 19:04:47.224: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
Nov  5 19:04:47.413: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:04:47.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7665" for this suite.

• [SLOW TEST:12.340 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":287,"skipped":4650,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:04:47.451: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-xz2t
STEP: Creating a pod to test atomic-volume-subpath
Nov  5 19:04:47.570: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xz2t" in namespace "subpath-1123" to be "Succeeded or Failed"
Nov  5 19:04:47.583: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Pending", Reason="", readiness=false. Elapsed: 12.287518ms
Nov  5 19:04:49.595: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024463842s
Nov  5 19:04:51.605: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Running", Reason="", readiness=true. Elapsed: 4.035071599s
Nov  5 19:04:53.614: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Running", Reason="", readiness=true. Elapsed: 6.043738301s
Nov  5 19:04:55.625: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Running", Reason="", readiness=true. Elapsed: 8.05409674s
Nov  5 19:04:57.636: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Running", Reason="", readiness=true. Elapsed: 10.065862725s
Nov  5 19:04:59.647: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Running", Reason="", readiness=true. Elapsed: 12.076839818s
Nov  5 19:05:01.657: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Running", Reason="", readiness=true. Elapsed: 14.086278184s
Nov  5 19:05:03.672: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Running", Reason="", readiness=true. Elapsed: 16.101698265s
Nov  5 19:05:05.694: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Running", Reason="", readiness=true. Elapsed: 18.123143247s
Nov  5 19:05:07.704: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Running", Reason="", readiness=true. Elapsed: 20.133856597s
Nov  5 19:05:09.718: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Running", Reason="", readiness=true. Elapsed: 22.147092699s
Nov  5 19:05:11.731: INFO: Pod "pod-subpath-test-configmap-xz2t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.160908656s
STEP: Saw pod success
Nov  5 19:05:11.731: INFO: Pod "pod-subpath-test-configmap-xz2t" satisfied condition "Succeeded or Failed"
Nov  5 19:05:11.739: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-subpath-test-configmap-xz2t container test-container-subpath-configmap-xz2t: <nil>
STEP: delete the pod
Nov  5 19:05:11.806: INFO: Waiting for pod pod-subpath-test-configmap-xz2t to disappear
Nov  5 19:05:11.814: INFO: Pod pod-subpath-test-configmap-xz2t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xz2t
Nov  5 19:05:11.814: INFO: Deleting pod "pod-subpath-test-configmap-xz2t" in namespace "subpath-1123"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:05:11.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1123" for this suite.

• [SLOW TEST:24.400 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":288,"skipped":4690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:05:11.852: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 19:07:11.984: INFO: Deleting pod "var-expansion-9ac575c4-a40c-4103-a2d0-a781de1b1159" in namespace "var-expansion-1231"
Nov  5 19:07:12.106: INFO: Wait up to 5m0s for pod "var-expansion-9ac575c4-a40c-4103-a2d0-a781de1b1159" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:07:14.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1231" for this suite.

• [SLOW TEST:122.300 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":289,"skipped":4712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:07:14.165: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6367
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6367
I1105 19:07:14.546691      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6367, replica count: 2
I1105 19:07:17.597129      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 19:07:20.597418      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 19:07:20.597: INFO: Creating new exec pod
Nov  5 19:07:25.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-6367 execpodbzgxx -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov  5 19:07:26.087: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov  5 19:07:26.088: INFO: stdout: ""
Nov  5 19:07:26.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-6367 execpodbzgxx -- /bin/sh -x -c nc -zv -t -w 2 10.3.53.233 80'
Nov  5 19:07:26.419: INFO: stderr: "+ nc -zv -t -w 2 10.3.53.233 80\nConnection to 10.3.53.233 80 port [tcp/http] succeeded!\n"
Nov  5 19:07:26.419: INFO: stdout: ""
Nov  5 19:07:26.419: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:07:26.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6367" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:12.333 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":290,"skipped":4742,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:07:26.498: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov  5 19:07:26.596: INFO: Waiting up to 5m0s for pod "downward-api-6986ed0c-f924-4847-af10-cf44ffbf9b03" in namespace "downward-api-9980" to be "Succeeded or Failed"
Nov  5 19:07:26.614: INFO: Pod "downward-api-6986ed0c-f924-4847-af10-cf44ffbf9b03": Phase="Pending", Reason="", readiness=false. Elapsed: 17.391855ms
Nov  5 19:07:28.625: INFO: Pod "downward-api-6986ed0c-f924-4847-af10-cf44ffbf9b03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028036251s
Nov  5 19:07:30.634: INFO: Pod "downward-api-6986ed0c-f924-4847-af10-cf44ffbf9b03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037350484s
STEP: Saw pod success
Nov  5 19:07:30.634: INFO: Pod "downward-api-6986ed0c-f924-4847-af10-cf44ffbf9b03" satisfied condition "Succeeded or Failed"
Nov  5 19:07:30.643: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downward-api-6986ed0c-f924-4847-af10-cf44ffbf9b03 container dapi-container: <nil>
STEP: delete the pod
Nov  5 19:07:30.758: INFO: Waiting for pod downward-api-6986ed0c-f924-4847-af10-cf44ffbf9b03 to disappear
Nov  5 19:07:30.765: INFO: Pod downward-api-6986ed0c-f924-4847-af10-cf44ffbf9b03 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:07:30.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9980" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":291,"skipped":4748,"failed":0}
SSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:07:30.791: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 19:07:30.898: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-42bfe188-e76f-4362-9866-4d65e9946281" in namespace "security-context-test-9115" to be "Succeeded or Failed"
Nov  5 19:07:30.907: INFO: Pod "busybox-privileged-false-42bfe188-e76f-4362-9866-4d65e9946281": Phase="Pending", Reason="", readiness=false. Elapsed: 9.079954ms
Nov  5 19:07:32.917: INFO: Pod "busybox-privileged-false-42bfe188-e76f-4362-9866-4d65e9946281": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019068557s
Nov  5 19:07:34.927: INFO: Pod "busybox-privileged-false-42bfe188-e76f-4362-9866-4d65e9946281": Phase="Running", Reason="", readiness=true. Elapsed: 4.028834114s
Nov  5 19:07:36.937: INFO: Pod "busybox-privileged-false-42bfe188-e76f-4362-9866-4d65e9946281": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039152059s
Nov  5 19:07:36.938: INFO: Pod "busybox-privileged-false-42bfe188-e76f-4362-9866-4d65e9946281" satisfied condition "Succeeded or Failed"
Nov  5 19:07:36.956: INFO: Got logs for pod "busybox-privileged-false-42bfe188-e76f-4362-9866-4d65e9946281": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:07:36.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9115" for this suite.

• [SLOW TEST:6.193 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with privileged
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":4754,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:07:36.984: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:07:37.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7358" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":293,"skipped":4760,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:07:37.103: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:07:37.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6318" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":294,"skipped":4767,"failed":0}

------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:07:37.272: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Nov  5 19:07:37.376: INFO: created test-podtemplate-1
Nov  5 19:07:37.385: INFO: created test-podtemplate-2
Nov  5 19:07:37.395: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Nov  5 19:07:37.403: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Nov  5 19:07:37.458: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:07:37.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-223" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":295,"skipped":4767,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:07:37.491: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 19:07:37.566: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:07:38.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8252" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":296,"skipped":4769,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:07:38.950: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov  5 19:07:39.062: INFO: Waiting up to 1m0s for all nodes to be ready
Nov  5 19:08:39.107: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:08:39.116: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Nov  5 19:08:43.279: INFO: found a healthy node: node-a7de9392-4cc3-4830-84ff-49e7615b291e
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 19:08:59.436: INFO: pods created so far: [1 1 1]
Nov  5 19:08:59.436: INFO: length of pods created so far: 3
Nov  5 19:09:09.458: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:09:16.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1026" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:09:16.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6851" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:97.745 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":297,"skipped":4794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:09:16.705: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov  5 19:09:16.808: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab0c2bed-91c9-47b8-a7e8-eea80c4f66ca" in namespace "projected-476" to be "Succeeded or Failed"
Nov  5 19:09:16.821: INFO: Pod "downwardapi-volume-ab0c2bed-91c9-47b8-a7e8-eea80c4f66ca": Phase="Pending", Reason="", readiness=false. Elapsed: 12.352414ms
Nov  5 19:09:18.832: INFO: Pod "downwardapi-volume-ab0c2bed-91c9-47b8-a7e8-eea80c4f66ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023214948s
Nov  5 19:09:20.840: INFO: Pod "downwardapi-volume-ab0c2bed-91c9-47b8-a7e8-eea80c4f66ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031550022s
STEP: Saw pod success
Nov  5 19:09:20.840: INFO: Pod "downwardapi-volume-ab0c2bed-91c9-47b8-a7e8-eea80c4f66ca" satisfied condition "Succeeded or Failed"
Nov  5 19:09:20.848: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod downwardapi-volume-ab0c2bed-91c9-47b8-a7e8-eea80c4f66ca container client-container: <nil>
STEP: delete the pod
Nov  5 19:09:20.961: INFO: Waiting for pod downwardapi-volume-ab0c2bed-91c9-47b8-a7e8-eea80c4f66ca to disappear
Nov  5 19:09:20.968: INFO: Pod downwardapi-volume-ab0c2bed-91c9-47b8-a7e8-eea80c4f66ca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:09:20.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-476" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":298,"skipped":4838,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:09:20.995: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov  5 19:09:22.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov  5 19:09:24.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740200162, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740200162, loc:(*time.Location)(0x7701840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63740200162, loc:(*time.Location)(0x7701840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63740200161, loc:(*time.Location)(0x7701840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov  5 19:09:27.068: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:09:27.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-829" for this suite.
STEP: Destroying namespace "webhook-829-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.353 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":299,"skipped":4845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:09:27.351: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov  5 19:09:27.455: INFO: Waiting up to 5m0s for pod "pod-508db19f-eea6-40fb-a82b-364a5e2664f9" in namespace "emptydir-3697" to be "Succeeded or Failed"
Nov  5 19:09:27.466: INFO: Pod "pod-508db19f-eea6-40fb-a82b-364a5e2664f9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.397334ms
Nov  5 19:09:29.474: INFO: Pod "pod-508db19f-eea6-40fb-a82b-364a5e2664f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018395092s
Nov  5 19:09:31.485: INFO: Pod "pod-508db19f-eea6-40fb-a82b-364a5e2664f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029603162s
STEP: Saw pod success
Nov  5 19:09:31.486: INFO: Pod "pod-508db19f-eea6-40fb-a82b-364a5e2664f9" satisfied condition "Succeeded or Failed"
Nov  5 19:09:31.495: INFO: Trying to get logs from node node-a7de9392-4cc3-4830-84ff-49e7615b291e pod pod-508db19f-eea6-40fb-a82b-364a5e2664f9 container test-container: <nil>
STEP: delete the pod
Nov  5 19:09:31.550: INFO: Waiting for pod pod-508db19f-eea6-40fb-a82b-364a5e2664f9 to disappear
Nov  5 19:09:31.558: INFO: Pod pod-508db19f-eea6-40fb-a82b-364a5e2664f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:09:31.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3697" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4877,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:09:31.589: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:09:31.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5167" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":301,"skipped":4893,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:09:31.840: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 19:09:31.966: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov  5 19:09:31.998: INFO: Number of nodes with available pods: 0
Nov  5 19:09:31.998: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 19:09:33.012: INFO: Number of nodes with available pods: 0
Nov  5 19:09:33.012: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 19:09:34.018: INFO: Number of nodes with available pods: 0
Nov  5 19:09:34.018: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 19:09:35.021: INFO: Number of nodes with available pods: 0
Nov  5 19:09:35.021: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 19:09:36.016: INFO: Number of nodes with available pods: 1
Nov  5 19:09:36.016: INFO: Node node-a7de9392-4cc3-4830-84ff-49e7615b291e is running more than one daemon pod
Nov  5 19:09:37.021: INFO: Number of nodes with available pods: 2
Nov  5 19:09:37.021: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov  5 19:09:37.076: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:37.076: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:38.095: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:38.095: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:39.101: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:39.101: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:40.094: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:40.094: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:40.095: INFO: Pod daemon-set-knq97 is not available
Nov  5 19:09:41.103: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:41.103: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:41.103: INFO: Pod daemon-set-knq97 is not available
Nov  5 19:09:42.096: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:42.096: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:42.096: INFO: Pod daemon-set-knq97 is not available
Nov  5 19:09:43.094: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:43.094: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:43.094: INFO: Pod daemon-set-knq97 is not available
Nov  5 19:09:44.102: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:44.102: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:44.102: INFO: Pod daemon-set-knq97 is not available
Nov  5 19:09:45.097: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:45.097: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:45.097: INFO: Pod daemon-set-knq97 is not available
Nov  5 19:09:46.097: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:46.098: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:46.098: INFO: Pod daemon-set-knq97 is not available
Nov  5 19:09:47.094: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:47.094: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:47.094: INFO: Pod daemon-set-knq97 is not available
Nov  5 19:09:48.098: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:48.098: INFO: Wrong image for pod: daemon-set-knq97. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:48.098: INFO: Pod daemon-set-knq97 is not available
Nov  5 19:09:49.099: INFO: Pod daemon-set-dcxvk is not available
Nov  5 19:09:49.099: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:50.100: INFO: Pod daemon-set-dcxvk is not available
Nov  5 19:09:50.100: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:51.098: INFO: Pod daemon-set-dcxvk is not available
Nov  5 19:09:51.098: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:52.097: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:53.105: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:53.105: INFO: Pod daemon-set-k9mgq is not available
Nov  5 19:09:54.100: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:54.100: INFO: Pod daemon-set-k9mgq is not available
Nov  5 19:09:55.096: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:55.096: INFO: Pod daemon-set-k9mgq is not available
Nov  5 19:09:56.097: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:56.097: INFO: Pod daemon-set-k9mgq is not available
Nov  5 19:09:57.094: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:57.094: INFO: Pod daemon-set-k9mgq is not available
Nov  5 19:09:58.097: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:58.098: INFO: Pod daemon-set-k9mgq is not available
Nov  5 19:09:59.097: INFO: Wrong image for pod: daemon-set-k9mgq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov  5 19:09:59.097: INFO: Pod daemon-set-k9mgq is not available
Nov  5 19:10:00.102: INFO: Pod daemon-set-l9wjx is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Nov  5 19:10:00.129: INFO: Number of nodes with available pods: 1
Nov  5 19:10:00.129: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 19:10:01.145: INFO: Number of nodes with available pods: 1
Nov  5 19:10:01.145: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 19:10:02.147: INFO: Number of nodes with available pods: 1
Nov  5 19:10:02.147: INFO: Node node-29f9289e-2d09-4bea-972e-88a5817d7ad8 is running more than one daemon pod
Nov  5 19:10:03.152: INFO: Number of nodes with available pods: 2
Nov  5 19:10:03.152: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8361, will wait for the garbage collector to delete the pods
Nov  5 19:10:03.269: INFO: Deleting DaemonSet.extensions daemon-set took: 19.472837ms
Nov  5 19:10:04.069: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.402204ms
Nov  5 19:10:09.879: INFO: Number of nodes with available pods: 0
Nov  5 19:10:09.880: INFO: Number of running nodes: 0, number of available pods: 0
Nov  5 19:10:09.887: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8361/daemonsets","resourceVersion":"1648440061"},"items":null}

Nov  5 19:10:09.894: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8361/pods","resourceVersion":"1648440065"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:10:09.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8361" for this suite.

• [SLOW TEST:38.110 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":302,"skipped":4912,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:10:09.954: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:10:10.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1674" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":303,"skipped":4913,"failed":0}

------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:10:10.132: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4393
STEP: creating service affinity-clusterip in namespace services-4393
STEP: creating replication controller affinity-clusterip in namespace services-4393
I1105 19:10:10.334259      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4393, replica count: 3
I1105 19:10:13.385109      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1105 19:10:16.385567      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  5 19:10:16.399: INFO: Creating new exec pod
Nov  5 19:10:21.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4393 execpod-affinity8f4gh -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Nov  5 19:10:21.789: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov  5 19:10:21.789: INFO: stdout: ""
Nov  5 19:10:21.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4393 execpod-affinity8f4gh -- /bin/sh -x -c nc -zv -t -w 2 10.3.238.237 80'
Nov  5 19:10:22.129: INFO: stderr: "+ nc -zv -t -w 2 10.3.238.237 80\nConnection to 10.3.238.237 80 port [tcp/http] succeeded!\n"
Nov  5 19:10:22.130: INFO: stdout: ""
Nov  5 19:10:22.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 exec --namespace=services-4393 execpod-affinity8f4gh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.238.237:80/ ; done'
Nov  5 19:10:22.598: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.238.237:80/\n"
Nov  5 19:10:22.598: INFO: stdout: "\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv\naffinity-clusterip-xggjv"
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Received response from host: affinity-clusterip-xggjv
Nov  5 19:10:22.598: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4393, will wait for the garbage collector to delete the pods
Nov  5 19:10:22.700: INFO: Deleting ReplicationController affinity-clusterip took: 18.791166ms
Nov  5 19:10:22.801: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.298605ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:10:38.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4393" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:28.542 seconds]
[sig-network] Services
/workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":304,"skipped":4913,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov  5 19:10:38.675: INFO: >>> kubeConfig: /tmp/kubeconfig-230475851
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov  5 19:10:38.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-230475851 version'
Nov  5 19:10:38.899: INFO: stderr: ""
Nov  5 19:10:38.899: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.2\", GitCommit:\"f5743093fd1c663cb0cbc89748f730662345d44d\", GitTreeState:\"clean\", BuildDate:\"2020-09-16T13:41:02Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.2\", GitCommit:\"f5743093fd1c663cb0cbc89748f730662345d44d\", GitTreeState:\"clean\", BuildDate:\"2020-09-16T13:32:58Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.2-rc.0.12+19706d90d87784/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov  5 19:10:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6349" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":305,"skipped":4918,"failed":0}
SSSSSSSSSNov  5 19:10:38.923: INFO: Running AfterSuite actions on all nodes
Nov  5 19:10:38.923: INFO: Running AfterSuite actions on node 1
Nov  5 19:10:38.923: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4927,"failed":0}

Ran 305 of 5232 Specs in 6305.889 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4927 Skipped
PASS

Ginkgo ran 1 suite in 1h45m7.733586231s
Test Suite Passed
